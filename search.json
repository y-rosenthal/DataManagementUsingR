[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Management With R (latest version)",
    "section": "",
    "text": "Intro\nThis course covers many different topics related to the processing and management of data. For a quick overview of many of the topics see the “Topics” section in the Appendix.",
    "crumbs": [
      "<yrChapterNumber></yrChapterNumber> Intro"
    ]
  },
  {
    "objectID": "intro00100-operatingSystems.html",
    "href": "intro00100-operatingSystems.html",
    "title": "1  1.  What is an “operating system”",
    "section": "",
    "text": "1.1 Operating Systems (Mac, Windows, Linux)\nBefore going into detail about how to work with files in R it is helpful to have some background regarding operating systmes (keep reading).",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'><yrChapterNumber>1. </yrChapterNumber> What is an \"operating system\"</span>"
    ]
  },
  {
    "objectID": "intro00100-operatingSystems.html#operating-systems-mac-windows-linux",
    "href": "intro00100-operatingSystems.html#operating-systems-mac-windows-linux",
    "title": "1  1.  What is an “operating system”",
    "section": "",
    "text": "1.1.1 An Operating System (OS) is the software that controls a computer\nAn operating system (OS for short) is the software that controls how a computer operates. Windows and Mac OS (i.e. Mac Operating System) are two very popular operating systems for personal computers.\nAnother operating system is Linux. This OS is often used to run backoffice computer systems, i.e. the computer systems that keep many critical business operations working - e.g. that keep websites working. Linux is also a popular option with many technical users for their own personal computers.\nWHile all operating systems allow you to control a computer, there are differences between operating systems when it comes to the exact ways to accomplish some tasks.\n\n\n1.1.2 Smartphones have operating systems too\nSmartphones are basically small computers. Therefore smartphones have operating systmes too. The very popular ones today are Android and Apple iOS (this is the operating system found on Apple iPhones).\n\n\n1.1.3 Mac and Linux are similar in a lot of ways\nBoth Mac and Linux were derived from an older operating system known as Unix. (Unix still exists but has largely been replaced by Linux). For this reason, Mac OS and Linux often do things the same way, while Microsoft Windows might be a little different from both of them. Keep this in mind as we go through this course. It is often the case that what will work on Mac also works on Linux and vice versa.\n\n\n1.1.4 Flavors of Linux\nLinux is “open source” software. That means that no one company owns Linux. The product is freely available for anyone to modify and release their own version. Therefore there are many different “flavors” of Linux. The different flavors of Linux share many similarities but are all different in one way or another. This is similar to how each car company has their own “flavor” of SUV. If you know how to drive one SUV, you can probably drive any SUV. However, the different car companies differentiate themselves with different slight details in the products.\nAs of today (Jan 2025) some of the popular flavors of Linux include:\n\nUbuntu\nDebian\nRed Hat Linux\nCentos\nthere are many, many, many other flavors of Linux with more coming out all the time.\n\n\n\n1.1.5 Know your OS version number\nOperating systems are constantly being updated and improved. When looking online for info about your computer or phone it often helps a lot to include the version number for your operating system. For example,\n\nIf you’re looking for info about how to start the “File Manager” program (see below) on Windows, don’t just search for “How do I start the file manager program”, rather search for “how do I start the file manager program on WINDOWS 11” (or Windows 10, etc.)\nSimilarly, if you’re looking for info about how to start the “Finder” program (see below) on Mac, don’t just search for “How do I start the finder program”, rather search for “how do I start the finder program on Mac OS Sequoia”\nIf you don’t know how to find your OS version number, I recommend that you search for that on your favorite search engine (e.g. google) or on your favorite AI tool (e.g. chatgpt). For example, “how do I find out exactly what version number of Mac OS I have”?\nNOTE: (as of today, to find the OS version on Mac, click the “apple button” at the top left of your menus and choose “About This Mac”.",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'><yrChapterNumber>1. </yrChapterNumber> What is an \"operating system\"</span>"
    ]
  },
  {
    "objectID": "intro00120-files.html",
    "href": "intro00120-files.html",
    "title": "2  2.  Working with files",
    "section": "",
    "text": "2.1 Mac Finder and Windows File Explorer\nThe Finder app on Mac and the File Explorer app on Windows are used to navigate the files on your computer. You should become familiar with how to use those apps on your computer. For more info search online. Youtube videos are often a good way to learn about how to use these features.",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'><yrChapterNumber>2. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "intro00120-files.html#filename-extensions",
    "href": "intro00120-files.html#filename-extensions",
    "title": "2  2.  Working with files",
    "section": "2.2 Filename extensions",
    "text": "2.2 Filename extensions\n\n2.2.1 What is a filename extension?\nA filename extension is the portion of the filename that appears after the final period. For example, in the file LetterToBob.docx, the filename extension is .docx. In a file named finances.xlsx, the filename extension is .xlsx.\n\n\n2.2.2 A filename extension indicates which program should be used to open a file\nModern Microsoft Word documents have a .docx filename extensions (older versions of Microsoft Word used a .doc extension).\nModern Microsoft Excel documents have a .xlsx filename extension (older versions of Excel used a .xls extension).\nWhen you double click on a file, that file is often opened inside a particular program. Most modern operating systems use the filename extension to determine which program to use with the file. For example files that end with .xlsx are typically automatically opened up in Excel and files that end with .docx are typically automatically opened up in Microsoft Word.\nOpening a file with the wrong program usually leads to problems. For example, if you try to open an Excel file with Microsoft Word or a Word file with Excel, the program that you opened will not be able to use the file correctly.\n\n\n2.2.3 Some files can be opened with different programs.\nYou typically would open a .docx file with Microsoft Windows. However, other programs exist that can also be used to open a .docx file. For example these other programs can also work with .docx files:\n\nLibreOffice Writer - Open-source software that can edit .docx files.\nWPS Office Writer - A lightweight alternative with .docx compatibility.\nApple Pages - Supports .docx on macOS and iOS.\nOnlyOffice - Free and online office suite for .docx editing.\nZoho Writer - Cloud-based editor with .docx support.\nSoftMaker FreeOffice - Free software supporting .docx files.\n\nIf you have more than one of these programs installed on your computer, one of them will be the “default” program to open when you double click on a .docx file. Both Mac and Windows computers have a way to configure which is the default program for each filename extension. The specifics of how to do so often change with different releases of an operating system. For info about how to do so on your computer, please search online with your favorite search engine or AI.\n\n\n2.2.4 How come I can’t see filename extensions\nChanging a filename extension can cause the computer to open up the wrong program when you double click on the filename. Many non-technical users are not aware of this and might inadvertantly change the filename extension without understanding the ramifications. For this reason, on new Windows and Mac computers, the filename extensions are usually hidden from view. The filename extensions exist, they are just hidden from view. There is always a way to show (or hide) the filename extensions. However, different versions of Windows and Mac OS provide different ways to do this.\nSearch online for instructions on how to either show or hide filename extensions on your computer.",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'><yrChapterNumber>2. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "intro00120-files.html#zip-files",
    "href": "intro00120-files.html#zip-files",
    "title": "2  2.  Working with files",
    "section": "2.3 ZIP files",
    "text": "2.3 ZIP files\nA ZIP file is a single file that contains multiple files.\n\n2.3.1 How to create a ZIP file?\nSearch online for info for your operating system.\nFor example “how can I create a ZIP file from a specific folder on Windows 11”\n\n\n2.3.2 How to “unzip”, i.e. extract the files, from a ZIP file?\nSearch online for info for your operating system.\nFor example “how can I unzip a zip file on Mac Sequoia”.",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'><yrChapterNumber>2. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "intro00130-chromeExtensions.html",
    "href": "intro00130-chromeExtensions.html",
    "title": "3  3.  Chrome Extensions",
    "section": "",
    "text": "3.1 Try it yourself",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'><yrChapterNumber>3. </yrChapterNumber> Chrome Extensions</span>"
    ]
  },
  {
    "objectID": "intro00130-chromeExtensions.html#try-it-yourself",
    "href": "intro00130-chromeExtensions.html#try-it-yourself",
    "title": "3  3.  Chrome Extensions",
    "section": "",
    "text": "Try installing an using the “google dictionary” chrome extention\nTry installing an using the “google translate” chrome extention",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'><yrChapterNumber>3. </yrChapterNumber> Chrome Extensions</span>"
    ]
  },
  {
    "objectID": "intro00130-chromeExtensions.html#using-other-browsers",
    "href": "intro00130-chromeExtensions.html#using-other-browsers",
    "title": "3  3.  Chrome Extensions",
    "section": "3.2 Using other browsers",
    "text": "3.2 Using other browsers\nChrome extensions may work in other browsers as well, but Chrome is the best option to make sure that the extension works as intended.\nThe following was generated by chatgpt on Jan 22, 2025 - As with all info generated by AI today, this may not be totally accurate. Proceed at your own risk :)\n\n Me: can i install a chrome extension in other browsers ?\nChatGPT said: Yes, many browsers support Chrome extensions, but compatibility depends on the browser:\n\nMicrosoft Edge: Directly supports Chrome extensions. Go to the Edge Add-ons Store, enable extensions from other stores, and install from the Chrome Web Store.\nOpera: Requires the “Install Chrome Extensions” add-on to enable Chrome extensions.\nBrave: Fully supports Chrome extensions without additional steps. Install them from the Chrome Web Store.\nVivaldi: Supports Chrome extensions. Install them directly from the Chrome Web Store.\nFirefox: Does not natively support Chrome extensions. Extensions must be specifically built or converted for Firefox.\nFor non-Chromium-based browsers, like Safari or Firefox, Chrome extensions won’t work directly without conversion or dedicated versions.",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'><yrChapterNumber>3. </yrChapterNumber> Chrome Extensions</span>"
    ]
  },
  {
    "objectID": "intro00130-chromeExtensions.html#developing-your-own-chrome-extensions",
    "href": "intro00130-chromeExtensions.html#developing-your-own-chrome-extensions",
    "title": "3  3.  Chrome Extensions",
    "section": "3.3 Developing your own Chrome Extensions",
    "text": "3.3 Developing your own Chrome Extensions\nUsing modern AI, it is often possible to develop your own custom Chrome extensions. We will not go into that here, but the adventurous are free to experiment on your own.\nWhile under development a Chrome extension consists of several files in a folder. Once the coding of the Chrome extension has been completed, the entire folder with all of the necessary files can be compressed into a single zip file. It is this zip file that can be upladed to the Chrome Extension store.\n\n3.3.1 “unpacked” Chrome extensions\nWhile under development, a Chrome extension can still be installed in Chrome without needing to ZIP up the folder. Follow the instructions here to use a chrome extension that is still under development. You will need to\n\nunzip the folder if it is zipped\nnavigate to the url chrome://extensions\nClick the “developer mode” switch\nClick “load unpacked”\n\nChrome extensions are created using standard web development languages (i.e. HTML, CSS, Javascript).\nYou can follow the instructions in this video:\nhttps://www.youtube.com/watch?v=yNFwFQrc27Q",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'><yrChapterNumber>3. </yrChapterNumber> Chrome Extensions</span>"
    ]
  },
  {
    "objectID": "intro00130-chromeExtensions.html#using-the-yrwebsnip-chrome-extension",
    "href": "intro00130-chromeExtensions.html#using-the-yrwebsnip-chrome-extension",
    "title": "3  3.  Chrome Extensions",
    "section": "3.4 Using the yrWebSnip chrome extension",
    "text": "3.4 Using the yrWebSnip chrome extension\n\n3.4.1 Download the code for the extension\n\nGo here https://github.com/y-rosenthal/yrWebSnap\nClick on the “&lt;&gt; Code” button and choose “Download ZIP”\nUnzip the file to a folder (search online if you don’t know how to do that)",
    "crumbs": [
      "Some computer basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'><yrChapterNumber>3. </yrChapterNumber> Chrome Extensions</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html",
    "href": "ai-000100-overview.html",
    "title": "4  4.  AI in Everyday Use",
    "section": "",
    "text": "4.1 General purpose LLM Chat Websites\nLarge Language Models (LLMs) are AI-powered tools capable of understanding and generating human-like text. Here are some well-known platforms where users can interact with these models. These are all roughly similar. Their features keep evolving. For example as of this writing ChatGPT could generate picture while Claude.ai could not. Exactly what is possible with each model might also depend on whether you are using a free version or are paying a fee.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#general-purpose-llm-chat-websites",
    "href": "ai-000100-overview.html#general-purpose-llm-chat-websites",
    "title": "4  4.  AI in Everyday Use",
    "section": "",
    "text": "ChatGPT (chat.openai.com)\nThis is a good overview of what is possible with chatgpt.com as of Feb 2025. I recommend that you watch (perhaps on double speed) even if you are already familiar with chatGPT.com.\nhttps://www.youtube.com/watch?v=PDw3Uk9dN9k&t=521s\nGoogle Gemini (gemini.google.com)\nClaude (claude.ai) - Developed by Anthropic\nGrok (grok.com) - Developed by X (formerly twitter)",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#search-tools",
    "href": "ai-000100-overview.html#search-tools",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.2 Search tools",
    "text": "4.2 Search tools\nSome LLMs have limited ability to search the Internet. The folloiwng naturally searches the Internet for current data.\n\nPerplexity AI (perplexity.ai)",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#image-generation-i.e.-pictures",
    "href": "ai-000100-overview.html#image-generation-i.e.-pictures",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.3 Image Generation (i.e. pictures)",
    "text": "4.3 Image Generation (i.e. pictures)\n\nDALL·E (openai.com/dall-e) - AI-powered image generation from text descriptions.\nMidjourney (midjourney.com) - Specializes in high-quality AI-generated art.\nStable Diffusion (stablediffusionweb.com) - Open-source image generation AI.\nchatgpt.com also has the ability to generate pictures (see link above)",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#music-and-audio-generation",
    "href": "ai-000100-overview.html#music-and-audio-generation",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.4 Music and Audio Generation",
    "text": "4.4 Music and Audio Generation\n\nSuno AI (suno.ai) - AI-generated music based on text prompts.\nBoomy (boomy.com) - Create and personalize AI-generated music.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#video-generation",
    "href": "ai-000100-overview.html#video-generation",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.5 Video Generation",
    "text": "4.5 Video Generation\n\nRunway (runwayml.com) - AI-powered video editing and generation.\nSynthesia (synthesia.io) - AI-generated videos with avatars and voice synthesis.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#code-generation-and-assistance",
    "href": "ai-000100-overview.html#code-generation-and-assistance",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.6 Code Generation and Assistance",
    "text": "4.6 Code Generation and Assistance\n\nGitHub Copilot (github.com/features/copilot) - AI-powered coding assistance.\nReplit Ghostwriter (replit.com) - AI-powered coding suggestions for developers.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "ai-000100-overview.html#ai-for-writing-and-content-creation",
    "href": "ai-000100-overview.html#ai-for-writing-and-content-creation",
    "title": "4  4.  AI in Everyday Use",
    "section": "4.7 AI for Writing and Content Creation",
    "text": "4.7 AI for Writing and Content Creation\n\nJasper (jasper.ai) - AI-generated marketing copy, blog posts, and content.\nCopy.ai (copy.ai) - Helps generate creative and engaging text for various use cases.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'><yrChapterNumber>4. </yrChapterNumber> AI in Everyday Use</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html",
    "href": "aiCoding00100-overview.html",
    "title": "5  5.  Coding with AI - an overview",
    "section": "",
    "text": "5.1 Some AI terminology\nThe following terms and concepts are related to modern AI. Some terms are defined differently by different people. There is no “official consensus” on how some of these terms are defined. In fact the very term “Artificial Intelligence” is often defined differently by different people. The following are “working definitions” that will be assumed in this text.\nThese terms and concepts can be described at defined at different levels of detail. The following aims for a target audience of an intelligent adult without a deep technical background.\n## data Raw facts, figures, or information that can be recorded and analyzed. Data comes in many forms, such as numbers, text, images, or videos. For example, a list of daily temperatures, customer comments or photos uploaded to social media are all types of data. In technology, data is the foundation for creating insights, decisions, and machine learning models.\n## data set A collection of data, usually presented in a structured format like a table, spreadsheet, or database, a CSV file, a JSON file, etc. It’s often used for analysis or training machine learning models. For example, a data set could include a list of student grades, a catalog of product prices, or medical records of patients.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#machine-learning",
    "href": "aiCoding00100-overview.html#machine-learning",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.2 machine learning",
    "text": "5.2 machine learning\n The act of teaching a computer system to recognize patterns in data.\nThe computer This involves “training” a machine learning model.\nA “machine learning model” is basically a computer program that can The “machine learning model” contains computer code and data. The machine learning process trains the machine learning model to recognize patterns in the data.\nA type of computer programming where machines (computers) learn to make decisions or predictions based on data. Instead of being explicitly programmed for every task, the machine identifies patterns in the data and uses those patterns to improve over time. For example, a machine learning system can learn to recognize faces in photos or recommend movies based on your preferences.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#machine-learning-model",
    "href": "aiCoding00100-overview.html#machine-learning-model",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.3 machine learning model",
    "text": "5.3 machine learning model",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#predictive-machine-learning-model",
    "href": "aiCoding00100-overview.html#predictive-machine-learning-model",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.4 Predictive machine learning model",
    "text": "5.4 Predictive machine learning model\nAn machine learning model is a program that learns from data\nto perform specific tasks, like predicting an outcome, sorting things into categories, or creating text. It’s like teaching a computer how to solve a problem by giving it examples.\nYou said: for AI model - explain a little bit how a simple model is trained with examples that have many attributes and a single target variable ChatGPT said: ChatGPT An AI model is trained by showing it examples that include many attributes (inputs) and a single target variable (output). Think of it like teaching the model how to match patterns.\nFor example, if you’re training a model to predict house prices, the attributes could be the size of the house, the number of bedrooms, and the location. The target variable would be the house price. You give the model a lot of these examples, and it learns the relationships between the attributes and the price.\nDuring training, the model makes guesses (predictions), compares them to the actual prices, and adjusts itself to improve. Over time, it gets better at predicting the target variable for new examples it hasn’t seen before.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#foundation-models",
    "href": "aiCoding00100-overview.html#foundation-models",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.5 foundation models",
    "text": "5.5 foundation models\nFoundation models are very large AI systems trained on huge amounts of data. They are like general-purpose tools that can be customized to do many tasks, such as writing, translating, or recognizing images.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#large-language-models-llm",
    "href": "aiCoding00100-overview.html#large-language-models-llm",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.6 Large Language Models (LLM)",
    "text": "5.6 Large Language Models (LLM)\nThere are different types of AI software systems. Foundation models The development of AI systems AI systems need to be “trained” to AI systems are developed as “models”. A Large The following are all “large language model” AI systems, similar to chatgpt that you can use for this assignment. The AI space is new and developing rapidly. There are other choices and new options and offerings become available\n\nhttps://claude.ai/ (this is probably the best at coding today)\nhttps://chatgpt.com/\nhttps://gemini.google.com/\nhttps://grok.com/\n\nYou are to use AI to code an R program. You are to keep making the program more complicated until the AI finally does NOT produce perfect results. Then you are to debug the code and post the final product.\nThis assignment is intended to start getting you familiar with using AI for coding tasks.\nAs an example, we used claude.ai to build a tic tac toe program in R. We used the following prompt:\nPlease write a tic tac toe game in R. \nThe board should be stored in a matrix. \nThe game should be played by two human players. \nA sample game might look like this:\n\nThe board looks like this:\n\n1 2 3 \n4 5 6\n7 8 9\n\nPlayer 1 (i.e. x) choose a number to move to: 5\n\nThe board looks like this:\n1 2 3 \n4 x 6\n7 8 9\n\nPlayer 2 (i.e. o) choose a number to move to: 5\nPlease choose a number on the board: apple\nPlease choose a number on the board: 7\n\nThe board looks like this:\n1 2 3 \n4 x 6\no 8 9\n\nKeep this going until someone wins or there is a tie.\nAfter the game is over, ask the users if they would like \nto play again.\n\nPlease only use Base R code - no packages.\nClaude.ai responded with (basically) the correct code for the requested tic tac toe program. If you copy/paste the following code into R and run it, you will be playing tic tac toe.\n\n# Create a function to display the board\ndisplay_board &lt;- function(board) {\n  cat(\"\\nThe board looks like this:\\n\\n\")\n  for(i in 1:3) {\n    cat(board[i,], \"\\n\")\n  }\n  cat(\"\\n\")\n}\n\n# Function to check if a player has won\ncheck_win &lt;- function(board, player) {\n  # Check rows\n  for(i in 1:3) {\n    if(all(board[i,] == player)) return(TRUE)\n  }\n  # Check columns\n  for(i in 1:3) {\n    if(all(board[,i] == player)) return(TRUE)\n  }\n  # Check diagonals\n  if(all(diag(board) == player)) return(TRUE)\n  if(all(diag(board[3:1,]) == player)) return(TRUE)\n  \n  return(FALSE)\n}\n\n# Function to check if the board is full (tie)\ncheck_tie &lt;- function(board) {\n  return(!any(board %in% 1:9))\n}\n\n# Main game function\nplay_tictactoe &lt;- function() {\n  # Initialize game\n  playing &lt;- TRUE\n  \n  while(playing) {\n    # Create board\n    board &lt;- matrix(1:9, nrow=3, byrow=TRUE)\n    game_over &lt;- FALSE\n    current_player &lt;- \"x\"\n    \n    # Main game loop\n    while(!game_over) {\n      # Display board\n      display_board(board)\n      \n      # Get player move\n      valid_move &lt;- FALSE\n      while(!valid_move) {\n        cat(sprintf(\"Player %s choose a number to move to: \", \n            ifelse(current_player == \"x\", \"1 (i.e. x)\", \"2 (i.e. o)\")))\n        move &lt;- readline()\n        \n        # Check if input is numeric\n        if(!grepl(\"^[1-9]$\", move)) {\n          cat(\"Please choose a number on the board.\\n\")\n          next\n        }\n        \n        move &lt;- as.numeric(move)\n        \n        # Check if move is valid (position is not taken)\n        if(!any(board == move)) {\n          cat(\"That position is already taken. Please choose another.\\n\")\n          next\n        }\n        \n        valid_move &lt;- TRUE\n      }\n      \n      # Update board\n      board[board == move] &lt;- current_player\n      \n      # Check for win\n      if(check_win(board, current_player)) {\n        display_board(board)\n        cat(sprintf(\"Player %s wins!\\n\", \n            ifelse(current_player == \"x\", \"1\", \"2\")))\n        game_over &lt;- TRUE\n      }\n      \n      # Check for tie\n      else if(check_tie(board)) {\n        display_board(board)\n        cat(\"It's a tie!\\n\")\n        game_over &lt;- TRUE\n      }\n      \n      # Switch players\n      current_player &lt;- ifelse(current_player == \"x\", \"o\", \"x\")\n    }\n    \n    # Ask to play again\n    cat(\"Would you like to play again? (y/n): \")\n    play_again &lt;- readline()\n    playing &lt;- tolower(play_again) == \"y\"\n  }\n  \n  cat(\"Thanks for playing!\\n\")\n}\n\n# Start the game\nplay_tictactoe()",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#suggestions-for-prompting-the-ai-models",
    "href": "aiCoding00100-overview.html#suggestions-for-prompting-the-ai-models",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.7 Suggestions for prompting the AI models",
    "text": "5.7 Suggestions for prompting the AI models\n\nSpeak to the AI as you would speak with a person.\nGive the AI a little more information than you might think necessary. Including the background information and reasons for why you are doing something can help a person and in this case the AI, tailor their responses to take into account things that you may not have thought of.",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "aiCoding00100-overview.html#different-types-of-errors",
    "href": "aiCoding00100-overview.html#different-types-of-errors",
    "title": "5  5.  Coding with AI - an overview",
    "section": "5.8 Different types of errors",
    "text": "5.8 Different types of errors\nThere can be two different types of errors in code:\n\nA “syntax error”. For this type of error R (or another programming language) can often displays an error message because you didn’t use the language correctly. For example, misspelling the name of a function. Some of these types of errors can be trickly, for example you might not get an error message, if there is a missing closing parentheses somewhere in the generated R code.\nA “logic error”.\n\nRunning the code requires the following R command:",
    "crumbs": [
      "Into to Coding With AI",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'><yrChapterNumber>5. </yrChapterNumber> Coding with AI - an overview</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html",
    "href": "0000022-InstallingRPackages-v003.html",
    "title": "6  6.  Intro to R packages",
    "section": "",
    "text": "6.1 What is CRAN?\nCRAN stands for the “Comprehensive R Archive Network”.\nR is free software that is supported by many different organizations. The official website for “R” is https://cran.r-project.org/. From this website you can download the “base R” software. This is also the official source of many things related to R. (NOTE that information about RStudio is NOT found on this website. RStudio is a program created by a for-profit company, Posit. The RStudio software is built on top of “base R”. The official website for RStudio is https://posit.co/)\nThe list of “officially recognized” R packages can be found here: https://cran.r-project.org/web/packages/available_packages_by_name.html From this page, you can get a lot more information about each of the different packages. To learn more about packages in general, click on the “packages” link located on this page https://cran.r-project.org/.",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#what-is-a-cran-mirror",
    "href": "0000022-InstallingRPackages-v003.html#what-is-a-cran-mirror",
    "title": "6  6.  Intro to R packages",
    "section": "6.2 What is a CRAN mirror?",
    "text": "6.2 What is a CRAN mirror?\nIt can cost a lot of money to maintain a popular website. Since R is a free program, the costs for hosting the offical website are divided among many of the organizations that have an interest in seeing R succeed. (Many of these organizations are educational institutions located around the world). Each of these organizations hosts a complete repository of the “base R” software as well as all of the R packages. All of the repositories together are known as CRAN, i.e. the “Comprehensive R Archive Network”. Because the repositories hosted by each organization have identical contents, the repositories are also known as “CRAN mirrors”. Whenever you install an R package (see install.packages below), the files for the package are downloaded from one of the CRAN mirror websites.",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#install.packages-some_package",
    "href": "0000022-InstallingRPackages-v003.html#install.packages-some_package",
    "title": "6  6.  Intro to R packages",
    "section": "6.3 install.packages( “SOME_PACKAGE” )",
    "text": "6.3 install.packages( “SOME_PACKAGE” )\nTo use a package you must first “install” the package with the install.packages function. install.packages downloads the package files from one of the CRAN mirror websites and installs them on your computer.\nYou only need to use install.packages on your computer once. After you’ve installed a package it will continue to be installed until you until you “remove” it from your computer with the function remove.packages().\nSee an example of installing the stringr package below. Pay special attention the the #comments in the code.\n\n# When calling install.packages, you must use \"quotes\" around the package name.\ninstall.packages(\"stringr\")   \n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\n# By default, install.packages will choose one of the CRAN mirror websites to \n# download the package from. The exact CRAN mirror website that it uses\n# is configured in your R software. However, if for some reason, it fails\n# to download from the default CRAN mirror website, you will need to \n# specify the exact CRAN mirror URLs in the repos argument as shown below.\n#\n# If you specify more than one website, install.packages will keep trying, \n# starting from the first one specified, until it is successful. The \n# following page contains the list of the CRAN mirror websites:\n#\n#    https://cran.r-project.org/mirrors.html\n#\n# It makes very little difference which mirror you choose. In order to \n# speed up the downloads it's recommended to choose a mirror that is \n# close to where you are located. The following command specifies the full\n# list of mirrors that are located in the USA.\n\ninstall.packages(\n  \"stringr\", \n  repos = c(\n    \"https://mirror.las.iastate.edu/CRAN/\",       # Iowa State University, Iowa\n    \"http://ftp.ussg.iu.edu/CRAN/\",               # Indiana University, Indiana\n    \"https://repo.miserver.it.umich.edu/cran/\", # University of Michigan\n    \"https://cran.wustl.edu/\",              # Washington University, Missouri\n    \"https://archive.linux.duke.edu/cran/\", # Duke University, NC\n    \"https://cran.case.edu/\",               # Case Western Reserve University, OH\n    \"https://ftp.osuosl.org/pub/cran/\",     # Oregon State University\n    \"http://lib.stat.cmu.edu/R/CRAN/\",    # Carnegie Mellon University, PA\n    \"https://cran.mirrors.hoobly.com/\",     # Hoobly Classifieds, PA\n    \"https://mirrors.nics.utk.edu/cran/\") # Nat. Inst. 4 Computational Sci, TN \n)\n\nWarning: unable to access index for repository https://mirror.las.iastate.edu/CRAN/src/contrib:\n  cannot open URL 'https://mirror.las.iastate.edu/CRAN/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository http://ftp.ussg.iu.edu/CRAN/src/contrib:\n  cannot open URL 'http://ftp.ussg.iu.edu/CRAN/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://repo.miserver.it.umich.edu/cran/src/contrib:\n  cannot open URL 'https://repo.miserver.it.umich.edu/cran/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.wustl.edu/src/contrib:\n  cannot open URL 'https://cran.wustl.edu/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://archive.linux.duke.edu/cran/src/contrib:\n  cannot open URL 'https://archive.linux.duke.edu/cran/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.case.edu/src/contrib:\n  cannot open URL 'https://cran.case.edu/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://ftp.osuosl.org/pub/cran/src/contrib:\n  cannot open URL 'https://ftp.osuosl.org/pub/cran/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository http://lib.stat.cmu.edu/R/CRAN/src/contrib:\n  cannot open URL 'http://lib.stat.cmu.edu/R/CRAN/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.mirrors.hoobly.com/src/contrib:\n  cannot open URL 'https://cran.mirrors.hoobly.com/src/contrib/PACKAGES'\n\n\nWarning: unable to access index for repository https://mirrors.nics.utk.edu/cran/src/contrib:\n  cannot open URL 'https://mirrors.nics.utk.edu/cran/src/contrib/PACKAGES'\n\n\nWarning: package 'stringr' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nWarning: unable to access index for repository https://mirror.las.iastate.edu/CRAN/bin/windows/contrib/4.4:\n  cannot open URL 'https://mirror.las.iastate.edu/CRAN/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository http://ftp.ussg.iu.edu/CRAN/bin/windows/contrib/4.4:\n  cannot open URL 'http://ftp.ussg.iu.edu/CRAN/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://repo.miserver.it.umich.edu/cran/bin/windows/contrib/4.4:\n  cannot open URL 'https://repo.miserver.it.umich.edu/cran/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.wustl.edu/bin/windows/contrib/4.4:\n  cannot open URL 'https://cran.wustl.edu/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://archive.linux.duke.edu/cran/bin/windows/contrib/4.4:\n  cannot open URL 'https://archive.linux.duke.edu/cran/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.case.edu/bin/windows/contrib/4.4:\n  cannot open URL 'https://cran.case.edu/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://ftp.osuosl.org/pub/cran/bin/windows/contrib/4.4:\n  cannot open URL 'https://ftp.osuosl.org/pub/cran/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/4.4:\n  cannot open URL 'http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://cran.mirrors.hoobly.com/bin/windows/contrib/4.4:\n  cannot open URL 'https://cran.mirrors.hoobly.com/bin/windows/contrib/4.4/PACKAGES'\n\n\nWarning: unable to access index for repository https://mirrors.nics.utk.edu/cran/bin/windows/contrib/4.4:\n  cannot open URL 'https://mirrors.nics.utk.edu/cran/bin/windows/contrib/4.4/PACKAGES'",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#help-for-the-package.",
    "href": "0000022-InstallingRPackages-v003.html#help-for-the-package.",
    "title": "6  6.  Intro to R packages",
    "section": "6.4 Help for the package.",
    "text": "6.4 Help for the package.\nOnce you’ve installed the package you can access the help pages for the package with the following command:\n\nhelp(package=“SOME_PACKAGE”)\n\n\n##############################################################################.\n# To see the functions in the package use the following command\n##############################################################################.\nhelp(package=\"stringr\")\n\nTo see examples of how to use a specific function. Click on the “Run examples” link that appears at the bottom of the help page. Try this for the following functions in the stringr package:\n\nstr_trunc\nstr_split",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#using-a-package-without-loading-it.",
    "href": "0000022-InstallingRPackages-v003.html#using-a-package-without-loading-it.",
    "title": "6  6.  Intro to R packages",
    "section": "6.5 Using a package without “loading” it.",
    "text": "6.5 Using a package without “loading” it.\nBelow you will learn how to “load” a package using the library and require functions. This functions make it easier to use a package. However, even before you “load” a package, you can still use it by specifying\nPACKAGE_NAME::FUNCTION_NAME(ARGUMENTS,GO,HERE) when you call the function.\nSee the example below:\n\n# After installing a package you can use the functions by specifying \n# the name of the package and the function as shown in the \n# example below. This example uses the str_trunc and the str_split\n# functions. Both of these functions are included in the stringr package.\n#\n# The str_trunc function \"truncates\" or \"chops off the end\" of long strings. \n# The str_split function splits up strings .\n#\n# See the documentation for more info about these functions:\n# help(package=\"stringr\")\n\n# Create a vector.\nsayings = c(\"roses are red, violets are blue\", \n            \"a stitch in time saves nine\")\n\n# Chop off each of the strings after in the vector after the 25th character\nstringr::str_trunc(sayings, width=25)\n\n[1] \"roses are red, violets...\" \"a stitch in time saves...\"\n\n# Split the words in each of the strings. This function returns a list.\nstringr::str_split(sayings, pattern=\" \")\n\n[[1]]\n[1] \"roses\"   \"are\"     \"red,\"    \"violets\" \"are\"     \"blue\"   \n\n[[2]]\n[1] \"a\"      \"stitch\" \"in\"     \"time\"   \"saves\"  \"nine\"",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#loading-packages---library-or-require",
    "href": "0000022-InstallingRPackages-v003.html#loading-packages---library-or-require",
    "title": "6  6.  Intro to R packages",
    "section": "6.6 loading packages - library() or require()",
    "text": "6.6 loading packages - library() or require()\n\n6.6.1 library(SOME_PACKAGE)\n\n# In the example above, to run the functions we used the syntax\n# PACKAGENAME::FUNCTIONNAME(), e.g. stringr::str_trunc( ARGUMENTS GO HERE )\n#\n# At this point, if you try to run the function without the package name you\n# will get an error, e.g. str_trun( ARGUMENTS GO HERE ) \n#\n# Compare the command below with the similar command above.\n\n# ERROR - function str_trunc not found.\nstr_trunc(sayings, width=25)\n\nError in str_trunc(sayings, width = 25): could not find function \"str_trunc\"\n\n# ERROR - function str_trunc not found.\nstr_split(sayings, pattern=\" \")\n\nError in str_split(sayings, pattern = \" \"): could not find function \"str_split\"\n\n\nThe reason why you get an error in the code above is because, while the package was “installed” on your computer, R doesn’t know to look in that package for functions.\nThe library function, e.g. library(stringr), tells R to add the package to the list of packages that R looks through to find functions. This allows you to call functions in a package without specifying the package name. For example, after calling library(stringr) you may call any of the functions in the stringr package without needing to specify the word stringr.\n\n# you don't need quotes\nlibrary(stringr)   \n\nWarning: package 'stringr' was built under R version 4.4.3\n\n# You can now call any functions in the stringr package without \n# specifying stringr::\n\n# Now it works without prefixing the function calls with stringr::\nstr_trunc(sayings, width=25)\n\n[1] \"roses are red, violets...\" \"a stitch in time saves...\"\n\nstr_split(sayings, pattern=\" \")\n\n[[1]]\n[1] \"roses\"   \"are\"     \"red,\"    \"violets\" \"are\"     \"blue\"   \n\n[[2]]\n[1] \"a\"      \"stitch\" \"in\"     \"time\"   \"saves\"  \"nine\"  \n\n\n\n\n6.6.2 install.packages vs library\nAs we said above, you only need to call install.packages(SOME_PACKAGE) once. Even if you reboot your computer, the package will still be installed.\nHowever, you need to call library(SOME_PACKAGE) every time you restart R if you want to be able to use the functions in the package without specifying the package name every time you call a function.\n\n\n6.6.3 require(SOME_PACKAGE)\nThere is another function, require(SOME_PACKAGE), that basically accomplishes the same thing as the library(SOME_PACKAGE) function (the differences between library and require are explained below). You may use either library or require. You never need to use both. For example, the commands below are an alternative to the commands shown above.\n\n#  Just like with library, after calling require(stringr) you no longer\n#  need to specify stringr:: before calling a function.\n\n#require(stringr)\n\n#str_trunc(c(\"roses are red, violets are blue\", \"a stitch in time saves nine\"),\n#              width = 25)\n\n# Now you can continue to call any functions from the stringr package \n# without specifying stringr::\n\n\n\n6.6.4 Calling library or require more than once is fine.\nIn case you were wondering, if you call library(SOME_PACKAGE) or require(SOME_PACKAGE) once and then call one of these functions again, nothing happens the 2nd time. No errors or warnings are generated.\n\n\n6.6.5 The difference between library and require\nThe difference between library(SOME_PACKAGE) vs require(SOME_PACKAGE) is regarding the return value of library vs require.\nBy default library and require do not display their return values - they have “invisible” return values. However, while the return values are not displayed, they do exist. To see an “invisible” return value you can either capture the return value in a variable. For example:\n\n# The return value of library is the vector of packages that have have \n# already been \"loaded\". Note that there are some packages that are \n# automatically loaded when you start R without you needing to \n# run the library/require command.\n\nreturnValue = library(stringr)\n\n# show the names of the loaded packages\nreturnValue\n\n[1] \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n# The require function returns a very different value.\n# The require function returns TRUE if the packages was loaded\n# successfully and returns FALSE otherwise. \n\nreturnValue = require(stringr)\nreturnValue\n\n[1] TRUE\n\n# One common reason for a package NOT loading successfully\n# is if the package hadn't been installed yet (using install.packages).\n# For example, the \"glue\" package contains some useful functions but we\n# haven't installed the glue package yet. Therefore the following\n# call to require will return FALSE. \n\nreturnValue = require(glue)   # FALSE - we haven't installed glue yet\n\nLoading required package: glue\n\n\n\n\n6.6.6 See invisible return values by using (parentheses)\n\n# Note you can also see the \"invisible\" return values by simply\n# surrounding the function calls with (parentheses). For example\n\nlibrary(stringr)   # return value is invisible\nrequire(stringr)   # return value is invisible\n\n(library(stringr))   # use (parentheses) to see the return value\n\n[1] \"glue\"      \"stringr\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n(require(stringr))   # use (parentheses) to see the return value\n\n[1] TRUE\n\n\n\n\n6.6.7 installing and loading the package in one line of code\nSome packages can take a while to install. Therefore you usually don’t want to install a package more than once. You can use the return value from require to write code that only installs the package if the call to require fails. For example\nNote that you can run several R commands in a single line by putting a semicolon between those commands. Similarly you can write a function on a single line. Therefore the following example shows how you can use a single line of code to install the package only if necessary and to then load the package. This type of code is very convenient to put at the top of .R files.\n\n# First try to load stringr with the require function.\n# If stringr is not installed, then require will return FALSE.\n#\n# Only if require returns FALSE will stringr be installed.\n# After it is installed the require function is called again to make sure \n# that the package is loaded.\n\nif(!require(stringr)){install.packages(\"stringr\");require(stringr)}\n\n# Now you should be able to use any function from stringr without specifying\n# stringr::\nstr_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#unloading-and-removing-packages",
    "href": "0000022-InstallingRPackages-v003.html#unloading-and-removing-packages",
    "title": "6  6.  Intro to R packages",
    "section": "6.7 unloading and removing packages",
    "text": "6.7 unloading and removing packages\nIt’s not usually necessary, but it is possible to unload packaes or remove packages entirely from your computer.\nYou may want to unload a package (without uninstalling it) if the name of a function in the package is interfering with another package that has a function with the same name.\nYou may want to uninstall a package to make room on your hard drive.\n\n6.7.1 detach(“package:PACKAGE_NAME”, unload=TRUE)\nThe “opposite” of libarary(SOME_PACKAGE) and require(SOME_PACKAGE) is detach(“package:PACKAGE_NAME”, unload=TRUE). See the documentation for more info : ?detach To call it you must specify the word “package” before the actual package name. See the example below:\n\n###################################.\n# Package is already loaded\n###################################.\n\n# We loaded stringr above (with library or require) so this works\nstr_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"\n\n# and so does this ... (but it's unnecessary to specify the package)\nstringr::str_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"\n\n###################################.\n# Detaching the package ...\n###################################.\ndetach(package:stringr, unload=TRUE)\n\n# Since we detached the package the function will not work without \n# specifying the name of the package:\n\n# This doesn't work now\nstr_trunc(\"An apple a day keeps the doc away\", 18)\n\nError in str_trunc(\"An apple a day keeps the doc away\", 18): could not find function \"str_trunc\"\n\n# However, this does:\nstringr::str_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"\n\n###################################.\n# loading the package again ...\n###################################.\n\n# loading the package again ...\nlibrary(stringr)  # or reqruire(stringr)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n# once again, both commands work:\nstr_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"\n\nstringr::str_trunc(\"An apple a day keeps the doc away\", 18)\n\n[1] \"An apple a day ...\"\n\n\n\n\n6.7.2 remove.packages(SOME_PACKAGE)\nThe “opposite” of install.packages(SOME_PACKAGE) is remove.packages(SOME_PACKAGE) NOT uninstall! See the documentation for more info : ?remove.packages\n\n# What is CRAN?\n# \n# See this - &lt;https://www.youtube.com/watch?v=GM6MCBkVNtQ&gt;",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#vignettes",
    "href": "0000022-InstallingRPackages-v003.html#vignettes",
    "title": "6  6.  Intro to R packages",
    "section": "6.8 vignettes",
    "text": "6.8 vignettes\nVignettes are tutorials on some of a package. These are often a good source of information. To see vignettes for a package that you haven’t installed, look at the “Vignettes” listing on its CRAN page.\nYou can find the CRAN page for a package here and then search for “vignettes” on the page.\n\nhttps://cran.r-project.org/web/packages/available_packages_by_name.html\n\nThe following is the page for the stringr package: &gt; https://cran.r-project.org/web/packages/stringr/index.html\nThe following are some vignettes listed on that page:\n\nhttps://cran.r-project.org/web/packages/stringr/vignettes/from-base.html\n\n\nhttps://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html\n\n\nhttps://cran.r-project.org/web/packages/stringr/vignettes/stringr.html",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000022-InstallingRPackages-v003.html#packages-from-other-sources",
    "href": "0000022-InstallingRPackages-v003.html#packages-from-other-sources",
    "title": "6  6.  Intro to R packages",
    "section": "6.9 Packages from other sources",
    "text": "6.9 Packages from other sources\nR packages can be created by anyone and then be submitted to be hosted on CRAN. The official procedures for doing so can be found by reviewing the information here https://cran.r-project.org/submit.html. There are also books and other sources of information about how to create R packages and submit them to CRAN.\nHowever, a package doesn’t have to be on CRAN to be used. Many software developers create R packages that are distributed in other ways.\ngithub.com is a website that hosts code “repositories”. Software developers often post code on github.com that may be freely downloaded and installed. Often developers of R packages first make the package available on github before the package becomes available on CRAN. Sometimes the package never becomes available on CRAN. Often a package is hosted on both github and CRAN. Often github contains the “bleeding edge” version of\na package while CRAN hosts packages that have been more fully tested.\nOther sites that are similar to github.com are bitbucket, gitlab and others.\nIf you come across a package that is on github, bitbucket, etc that you’d like to install you can do so\nThe “devtools” and “remotes” packages (that are available on CRAN) contain functions, similar to install.packages, that allow you to install R packages directly from repositories such as github, etc. It’s pretty easy to find information online about how to use these packages to install R packages from github, bitbucket, etc.",
    "crumbs": [
      "Intro to R packages",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'><yrChapterNumber>6. </yrChapterNumber> Intro to R packages</span>"
    ]
  },
  {
    "objectID": "0000017-introToQuarto-v001.html",
    "href": "0000017-introToQuarto-v001.html",
    "title": "7  7.  Intro to Quarto",
    "section": "",
    "text": "Official into: https://quarto.org/docs/get-started/hello/rstudio.html\nMarkdown syntax: https://quarto.org/docs/authoring/markdown-basics.html\nOverview of Quarto in textbook: R for Data Science, 2e\nby Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund:\nhttps://r4ds.hadley.nz/quarto.html",
    "crumbs": [
      "Intro to Quarto",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'><yrChapterNumber>7. </yrChapterNumber> Intro to Quarto</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html",
    "href": "0400050-whatIsTidyverse-v001.html",
    "title": "8  8.  What is the “tidyverse”",
    "section": "",
    "text": "8.1 Installation and Loading\nTo install all tidyverse packages:\nTo load the core tidyverse packages:\nAlternatively, you can install and load each package separately.",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#installation-and-loading",
    "href": "0400050-whatIsTidyverse-v001.html#installation-and-loading",
    "title": "8  8.  What is the “tidyverse”",
    "section": "",
    "text": "install.packages(\"tidyverse\")\n\nlibrary(tidyverse)",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#core-tidyverse-packages",
    "href": "0400050-whatIsTidyverse-v001.html#core-tidyverse-packages",
    "title": "8  8.  What is the “tidyverse”",
    "section": "8.2 Core Tidyverse Packages",
    "text": "8.2 Core Tidyverse Packages\nThe core tidyverse includes packages that you’ll likely use in almost every analysis. When you run library(tidyverse), these packages are automatically loaded:\n\n8.2.1 ggplot2\n\nPurpose: Creates elegant data visualizations using the Grammar of Graphics\nKey features: Layered approach to building plots, extensive customization options\nCommon uses: Creating statistical graphics, from basic scatter plots to complex multi-layered visualizations\nLearn more: https://ggplot2.tidyverse.org\n\n\n\n8.2.2 dplyr\n\nPurpose: Data manipulation and transformation\nKey features: Functions for filtering, selecting, mutating, summarizing, and joining data\nCommon uses: Subset rows, select columns, create new variables, calculate summaries\nLearn more: https://dplyr.tidyverse.org\n\n\n\n8.2.3 tidyr\n\nPurpose: Data tidying and reshaping\nKey features: Functions for pivoting data between wide and long formats, handling missing values\nCommon uses: Restructuring data to make it tidy (each variable in a column, each observation in a row)\nLearn more: https://tidyr.tidyverse.org\n\n\n\n8.2.4 readr\n\nPurpose: Fast and friendly reading of rectangular data (like CSV files)\nKey features: Consistent parsing, progress bars, warnings about parsing problems\nCommon uses: Reading CSV, TSV, and other delimited files\nLearn more: https://readr.tidyverse.org\n\n\n\n8.2.5 purrr\n\nPurpose: Functional programming tools\nKey features: Functions for working with vectors and lists, replacing many loop functions\nCommon uses: Applying functions to lists, working with nested data structures\nLearn more: https://purrr.tidyverse.org\n\n\n\n8.2.6 tibble\n\nPurpose: Modern reimagining of data frames\nKey features: Better printing, stricter subsetting, and enhanced creation of data frames\nCommon uses: Creating and working with data frames in a more consistent way\nLearn more: https://tibble.tidyverse.org\n\n\n\n8.2.7 stringr\n\nPurpose: String manipulation\nKey features: Consistent interface for working with strings\nCommon uses: Pattern matching, string extraction, modification, and joining\nLearn more: https://stringr.tidyverse.org\n\n\n\n8.2.8 forcats\n\nPurpose: Working with categorical variables (factors)\nKey features: Tools for reordering factor levels, recoding, and collapsing factors\nCommon uses: Managing and modifying factor variables\nLearn more: https://forcats.tidyverse.org\n\n\n\n8.2.9 lubridate\n\nPurpose: Working with dates and times\nLoad with: library(lubridate)\nLearn more: https://lubridate.tidyverse.org",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#non-core-tidyverse-packages",
    "href": "0400050-whatIsTidyverse-v001.html#non-core-tidyverse-packages",
    "title": "8  8.  What is the “tidyverse”",
    "section": "8.3 Non-Core Tidyverse Packages",
    "text": "8.3 Non-Core Tidyverse Packages\nThese packages are part of the tidyverse but aren’t loaded automatically with library(tidyverse). You’ll need to load them individually when needed:\n\n8.3.1 readxl\n\nPurpose: Reading Excel files\nLoad with: library(readxl)\nLearn more: https://readxl.tidyverse.org\n\n\n\n8.3.2 haven\n\nPurpose: Reading SPSS, Stata, and SAS files\nLoad with: library(haven)\nLearn more: https://haven.tidyverse.org\n\n\n\n8.3.3 hms\n\nPurpose: Working with time-of-day values\nLoad with: library(hms)\nLearn more: https://hms.tidyverse.org\n\n\n\n8.3.4 jsonlite\n\nPurpose: Working with JSON data\nLoad with: library(jsonlite)\nLearn more: https://jsonlite.tidyverse.org\n\n\n\n8.3.5 xml2\n\nPurpose: Working with XML data\nLoad with: library(xml2)\nLearn more: https://xml2.tidyverse.org\n\n\n\n8.3.6 rvest\n\nPurpose: Web scraping\nLoad with: library(rvest)\nLearn more: https://rvest.tidyverse.org",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#finding-help-and-learning-more",
    "href": "0400050-whatIsTidyverse-v001.html#finding-help-and-learning-more",
    "title": "8  8.  What is the “tidyverse”",
    "section": "8.4 Finding Help and Learning More",
    "text": "8.4 Finding Help and Learning More\n\nEach package has its own website (links provided above) with comprehensive documentation, vignettes, and examples.\nWithin R, you can access help for any function using the ? operator:\n\n?select  # Get help for dplyr's select function\n\nCheatsheets are available for many tidyverse packages:\n\n\nAccess in RStudio: Help &gt; Cheatsheets\nDownload from: https://posit.co/resources/cheatsheets/\n\n\nThe official tidyverse website (https://www.tidyverse.org) provides additional resources and learning materials.\nThe “R for Data Science” book (https://r4ds.hadley.nz/) is an excellent resource for learning the tidyverse ecosystem.",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#understanding-core-vs-non-core-packages",
    "href": "0400050-whatIsTidyverse-v001.html#understanding-core-vs-non-core-packages",
    "title": "8  8.  What is the “tidyverse”",
    "section": "8.5 Understanding Core vs Non-Core Packages",
    "text": "8.5 Understanding Core vs Non-Core Packages\nThe distinction between core and non-core tidyverse packages is primarily about frequency of use and interdependence:\n\n8.5.1 Core Packages:\n\nLoaded automatically with library(tidyverse)\nUsed in most data analysis workflows\nWork together seamlessly\nShare common design principles and documentation styles\nUpdated and maintained on a coordinated release schedule\n\n\n\n8.5.2 Non-Core Packages:\nThese packages are part of the tidyverse. They are installed with install.packages(“tidyverse”) but aren’t loaded automatically with library(tidyverse). You’ll need to load them individually (with library(PACKAGE_NAME) or require(PACKAGE_NAME).\n\nMust be loaded individually\nServe more specialized purposes\nStill follow tidyverse design principles\nMay be used less frequently\nMay have different release schedules\nOften depend on core packages but are not required for basic data analysis workflows\n\nRemember that being “non-core” doesn’t mean less important—it simply means the package serves a more specialized purpose that isn’t needed in every analysis.",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-whatIsTidyverse-v001.html#non-core-tidyverse-packages-as-of-feb-2025",
    "href": "0400050-whatIsTidyverse-v001.html#non-core-tidyverse-packages-as-of-feb-2025",
    "title": "8  8.  What is the “tidyverse”",
    "section": "8.6 Non-Core Tidyverse Packages (as of Feb 2025)",
    "text": "8.6 Non-Core Tidyverse Packages (as of Feb 2025)\nThese packages are part of the tidyverse but aren’t loaded automatically with library(tidyverse). You’ll need to load them individually:\n\n8.6.1 File Reading and Writing\n\nreadxl: Efficiently reads Excel files (.xls and .xlsx formats) into R. Provides functions to read entire sheets or specific cells, and handles various Excel formatting quirks.\nhaven: Specializes in reading data from other statistical software like SPSS, Stata, and SAS. Preserves value labels, variable labels, and other metadata from the original files.\nfs: Provides a cross-platform interface for working with files and directories. Includes tools for creating, moving, copying, and deleting files, as well as working with file permissions and paths in a consistent way across operating systems.\njsonlite: Handles reading and writing JSON data with robust encoding support. Specializes in converting between JSON data and R objects while maintaining data types and structures.\nxml2: Provides tools for working with XML files. Includes functions for reading, writing, and manipulating XML data, with support for XPath expressions and namespace handling.\n\n\n\n8.6.2 Web and Database\n\nhttr2: Modern toolkit for working with web APIs and HTTP. Includes features for authentication, request/response handling, and working with different HTTP methods.\nrvest: Makes it easy to scrape data from web pages. Provides tools for extracting specific elements from HTML pages, handling forms, and navigating web pages programmatically.\ndbplyr: Allows you to work with databases using familiar dplyr syntax. Translates R code into SQL queries behind the scenes, making it easier to work with large datasets stored in databases.\ngoogledrive: Provides a full-featured interface to Google Drive from R. Allows you to upload, download, and manage files on Google Drive programmatically.\n\n\n\n8.6.3 Modeling and Development\n\nmodelr: Contains helper functions for modeling and model visualization. Includes tools for data partitioning, cross-validation, and creating model predictions.\nbroom: Converts statistical model objects into tidy tibbles. Makes it easier to work with model outputs using tidyverse tools by providing consistent data structures.\nreprex: Creates reproducible examples for sharing code problems or solutions. Formats R code and its output in a way that’s perfect for posting to GitHub issues, Stack Overflow, or other forums.\ndevtools: Essential toolkit for R package development. Provides functions for creating, testing, and maintaining R packages, including tools for documentation and testing.\n\n\n\n8.6.4 Time and Dates\n\nhms: Specialized package for working with time-of-day values. Provides a simple class for storing and manipulating time measurements without dates.\nclock: Comprehensive toolkit for working with dates and times. Offers more precise control over calendars, time zones, and date-time arithmetic than base R or lubridate.",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'><yrChapterNumber>8. </yrChapterNumber> What is the \"tidyverse\"</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html",
    "href": "0400050-tibbles-v001.html",
    "title": "9  9.  tibbles and tribbles",
    "section": "",
    "text": "9.1 Introduction to Tibbles\n# Tibbles are part of the tibble package, which is included in tidyverse\n# You can install/load tibble directly\nif(!require(tibble)) {install.packages(\"tibble\");require(tibble);}\n\nLoading required package: tibble\n\n# For more info see:\n#\n# help(package=\"tibble\")",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#what-are-tibbles",
    "href": "0400050-tibbles-v001.html#what-are-tibbles",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.2 What are Tibbles?",
    "text": "9.2 What are Tibbles?\nTibbles are a modern reimagining of R’s traditional data.frame. They are designed to make working with data frames easier and more consistent. Here’s how to create a basic tibble:\n\n9.2.1 tibble() function is similar to data.frame() function\nCreate a tibble with the tibble function. It’s use is very similar to how you would use the data.frame function to create a dataframe (we’re assuming that you’re familiar with creating dataframes in R)\n\n# Create a tibble directly\nmy_tibble = tibble(\n  x = 1:3,\n  y = letters[1:3],\n  z = LETTERS[1:3]\n)\nmy_tibble\n\n# A tibble: 3 × 3\n      x y     z    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     A    \n2     2 b     B    \n3     3 c     C    \n\n\n\n\n9.2.2 as_tibble() to convert data.frame or matrix to a tibble\n# convert a dataframe to a tibble\n\ndf = data.frame(\n  numbers = 1:3,\n  letters = c(\"a\", \"b\", \"c\")\n)\n\ntbl = as_tibble(df)\ntbl\n\n# convert a matrix to a tibble\n\nmat = matrix(seq(10,120,by=10), nrow=3,ncol = 4) \ntbl = as_tibble(mat)\ntbl",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#key-differences-from-data.frames",
    "href": "0400050-tibbles-v001.html#key-differences-from-data.frames",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.3 Key Differences from data.frames",
    "text": "9.3 Key Differences from data.frames\nTibbles have several important differences from traditional data.frames:\n\n9.3.1 Default Printing Behavior\n\n# Create a wide tibble with many columns 25 rows.\n\nhealth_data = tibble(\n  patient_id = sprintf(\"P%04d\", 1:25),\n  admit_date = as.Date(\"2024-01-01\") + sample(0:30, 25, replace = TRUE),\n  age = sample(18:95, 25, replace = TRUE),\n  heart_rate = round(rnorm(25, 75, 10)),\n  blood_pressure_sys = round(rnorm(25, 120, 15)),\n  blood_pressure_dia = round(rnorm(25, 80, 10)),\n  temperature = round(rnorm(25, 98.6, 0.5), 1),\n  oxygen_saturation = round(rnorm(25, 98, 2)),\n  cholesterol = round(rnorm(25, 190, 30)),\n  glucose = round(rnorm(25, 100, 20)),\n  weight_kg = round(rnorm(25, 70, 15), 1),\n  height_cm = round(rnorm(25, 170, 10))\n)\n\n# Tibbles show only the first 10 rows by default\n# and only columns that fit on screen\nhealth_data\n\n# A tibble: 25 × 12\n   patient_id admit_date   age heart_rate blood_pressure_sys\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 P0001      2024-01-11    20         77                106\n 2 P0002      2024-01-30    39         79                129\n 3 P0003      2024-01-08    21         67                121\n 4 P0004      2024-01-20    93         80                122\n 5 P0005      2024-01-01    43         57                131\n 6 P0006      2024-01-02    41         64                135\n 7 P0007      2024-01-27    21         76                157\n 8 P0008      2024-01-29    83         75                127\n 9 P0009      2024-01-11    76         80                 88\n10 P0010      2024-01-20    47         69                115\n# ℹ 15 more rows\n# ℹ 7 more variables: blood_pressure_dia &lt;dbl&gt;,\n#   temperature &lt;dbl&gt;, oxygen_saturation &lt;dbl&gt;,\n#   cholesterol &lt;dbl&gt;, glucose &lt;dbl&gt;, weight_kg &lt;dbl&gt;,\n#   height_cm &lt;dbl&gt;\n\n# Compare to data.frame which tries to print everything\nas.data.frame(health_data)\n\n   patient_id admit_date age heart_rate blood_pressure_sys\n1       P0001 2024-01-11  20         77                106\n2       P0002 2024-01-30  39         79                129\n3       P0003 2024-01-08  21         67                121\n4       P0004 2024-01-20  93         80                122\n5       P0005 2024-01-01  43         57                131\n6       P0006 2024-01-02  41         64                135\n7       P0007 2024-01-27  21         76                157\n8       P0008 2024-01-29  83         75                127\n9       P0009 2024-01-11  76         80                 88\n10      P0010 2024-01-20  47         69                115\n11      P0011 2024-01-05  81         64                139\n12      P0012 2024-01-08  65         66                121\n13      P0013 2024-01-03  34         71                119\n14      P0014 2024-01-25  93         66                102\n15      P0015 2024-01-28  20         83                109\n16      P0016 2024-01-24  94         70                106\n17      P0017 2024-01-30  45         74                148\n18      P0018 2024-01-17  21         98                121\n19      P0019 2024-01-14  74         64                117\n20      P0020 2024-01-23  51         71                112\n21      P0021 2024-01-25  46         51                117\n22      P0022 2024-01-24  51         66                136\n23      P0023 2024-01-12  30         68                126\n24      P0024 2024-01-25  52         68                 95\n25      P0025 2024-01-15  91         70                119\n   blood_pressure_dia temperature oxygen_saturation\n1                  83        98.5                98\n2                  86        99.6               100\n3                  74        98.6                98\n4                  80        98.9               100\n5                  98        99.0               100\n6                  76        98.0                97\n7                  63        98.3               102\n8                  84        98.6                95\n9                  88        98.3                97\n10                 96        98.5                99\n11                 92        98.7                97\n12                 98        98.9                99\n13                 84        98.6               101\n14                 91        98.6                98\n15                 92        98.7               101\n16                 80        98.4                99\n17                102        98.3                96\n18                 81        98.7                96\n19                 81        98.9               100\n20                 87        98.1               100\n21                 83        98.6                98\n22                 85        98.4                95\n23                 77        98.3                99\n24                 77        98.6               100\n25                 80        98.3                97\n   cholesterol glucose weight_kg height_cm\n1          142      52      79.0       162\n2          164      92      64.2       184\n3          216     133      81.4       194\n4          195      93      85.0       172\n5          203     102      69.5       171\n6          164     106      58.1       164\n7          268     100      48.2       177\n8          190     116      99.8       178\n9          186      94      85.8       167\n10         218      88      59.3       166\n11         148     113      45.2       164\n12         182     103      70.0       160\n13         227      67      70.7       181\n14         174     106      82.0       159\n15         194      88      77.2       180\n16         100      58     118.0       175\n17         179      64      46.7       170\n18         170      92      56.8       167\n19         231     100      70.5       156\n20         168     109      43.4       171\n21         183      77      73.1       166\n22         230     108      61.0       183\n23         214     116      70.8       158\n24         158      85      65.6       187\n25         227     119      51.6       187",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#printing-more-rowscolumns-of-a-tibble",
    "href": "0400050-tibbles-v001.html#printing-more-rowscolumns-of-a-tibble",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.4 Printing More Rows/Columns of a Tibble",
    "text": "9.4 Printing More Rows/Columns of a Tibble\n\n# By default, print() shows 10 rows. Use n= to show more rows\nprint(health_data, n = 20)  # Shows 20 rows\n\n# A tibble: 25 × 12\n   patient_id admit_date   age heart_rate blood_pressure_sys\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 P0001      2024-01-11    20         77                106\n 2 P0002      2024-01-30    39         79                129\n 3 P0003      2024-01-08    21         67                121\n 4 P0004      2024-01-20    93         80                122\n 5 P0005      2024-01-01    43         57                131\n 6 P0006      2024-01-02    41         64                135\n 7 P0007      2024-01-27    21         76                157\n 8 P0008      2024-01-29    83         75                127\n 9 P0009      2024-01-11    76         80                 88\n10 P0010      2024-01-20    47         69                115\n11 P0011      2024-01-05    81         64                139\n12 P0012      2024-01-08    65         66                121\n13 P0013      2024-01-03    34         71                119\n14 P0014      2024-01-25    93         66                102\n15 P0015      2024-01-28    20         83                109\n16 P0016      2024-01-24    94         70                106\n17 P0017      2024-01-30    45         74                148\n18 P0018      2024-01-17    21         98                121\n19 P0019      2024-01-14    74         64                117\n20 P0020      2024-01-23    51         71                112\n# ℹ 5 more rows\n# ℹ 7 more variables: blood_pressure_dia &lt;dbl&gt;,\n#   temperature &lt;dbl&gt;, oxygen_saturation &lt;dbl&gt;,\n#   cholesterol &lt;dbl&gt;, glucose &lt;dbl&gt;, weight_kg &lt;dbl&gt;,\n#   height_cm &lt;dbl&gt;\n\n# To see all rows\nprint(health_data, n = Inf)\n\n# A tibble: 25 × 12\n   patient_id admit_date   age heart_rate blood_pressure_sys\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 P0001      2024-01-11    20         77                106\n 2 P0002      2024-01-30    39         79                129\n 3 P0003      2024-01-08    21         67                121\n 4 P0004      2024-01-20    93         80                122\n 5 P0005      2024-01-01    43         57                131\n 6 P0006      2024-01-02    41         64                135\n 7 P0007      2024-01-27    21         76                157\n 8 P0008      2024-01-29    83         75                127\n 9 P0009      2024-01-11    76         80                 88\n10 P0010      2024-01-20    47         69                115\n11 P0011      2024-01-05    81         64                139\n12 P0012      2024-01-08    65         66                121\n13 P0013      2024-01-03    34         71                119\n14 P0014      2024-01-25    93         66                102\n15 P0015      2024-01-28    20         83                109\n16 P0016      2024-01-24    94         70                106\n17 P0017      2024-01-30    45         74                148\n18 P0018      2024-01-17    21         98                121\n19 P0019      2024-01-14    74         64                117\n20 P0020      2024-01-23    51         71                112\n21 P0021      2024-01-25    46         51                117\n22 P0022      2024-01-24    51         66                136\n23 P0023      2024-01-12    30         68                126\n24 P0024      2024-01-25    52         68                 95\n25 P0025      2024-01-15    91         70                119\n# ℹ 7 more variables: blood_pressure_dia &lt;dbl&gt;,\n#   temperature &lt;dbl&gt;, oxygen_saturation &lt;dbl&gt;,\n#   cholesterol &lt;dbl&gt;, glucose &lt;dbl&gt;, weight_kg &lt;dbl&gt;,\n#   height_cm &lt;dbl&gt;\n\n\n\n\n9.4.1 Controlling Column Width\n\n# width argument to print specifies the number of characters that should\n# be printed in the widest row. In effect, this limits the number of columns\n# being output to those columns that fit in the specified width.\nprint(health_data, width = 75)\n\n# A tibble: 25 × 12\n   patient_id admit_date   age heart_rate blood_pressure_sys\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n 1 P0001      2024-01-11    20         77                106\n 2 P0002      2024-01-30    39         79                129\n 3 P0003      2024-01-08    21         67                121\n 4 P0004      2024-01-20    93         80                122\n 5 P0005      2024-01-01    43         57                131\n 6 P0006      2024-01-02    41         64                135\n 7 P0007      2024-01-27    21         76                157\n 8 P0008      2024-01-29    83         75                127\n 9 P0009      2024-01-11    76         80                 88\n10 P0010      2024-01-20    47         69                115\n# ℹ 15 more rows\n# ℹ 7 more variables: blood_pressure_dia &lt;dbl&gt;, temperature &lt;dbl&gt;,\n#   oxygen_saturation &lt;dbl&gt;, cholesterol &lt;dbl&gt;, glucose &lt;dbl&gt;,\n#   weight_kg &lt;dbl&gt;, height_cm &lt;dbl&gt;\n\n\n\n# Show all columns by setting width to Inf\nprint(health_data, width = Inf)\n\n# A tibble: 25 × 12\n   patient_id admit_date   age heart_rate blood_pressure_sys blood_pressure_dia temperature oxygen_saturation cholesterol glucose weight_kg height_cm\n   &lt;chr&gt;      &lt;date&gt;     &lt;int&gt;      &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 P0001      2024-01-11    20         77                106                 83        98.5                98         142      52      79         162\n 2 P0002      2024-01-30    39         79                129                 86        99.6               100         164      92      64.2       184\n 3 P0003      2024-01-08    21         67                121                 74        98.6                98         216     133      81.4       194\n 4 P0004      2024-01-20    93         80                122                 80        98.9               100         195      93      85         172\n 5 P0005      2024-01-01    43         57                131                 98        99                 100         203     102      69.5       171\n 6 P0006      2024-01-02    41         64                135                 76        98                  97         164     106      58.1       164\n 7 P0007      2024-01-27    21         76                157                 63        98.3               102         268     100      48.2       177\n 8 P0008      2024-01-29    83         75                127                 84        98.6                95         190     116      99.8       178\n 9 P0009      2024-01-11    76         80                 88                 88        98.3                97         186      94      85.8       167\n10 P0010      2024-01-20    47         69                115                 96        98.5                99         218      88      59.3       166\n# ℹ 15 more rows\n\n\n\n\n\n\n9.4.2 Row Names\n\n# data.frames can have row names\ndf_rownames = data.frame(\n  x = 1:3,\n  y = letters[1:3],\n  row.names = c(\"row1\", \"row2\", \"row3\")\n)\ndf_rownames\n\n     x y\nrow1 1 a\nrow2 2 b\nrow3 3 c\n\n# Tibbles don't support row names\n# If you convert a data.frame with row names to a tibble,\n# the row names become a regular column called 'rowname'\nas_tibble(df_rownames, rownames = \"id\")\n\n# A tibble: 3 × 3\n  id        x y    \n  &lt;chr&gt; &lt;int&gt; &lt;chr&gt;\n1 row1      1 a    \n2 row2      2 b    \n3 row3      3 c",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#creating-tibbles",
    "href": "0400050-tibbles-v001.html#creating-tibbles",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.5 Creating Tibbles",
    "text": "9.5 Creating Tibbles\nYou can create tibbles in several ways:\n\n# Using tibble()\nt1 = tibble(\n  x = 1:5,\n  y = x * 2,  # Note: you can refer to columns created earlier\n  z = letters[1:5]\n)\nt1\n\n# A tibble: 5 × 3\n      x     y z    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     2 a    \n2     2     4 b    \n3     3     6 c    \n4     4     8 d    \n5     5    10 e",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#creating-tibble-row-by-row-using-tribbles",
    "href": "0400050-tibbles-v001.html#creating-tibble-row-by-row-using-tribbles",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.6 creating tibble row by row using tribbles",
    "text": "9.6 creating tibble row by row using tribbles\nWhile reading the raw code for creating a dataframe or a tibble, it can be challenging to visualize what the actual dataframe/tibble will look like. This is because when typing the data into the code, each column is typed horrizontally instead of vertically. For example:\n# Using tribble() for transposed input\n# Useful for small, manual data entry\nstuff = tribble(\n  col1 = c(\"a\",  \"b\",   \"c\"),\n  col2 = c( 1,    2,     3)\n  col3 = c(TRUE, FALSE, TRUE))\n\n# The code above lays out columns horizontally. \n# The actual dataframe displays columns vertically.\nstuff\nA “tribble” (i.e. TRansposed tIBBLE) is just a different way of typing the code that becomes a tibble. Each column heading is prefixed with a tilde (~). The columns can be laid out vertically in the code, making the code much more readable. See the example below.\n# Using tribble() for transposed input\n# Useful for small, manual data entry\nstuff = tribble(\n  ~col1, ~col2, ~col3,\n  \"a\",   1,     TRUE,\n  \"b\",   2,     FALSE,\n  \"c\",   3,     TRUE\n)\n\n# The following looks much more similar to the code that created it.\nstuff",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#converting-between-tibbles-and-data.frames",
    "href": "0400050-tibbles-v001.html#converting-between-tibbles-and-data.frames",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.7 Converting Between Tibbles and data.frames",
    "text": "9.7 Converting Between Tibbles and data.frames\n\n# Convert data.frame to tibble\ndf = data.frame(\n  x = 1:3,\n  y = letters[1:3]\n)\ntbl = as_tibble(df)\n\n# Convert tibble back to data.frame\ndf_again = as.data.frame(tbl)\n\n# Check classes\nclass(tbl)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(df_again)\n\n[1] \"data.frame\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400050-tibbles-v001.html#other-differences-between-tibbles-and-dataframes",
    "href": "0400050-tibbles-v001.html#other-differences-between-tibbles-and-dataframes",
    "title": "9  9.  tibbles and tribbles",
    "section": "9.8 Other differences between tibbles and dataframes",
    "text": "9.8 Other differences between tibbles and dataframes\n\n9.8.1 Variable Names and Subsetting\n\n# data.frames modify non-syntactic names\ndf_names = data.frame(\n  `1` = 1:3,\n  `2+2` = 4:6,\n  check.names = TRUE  # default behavior\n)\nnames(df_names)  # Names are modified\n\n[1] \"X1\"   \"X2.2\"\n\n# Tibbles preserve original names\ntbl_names = tibble(\n  `1` = 1:3,\n  `2+2` = 4:6\n)\nnames(tbl_names)  # Original names kept\n\n[1] \"1\"   \"2+2\"\n\n# Subsetting differences\n# data.frame allows partial matching of variable names\ndf = data.frame(numbers = 1:3, letters = c(\"a\", \"b\", \"c\"))\ndf$num  # Partial matching works\n\n[1] 1 2 3\n\n# Tibbles require exact matching\ntbl = tibble(numbers = 1:3, letters = c(\"a\", \"b\", \"c\"))\ntry(tbl$num)  # This will raise an error\n\nWarning: Unknown or uninitialised column: `num`.\n\n\nNULL",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'><yrChapterNumber>9. </yrChapterNumber> tibbles and tribbles</span>"
    ]
  },
  {
    "objectID": "0400100-usingPipes-v001.html",
    "href": "0400100-usingPipes-v001.html",
    "title": "10  10.  Using pipes, i.e. %>% and |>",
    "section": "",
    "text": "10.1 Intro to the “pipe”, i.e. %&gt;% from the magrittr package\n# The pipe symbol, i.e. %&gt;% that we will be discussing below\n# is part of the magrittr package. You can install/load magrittr directly.\n# Alternatively, magrittr is automatically installed/loaded with most of the\n# tidyverse packages. \nif(!require(magrittr)) {install.packages(\"magrittr\");require(magrittr);}\n\nLoading required package: magrittr\n\n# For more info see:\n#\n# help(package=\"magrittr\")`",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'><yrChapterNumber>10. </yrChapterNumber> Using pipes, i.e. %>% and |></span>"
    ]
  },
  {
    "objectID": "0400100-usingPipes-v001.html#using-the-operator.",
    "href": "0400100-usingPipes-v001.html#using-the-operator.",
    "title": "10  10.  Using pipes, i.e. %>% and |>",
    "section": "10.2 Using the %>% operator.",
    "text": "10.2 Using the %&gt;% operator.\nThe %&gt;% operator is used to send the output of one command into the into a 2nd commmand. Specifically, the info on the left side of the %&gt;% is sent into the first argument of the function on the right hand side of the %&gt;% See the example below.\n\n# The following is code that does NOT use a \"pipe\".\nx = seq(4,6)\nrep(x, 3)\n\n[1] 4 5 6 4 5 6 4 5 6\n\n# The following code uses a \"pipe\", i.e. %&gt;%, to send\n# the info from the left of the %&gt;% into the first argument\n# of the function that appears on the right hand side\n# of the %&gt;%\nseq(4, 6) %&gt;% rep(3)\n\n[1] 4 5 6 4 5 6 4 5 6\n\n# We can extend this further\nseq(4, 6)  %&gt;%  rep(3)  %&gt;%  sum()\n\n[1] 45\n\n# To make it easier to read, we can put each function\n# call on a separate line.\nseq(4, 6) %&gt;%\n  rep(3) %&gt;%\n  sum()\n\n[1] 45\n\n# Notice in the code above that the %&gt;% appears at the end of \n# each line. This is necessary to indicate to R that the command\n# isn't finished yet.\n\n# Without pipes, we could have coded the same thing using the\n# following R code. This still works but the pipes make it easier\n# to read and understand the code.\n\nx = seq(4,6)\ny = rep(x, 3)\nsum(y)\n\n[1] 45",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'><yrChapterNumber>10. </yrChapterNumber> Using pipes, i.e. %>% and |></span>"
    ]
  },
  {
    "objectID": "0400100-usingPipes-v001.html#passing-a-value-to-an-argument-that-is-not-the-1st-argument",
    "href": "0400100-usingPipes-v001.html#passing-a-value-to-an-argument-that-is-not-the-1st-argument",
    "title": "10  10.  Using pipes, i.e. %>% and |>",
    "section": "10.3 Passing a value to an argument that is not the 1st argument",
    "text": "10.3 Passing a value to an argument that is not the 1st argument\nIf you want to send a value to an argument that is not the first, you can use the period as shown in the code below.\n\n# define a vector of numbers\nnums = c(1,3)\n\n# This is without the pipe:\nrep(\"hello\", sum(nums))\n\n[1] \"hello\" \"hello\" \"hello\" \"hello\"\n\n# This is with the pipe. We use the . in the rep function to show\n# where the output of the pipe should go.\n\nsum(nums) %&gt;%\n  rep(\"hello\", .)\n\n[1] \"hello\" \"hello\" \"hello\" \"hello\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'><yrChapterNumber>10. </yrChapterNumber> Using pipes, i.e. %>% and |></span>"
    ]
  },
  {
    "objectID": "0400100-usingPipes-v001.html#you-can-also-use-the-operator",
    "href": "0400100-usingPipes-v001.html#you-can-also-use-the-operator",
    "title": "10  10.  Using pipes, i.e. %>% and |>",
    "section": "10.4 You can also use the |> operator",
    "text": "10.4 You can also use the |&gt; operator\nThe %&gt;% was and is still part of the magrittr package. The pipe was seen as such a useful tool that as of R version 4.1 “base-R” started to include its own version of the pipe. The base-R version of the pipe is |&gt; while the magrittr version of the pipe is %&gt;%\nThe pipe symbol that is now built into R (without needing to load the magrittr package) provides basic pipe functionality. However, if you want extra features (e.g. using the . to show where the data should be placed) then you still need to use the magrittr version of the pipe.\n\n# The following is the new base-R version of the pipe\nseq(4, 6) |&gt;\n  rep(3) |&gt;\n  sum()\n\n[1] 45\n\n# Unfortunately you cannot use |&gt; to send the output to any location\n# other than the 1st argument. For example:\nnums = c(1,3)\n\n# this doesn't work:\nsum(nums) |&gt;        # using the new base-R pipe symbol |&gt;\n  rep(\"hello\", .)   \n\nError: object '.' not found\n\n# but this does\nsum(nums) %&gt;%       # using magrittr pipe symbol %&gt;%\n  rep(\"hello\", .)   \n\n[1] \"hello\" \"hello\" \"hello\" \"hello\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'><yrChapterNumber>10. </yrChapterNumber> Using pipes, i.e. %>% and |></span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html",
    "href": "0400200-stringr-tutorials-v003.html",
    "title": "11  11.  stringr tutorial",
    "section": "",
    "text": "11.1 stringr: Basic String Manipulation\nchr_data &lt;- c(\"Data\", \"Daft\", \"YouTube\", \"channel\",\n             \"learn\", \"and\", \"have\", \"FUN!\")\n# Check the length of a string\nstr_length(\"Hi there! How are you?\")\n\n[1] 22\n\nstr_length(chr_data)\n\n[1] 4 4 7 7 5 3 4 4\n# Convert string letters to uppercase\nstr_to_upper(chr_data)\n\n[1] \"DATA\"    \"DAFT\"    \"YOUTUBE\" \"CHANNEL\" \"LEARN\"   \"AND\"     \"HAVE\"   \n[8] \"FUN!\"\n# Convert string letters to lowercase\nstr_to_lower(chr_data)\n\n[1] \"data\"    \"daft\"    \"youtube\" \"channel\" \"learn\"   \"and\"     \"have\"   \n[8] \"fun!\"\n# Convert string to title (first letter uppercase)\nstr_to_title(chr_data)\n\n[1] \"Data\"    \"Daft\"    \"Youtube\" \"Channel\" \"Learn\"   \"And\"     \"Have\"   \n[8] \"Fun!\"\n# Convert string to sentence (only first letter of first word uppercase)\nstr_to_sentence(\"make me into a SENTENCE!\")\n\n[1] \"Make me into a sentence!\"\n# Trim whitespace\nstr_trim(\"  Trim Me!   \")\n\n[1] \"Trim Me!\"\n# Pad strings with whitespace\nstr_pad(\"Pad Me!\", width = 15, side=\"both\")\n\n[1] \"    Pad Me!    \"\n# Truncate strings to a given length\nstr_trunc(\"If you have a long string, you might want to truncate it!\", \n          width = 50)\n\n[1] \"If you have a long string, you might want to tr...\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-split-and-join-strings",
    "href": "0400200-stringr-tutorials-v003.html#stringr-split-and-join-strings",
    "title": "11  11.  stringr tutorial",
    "section": "11.2 stringr: Split and Join Strings",
    "text": "11.2 stringr: Split and Join Strings\n\n# Split strings\nstr_split(\"Split Me!\", pattern = \" \")\n\n[[1]]\n[1] \"Split\" \"Me!\"  \n\nfood &lt;- c(\n  \"apples and oranges and pears and bananas\",\n  \"pineapples and mangos and guavas\"\n)\n\nstringr::str_split(food, \" and \")\n\n[[1]]\n[1] \"apples\"  \"oranges\" \"pears\"   \"bananas\"\n\n[[2]]\n[1] \"pineapples\" \"mangos\"     \"guavas\"    \n\n\n\n# Join strings (equivalent to base R paste())\nstr_c(\"Join\", \"Me!\", sep=\"_\")\n\n[1] \"Join_Me!\"\n\n# Join strings (equivalent to base R paste())\nstr_c(c(\"Join\", \"vectors\"), c(\"Me!\", \"too!\"), sep=\"_\")\n\n[1] \"Join_Me!\"     \"vectors_too!\"\n\n\n\n# Collapse a vector of strings into a single string\nstr_c(c(\"Turn\", \"me\", \"into\", \"one\", \"string!\"), collapse= \" \")\n\n[1] \"Turn me into one string!\"\n\n\n\n# Convert NA values in character vector to string \"NA\"\nstr_replace_na(c(\"Make\", NA, \"strings!\"))\n\n[1] \"Make\"     \"NA\"       \"strings!\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-sorting-strings",
    "href": "0400200-stringr-tutorials-v003.html#stringr-sorting-strings",
    "title": "11  11.  stringr tutorial",
    "section": "11.3 stringr: Sorting Strings",
    "text": "11.3 stringr: Sorting Strings\n\nsort_data &lt;- c(\"sort\", \"me\", \"please!\")\n\n# Get vector of indicies that would sort a string alphabetically\nstr_order(sort_data)\n\n[1] 2 3 1\n\n\n\n# Use discovered ordering to extract data in sorted order\nsort_data[str_order(sort_data)]\n\n[1] \"me\"      \"please!\" \"sort\"   \n\n\n\n# Directly extract sorted strings\nstr_sort(sort_data)\n\n[1] \"me\"      \"please!\" \"sort\"   \n\n\n\n# Extract in reverse sorted order\nstr_sort(sort_data, decreasing = TRUE)\n\n[1] \"sort\"    \"please!\" \"me\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-string-interpolation",
    "href": "0400200-stringr-tutorials-v003.html#stringr-string-interpolation",
    "title": "11  11.  stringr tutorial",
    "section": "11.4 stringr: String Interpolation",
    "text": "11.4 stringr: String Interpolation\n\nfirst &lt;- c(\"Luke\", \"Han\", \"Jean-Luc\")\nlast &lt;- c(\"Skywalker\", \"Solo\", \"Picard\")\n\n# Interpolate (insert variable values) into strings with str_glue()\nstr_glue(\"My name is {first}. {first} {last}.\")\n\nMy name is Luke. Luke Skywalker.\nMy name is Han. Han Solo.\nMy name is Jean-Luc. Jean-Luc Picard.\n\n\n\nminimum_age &lt;- 18\nover_minimum &lt;- c(5, 17, 33)\n\n# Interpolate the result of an execution into a string\nstr_glue(\"{first} {last} is {minimum_age + over_minimum} years old.\")\n\nLuke Skywalker is 23 years old.\nHan Solo is 35 years old.\nJean-Luc Picard is 51 years old.\n\n\n\nnum &lt;- c(1:5)\n\n# Interpolate the result of function calls\nstr_glue(\"The square root of {num} is {round(sqrt(num), 3)}.\")\n\nThe square root of 1 is 1.\nThe square root of 2 is 1.414.\nThe square root of 3 is 1.732.\nThe square root of 4 is 2.\nThe square root of 5 is 2.236.\n\n\n\nfuel_efficiency &lt;- 30\n\n# Interpolate strings using data from a data frame\nmtcars %&gt;% rownames_to_column(\"Model\") %&gt;%\n         filter(mpg &gt; fuel_efficiency) %&gt;%\n         str_glue_data(\"The {Model} gets {mpg} mpg.\")\n\nThe Fiat 128 gets 32.4 mpg.\nThe Honda Civic gets 30.4 mpg.\nThe Toyota Corolla gets 33.9 mpg.\nThe Lotus Europa gets 30.4 mpg.",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-string-matching",
    "href": "0400200-stringr-tutorials-v003.html#stringr-string-matching",
    "title": "11  11.  stringr tutorial",
    "section": "11.5 stringr: String Matching",
    "text": "11.5 stringr: String Matching\n\nhead(data,8)\n\n       author score\n1  butt_ghost     3\n2 buntaro_pup     1\n3  iidealized     2\n4   [deleted]     1\n5   stathibus     6\n6 soulslicer0     2\n7 swiftsecond     1\n                                                                                                                                                                                                                       body\n1                                                                                      Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.\n2                                                                                                                                                                                                          yep, good point.\n3                                                            Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P\n4                                                                                                                                                                                                                 [deleted]\n5                                                                                                   Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.\n6 This. Such a legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. Read these 3 for starts from the book, then come back and ask the questions\n7                                                                                                                                                                                                   Do you still need help?\n\n\n\n# Detecting the presence of a pattern in strings\nstr_detect(data$body[1:100], pattern=\"deep\")\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE    NA    NA    NA    NA    NA\n [13]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [25]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [37]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [49]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [61]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [73]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [85]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n [97]    NA    NA    NA    NA\n\n\n\n# Get the indicies of matched strings\nstr_inds &lt;- str_which(data$body[1:100], pattern=\"deep\")\nstr_inds\n\ninteger(0)\n\n\n\n# Extract matched strings using detected indicies\ndata$body[str_inds]\n\ncharacter(0)\n\n\n\n# Count the number of matches\nstr_count(data$body[1:100], \"deep\")\n\n  [1]  0  0  0  0  0  0  0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n\n\n\n# Get the position of matches\nstr_locate_all(data$body[1], \"deep\")\n\n[[1]]\n     start end\n\n\n\n# Get a list of the first match found in each string as a vector\nstr_extract(data$body[1:3], \"deep|the|and\")\n\n[1] \"and\" NA    \"and\"\n\n\n\n# Get a list of the first match found in each string as matrix\nstr_match(data$body[1:3], \"deep|the|and\")\n\n     [,1] \n[1,] \"and\"\n[2,] NA   \n[3,] \"and\"\n\n\n\n# Get a list of the all matches found in each string as list of matricies\nstr_match_all(data$body[1:3], \"deep|the|and\")\n\n[[1]]\n     [,1] \n[1,] \"and\"\n[2,] \"and\"\n[3,] \"the\"\n\n[[2]]\n     [,1]\n\n[[3]]\n     [,1] \n[1,] \"and\"\n[2,] \"the\"\n[3,] \"and\"\n[4,] \"the\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-subset-and-replace-strings",
    "href": "0400200-stringr-tutorials-v003.html#stringr-subset-and-replace-strings",
    "title": "11  11.  stringr tutorial",
    "section": "11.6 stringr: Subset and Replace Strings",
    "text": "11.6 stringr: Subset and Replace Strings\n\nhead(data,8)\n\n       author score\n1  butt_ghost     3\n2 buntaro_pup     1\n3  iidealized     2\n4   [deleted]     1\n5   stathibus     6\n6 soulslicer0     2\n7 swiftsecond     1\n                                                                                                                                                                                                                       body\n1                                                                                      Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.\n2                                                                                                                                                                                                          yep, good point.\n3                                                            Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P\n4                                                                                                                                                                                                                 [deleted]\n5                                                                                                   Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.\n6 This. Such a legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. Read these 3 for starts from the book, then come back and ask the questions\n7                                                                                                                                                                                                   Do you still need help?\n\n\n\n# Get a string subset based on character position\nstr_sub(data$body[1], start=1, end=100)\n\n[1] \"Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ev\"\n\n\n\n# Get a string subset based on words\nword(data$body[1], start=1, end=10)\n\n[1] \"Hdf5. It's structured, it's easy to get data in and\"\n\n\n\n# Get the strings that contain a certain pattern\nstr_subset(data$body[1:100], pattern=\"deep\")\n\ncharacter(0)\n\n\n\n# Replace a substring with a new string by substring position\nstr_sub(data$body[1], start=1, end=100) &lt;- str_to_upper(str_sub(data$body[1], \n                                                                start=1, \n                                                                end=100))\nstr_sub(data$body[1], start=1, end=100)\n\n[1] \"HDF5. IT'S STRUCTURED, IT'S EASY TO GET DATA IN AND OUT, AND IT'S FAST. PLUS IT WILL SCALE IF YOU EV\"\n\n\n\n# Replace first occurrence of a substring with a new string by matching\nstr_replace(data$body[1], pattern=\"deep|DEEP\", replacement=\"multi-layer\")\n\n[1] \"HDF5. IT'S STRUCTURED, IT'S EASY TO GET DATA IN AND OUT, AND IT'S FAST. PLUS IT WILL SCALE IF YOU EVer get up there in dataset size.\"\n\n\n\n# Replace all occurrences of a substring with a new string by matching\nstr_replace_all(data$body[1], pattern=\"deep|DEEP\", replacement=\"multi-layer\")\n\n[1] \"HDF5. IT'S STRUCTURED, IT'S EASY TO GET DATA IN AND OUT, AND IT'S FAST. PLUS IT WILL SCALE IF YOU EVer get up there in dataset size.\"",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0400200-stringr-tutorials-v003.html#stringr-viewing-strings",
    "href": "0400200-stringr-tutorials-v003.html#stringr-viewing-strings",
    "title": "11  11.  stringr tutorial",
    "section": "11.7 stringr: Viewing Strings",
    "text": "11.7 stringr: Viewing Strings\n\n# Basic printing\nprint(data$body[1:10])\n\n [1] \"HDF5. IT'S STRUCTURED, IT'S EASY TO GET DATA IN AND OUT, AND IT'S FAST. PLUS IT WILL SCALE IF YOU EVer get up there in dataset size.\"                                                                                     \n [2] \"yep, good point.\"                                                                                                                                                                                                         \n [3] \"Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P\"                                                           \n [4] \"[deleted]\"                                                                                                                                                                                                                \n [5] \"Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.\"                                                                                                  \n [6] \"This. Such a legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. Read these 3 for starts from the book, then come back and ask the questions\"\n [7] \"Do you still need help?\"                                                                                                                                                                                                  \n [8] NA                                                                                                                                                                                                                         \n [9] NA                                                                                                                                                                                                                         \n[10] NA                                                                                                                                                                                                                         \n\n\n\ndeep_learning_posts &lt;- data$body[str_which(data$body, \"deep learning\")]\n\n# View strings in HTML format with the first occurence of a pattern highlighted\nstr_view(deep_learning_posts, pattern=\"deep\")\n\n\n# View strings in HTML format with the first all occurences highlighted\nstr_view_all(deep_learning_posts, pattern=\"deep\")\n\nWarning: `str_view_all()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view()` instead.\n\n\n\n# Format strings into paragraphs of a given width with str_wrap()\nwrapped &lt;- str_wrap(data$body[str_which(data$body, \"deep learning\")][1], \n                    width = 50)\nwrapped \n\n[1] NA\n\n\n\n# Print wrapped string with output obeying newlines\nwrapped %&gt;% cat()\n\nNA\n\n\n\n# Display wrapped paragraph as HTML, inserting paragraph breaks\nstr_wrap(data$body[str_which(data$body, \"deep learning\")][1], width = 50) %&gt;%\nstr_replace_all(\"\\n\", \"&lt;br&gt;\") %&gt;%\nstr_view_all(pattern = \"deep\")\n\n[1] │ NA",
    "crumbs": [
      "Intro to tidyverse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'><yrChapterNumber>11. </yrChapterNumber> stringr tutorial</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html",
    "href": "0000055-workingWithFiles.html",
    "title": "12  12.  Working with files",
    "section": "",
    "text": "12.1 Download files to work with.\nUse this link to download a set of files in a ZIP file for use with these lessons.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#a-directory-means-the-same-thing-as-a-folder",
    "href": "0000055-workingWithFiles.html#a-directory-means-the-same-thing-as-a-folder",
    "title": "12  12.  Working with files",
    "section": "12.2 A “directory” means the same thing as a “folder”",
    "text": "12.2 A “directory” means the same thing as a “folder”\nThe terms “directory” and “folder” mean exactly the same thing - they’re just two different words for places where we store files on a computer. The word “directory” came first, used in early computer systems in the 1960s. It comes from the word’s original meaning of an organized listing or index, like a phone directory. In the 1980s, when computers started using visual interfaces with icons, Apple introduced the word “folder” because it was more intuitive for regular users - it referenced something familiar that people used to organize paper documents. Today, both terms are used interchangeably, with “folder” being more common in everyday use and “directory” often preferred in technical contexts.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#obvious-stuff-folders-and-files",
    "href": "0000055-workingWithFiles.html#obvious-stuff-folders-and-files",
    "title": "12  12.  Working with files",
    "section": "12.3 Obvious stuff … folders and files",
    "text": "12.3 Obvious stuff … folders and files\nAs I’m sure you know\n\nfiles on a computer are stored in folders.\nThose folders can be in other (higher level) folders, etc.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#the-root-folder-i.e.",
    "href": "0000055-workingWithFiles.html#the-root-folder-i.e.",
    "title": "12  12.  Working with files",
    "section": "12.4 The “root” folder, i.e. /",
    "text": "12.4 The “root” folder, i.e. /\nMost folders are contained in other “higher level” folders. The “top” level folder on a computer is known as the “root” folder and is referred to as just “/”. The root folder (i.e. /) is not contained in any other folder.\n\n12.4.1 full path\nA full path of a file (or folder) specifies the EXACT location of the file (or folder) on a computer, i.e. exactly which folder contains the file (or folder).\n(see below for an exmaple)\nA full path always starts with a slash, i.e. /\nIt consists of a list of folder names, starting from the “root” folder, i.e. / and continuing with folder names separated by slashes. The final entry in the path is the folder or filename being identified.\nExample:\nFor example,\n\nThe “full path” /Users/yrosenth/myFiles/personal/work/endOfMonthReport.txt identifies the following file\n\nthe file is named “endOfMonthReport.txt” (the last part of the path)\nthe file is in a folder named “work” (the 2nd to last part of the path).\n\ncontinuing on backwards through the path …\n\nthe work folder is a folder named “personal”\nwhich is in a folder named “myFiles”\nwhich is in a folder named “yrosenth”\nwhich is in a folder named “Users”.\nThe folder “Users” is in the “root” folder (i.e. the “highest” level folder - see above) on the computer.\n\n\n\n\n12.4.2 two different files with the same names\nJust like two people named Sarah Johnson can live at different addresses (one in Chicago, one in New York), you can have two files with the same name in different directories (i.e. folders).\nFor example, the following are the “full paths” for two different files with the same name, “budget.xlsx”. The full path helps you tell the two files apart by showing exactly where each file is located.\n\nFull path 1: /Users/Joe/work/budget.xlsx\n\nThis file is named “budget.xlsx”.\nIt is in the “work” folder,\nwhich is in the folder named “Joe”\n\nwhich is in the folder name “Users”\nwhich is in the “root” (i.e. /) folder (see above)\n\nFull path 2: /Users/Sue/personal/budget.xlsx\n\nThis file is also named “budget.xlsx”.\nIt is in the “personal” folder\nwhich is in the folder named “Sue”\nwhich is in the “Users” folder\nwhich is in the “root” (i.e. /) folder (see above)",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#relative-paths-vs-full-paths",
    "href": "0000055-workingWithFiles.html#relative-paths-vs-full-paths",
    "title": "12  12.  Working with files",
    "section": "12.5 “relative paths” vs “full paths”",
    "text": "12.5 “relative paths” vs “full paths”\nAnother way to describe which folder a file is in is to use what’s known as a “relative path”. We will describe “relative paths” in detail later. For now, we will stick to use “full paths” as described above. However, one thing should be made absolutely clear now:\nDifference between a “full path” and a “relative path”\nA “full path” always starts with a “/”.\nIMPORTANT: MAKE SURE THAT YOU KNOW THIS …\nLater, we will study another type of file path called a relative path. A realtive path is simiilar to a “full path” but a relative path does NOT start with a “/”. We’ll learn more about relative paths later and how they differ from full paths.\n\n12.5.1 Mac and Linux style paths VS Windows style paths\nMac and Linux systems use forward slashes (/) as path separators - i.e.  see the paths above, eg. /home/Joe/work/budget.xlsx\nWindows traditionally uses backslashes (\\) as separators, like this:\n\\\\Users\\\\Joe\\\\work\\\\budget.xlsx\nNote that R understands the Linux/Mac style paths (with forward slashes) even on Windows. It is recommended therefore to always use Linux/Mac style paths in R code so that the code will work on both Linux/Mac and on Windows.\nHowever, since backslashes have a special meaning in R if you need to use Windows-style paths in R, you must use double backslashes (\\), e.g.\ndir_tree(\"\\\\Users\\\\Joe\\\\work\")\n\n\n12.5.2 Windows drive letters\nWindows systems use letters (like C:, D:) at the start of paths to represent different drives or storage devices. The C: drive is typically your main hard drive where Windows is installed. Other letters might be additional hard drives, USB drives, or network drives.\nFor example:\nC:\\Users\\username\\Documents\\file.txt (your main hard drive)\nD:\\Backups\\file.txt (maybe an external drive)\nE:\\Photos\\vacation.jpg (could be a USB drive)\nMac and Linux systems don’t use drive letters. Instead, all hard drives are “mounted” as folders within the single root directory (/). For example, a USB drive might appear at /media/username/USB_DRIVE/. When working in R on Windows, you may need to include the drive letter at the start of full paths, like: “C:/Users/username/Documents/file.txt” (using forward slashes) or \"C:\\Users\\username\\Documents\\file.txt\" (using double backslashes)",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#dir-some_folder-same-as-list.files-some_folder",
    "href": "0000055-workingWithFiles.html#dir-some_folder-same-as-list.files-some_folder",
    "title": "12  12.  Working with files",
    "section": "12.6 dir( SOME_FOLDER ) # same as list.files( SOME_FOLDER )",
    "text": "12.6 dir( SOME_FOLDER ) # same as list.files( SOME_FOLDER )\nThe dir() function displays the names of all files and folders in a particular directory (remember - “directory” and “folder” mean the same thing).\ndir() does exactly the same thing as list.files() - they’re just two names for the same function. Some people prefer dir() because it’s shorter to type. See ?dir\nFor example the following shows the contents of a folder located at the specified full path.\n\ndir(\"/Users/yrosenth/Downloads/myFiles/personal\")\n\n[1] \"school\"           \"shakepereBio.txt\" \"work\"            \n\n\nThe following shows the contents of the school and work folders that are located inside the personal folder shown above.\n\ndir(\"/Users/yrosenth/Downloads/myFiles/personal/school\")\n\n[1] \"datamgmt\" \"english\"  \"finance\" \n\ndir(\"/Users/yrosenth/Downloads/myFiles/personal/work\")\n\n[1] \"bigProject\"       \"endOfMonthReport\"",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#the-working-directory-getwd-setwd-dir",
    "href": "0000055-workingWithFiles.html#the-working-directory-getwd-setwd-dir",
    "title": "12  12.  Working with files",
    "section": "12.7 The “working directory”, getwd(), setwd(), dir()",
    "text": "12.7 The “working directory”, getwd(), setwd(), dir()\nR maintains a “working directory” - i.e. a default directory - that is used when you don’t otherwise specify a path. For example, if you call dir() without a path, the listing shows the files in the “working directory” (see the example below).\nThe getwd() function returns the full path of the “working directory”.\n\ngetwd()\n\n[1] \"C:/Users/yrosenth/Dropbox/_yrQuarto-master/DataManagementUsingRSrc\"\n\n\nThe setwd( SOME_PATH ) - allows you to set the “working directory” to a path of your choice.\n\n# set the working directory\nsetwd(\"/Users/yrosenth/Downloads/myFiles\")\n\n# show that it worked\ngetwd()\n\n[1] \"C:/Users/yrosenth/Downloads/myFiles\"\n\n# Using dir() without a specified directory returns the contents\n# of the working directory. \ndir()\n\n[1] \"bashShellScripts\" \"mixOfFiles\"       \"personal\"        \n\n# Since the working directory is currently \n#\n#    /Users/yrosenth/Downloads/myFiles\n#\n# the above call to \n#\n#    dir() \n# \n# without a path is the same as the following call to\n#\n#    dir(\"/Users/yrosenth/Downloads/myFiles\")\n\ndir(\"/Users/yrosenth/Downloads/myFiles\")\n\n[1] \"bashShellScripts\" \"mixOfFiles\"       \"personal\"        \n\n\n\n12.7.1 Specifying a pattern to dir\nLater we will learn more about “regular expressions”.\n\n\n12.7.2 Summary - different ways of using dir function:\n\n# list everything in the \"working directory\"\ndir()  \n\n  [1] \"_book\"                                                                \n  [2] \"_book-save2\"                                                          \n  [3] \"_book.save\"                                                           \n  [4] \"_inProgressWork\"                                                      \n  [5] \"_quarto-backup.yml\"                                                   \n  [6] \"_quarto-before-fix.yml\"                                               \n  [7] \"_quarto-fixed.yml\"                                                    \n  [8] \"_quarto-save.yml\"                                                     \n  [9] \"_quarto.yml\"                                                          \n [10] \"_quarto.yml.bak\"                                                      \n [11] \"_quarto2.yml\"                                                         \n [12] \"_quarto3.yml\"                                                         \n [13] \"_quartoDiff-fixed.yml\"                                                \n [14] \"_quartoDiff-v002-fixed.yml\"                                           \n [15] \"_quartoDiff-v002.yml\"                                                 \n [16] \"_quartoDiff.yml\"                                                      \n [17] \"00000-InstallingRPackages-v002_files.save\"                            \n [18] \"00000-tmpMermaid.qmd\"                                                 \n [19] \"0000011-WhatIsACli-v010.qmd\"                                          \n [20] \"0000011-WhatIsACli-v010_files-save\"                                   \n [21] \"0000013-HowToRunTheBashShellCli-v011.qmd\"                             \n [22] \"0000013-introToBash-image01.png\"                                      \n [23] \"0000013-introToBash-image02.png\"                                      \n [24] \"0000013-introToBash-image03.png\"                                      \n [25] \"0000013-introToBash-image04.png\"                                      \n [26] \"0000013-introToBash-image05.png\"                                      \n [27] \"0000013-IntroToTheBashCLI-v009.qmd\"                                   \n [28] \"0000017-introToQuarto-v001.html\"                                      \n [29] \"0000017-introToQuarto-v001.qmd\"                                       \n [30] \"0000022-InstallingRPackages-v003.html\"                                \n [31] \"0000022-InstallingRPackages-v003.qmd\"                                 \n [32] \"0000022-InstallingRPackages-v003_files-save\"                          \n [33] \"0000025-introToBash-v007.ipynb\"                                       \n [34] \"0000025-introToBash-v007.qmd\"                                         \n [35] \"0000025-introToBash-v007_files-save\"                                  \n [36] \"0000031-IntroToJson-v001.qmd\"                                         \n [37] \"0000031-IntroToJson-v001_files\"                                       \n [38] \"0000033-IntroToCli-v001.qmd\"                                          \n [39] \"0000051-filenames-v001.qmd\"                                           \n [40] \"0000055-workingWithFiles.qmd\"                                         \n [41] \"0000055-workingWithFiles.rmarkdown\"                                   \n [42] \"0000075-whatIsJsonData.qmd\"                                           \n [43] \"0000075-whatIsJsonData.quarto_ipynb\"                                  \n [44] \"0000080-editingAndValidatingJsonFiles.qmd\"                            \n [45] \"0000085-jsonDataTypes.qmd\"                                            \n [46] \"0000088-nestedDataframes.qmd\"                                         \n [47] \"0000088-nestedDataframes_files\"                                       \n [48] \"0000090-usingJsonInR.qmd\"                                             \n [49] \"0000093-jsonParsingPractice-v001.qmd\"                                 \n [50] \"0000100-topics-v001.qmd\"                                              \n [51] \"000013-IntroToTheBashShellCLI-v008.docx\"                              \n [52] \"000013-IntroToTheBashShellCLI-v008.qmd\"                               \n [53] \"000013-IntroToTheBashShellCLI-v009.qmd\"                               \n [54] \"000013-IntroToTheBashShellCLI-v009_files-save\"                        \n [55] \"0000200-bibliography-v001.qmd\"                                        \n [56] \"000025-introToCliCommands-v003.zip\"                                   \n [57] \"000025-introToCliCommands-v004\"                                       \n [58] \"000025-introToCliCommands-v004.zip\"                                   \n [59] \"000025-introToCliCommands-v005.qmd\"                                   \n [60] \"000025-introToCliCommands-v006.ipynb\"                                 \n [61] \"000025-introToCliCommands-v006.qmd\"                                   \n [62] \"0000300-whereToFindDatasets-v001.qmd\"                                 \n [63] \"0000300-whereTogetDatasets-v001.qmd\"                                  \n [64] \"0000310-whereToFindApis-v001.qmd\"                                     \n [65] \"000130-WebscrapingJavascriptSitesWithRselenium-v002.docx\"             \n [66] \"000200-IntroToJSONDataFormat-v008.docx\"                               \n [67] \"000260-PracticeWithRegulaExpressions-v003.qmd\"                        \n [68] \"000260-PracticeWithRegulaExpressions-v003_files-save\"                 \n [69] \"000420-practiceWithWebScraping-v001.docx\"                             \n [70] \"0100100-stringr-tutorials-v001.ipynb\"                                 \n [71] \"0100100-stringr-tutorials-v001.qmd\"                                   \n [72] \"0100100-stringr-tutorials-v002.ipynb\"                                 \n [73] \"0200200-a-introToRegularExpressions-v036.qmd\"                         \n [74] \"0200200-b-introToRegularExpressions-v036.qmd\"                         \n [75] \"0200200-introToRegularExpressions-v034.qmd\"                           \n [76] \"0200200-introToRegularExpressions-v036.qmd\"                           \n [77] \"0200200-rstudioFolding-v033.qmd\"                                      \n [78] \"0300100-excelAndR-v001.qmd\"                                           \n [79] \"0400050-tibbles-v001.html\"                                            \n [80] \"0400050-tibbles-v001.qmd\"                                             \n [81] \"0400050-whatIsTidyverse-v001.html\"                                    \n [82] \"0400050-whatIsTidyverse-v001.qmd\"                                     \n [83] \"0400100-usingPipes-v001.html\"                                         \n [84] \"0400100-usingPipes-v001.qmd\"                                          \n [85] \"0400200-stringr-tutorials-v003.html\"                                  \n [86] \"0400200-stringr-tutorials-v003.qmd\"                                   \n [87] \"0500100-a-tdyvrsDplyr-v005.qmd\"                                       \n [88] \"0500100-b-tdyvrsDplyr-v005.qmd\"                                       \n [89] \"0500100-tdyvrsDplyr-v0001.qmd\"                                        \n [90] \"0500100-tdyvrsDplyr-v002.qmd\"                                         \n [91] \"0500100-tdyvrsDplyr-v003.html\"                                        \n [92] \"0500100-tdyvrsDplyr-v003.qmd\"                                         \n [93] \"0500100-tdyvrsDplyr-v003_files-save\"                                  \n [94] \"0500100-tdyvrsDplyr-v004.html\"                                        \n [95] \"0500100-tdyvrsDplyr-v004.qmd\"                                         \n [96] \"0500100-tdyvrsDplyr-v004_files-save\"                                  \n [97] \"0500400-sqlSelectIntro-v003.qmd\"                                      \n [98] \"050050-tdyvrsGgplot2-v0001.qmd\"                                       \n [99] \"050050-tdyvrsGgplot2-v0001_files-save\"                                \n[100] \"0600100-introToDb-v001.qmd\"                                           \n[101] \"0990000-part-Questions-v001.qmd\"                                      \n[102] \"0990000-partQuestions-v001_files.save\"                                \n[103] \"0990100-cleaningData-v001.qmd\"                                        \n[104] \"0990100-cleaningData-v001_files.save\"                                 \n[105] \"0999000-appendices-v001.qmd\"                                          \n[106] \"0999000-toDo-v001.qmd\"                                                \n[107] \"83spr23\"                                                              \n[108] \"83spr23-datamgmt-000600-WebScraping\"                                  \n[109] \"ai-000100-overview.html\"                                              \n[110] \"ai-000100-overview.qmd\"                                               \n[111] \"aiCoding00100-overview.html\"                                          \n[112] \"aiCoding00100-overview.qmd\"                                           \n[113] \"appointments_2nf.csv\"                                                 \n[114] \"appointments_unnormalized.csv\"                                        \n[115] \"assignment-api-0000100-weatherDataApi-v001.html\"                      \n[116] \"assignment-api-0000100-weatherDataApi-v001.qmd\"                       \n[117] \"assignment-api-0000100-weatherDataApi-v001.quarto_ipynb\"              \n[118] \"assignment-api-0000100-weatherDataApi-v001_files\"                     \n[119] \"assignment-api-0000100-weatherDataApi-v002.qmd\"                       \n[120] \"authors.csv\"                                                          \n[121] \"authors_2nf.csv\"                                                      \n[122] \"books_2nf.csv\"                                                        \n[123] \"books_unnormalized.csv\"                                               \n[124] \"booksDatabase-AnswersWithDplyr-v001.R\"                                \n[125] \"booksDatabaseDescription-withQuestions-ANSWERS-v021.docx\"             \n[126] \"booksDatabaseDescription-withQuestions-ANSWERS-v022.docx\"             \n[127] \"booksDatabaseDescription-withQuestions-ANSWERS-v023.docx\"             \n[128] \"categories_2nf.csv\"                                                   \n[129] \"cellsize.xlsx\"                                                        \n[130] \"claude-json-parsing-exercises.md\"                                     \n[131] \"copy_quartoYml_to_bookFolder-v001.R\"                                  \n[132] \"courses_2nf.csv\"                                                      \n[133] \"cover.png\"                                                            \n[134] \"createData-famousQuotesDf-v001.R\"                                     \n[135] \"createData-vecAddresses-v001.R\"                                       \n[136] \"createData-vecFruit-v001.R\"                                           \n[137] \"cssSelectorExamples-v001.html\"                                        \n[138] \"data\"                                                                 \n[139] \"data.csv\"                                                             \n[140] \"data.lnk\"                                                             \n[141] \"data2.csv\"                                                            \n[142] \"data3.csv\"                                                            \n[143] \"DataManagementUsingR.Rproj\"                                           \n[144] \"datamgmt\"                                                             \n[145] \"departments_2nf.csv\"                                                  \n[146] \"departments_3nf.csv\"                                                  \n[147] \"doctors_2nf.csv\"                                                      \n[148] \"enrollment_2nf.csv\"                                                   \n[149] \"enrollments_unnormalized.csv\"                                         \n[150] \"files\"                                                                \n[151] \"forecastDfRows-v002.png\"                                              \n[152] \"forecastDfRows.png\"                                                   \n[153] \"github-000700-markdownGithubVsQuarto.qmd\"                             \n[154] \"grades.csv\"                                                           \n[155] \"guests_2nf.csv\"                                                       \n[156] \"hello\"                                                                \n[157] \"htmlExamples\"                                                         \n[158] \"htmlExamples-v001.zip\"                                                \n[159] \"httpdocs-web.config\"                                                  \n[160] \"hw1-v002.R\"                                                           \n[161] \"hw1-v003-separateTitles.R\"                                            \n[162] \"images\"                                                               \n[163] \"index.html\"                                                           \n[164] \"index.qmd\"                                                            \n[165] \"index_files.save\"                                                     \n[166] \"instructors_2nf.csv\"                                                  \n[167] \"instructors_3nf.csv\"                                                  \n[168] \"intro.qmd\"                                                            \n[169] \"intro00100-operatingSystems.html\"                                     \n[170] \"intro00100-operatingSystems.qmd\"                                      \n[171] \"intro00120-files.html\"                                                \n[172] \"intro00120-files.qmd\"                                                 \n[173] \"intro00130-chromeExtensions.html\"                                     \n[174] \"intro00130-chromeExtensions.qmd\"                                      \n[175] \"inventory_items_2nf.csv\"                                              \n[176] \"inventory_unnormalized.csv\"                                           \n[177] \"javascriptExample-v002.html\"                                          \n[178] \"json-0001000-processingJsonInR-v003.qmd\"                              \n[179] \"jsonFiles\"                                                            \n[180] \"jsonStuff\"                                                            \n[181] \"mydb.sqlite\"                                                          \n[182] \"myf\"                                                                  \n[183] \"myFiles\"                                                              \n[184] \"myFiles.zip\"                                                          \n[185] \"nyc_weather\"                                                          \n[186] \"NYTimes-APIs-v001.R\"                                                  \n[187] \"oldfiles\"                                                             \n[188] \"otherSqlWebsites-dataMgmt-spring2024-v001.docx\"                       \n[189] \"otherSqlWebsites-ids1020-spring2024-v001.docx\"                        \n[190] \"patients_2nf.csv\"                                                     \n[191] \"payments_2nf.csv\"                                                     \n[192] \"penguin-plot.png\"                                                     \n[193] \"products_2nf.csv\"                                                     \n[194] \"publishers.csv\"                                                       \n[195] \"publishers_2nf.csv\"                                                   \n[196] \"references.bib\"                                                       \n[197] \"references.qmd\"                                                       \n[198] \"regex-0200300-practice-TMP.qmd\"                                       \n[199] \"regex-0200300-practice-v001.qmd\"                                      \n[200] \"regex-0200300-practice-v002.html\"                                     \n[201] \"regex-0200300-practice-v002.knit.md\"                                  \n[202] \"regex-0200300-practice-v002.qmd\"                                      \n[203] \"regex-0200300-practice-v002_files\"                                    \n[204] \"regex-0200300-practice-v003.html\"                                     \n[205] \"regex-0200300-practice-v003.qmd\"                                      \n[206] \"regex-0200300-practice-v003_files\"                                    \n[207] \"regex-0200300-practice-v004.html\"                                     \n[208] \"regex-0200300-practice-v004.qmd\"                                      \n[209] \"regex-0200300-practice-v004_files\"                                    \n[210] \"regex-0200300-practice.html\"                                          \n[211] \"regex-0200300-practice_files\"                                         \n[212] \"reservations_2nf.csv\"                                                 \n[213] \"reservations_unnormalized.csv\"                                        \n[214] \"rooms_2nf.csv\"                                                        \n[215] \"royalties.csv\"                                                        \n[216] \"salespeople-v002.csv\"                                                 \n[217] \"site_libs\"                                                            \n[218] \"site_libs.save\"                                                       \n[219] \"sql0006-introToDatabases-v023.qmd\"                                    \n[220] \"sql0010-introToSql-v023.qmd\"                                          \n[221] \"sql0014-theBooksDatabase-v001.qmd\"                                    \n[222] \"sql0017-RDataFiles-v001.qmd\"                                          \n[223] \"sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.qmd\"      \n[224] \"sql0035-workingWithMultipleTables-v012.qmd\"                           \n[225] \"sql0040-a-crossJoin-leftJoin-subquery-selfJoins-v023.qmd\"             \n[226] \"sql0040-b-crossJoin-leftJoin-subquery-selfJoins-v023.qmd\"             \n[227] \"sql0040-c-crossJoin-leftJoin-subquery-selfJoins-v023.qmd\"             \n[228] \"sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.qmd\"               \n[229] \"sql0040-d-crossJoin-leftJoin-subquery-selfJoins-v023.qmd\"             \n[230] \"sql0050-commonTableExpressions-v001.qmd\"                              \n[231] \"sql0200-otherSqlCommands-v001.R\"                                      \n[232] \"sql0200-otherSqlCommands-v002.qmd\"                                    \n[233] \"sql0900-sqlWindowFunctions-v001.R\"                                    \n[234] \"sql0900-sqlWindowFunctions-v002.qmd\"                                  \n[235] \"sql0915-workingWithSqliteDatabasesInR-v001.qmd\"                       \n[236] \"sql0935-dbNormalization-v001.qmd\"                                     \n[237] \"sql0950-workingWithDatabasesInR-v009.R\"                               \n[238] \"sql0950-workingWithDatabasesInR-v010.qmd\"                             \n[239] \"sql3000-sqlQuestions-v001-tmpSAVE.qmd\"                                \n[240] \"sql3000-sqlQuestions-v001.qmd\"                                        \n[241] \"sqlbolt_tables-v007-xxxx.RData\"                                       \n[242] \"sqlbolt_tables-v007.RData\"                                            \n[243] \"students_2nf.csv\"                                                     \n[244] \"stuffToDo-withCss-v002.html\"                                          \n[245] \"stuffToDo.html\"                                                       \n[246] \"summary.qmd\"                                                          \n[247] \"suppliers_2nf.csv\"                                                    \n[248] \"The jsonlite Package A Practical and Consistent Mapping-1403.2805.pdf\"\n[249] \"title_authors.csv\"                                                    \n[250] \"titles.csv\"                                                           \n[251] \"tmp\"                                                                  \n[252] \"tmp.qmd\"                                                              \n[253] \"tmp1.txt\"                                                             \n[254] \"tmp2\"                                                                 \n[255] \"tmp2.txt\"                                                             \n[256] \"tmp3\"                                                                 \n[257] \"tmp4\"                                                                 \n[258] \"tmptmp\"                                                               \n[259] \"todo\"                                                                 \n[260] \"Use the following websites to practice CSS selectors.docx\"            \n[261] \"warehouses_2nf.csv\"                                                   \n[262] \"weather-nyc-3-5\"                                                      \n[263] \"web-000130-WebscrapingJavascriptSites-v003.qmd\"                       \n[264] \"web-000300-internetArchitecture-v001.ipynb\"                           \n[265] \"web-000300-internetArchitecture-v001.qmd\"                             \n[266] \"web-000400-webscrapingContinued-011.qmd\"                              \n[267] \"web-000400-webscrapingContinued-015.qmd\"                              \n[268] \"web-000400-webscrapingContinued-015_files-save\"                       \n[269] \"web-000420-practiceWithWebScraping-v001-ANSWERS-v002.qmd\"             \n[270] \"web-000420-practiceWithWebScraping-v001.qmd\"                          \n[271] \"web-current-changed.config\"                                           \n[272] \"web-current.config\"                                                   \n[273] \"web-RSelenium-ScrapingJavascriptEnabledSites-v003.qmd\"                \n[274] \"web.config\"                                                           \n[275] \"web.config-save-old\"                                                  \n[276] \"web.config-suggestionFromChatGPT.config\"                              \n[277] \"yellowUmbrella.png\"                                                   \n[278] \"yrBooksDatabase-common-v001.html\"                                     \n[279] \"yrBooksDatabase-common-v001.qmd\"                                      \n[280] \"yrBooksDatabase-common-v001_files\"                                    \n[281] \"yrBuildQuarto-v001.sh\"                                                \n[282] \"yrChapterNumber-tmp.sh\"                                               \n[283] \"yrChapterNumber-v001.sh\"                                              \n[284] \"yrChapterNumber-v002.sh\"                                              \n[285] \"yrChapterNumber-v003.sh\"                                              \n[286] \"yrChapterNumber-v004.sh\"                                              \n[287] \"yrNocacheHeaders-v001.html\"                                           \n[288] \"yrReadme-DatamanagementUsingRSrc.md\"                                  \n[289] \"yrRenumberChapters-v001.sh\"                                           \n[290] \"yrRenumberChapters-v002.sh\"                                           \n[291] \"yrRenumberChapters-v003.sh\"                                           \n[292] \"yrRenumberChapters-v004.sh\"                                           \n[293] \"yrstuff-v001.js\"                                                      \n[294] \"yrStyles-main-v001.css\"                                               \n[295] \"yrUtil-v001.R\"                                                        \n[296] \"yrZipQmdFiles.R\"                                                      \n\n# list everything in the specified directory\ndir(\"/Users/yrosenth/Downloads/myFiles/work\")  \n\ncharacter(0)\n\n# list only files and folders containing .xlsx in the name\ndir(\"/Users/yrosenth/Downloads/myFiles/work\", pattern = \".xlsx\")  \n\ncharacter(0)\n\n\n\n\n12.7.3 Other R functions also use the “working directory”\nThere are many R functions that work with files or folders. dir() is just one exmple. Some other examples are read.csv(), write.csv(), readLines(), writeLines(), read.fwf(), etc as well as most of the functions in the “fs” package.\nMost (if not all) of these functions allow you to leave out the path information when you call the function. If you don’t specify the path then R uses the “working directory” as the path.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#finding-a-full-path-to-a-specific-file",
    "href": "0000055-workingWithFiles.html#finding-a-full-path-to-a-specific-file",
    "title": "12  12.  Working with files",
    "section": "12.8 Finding a full path to a specific file",
    "text": "12.8 Finding a full path to a specific file\nYou can do this with the built in Windows or Mac tools or by using R functions.\n\n12.8.1 Using “File Explorer” (win) or “Finder” (mac)\nTo find the full path for a file, navigate to it using the “Finder” app on Mac or the “File Explorer” app on Windows. Each of these tools have a way to show you the full path to the file. These features keep changing with different releases of each operating system. Therefore you should search online for how to find the full path of a file using your specific operating system version. The following are some videos that might work for your operating system. If they don’t please search online for specific instructions for your operating system.\n\nWindows 10 - https://www.youtube.com/watch?v=QZUpzuQ0X5I\nMac - https://www.youtube.com/watch?v=Z70-MqAd8oU\n\n\n\n12.8.2 Using the file.choose() or choose.files() R functions\nSee the R documentation for ?file.choose and ?choose.files.\nThese commands open up a window that allows you to choose a file (or files). When you finally select the file (or files) the full path is returned to R.\nIMPORTANT* - The window that opens in response to calling file.choose() or choose.files() can sometimes open up “in back of” RStudio or other programs. This seems to be a bug. If you don’t see the window for choosing the files try minimizing the RStudio window (and possibly other windows) and see if there is a file chooser window in back of RStudio (or other programs).\n\n\n12.8.3 Finding a full path to a “Special folder”\nTo find the full path for a file, navigate to it using the “Finder” app on Mac or the “File Explorer” app on Windows. Each of these tools have a way to show you the full path to the file.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#functions-to-read-and-write-files-in-base-r",
    "href": "0000055-workingWithFiles.html#functions-to-read-and-write-files-in-base-r",
    "title": "12  12.  Working with files",
    "section": "12.9 functions to read and write files in “Base R”",
    "text": "12.9 functions to read and write files in “Base R”\nR has numerous functions that allow reading files and writing to files (i.e. creating or modifying files). The following are built into R “out of the box” (i.e. the “base R”).\nFor help with these functions see the R documentation (i.e. ?functionName)\n\n12.9.1 readLines, writeLines\nThese functions read and write lines of a file into/out-of a character vector.\n\n\n12.9.2 read.csv, write.csv\nread.csv: reads a .csv file into R as a data.frame\nwrite.csv: write a dataframe to a csv file\n\n\n12.9.3 read.fwf\nread a “fixed width file” into a data.frame. A “fixed width file” is different from a CSV file. A CSV file separates the data with commas. A “fixed width file” places each column of data in a “fixed width”. See the example below. Each column of info is typed into a specific number of spaces. If a data value is shorter than the width of the field for it’s column then the rest of the info in the column is filled in with spaces. Note that you need to use a monospace font in your editor to view this type of information in the way it was intended.\nExample: notice how each piece of data is padded with spaces if it doesn’t take up the entire space for the column.\nstudent        year       test1  test2   \njoe            freshman   99     100\nsamantha       senior     100    97\nbartholemeu    sophomore  77     65\nike            senior     82     81",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#fs-package-a-non-core-part-of-the-tidyverse",
    "href": "0000055-workingWithFiles.html#fs-package-a-non-core-part-of-the-tidyverse",
    "title": "12  12.  Working with files",
    "section": "12.10 fs package (a non-core part of “the tidyverse”)",
    "text": "12.10 fs package (a non-core part of “the tidyverse”)\nThe fs package provides modern file system operations in R. It makes working with files and folders more consistent across operating systems (Windows, Mac, Linux) and safer than base R functions. Think of it as a toolkit for handling files - copying, moving, creating directories, checking paths, and viewing directory structures. All functions start with clear prefixes like file_ or dir_ which makes them easy to remember and use.\nInstall the “fs” package with: install.packages(“fs”)\n\n12.10.1 dir_ls() function\nThis works similarly to the dir() funciton in Base R. However, dir_ls() allows you to get much more information about each file, e.g.\n\nsize of the file\nwhen the file was created\nwhen the file was last modified\nother info\n\nSee the R documentation (i.e. ?fs::dir_ls after you’ve already installed the fs package with install.packages(“fs”).\n\n\n12.10.2 dir_tree\ndir_tree() from the fs package displays a directory structure in a visual tree format, similar to how you might see folders indented in a file explorer. It shows the hierarchy of files and folders using lines and indentation. For example, if you run:\nfs::dir_tree(\"my_project\")\nIt might show something like:\nmy_project\n├── data\n│   ├── raw.csv\n│   └── clean.csv\n├── scripts\n│   ├── analysis.R\n│   └── cleaning.R\n└── README.md\nThis makes it easy to visualize how your files and folders are organized. The vertical and horizontal lines show how everything connects, and each level of folders is indented further to the right.\n\n\n12.10.3 Other functions in the fs package\nThe fs package provides modern, user-friendly functions for working with files. Here are some commonly used ones:\nfs::dir_ls() - Lists directory contents (similar to dir() and list.files())\nfs::dir_create() - Creates directories, can create multiple levels at once\nfs::file_copy() - Copies files from one location to another\nfs::file_move() - Moves files from one location to another\nfs::file_delete() - Deletes files\nfs::file_exists() - Checks if files exist",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#readlines-and-writelines-to-readwrite-lines-of-text-files",
    "href": "0000055-workingWithFiles.html#readlines-and-writelines-to-readwrite-lines-of-text-files",
    "title": "12  12.  Working with files",
    "section": "12.11 readLines() and writeLines() to read/write lines of text files",
    "text": "12.11 readLines() and writeLines() to read/write lines of text files\nThese are built into Base R. see the R documentation",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#read_csv-and-write_csv",
    "href": "0000055-workingWithFiles.html#read_csv-and-write_csv",
    "title": "12  12.  Working with files",
    "section": "12.12 read_csv and write_csv",
    "text": "12.12 read_csv and write_csv\nread_csv() and write_csv() from the readr package are the standard functions for working with CSV files in modern R: read_csv() loads a CSV file into R as a data frame:\ndata &lt;- read_csv(\"myfile.csv\")\nwrite_csv() saves a data frame to a CSV file:\nwrite_csv(data, \"output.csv\")\nThese functions are faster and have better defaults than the older read.csv() and write.csv() functions from base R. They automatically handle common issues like string encoding and they don’t add row numbers to your output files.\nYou’ll need the readr package installed and loaded to use them: library(readr) (though readr is automatically loaded if you use the tidyverse).",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#readwrite-excel-files",
    "href": "0000055-workingWithFiles.html#readwrite-excel-files",
    "title": "12  12.  Working with files",
    "section": "12.13 Read/Write Excel files",
    "text": "12.13 Read/Write Excel files\nTo work with Excel files in R, you’ll need either the readxl package (for reading) or the writexl package (for writing): Reading Excel files:\nlibrary(readxl)\ndata &lt;- read_excel(\"myfile.xlsx\")\n\n# For specific sheet: \ndata &lt;- read_excel(\"myfile.xlsx\", sheet = \"Sheet2\")\nWriting Excel files:\nlibrary(writexl)\nwrite_xlsx(data, \"output.xlsx\")\nNote that readxl comes with the tidyverse, but writexl needs to be installed separately. These functions work with both newer .xlsx and older .xls files.",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#reading-fixed-width-field-files",
    "href": "0000055-workingWithFiles.html#reading-fixed-width-field-files",
    "title": "12  12.  Working with files",
    "section": "12.14 Reading fixed width field files",
    "text": "12.14 Reading fixed width field files\nUse read_fwf() from the readr package (part of tidyverse) to read fixed-width files where each column takes up a specific number of characters. You specify the width of each field:\nlibrary(readr)\n\n# Define the widths of each column\ndata &lt;- read_fwf(\"myfile.txt\", \n                 fwf_widths(c(5, 10, 15),  # each number is the width of a column\n                           col_names = c(\"id\", \"name\", \"address\")))\n\n# Or specify exact positions\ndata &lt;- read_fwf(\"myfile.txt\",\n                 fwf_positions(start = c(1, 6, 16),  # where each column starts\n                             end = c(5, 15, 30),     # where each column ends\n                             col_names = c(\"id\", \"name\", \"address\")))\nHere’s a simple example. Imagine you have a file named employees.txt that looks like this:\n12345John Smith           123 Main St          \n98765Mary Johnson         456 Oak Ave          \n54321Bob Wilson          789 Pine Rd\nEach line represents an employee record where:\n\nEmployee ID takes exactly 5 characters\nName takes exactly 20 characters (including spaces)\nAddress takes exactly 20 characters\n\nHere’s how to read it:\nrCopylibrary(readr)\n\nemployees &lt;- read_fwf(\"employees.txt\",\n                     fwf_widths(c(5, 20, 20),  # width of each column\n                               col_names = c(\"id\", \"name\", \"address\")))\nThe fwf_widths(c(5, 20, 20)) tells R:\nRead first 5 characters as ‘id’ Read next 20 characters as ‘name’ Read next 20 characters as ‘address’\nThe resulting data frame would look like this:\n# A tibble:\n#      id  name                address            \n#   &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;              \n# 1 12345  John Smith         123 Main St        \n# 2 98765  Mary Johnson       456 Oak Ave        \n# 3 54321  Bob Wilson         789 Pine Rd",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#capture-output-of-r-commands",
    "href": "0000055-workingWithFiles.html#capture-output-of-r-commands",
    "title": "12  12.  Working with files",
    "section": "12.15 Capture output of R commands",
    "text": "12.15 Capture output of R commands\n\n12.15.1 capture.output(someCommand, file=“someFile.txt”)\nTo save or “capture” the output from an R command, you can use the capture.output() function. It lets you store text output in a file or in a character vector:\n# Save to a file:\ncapture.output(summary(mydata), file = \"output.txt\")\n\n# Save to a variable:\nmy_output &lt;- capture.output(summary(mydata))",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000055-workingWithFiles.html#sinksomefile-sink",
    "href": "0000055-workingWithFiles.html#sinksomefile-sink",
    "title": "12  12.  Working with files",
    "section": "12.16 sink(“someFile”), sink()",
    "text": "12.16 sink(“someFile”), sink()\nThe sink() function is another option - it redirects all output to a file until you turn it off:\nsink(\"output.txt\")\nprint(summary(mydata))\nsink()  # turns off the redirection",
    "crumbs": [
      "Working with files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'><yrChapterNumber>12. </yrChapterNumber> Working with files</span>"
    ]
  },
  {
    "objectID": "0000075-whatIsJsonData.html",
    "href": "0000075-whatIsJsonData.html",
    "title": "13  13.  Intro to JSON data",
    "section": "",
    "text": "13.1 What is JSON?\nJSON is pronounced like the name “Jason” or as “j sahn” - either is Acceptable. JSON is a “text” format for storing and distributing data. The following small example shows information about the USA Postal Code (“i.e. zip code”) 60527. Many more examples appear throughout this document.\nA “text” format means that it is written all in ASCII or Unicode. ASCII and Unicode are standard technologies; every computer system has built in tools for working with ASCII and Unicode data. Therefore, JSON can be read and used on any computer system with a simple text editor without the need to install special software to read it. CSV formats are similar in this respect. However, while CSV is suited specifically to storing information that can be arranged into “rows and columns”, the JSON data format is much more flexible in how the data is arranged.\nJSON was originally developed for the JavaScript language. However, today it has become a popular format that is used in many areas of technology. You can process JSON data with R or any other programming language. The name JSON stands for “JavaScript Object Notation:” - but seriously – you do NOT need to know ANY JavaScript to make use of JSON data. Unfortunately, many resources for learning JSON assume knowledge of JavaScript. That really isn’t necessary if you are using JSON in another language. This document describes JSON for people who DO NOT know JavaScript.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'><yrChapterNumber>13. </yrChapterNumber> Intro to JSON data</span>"
    ]
  },
  {
    "objectID": "0000075-whatIsJsonData.html#what-is-json",
    "href": "0000075-whatIsJsonData.html#what-is-json",
    "title": "13  13.  Intro to JSON data",
    "section": "",
    "text": "{\n    \"post code\": \"60527\",\n    \"country\": \"United States\",\n    \"places\": [{\n            \"place name\": \"Willowbrook\",\n            \"latitude\": \"41.7447\",\n            \"longitude\": \"-87.9334\",\n            \"state\": \"Illinois\"\n        },\n        {\n            \"place name\": \"Hinsdale\",\n            \"latitude\": \"41.7447\",\n            \"longitude\": \"-87.9334\",\n            \"state\": \"Illinois\"\n        }\n    ]\n}",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'><yrChapterNumber>13. </yrChapterNumber> Intro to JSON data</span>"
    ]
  },
  {
    "objectID": "0000075-whatIsJsonData.html#some-examples-of-json-data",
    "href": "0000075-whatIsJsonData.html#some-examples-of-json-data",
    "title": "13  13.  Intro to JSON data",
    "section": "13.2 Some Examples of JSON Data",
    "text": "13.2 Some Examples of JSON Data\nThe full rules of how to create a JSON file are listed later. For now, it may help to see some typical examples of what JSON data looks like. The exact explanations of how to format a JSON file appear below the examples.\n\n13.2.1 EXAMPLE 1 - storing the contents of a company “org chart” in JSON format:\nThe following is an “organization chart” for a company. This is NOT in JSON format. One possible JSON format of the data appears below. The chart shows that Sue is the CEO. Frank and Anne report to Sue. Bob, Lisa and Joe report to Frank. etc.\n         Widgets R Us (founded 2022)\n \n                 ---------------Sue (CEO) ------------\n                 |                                   |\n                 |                                   |\n         --Frank (Dir of Operations)---         -----Anne (CFO)-----\n         |             |              |         |                  |\n         |             |              |         |                  |\n         Bob          Lisa          Joe        Pat (CPA)         Pete\n                       |\n                      Larry\n\nONE POSSIBLE JSON ARRANGEMENT OF THIS DATA IN JSON FORMAT\nNote that the JSON shown below repeats some data (i.e. the employee names are repeated in both the particular section for that employee and in the list of “direct reports” for that employee’s mananger). There are other JSON organizations of this data that would not require repeating any data. However, the goal now is not to debate different alternative JSON representations, but rather to give you a flavor of what JSON looks like in general.\n         {\n             \"company name\": \"Widgets R Us\",\n             \"founded\": 2022,\n             \"employees\": [\n                 {\n                     \"name\": \"Sue\",\n                     \"title\": \"CEO\",\n                     \"directReports\": [\"Frank\", \"Anne\"]\n                 },\n                 {\n                     \"name\": \"Frank\",\n                     \"title\": \"Director of Operations\",\n                     \"directReports\": [\"Bob\",\"Lisa\",\"Joe\"]\n                 },\n                 {\n                     \"name\": \"Anne\",\n                     \"title\": \"CFO\",\n                     \"directReports\": [\"Pat\",\"Pete\"]\n                 },\n                 {\n                     \"name\": \"Bob\"\n                 },\n                 {\n                     \"name\": \"Lisa\",\n                     \"directReports\": [\"Larry\"]\n                 },\n                 {\n                     \"name\": \"Joe\"\n                 },\n                 {\n                     \"name\": \"Larry\"\n                 },\n                 {\n                     \"name\": \"Pat\",\n                     \"title\": \"accountant\",\n                     \"certified\": true\n                 },\n                 {\n                     \"name\": \"Pete\",\n                     \"title\": \"accountant\",\n                     \"certified\": false\n                 }\n             ]\n         }\n\n\n\n13.2.2 EXAMPLE 2 - JSON data - books for sale on a website\nThe following example shows a JSON file that contains information about some books.\n     [\n       {\n         \"hardcover\": false,\n         \"title\": \"How to program in R\",\n         \"copyright\": 2017,\n         \"publisher\": {\n           \"pubName\": \"Books R Us\",\n           \"pubWebsite\": \"https://booksrus.com/\"\n         },\n         \"categories\": [\"R\",\"technology\",\"statistics\"],\n         \"author\" : \n           {\n             \"first\": \"Robert\",\n             \"last\": \"Rosen\"\n           }\n       },\n       {\n         \"hardcover\": false,\n         \"title\": \"Python For Perfectionists\",\n         \"copyright\": 2018,\n         \"categories\": [\"Python\", \"technology\"],\n         \"author\" : \n           {\n             \"title\": \"Dr.\",\n             \"first\": \"Sue\",\n             \"last\": \"Smith\"\n           }\n       },\n       {\n         \"hardcover\": true,\n         \"title\": \"Cooking For Programmers\",\n         \"copyright\": 2018,\n         \"categories\": [\"cooking\"],\n         \"author\" : \n           {\n             \"first\": \"Joe\",\n             \"middle\": \"J\",\n             \"last\": \"Johnson\"\n           }\n       }\n     ]",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'><yrChapterNumber>13. </yrChapterNumber> Intro to JSON data</span>"
    ]
  },
  {
    "objectID": "0000080-editingAndValidatingJsonFiles.html",
    "href": "0000080-editingAndValidatingJsonFiles.html",
    "title": "14  14.  Creating, editing and validating JSON files",
    "section": "",
    "text": "14.1 USING A JSON VALIDATOR TOOL\nAnother way to determine if a JSON file is “valid” is to copy the contents into a JSON validation service such as https://jsonlint.com/. Try doing so with the JSON examples shown above. If you paste one of them into https://jsonlint.com/ and press the “Validate JSON” button, you should see “valid JSON” in the Results section.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'><yrChapterNumber>14. </yrChapterNumber> Creating, editing and validating JSON files</span>"
    ]
  },
  {
    "objectID": "0000080-editingAndValidatingJsonFiles.html#extra-whitespace-in-json-is-ignored-i.e.-spaces-tabs-and-newlines",
    "href": "0000080-editingAndValidatingJsonFiles.html#extra-whitespace-in-json-is-ignored-i.e.-spaces-tabs-and-newlines",
    "title": "14  14.  Creating, editing and validating JSON files",
    "section": "14.2 EXTRA WHITESPACE IN JSON IS IGNORED (i.e. spaces, tabs and newlines)",
    "text": "14.2 EXTRA WHITESPACE IN JSON IS IGNORED (i.e. spaces, tabs and newlines)\nFormatting JSON files with spaces, tabs and newlines to clearly show the structure of the data is highly recommended. However, strictly speaking, whitespace (i.e. spaces, tabs and newlines) that does not appear between quotes do not affect the “validity” of the JSON. For example, the following would also be a valid JSON representation of the data from the last example. Of course, spaces that appear between quotes, such as “Books R Us” are not ignored.\nThe following IS valid JSON (but don’t do this):\n[{\"hardcover\": false,\"title\": \"How to Program in R\"\n,\"copyright\":2017,\"publisher\": {\"pubName\": \"Books R Us\",\"pubWebsite\":\n\"https://booksrus.com/\"},\"categories\":\n[\"R\",\"technology\",\"statistics\"],\"author\" :{\"first\": \"Robert\",\"last\":\n\"Rosen\"}},{\"hardcover\": true,\"title\": \"Python For Perfectionists\",\"copyright\":\n2018,\"categories\": [\"Python\", \"technology\"],\"author\" :{\"title\": \"Dr.\",\"first\":\n\"Sue\",\"last\": \"Smith\"}},{\"hardcover\": true,\"title\": \"Cooking For Programmers\",\"copyright\":\n2018,\"categories\": [\"cooking\"],\"author\" :{\"first\":\n\"Joe\",\"middle\": \"J\",\"last\": \"Johnson\"}}]\nNote that any “quoted information” must all appear on one line. For example, the quoted title “How to Program in R” appears at the very end of the fist line of the JSON shown above. If the title were split between the 1st and 2nd lines, the JSON would NOT be valid.\nRemoving all the unnecessary whitespace (spaces, tabs and newlines) from a JSON file is known as “minifying” the JSON file. Sometimes JSON files are intentionally “minified”. Specifically, this often is done to reduce the amount of data that needs to be transferred when JSON data is sent from one computer to another via an API. For right now, don’t worry about APIs. We will cover APIs very soon.\nMost JSON editors have features for automatically “prettifying” the JSON file so that it is indented properly. For example, in VSCode you can press Shift + Alt + F (Windows) or Shift + Option + F (Mac) to format a JSON document. See this page (or search online for others) for more tips on using VSCode to edit JSON documents: https://linuxpip.org/vscode-format-json/",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'><yrChapterNumber>14. </yrChapterNumber> Creating, editing and validating JSON files</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html",
    "href": "0000085-jsonDataTypes.html",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "",
    "text": "15.1 “PRIMATIVE” (or SIMPLE) JSON DATATYPES (string, number, boolean, null)",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#primative-or-simple-json-datatypes-string-number-boolean-null",
    "href": "0000085-jsonDataTypes.html#primative-or-simple-json-datatypes-string-number-boolean-null",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "",
    "text": "Each individual piece of data in a JSON file must be one of the following data types: string, number, boolean, null. Technically a file is a valid JSON file if it contains just a single number, a single string, a single true or false or a single null. Practically speaking though, most JSON files arrange multiple pieces of data into objects and arrays as described in the next section.\n\nThe simple (or primative) datatypes are:\n \n   - \"string\": string is just another way of saying \"character\" data.\n               Strings in JSON must be enclosed in \"double quotes\" (i.e. quotation marks).\n               You may NOT use 'single quotes' (i.e. apostrophes) to enclose strings.\n               Strings may contain the following \"backslash escape sequences\", i.e.\n               characters preceded by a backslash:\n \n               \\\"  -  Double quote                \\\\  -  Backslash\n               \\/  -  Forward slash               \\b  -  Backspace\n               \\f  -  Form feed                   \\n  -  Newline\n               \\r  -  Carriage return             \\t  -  Tab\n\n               \\u  - followed by four hexadecimal digits indicates the\n                     Unicode character with the specified \"code point\". For\n                     example, \\u05D0 represents a Hebrew letter Aleph. Here are\n                     all codepoints for the Hebrew alphabet:\n                     https://unicodeplus.com/block/0590 (for more info see:\n                     https://developer.mozilla.org/en-US/docs/Glossary/Code_point)\n \n   - number: 123 and 123.45 are both valid numbers. Do not put quotes around\n             numbers. A number with quotes around it is considered a \"string\". Software\n             that processes the JSON will not associate a numeric value with numbers that\n             have \"quotes\" around them.\n\n   - boolean: boolean is just another way of saying \"logical\" data. A boolean\n              value can be either true or false. The words true/false must be lowercase.\n              Do not enclose the true and false in quotes. A true or false with quotes around\n              it is considered a \"string\".\n\n   - null: A value that is null is considered non-existant. For example, the\n           following JSON is used to represent some data about the ACME Shipping\n           Company. Notice that some employees have a middle name and some don't. The\n           ones who don't have a middle name have a value of null for the middle name.\n\n               {\n                 \"company\": \"ACME Shipping Company\",\n                 \"employees\": [ {\"first\": \"Joe\",     \"middle\": null,     \"last\": \"Smith\"},\n                                {\"first\": \"Sheila\",  \"middle\": null,     \"last\": \"Cohen\"},\n                                {\"first\": \"Jack\",    \"middle\": \"Edgar\",  \"last\": \"Jones\"} ]\n               }\n\n           Note that another way to indicate that an employee doesn't have a\n           middle name is just to leave out the middle name for that employee.\n           For example, the following is also valid JSON:\n\n               {\n                 \"company\": \"ACME Shipping Company\",\n                 \"employees\": [ {\"first\": \"Joe\",                          \"last\": \"Smith\"},\n                                {\"first\": \"Sheila\",                       \"last\": \"Cohen\"},\n                                {\"first\": \"Jack\",     \"middle\": \"Edgar\",  \"last\": \"Jones\"} ]\n               }\n\n           The choice of whether to use null or to simply leave out the data is\n           a choice made by the people who design the data format. Either are\n           valid choices. As mentioned above, the exact \"spacing\" of the data does\n           not affect the validity of the JSON file. The spacing is very useful\n           though to make the file easier to read.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#complex-data-types---objects-and-arrays",
    "href": "0000085-jsonDataTypes.html#complex-data-types---objects-and-arrays",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "15.2 COMPLEX DATA TYPES - “OBJECTS” and “ARRAYS”",
    "text": "15.2 COMPLEX DATA TYPES - “OBJECTS” and “ARRAYS”\n“Objects” and “arrays” are two different “complex data types” that allow you to organize multiple data values in different ways.\n\n15.2.1 OBJECTS\nA JSON “object” is surrounded by a single set of {curly braces}. Inside the curly braces are zero or more “name”:value pairs (also known as “key”:value pairs).\nFor example, the following is a valid JSON object:\n                 {\n                     \"recipeName\": \"Midnight Chocolate Cake\",\n                     \"submittedBy\": \"Jack Sep 1, 2022 9:00am\",\n                     \"caloriesPerServing\": 450,\n                     \"dietMenu\": false,\n                     \"prepTime\": \"2 hours 45 minutes\"\n                 }\nNotice a few things about this example:\n\nThe data for the “object” is surrounded by { curly braces }.\nEach line inside the {curly braces} contains a “name”:value pair (AKA “key”:value pair).\nNote that it is NOT required that each “name”:value pair be on a separate line but it is considered good practice to make the files more readable.\nThe “name” in each “name”:value pair (AKA the “key” in each “key”:value pair) is enclosed in quotes.\nIt’s not uncommon to see JSON files that incorrectly leave out the quotes around the names. This is wrong. While these files may sometimes still be able to be processed correctly by software that accounts for this mistake, you should always make sure to include the “quotes” around the “names” in JSON files that you create yourself.\nNote that technically the “name” (AKA “key”) in a name value pair may contain spaces. However, including spaces in names can sometimes cause problems when using software to process a JSON file. Therefore it is recommended not to use spaces in names. The example above uses “camelCase” for the names to clearly identify each word in a multiWord name.\nThe value in each “name”:value pair may be any valid JSON datatype. The different “name”:value pairs in the example above use different data types. For example “Midnight Chocolate Cake” is a string, 450 is a number and false is a boolean value. Note that “45 minutes” is a string and not a number.\nThe “name”:value pairs are separated from each other with commas. Note that there is NO comma after the last “name”:value pair in the object (i.e. no comma after “dietMenu”: false). Extra commas and missing commas are a common source of errors that can be hard to find.\nThe same “name” may not appear twice in a single object. For example, the following is NOT valid!\n{ “thing”: “ball”, “thing”: “cookie” } # THIS IS NOT VALID - DUPLICATE NAMES",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#nested-objects",
    "href": "0000085-jsonDataTypes.html#nested-objects",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "15.3 NESTED OBJECTS",
    "text": "15.3 NESTED OBJECTS\nThe values in “name”:value pairs (AKA “key”:value pairs) may be objects. Below, is a modified version of the recipe example shown above. Notice that in the following version of the JSON, the value for “prepTime” is now itself an object. An object inside an object is known as a “nested object”.\n                 {\n                     \"recipeName\": \"Midnight Chocolate Cake\",\n                     \"submittedBy\": \"Jack Sep 1, 2022 9:00am\",\n                     \"caloriesPerServing\": 450,\n                     \"dietMenu\": false,\n                     \"prepTime\": {\n                         \"hours\": 2,\n                         \"minutes\": 45\n                     }\n                 }\nContinuing with the same example … we added another “name”:value pair whose name is “submittedBy”. The value of “submittedBy” is itself a nested object.\n                {\n                    \"recipeName\": \"Midnight Chocolate Cake\",\n                    \"submittedBy\": {\n                        \"userName\": \"Jack\",\n                        \"when\": {\n                            \"date\": \"Sep 1, 2022\",\n                            \"time\": \"9:00 am\"\n                        }\n                    },\n                    \"caloriesPerServing\": 450,\n                    \"dietMenu\": false,\n                    \"prepTime\": {\n                        \"hours\": 2,\n                        \"minutes\": 45\n                    }\n                }\nContinuing once again with the same example … we can also make the values for “date” and “time” into nested objects:\n                 {\n                     \"recipeName\": \"Midnight Chocolate Cake\",\n                     \"submittedBy\": {\n                         \"userName\": \"Jack\",\n                         \"when\": {\n                             \"date\": {\n                                 \"month\": \"Sep\",\n                                 \"day\":  1, \n                                 \"year\": 2022\n                             },\n                             \"time\": {\n                               \"hour\": 9,\n                               \"minute\": 0,\n                               \"am\": true\n                             }\n                         }\n                     },\n                     \"caloriesPerServing\": 450,\n                     \"dietMenu\": false,\n                     \"prepTime\": {\n                         \"hours\": 2,\n                         \"minutes\": 45\n                     }\n                 }\nObjects may be nested for many levels. Without proper indentation, nested objects (and arrays - see below) can become very difficult to read. Make sure that your JSON objects are indented properly. (Note that the value in a “name”:value pair may also be an array as explained in the next section). ARRAYS\nJSON arrays combine a sequence of values between [square brackets]. The values in an array are separated from each other using commas. There is no comma after the last value. For example, the following is a valid JSON array that contains several strings:\n                 [\"apple\", \"pear\", \"plum\"]\nNote that the values in an array are not “named” as they are in objects (see above). The following JSON array contains numbers:\n                 [3.14159, 2.7182, 1.6180, 186282]\nThere is technically no requirement that all values in a JSON array must be the same datatype (i.e. a “homogeneous array”). JSON does allow for “heterogeneous arrays”, or arrays that contain values of different data types. These are not as common as homogenous arrays, however they are still considered valid JSON. For example, the following is a JSON array that contains values of different data types.\n                 [ \"apple\" , 3.14159 , true , null , false , true, 92 , \"abc\"]",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#nested-arrays",
    "href": "0000085-jsonDataTypes.html#nested-arrays",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "15.4 Nested Arrays",
    "text": "15.4 Nested Arrays\nIn addition to the simple data types (string, number, boolean, null), JSON arrays may also contain nested arrays as well as objects. For example, the following is an array that contains nested arrays.\n   [ [92, 80, 70, 83, 98, 77, 86, 99, 25] , [ 96, 72 , 26, 100, 92] , [ 93, 82, 92, 85] ]\nThe above example contains three different nested arrays. Often such arrays are arranged with each “inner” array (that is contained in the “outer” array) on a different line. For example, the above JSON is equivalent to the following:\n         [\n             [92, 80, 70, 83, 98, 77, 86, 99, 25],\n             [96, 72, 26, 100, 92],\n             [93, 82, 92, 85]\n         ]\n```json\n \n## Arrays that contain objects\n\nArrays may also contain objects. For example, the following shows an array that\ncontains 3 objects. Notice how the indentation is arranged to clearly show the\nstructure of the array and the objects.\n \n```json\n             [\n                 {\n                     \"first\": \"Sue\",\n                     \"last\": \"Smith\"\n                 },\n                 {\n                     \"first\": \"Frank\",\n                     \"last\": \"Jones\"\n                 },\n                 {\n                     \"name\": \"Anne\",\n                     \"title\": \"Applebaum\"\n                 }\n             ]",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#objects-that-contain-arrays",
    "href": "0000085-jsonDataTypes.html#objects-that-contain-arrays",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "15.5 Objects that contain arrays",
    "text": "15.5 Objects that contain arrays\nJust as arrays can contain objects, objects can contain arrays. The value of a “name”:value pair in an object may be an array (as well as an object). For example, the following is a JSON object that describes a company. Below, the array of objects from the last example is used as the value for the “employees” key in the outermost object.\n{\n     \"companyName\": \"Widgets R Us\",\n     \"founded\": 2022,\n     \"employees\":\n             [\n                 {\n                     \"first\": \"Sue\",\n                     \"last\": \"Smith\"\n                 },\n                 {\n                     \"first\": \"Frank\",\n                     \"last\": \"Jones\"\n                 },\n                 {\n                     \"name\": \"Anne\",\n                     \"title\": \"Applebaum\"\n                 }\n             ]\n }",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000085-jsonDataTypes.html#complex-json-files",
    "href": "0000085-jsonDataTypes.html#complex-json-files",
    "title": "15  15.  DATA TYPES IN JSON",
    "section": "15.6 Complex JSON Files",
    "text": "15.6 Complex JSON Files\nVery complex JSON files can be constructed from a deep nesting of arrays that contain objects and objects that contain arrays, etc. etc. etc. The outermost structure in the example above is an object. It contains information about a single compmany. If we want to capture information about many companies, we could create an array of different company objects as shown below.\n [\n     {\n         \"companyName\": \"Widgets R Us\",\n         \"founded\": 2022,\n         \"employees\":\n                 [\n                     {\n                         \"first\": \"Sue\",\n                         \"last\": \"Smith\"\n                     },\n                     {\n                         \"first\": \"Frank\",\n                         \"last\": \"Jones\"\n                     },\n                     {\n                         \"first\": \"Anne\",\n                         \"last\": \"Applebaum\"\n                     }\n                 ]\n     },\n     {\n         \"companyName\": \"Pete's Pizza\",\n         \"founded\": 2020,\n         \"employees\":\n                 [\n                     {\n                         \"first\": \"Pete\",\n                         \"last\": \"Jones\"\n                     },\n                     {\n                         \"first\": \"Paula\",\n                         \"last\": \"Jones\"\n                     }\n     }\n ]",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'><yrChapterNumber>15. </yrChapterNumber> DATA TYPES IN JSON</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html",
    "href": "0000090-usingJsonInR.html",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "",
    "text": "16.1 jsonlite vignettes\nThe jsonlite R pacakge is used to read JSON files into R. The following is the link to the CRAN webpage for the jsonlite package. See the vignettes and reference manual linked there.\nBelow, I copied the links to some of the vignettes from that page. However, it’s best to check the official CRAN page in case there are modifications or if there are new vignettes added there (this page was compiled in Jan 2025).",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html#jsonlite-vignettes",
    "href": "0000090-usingJsonInR.html#jsonlite-vignettes",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "",
    "text": "jsonlite: A Simple and Robust JSON Parser and Generator for R\n\n\n\nIMPORTANT: The following describes how to use the fromJSON and toJSON functions from the jsonlite package. Getting started with JSON and jsonlite\nThe following will be covered later when we discuss APIs:\nFetching JSON data from REST APIs (2022-01-16)\nThe following is nice to know but is something that you can figure out yourself without needing this:\nCombining pages of JSON data with jsonlite (2022-01-16)\nThe following is a more detailed definition of how JSON is converted to R data and how R data is converted to JSON. You should NOT need to look at this but it is nice to know that it is available for reference in case you get stuck.\nA mapping between JSON data and R objects (March 2014)\nThere are some other vignettes on the CRAN page, but you probably don’t need to look at them.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html#summary-of-above",
    "href": "0000090-usingJsonInR.html#summary-of-above",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "16.2 Summary of above",
    "text": "16.2 Summary of above\nThe above vignettes are great for understanding this material. Below I present some of my own material, however, reading the vignettes linked above is just as good (or maybe better) than reading my material.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html#working-with-json-in-r-using-jsonlite",
    "href": "0000090-usingJsonInR.html#working-with-json-in-r-using-jsonlite",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "16.3 Working with JSON in R using jsonlite",
    "text": "16.3 Working with JSON in R using jsonlite\nUsing R to read/write JSON files and process their contents comes down to knowing how to do the following\n\nread/write JSON text to/from R vectors using the baseR functions readLines() and writeLines()\nconvert between JSON text and R objects (lists, dataframes, matrices, arrays, etc) using the fromJSON() and toJSON() functions from the jsonlite package\nknowing how to navigate the R objects that were created from the JSON to retrieve the data you want. This is just standard R knowledge about how to navigate R objects.\n\nThese topics are covered below.\nTo use the toJSON or fromJSON functions we will need to use the jsonlite package.\n\nif(!require(jsonlite)){install.packages(\"jsonlite\");require(jsonelite);}\n\nLoading required package: jsonlite\n\n\nThe jsonlite package in R provides numerous functions for working with JSON. We will discuss the following functions below:\n\ntoJSON(): Convert R objects into a JSON format character vector (You can then write this vector to a file using writeLines() - see above)\nfromJSON(): Convert JSON data into R objects\nprettify() : Format the JSON text on different lines and indent the text in a readable way\nminify() : Convert JSON text to a single line and remove all extra spaces and tabs",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html#tojson---convert-r-objects-to-json-text",
    "href": "0000090-usingJsonInR.html#tojson---convert-r-objects-to-json-text",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "16.4 toJSON() - Convert R objects to JSON text",
    "text": "16.4 toJSON() - Convert R objects to JSON text\nAny R object (e.g. vector, list, dataframe, etc) can be converted to JSON text and written to a JSON file. The steps are:\n\nConvert the R object to JSON text :\njsonText = jsonlite::toJSON( SOME_R_OBJECT )\nWrite the JSON text to a file :\nwriteLines(jsonText, filename)\n\nExample:\n\n# create a dataframe\ndf = data.frame(\n  student = c(\"joe\", \"sue\", \"anne\"),\n  honors = c(FALSE, FALSE, TRUE),\n  test1 = c(NA,80,90),\n  test2 = c(75,85,95)\n)\n\ndf\n\n  student honors test1 test2\n1     joe  FALSE    NA    75\n2     sue  FALSE    80    85\n3    anne   TRUE    90    95\n\n# convert the dataframe to JSON text\njson = toJSON(df, pretty=TRUE)\n\njson\n\n[\n  {\n    \"student\": \"joe\",\n    \"honors\": false,\n    \"test2\": 75\n  },\n  {\n    \"student\": \"sue\",\n    \"honors\": false,\n    \"test1\": 80,\n    \"test2\": 85\n  },\n  {\n    \"student\": \"anne\",\n    \"honors\": true,\n    \"test1\": 90,\n    \"test2\": 95\n  }\n] \n\n# Create a new directory (i.e. folder) to contain the json files.\n# (We create a folder to contain the file. The folder isn't necessary but\n# in general, it's a good idea to organize your files into folders that make sense.)\ndir.create(\"jsonFiles\", \n           showWarnings=FALSE) # don't warn us if the folder already exists\n\n# Create a file that contains the JSON data\nwriteLines(json, \"jsonFiles/sampleJsonFile.json\")\n\n# Use dir() to see that the file has been created.\ndir(\"jsonFiles\")\n\n[1] \"sampleJsonFile.json\"\n\n# We can see the text of the file by reading the lines of \n# the file into a character vector with the BaseR readLines() function.\nlines = readLines(\"jsonFiles/sampleJsonFile.json\")\n\n# A character vector - each entry is one line from the file.\n# This can be very hard to read.\nlines\n\n [1] \"[\"                          \"  {\"                       \n [3] \"    \\\"student\\\": \\\"joe\\\",\"  \"    \\\"honors\\\": false,\"    \n [5] \"    \\\"test2\\\": 75\"          \"  },\"                      \n [7] \"  {\"                        \"    \\\"student\\\": \\\"sue\\\",\" \n [9] \"    \\\"honors\\\": false,\"     \"    \\\"test1\\\": 80,\"        \n[11] \"    \\\"test2\\\": 85\"          \"  },\"                      \n[13] \"  {\"                        \"    \\\"student\\\": \\\"anne\\\",\"\n[15] \"    \\\"honors\\\": true,\"      \"    \\\"test1\\\": 90,\"        \n[17] \"    \\\"test2\\\": 95\"          \"  }\"                       \n[19] \"]\"                         \n\n# It is much easier to read the data with cat\ncat(lines, sep=\"\\n\")\n\n[\n  {\n    \"student\": \"joe\",\n    \"honors\": false,\n    \"test2\": 75\n  },\n  {\n    \"student\": \"sue\",\n    \"honors\": false,\n    \"test1\": 80,\n    \"test2\": 85\n  },\n  {\n    \"student\": \"anne\",\n    \"honors\": true,\n    \"test1\": 90,\n    \"test2\": 95\n  }\n]\n\n\n\n\n\n\n16.4.1 toJSON( … pretty=TRUE …)\nNote that toJSON call above included the argument “pretty=TRUE”. This formats the JSON with easy to read indentation.\nBy contrast, if you leave out “pretty=TRUE” (or if you specify pretty=FALSE) the JSON is created in “minified” format - i.e. all in one line without any extra “whitespace” (no extra spaces, tabs or newline characters), as shown below. The minified version is easy for computers to read and takes up less space. The “pretty” version is appropriate when you want people to read and understand the JSON.\nThere are also standalone functions prettify() and minify() that convert JSON text - see the next section.\n\n\n16.4.2 prettify() , minify()\nThe jsonlite package also provides standalone functions for formatting JSON text.\n\njsonlite::prettify() formats JSON text on different lines and indents the text in a readable way.\njsonlite::minify() removes all extra “whitespace” from JSON text. In other words, it converts JSON text to a single line and removes all extra spaces and tabs.\n\nExamples\n\n# Create a compact JSON string\njson_string &lt;- '{\"name\":\"Alice\",\"age\":25,\"hobbies\":[\"reading\",\"hiking\"]}'\n\n# Make it pretty\nprettify(json_string)\n\n{\n    \"name\": \"Alice\",\n    \"age\": 25,\n    \"hobbies\": [\n        \"reading\",\n        \"hiking\"\n    ]\n}\n \n\n# Make it compact\nminify(prettify(json_string))  # Converting pretty JSON back to compact form\n\n{\"name\":\"Alice\",\"age\":25,\"hobbies\":[\"reading\",\"hiking\"]} \n\n\n\n\n16.4.3 toJSON() - How R objects are converted into JSON\nThe information below reflects the default approach that toJSON() uses in converting R data into JSON. However, toJSON() takes numerous arguments that can be used to adjust exactly what form the resulting JSON takes. For more info see the R help ?toJSON\n\nR vectors are converted into JSON arrays\n\nnums = c(10,20,30)\ntoJSON(nums, pretty=TRUE)\n\n[10, 20, 30] \n\n\nR dataframes are converted into a JSON array of JSON classes. Each class represents a row of the dataframe and contains a “name”:value pair for each value in the row. If a row contains NA in a column, the resulting JSON class will be missing the “name”:value pair that corresponds to that column (for an example of NA, see Joe’s grade for test1 in the example below)\nExample:\n\ndf = data.frame(\n  student = c(\"joe\", \"sue\", \"anne\"),\n  honors = c(FALSE, FALSE, TRUE),\n  test1 = c(NA,80,90),\n  test2 = c(75,85,95)\n)\n\ntoJSON(df, pretty=TRUE)\n\n[\n  {\n    \"student\": \"joe\",\n    \"honors\": false,\n    \"test2\": 75\n  },\n  {\n    \"student\": \"sue\",\n    \"honors\": false,\n    \"test1\": 80,\n    \"test2\": 85\n  },\n  {\n    \"student\": \"anne\",\n    \"honors\": true,\n    \"test1\": 90,\n    \"test2\": 95\n  }\n] \n\n\nR matrices are converted into a JSON array of JSON arrays\nExample\n\nmat = matrix(seq(10,120,by=10), nrow=3, ncol=4)\nmat\n\n     [,1] [,2] [,3] [,4]\n[1,]   10   40   70  100\n[2,]   20   50   80  110\n[3,]   30   60   90  120\n\ntoJSON(mat, pretty=TRUE)\n\n[\n  [10, 40, 70, 100],\n  [20, 50, 80, 110],\n  [30, 60, 90, 120]\n] \n\n\n\n\n\nR lists whose values have names are converted into JSON classes.\nExample\n\n# named lists\n\nnamedList = list( first=\"joe\", middle=NA, last=\"jones\", gpa=3.5, honors=FALSE )\ntoJSON(namedList, pretty=TRUE)\n\n{\n  \"first\": [\"joe\"],\n  \"middle\": [null],\n  \"last\": [\"jones\"],\n  \"gpa\": [3.5],\n  \"honors\": [false]\n}",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000090-usingJsonInR.html#fromjson---convert-json-text-into-r-objects",
    "href": "0000090-usingJsonInR.html#fromjson---convert-json-text-into-r-objects",
    "title": "16  16.  Using JSON in R with jsonlite package",
    "section": "16.5 fromJSON() - Convert JSON text into R objects",
    "text": "16.5 fromJSON() - Convert JSON text into R objects\nOften we start with JSON text - either from a JSON file or JSON data that is returned from a call to a Web API - and we want to convert the JSON into R objects (lists, dataframes, etc) so that we can work with the data in R.\nWe can use the fromJSON() function to convert JSON text into an R object. fromJSON() can accept (a) a filename that contains JSON text (b) a Web API URL that returns JSON text or (c) a character vector that contains JSON text. fromJSON() returns an R object (e.g.  list, dataframe, matrix, etc) that can be manipulated with R code to get at the exact data you want.\nAfter converting the JSON text to an R object, it is helpful to use the str() or View() functions to see the structure of the R object so that you can figure out how to access the specific data you want.\nExample:\nWe use fromJSON() to read the file that we created above from the dataframe and get an R object that contains the data.\n\nx = fromJSON(\"jsonFiles/sampleJsonFile.json\")\n\nx\n\n  student honors test2 test1\n1     joe  FALSE    75    NA\n2     sue  FALSE    85    80\n3    anne   TRUE    95    90\n\n\nIn this case it’s easy to understand that fromJSON() returns a dataframe since we know that we had originally created this JSON from an R dataframe. However, when reading complex JSON files, it can be helpfult to use the str() or View() functions to view the structure of the data so that you can understand how to retrieve the specific data that you want.\n\n\n\n\n\n16.5.1 fromJSON() - The JSON can come from a character vector, a file, or an API\nThe fromJSON() function can accept JSON data from any of the following sources:\n\nFrom a character vector that contains JSON text. The vector might consist only of a single character value that contains the full JSON text\n(see the example above). The vector can also contain the JSON split up into multiple character values (e.g. one line of JSON per character value) The JSON is treated as the concatenation of all the character values. (see this example)\n\n# JSON as a character string\njson_text &lt;- c('{', \n               '  \"name\": \"Alice\",', \n               '  \"age\": 25', \n               '}')\n\nrData = fromJSON(json_text)\n\nrData\n\n$name\n[1] \"Alice\"\n\n$age\n[1] 25\n\n# see the structure of the R object\nstr(rData)\n\nList of 2\n $ name: chr \"Alice\"\n $ age : int 25\n\n\nFrom a JSON file\n\n# Assuming you have a file named 'data.json'\n# fromJSON(\"data.json\")  # Commented out as file doesn't exist\n\nFrom a Web API URL\n\n# Example with a public API\n# fromJSON(\"https://api.example.com/data\")  # Commented out as URL doesn't exist\n\n\n\n\n16.5.2 fromJSON() - How the JSON is converted to an R object.\nThe general rule is as follows:\nThe fromJSON function has a few “simplifyXXX” arguments (see below). The default is that all “simplifyXXX” arguments are set to TRUE. When that is the case then the fromJSON function usually operates in a way that is the opposite to the toJSON function. If toJSON converts a particular type of R object into a particular type of JSON, then fromJSON will convert that type of JSON into the correponding type of R object. See the discussion of the toJSON function above for details.\nThis is usually enough to understand what’s going on. To get specific data from the R object, it helps a lot to understand the structure of the object by using the str() or View() functions.\nThe following goes into more detail, but you really don’t have to study this too deeply. In general, looking at the str() function and navigating through the data using your knowledge of R is usually enough.\nMore details\n\nUsually JSON consists of a JSON class or a JSON array. However, a single number, character value, true/false or null is also valid JSON. In the unusual case that the JSON consists of just one number, character value or logical value then the corresponding R object is just a single R numeric, logical or character value. For exmaple:\n\n# Returns a single numeric value, not a list\nfromJSON('42')\n\n[1] 42\n\n# Returns a single character value, not a list\nfromJSON('\"apple\"')\n\n[1] \"apple\"\n\n# Returns a single TRUE value, not a list\nfromJSON('true')\n\n[1] TRUE\n\n\nSimilarly, if the JSON consists of a single null value then the\nresult is a single NULL value in R.\n\n# Returns a single NULL value, not a list\nfromJSON('null')\n\nNULL\n\n\nIf the JSON consists of [\"a\",\"JSON\",\"array\"] or a {\"JSON\": \"class\"}, then the type of R object that is created depends on the values of the structure of the JSON and on the values of the three “simplify” arguments to the fromJSON function (see the next section)\n\n\n\n16.5.3 fromJSON( … simplifyVector, simplifyDataFrame, simplifyMatrix …)\nThe help for fromJSON() shows the arguments and their default values:\n```\nfromJSON(\n  txt,\n  simplifyVector = TRUE,\n  simplifyDataFrame = simplifyVector,\n  simplifyMatrix = simplifyVector,\n  flatten = FALSE,\n  ...\n)\n```\nThe arguments simplifyVector, simplifyDataFrame and simplifyMatrix control how the R object is created when the JSON consists of a JSON array or a JSON class.\nBy default all values are TRUE. Notice that by default, the value for simplifyDataframe and simplifyMatrix are set to the same value as is supplied for simplifyVector. Therefore to set all the values to FALSE, you can simply pass as single FALSE value.\nIf all of these values are FALSE, the function will returns an R list. For example:\n\n# Returns a list\nx = fromJSON('{\"a\": 1}', FALSE)\nx\n\n$a\n[1] 1\n\nstr(x)\n\nList of 1\n $ a: int 1\n\n# Returns a list\nx = fromJSON('[1, 2, 3]', FALSE)\nx\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\nstr(x)\n\nList of 3\n $ : int 1\n $ : int 2\n $ : int 3\n\n\nWhen the simplifyVector, simplifyDataFrame or simplifyMatrix arguements are TRUE, then the R object being returned might be a vector, dataframe, matrix or list (depending on the structure of the JSON).\n\nfromJSON() - simplifyVector\n\n# JSON array example\njson_array &lt;- '[\"a\", \"b\", \"c\"]'\n\n# Default behavior (simplifyVector = TRUE) - you get an R vector\nfromJSON(json_array)\n\n[1] \"a\" \"b\" \"c\"\n\n# Without simplification (simplifyVector = FALSE) - you get an R LIST\nfromJSON(json_array, simplifyVector = FALSE)\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] \"b\"\n\n[[3]]\n[1] \"c\"\n\n\n\n\nfromJSON() - simplifyDataFrame\nWhen simplifyDataframe is set to TRUE JSON data that consists of an array of classes, is returned as an R dataframe. Each row of the dataframe comes from the data of a single class in the JSON array. This makes perfect sense when all of the classes contain the same names. Each name in the classes becomees a column in the resulting dataframe.\nIf some of the classes contain names that don’t appear in other classes, then there will be NAs in the resulting dataframe in those columns for rows that correspond to classes that didn’t contain that name.\nSee the example:\n\n# A JSON array of JSON classes\njsonText &lt;- '[\n  {\"first\": \"Bob\",   \"middle\": \"Michael\", \"last\": \"jones\", \"age\": 30},\n  {\"first\": \"Alice\", \"last\": \"cohen\",     \"age\": 25},\n  {\"first\": \"Jane\",  \"last\": \"springer\",  \"age\": 25}\n]'\n\n# Default behavior (simplifyDataFrame = TRUE) returns an R dataframe\nx = fromJSON(jsonText)\nx\n\n  first  middle     last age\n1   Bob Michael    jones  30\n2 Alice    &lt;NA&gt;    cohen  25\n3  Jane    &lt;NA&gt; springer  25\n\nstr(x)\n\n'data.frame':   3 obs. of  4 variables:\n $ first : chr  \"Bob\" \"Alice\" \"Jane\"\n $ middle: chr  \"Michael\" NA NA\n $ last  : chr  \"jones\" \"cohen\" \"springer\"\n $ age   : int  30 25 25\n\n# Without simplification (simplifyDataFrame = FALSE)\nx = fromJSON(jsonText, simplifyDataFrame = FALSE)\nx\n\n[[1]]\n[[1]]$first\n[1] \"Bob\"\n\n[[1]]$middle\n[1] \"Michael\"\n\n[[1]]$last\n[1] \"jones\"\n\n[[1]]$age\n[1] 30\n\n\n[[2]]\n[[2]]$first\n[1] \"Alice\"\n\n[[2]]$last\n[1] \"cohen\"\n\n[[2]]$age\n[1] 25\n\n\n[[3]]\n[[3]]$first\n[1] \"Jane\"\n\n[[3]]$last\n[1] \"springer\"\n\n[[3]]$age\n[1] 25\n\nstr(x)\n\nList of 3\n $ :List of 4\n  ..$ first : chr \"Bob\"\n  ..$ middle: chr \"Michael\"\n  ..$ last  : chr \"jones\"\n  ..$ age   : int 30\n $ :List of 3\n  ..$ first: chr \"Alice\"\n  ..$ last : chr \"cohen\"\n  ..$ age  : int 25\n $ :List of 3\n  ..$ first: chr \"Jane\"\n  ..$ last : chr \"springer\"\n  ..$ age  : int 25\n\n\n\n\n\nfromJSON() - simplifyMatrix\n\n# JSON representing matrix-like data\njson_matrix &lt;- '[\n                   [1, 2, 3], \n                   [4, 5, 6]\n                ]'\n\n# Default behavior (simplifyMatrix = TRUE)\nfromJSON(json_matrix)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n# Without simplification (simplifyMatrix = FALSE)\nfromJSON(json_matrix, simplifyMatrix = FALSE)\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 4 5 6\n\n\n\n\n\n16.5.4 Working with Nested Data Frames\nWhen JSON contains nested objects or arrays, fromJSON() can create nested data frames. A typical dataframe contains only simple individual values (e.g. a single number, text value or logical value) in each cell of the dataframe. A nested dataframe can contain an entire “inner” dataframe or list inside the cells of the “outer” dataframe. The data can be accessed via typical R dataframe techniques. However, it’s important to analyze the structure of the data - e.g. using str() or View() - to understand how to access the data you want.\nExample:\n\n# JSON with nested structure\nnested_json &lt;- '[\n  {\n    \"name\": \"Alice\",\n    \"pets\": [\n      {\"type\": \"cat\", \"name\": \"Whiskers\"},\n      {\"type\": \"dog\", \"name\": \"Rex\"}\n    ]\n  },\n  {\n    \"name\": \"Bob\",\n    \"pets\": [\n      {\"type\": \"fish\", \"name\": \"Bubbles\"}\n    ]\n  }\n]'\n\n# Convert to nested data frame\nnested_df &lt;- fromJSON(nested_json)\nnested_df\n\n   name                    pets\n1 Alice cat, dog, Whiskers, Rex\n2   Bob           fish, Bubbles\n\n# When trying to retrieve the data it is VERY important to \n# understand the structure of the R data. \n# You can use the following tools to do so:\n#\n#    str() function in baseR\n#    View()  function in baseR (View with a capital \"V\")\n#    tidyverse::glimpse() function in tidyverse's dplyr package - similar to str()\n#    the variable in the \"Environment\" window pane in RStudio\n\n# See the structure of the data to understand how to extract specific data\nstr(nested_df)\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Alice\" \"Bob\"\n $ pets:List of 2\n  ..$ :'data.frame':    2 obs. of  2 variables:\n  .. ..$ type: chr  \"cat\" \"dog\"\n  .. ..$ name: chr  \"Whiskers\" \"Rex\"\n  ..$ :'data.frame':    1 obs. of  2 variables:\n  .. ..$ type: chr \"fish\"\n  .. ..$ name: chr \"Bubbles\"\n\n# By analyzing the output from str() we can understand how to access \n# specific data that we might want.\n\n# Get the pet owner's names\nnested_df$name\n\n[1] \"Alice\" \"Bob\"  \n\n# Get info for all pets\nnested_df$pets\n\n[[1]]\n  type     name\n1  cat Whiskers\n2  dog      Rex\n\n[[2]]\n  type    name\n1 fish Bubbles\n\n# Get all info for pets of person #1\nnested_df$pets[[1]]\n\n  type     name\n1  cat Whiskers\n2  dog      Rex\n\n# Use str to see the structure of this\nstr(nested_df$pets[[1]])\n\n'data.frame':   2 obs. of  2 variables:\n $ type: chr  \"cat\" \"dog\"\n $ name: chr  \"Whiskers\" \"Rex\"\n\n# See all info for the 1st person's 2nd pet\nnested_df$pets[[1]][2,]\n\n  type name\n2  dog  Rex\n\n# Get all info for pets of person #2\nnested_df$pets[[2]]\n\n  type    name\n1 fish Bubbles\n\n# Get the types of pets for person #1\nnested_df$pets[[1]]$type\n\n[1] \"cat\" \"dog\"\n\n\nIn this example, the nested data frame has a column pets that contains another data frame. Each row in the pets column corresponds to the pets owned by the person in that row of the main data frame.\n\n\n16.5.5 three dimensional (3d) arrays\nSome JSON contains JSON arrays nested inside JSON arrays, nested inside a JSON array. This type of data gets converted into an R three dimensional (3d) array. The following shows an example of how to extract data from such a structure in R. There is a lot of similarity to how this data is extracted from 2d matrices. (NOTE - an R matrix is also known as a 2d array).\n\n# The following JSON gets converted into a 3 dimensinal array in R\n# which is basically an array of matrices\n\nx=fromJSON('[\n              [\n                [1,2,3,4],\n                [5,6,7,8]\n              ],\n              [\n                [9,10,11,12],\n                [13,14,15,16]\n              ],\n              [\n                [17,18,19,20],\n                [21,22,23,24]\n              ]\n            ]')\n\n# 1st matrix\nx[1,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n\n# 2nd matrix\nx[2,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    9   10   11   12\n[2,]   13   14   15   16\n\n# 3rd matrix\nx[3,,]\n\n     [,1] [,2] [,3] [,4]\n[1,]   17   18   19   20\n[2,]   21   22   23   24\n\n# The first index specifies which matrix or matrices\n# The 2nd index specifies which row or rows\n# The 3rd index specifies which column or columns\n\n# all values from 1st row of 3rd matrix\nx[3,1,]\n\n[1] 17 18 19 20\n\n# all values from 2nd column of 1st matrix\nx[1,,2]\n\n[1] 2 6\n\n# The 2nd matrix, 1st row, 3rd column\nx[2,1,3]\n\n[1] 11\n\n# All 3 matrices, just the numbers in the first row and first column\nx[,1,1]\n\n[1]  1  9 17\n\n# The 2nd rows from each matrix\nx[,2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    6    7    8\n[2,]   13   14   15   16\n[3,]   21   22   23   24",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'><yrChapterNumber>16. </yrChapterNumber> Using JSON in R with jsonlite package</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html",
    "href": "0000093-jsonParsingPractice-v001.html",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "",
    "text": "17.1 Exercise: Basic JSON Object Access\nAnswer the questions below by referring to the following JSON response from an API.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-basic-json-object-access",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-basic-json-object-access",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "",
    "text": "Click to see JSON\n\n\n# API response stored in variable 'weather_data'\nweather_data &lt;- fromJSON('{\n  \"location\": \"Seattle\",\n  \"current\": {\n    \"temperature\": 12.5,\n    \"conditions\": \"Cloudy\",\n    \"wind\": {\n      \"speed\": 15.5,\n      \"direction\": \"NW\"\n    }\n  },\n  \"forecast\": [\n    {\"day\": \"Monday\", \"high\": 14, \"low\": 8},\n    {\"day\": \"Tuesday\", \"high\": 16, \"low\": 9},\n    {\"day\": \"Wednesday\", \"high\": 15, \"low\": 7}\n  ]\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(weather_data)\n\nList of 3\n $ location: chr \"Seattle\"\n $ current :List of 3\n  ..$ temperature: num 12.5\n  ..$ conditions : chr \"Cloudy\"\n  ..$ wind       :List of 2\n  .. ..$ speed    : num 15.5\n  .. ..$ direction: chr \"NW\"\n $ forecast:'data.frame':   3 obs. of  3 variables:\n  ..$ day : chr [1:3] \"Monday\" \"Tuesday\" \"Wednesday\"\n  ..$ high: int [1:3] 14 16 15\n  ..$ low : int [1:3] 8 9 7\n\n# THE DATA\nweather_data\n\n$location\n[1] \"Seattle\"\n\n$current\n$current$temperature\n[1] 12.5\n\n$current$conditions\n[1] \"Cloudy\"\n\n$current$wind\n$current$wind$speed\n[1] 15.5\n\n$current$wind$direction\n[1] \"NW\"\n\n\n\n$forecast\n        day high low\n1    Monday   14   8\n2   Tuesday   16   9\n3 Wednesday   15   7\n\n\n\n\n17.1.1 Question (easy): Write R code to extract the current temperature.\n\n\nClick for answer\n\n\nweather_data$current$temperature\n\n[1] 12.5\n\n# or alternatively\nweather_data[[\"current\"]][[\"temperature\"]]\n\n[1] 12.5\n\n\n\n\n\n17.1.2 Question (easy): Write R code to create a vector of all forecasted high temperatures.\n\n\nClick for answer\n\n\nweather_data$forecast$high\n\n[1] 14 16 15\n\n# or\nweather_data$forecast[,\"high\"]\n\n[1] 14 16 15",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-e-commerce-product-data",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-e-commerce-product-data",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "17.2 Exercise: E-commerce Product Data",
    "text": "17.2 Exercise: E-commerce Product Data\nAnswer the questions below by referring to the following JSON response from an e-commerce API.\n\n\nClick to see JSON\n\n\n# API response stored in variable 'product_data'\nproduct_data &lt;- fromJSON('{\n  \"store\": \"TechStore\",\n  \"products\": [\n    {\n      \"id\": \"p123\",\n      \"name\": \"Wireless Headphones\",\n      \"price\": 89.99,\n      \"stock\": 45,\n      \"colors\": [\"black\", \"white\", \"blue\"],\n      \"specs\": {\n        \"battery\": \"20 hours\",\n        \"connectivity\": \"Bluetooth 5.0\",\n        \"weight\": \"250g\"\n      }\n    },\n    {\n      \"id\": \"p124\",\n      \"name\": \"Smart Watch\",\n      \"price\": 199.99,\n      \"stock\": 28,\n      \"colors\": [\"black\", \"silver\"],\n      \"specs\": {\n        \"battery\": \"48 hours\",\n        \"connectivity\": \"Bluetooth 5.0\",\n        \"weight\": \"45g\"\n      }\n    },\n    {\n      \"id\": \"p125\",\n      \"name\": \"Wireless Charger\",\n      \"price\": 29.99,\n      \"stock\": 120,\n      \"colors\": [\"black\", \"white\"],\n      \"specs\": {\n        \"input\": \"QC 3.0\",\n        \"output\": \"15W\",\n        \"weight\": \"80g\"\n      }\n    }\n  ],\n  \"last_updated\": \"2024-02-20\"\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(product_data)\n\nList of 3\n $ store       : chr \"TechStore\"\n $ products    :'data.frame':   3 obs. of  6 variables:\n  ..$ id    : chr [1:3] \"p123\" \"p124\" \"p125\"\n  ..$ name  : chr [1:3] \"Wireless Headphones\" \"Smart Watch\" \"Wireless Charger\"\n  ..$ price : num [1:3] 90 200 30\n  ..$ stock : int [1:3] 45 28 120\n  ..$ colors:List of 3\n  .. ..$ : chr [1:3] \"black\" \"white\" \"blue\"\n  .. ..$ : chr [1:2] \"black\" \"silver\"\n  .. ..$ : chr [1:2] \"black\" \"white\"\n  ..$ specs :'data.frame':  3 obs. of  5 variables:\n  .. ..$ battery     : chr [1:3] \"20 hours\" \"48 hours\" NA\n  .. ..$ connectivity: chr [1:3] \"Bluetooth 5.0\" \"Bluetooth 5.0\" NA\n  .. ..$ weight      : chr [1:3] \"250g\" \"45g\" \"80g\"\n  .. ..$ input       : chr [1:3] NA NA \"QC 3.0\"\n  .. ..$ output      : chr [1:3] NA NA \"15W\"\n $ last_updated: chr \"2024-02-20\"\n\n# THE DATA\nproduct_data\n\n$store\n[1] \"TechStore\"\n\n$products\n    id                name  price stock             colors specs.battery\n1 p123 Wireless Headphones  89.99    45 black, white, blue      20 hours\n2 p124         Smart Watch 199.99    28      black, silver      48 hours\n3 p125    Wireless Charger  29.99   120       black, white          &lt;NA&gt;\n  specs.connectivity specs.weight specs.input specs.output\n1      Bluetooth 5.0         250g        &lt;NA&gt;         &lt;NA&gt;\n2      Bluetooth 5.0          45g        &lt;NA&gt;         &lt;NA&gt;\n3               &lt;NA&gt;          80g      QC 3.0          15W\n\n$last_updated\n[1] \"2024-02-20\"\n\n\n\n\n17.2.1 Question (easy): Write R code to extract the name and price of the second product.\n\n\nClick for answer\n\n\n# Extract name\nproduct_data$products$name[2]\n\n[1] \"Smart Watch\"\n\n# or\nproduct_data$products[2, \"name\"]\n\n[1] \"Smart Watch\"\n\n# Extract price\nproduct_data$products$price[2]\n\n[1] 199.99\n\n# or\nproduct_data$products[2, \"price\"]\n\n[1] 199.99\n\n# Combined as a named vector\nc(name = product_data$products$name[2], price = product_data$products$price[2])\n\n         name         price \n\"Smart Watch\"      \"199.99\" \n\n\n\n\n\n17.2.2 Question (easy): Write R code to extract all available colors for the Wireless Headphones product.\n\n\nClick for answer\n\n\n# Find the row index for the Wireless Headphones\nheadphone_index &lt;- which(product_data$products$name == \"Wireless Headphones\")\n\n# Extract the colors\nproduct_data$products$colors[[headphone_index]]\n\n[1] \"black\" \"white\" \"blue\" \n\n\n\n\n\n17.2.3 Question (easy): Write R code to create a data frame showing each product’s name, price, and stock.\n\n\nClick for answer\n\n\ndata.frame(\n  name = product_data$products$name,\n  price = product_data$products$price,\n  stock = product_data$products$stock\n)\n\n                 name  price stock\n1 Wireless Headphones  89.99    45\n2         Smart Watch 199.99    28\n3    Wireless Charger  29.99   120\n\n# Alternatively\nproduct_data$products[, c(\"name\", \"price\", \"stock\")]\n\n                 name  price stock\n1 Wireless Headphones  89.99    45\n2         Smart Watch 199.99    28\n3    Wireless Charger  29.99   120",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-restaurant-menu-api",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-restaurant-menu-api",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "17.3 Exercise: Restaurant Menu API",
    "text": "17.3 Exercise: Restaurant Menu API\nAnswer the questions below by referring to the following JSON response from a restaurant menu API.\n\n\nClick to see JSON\n\n\n# API response stored in variable 'menu_data'\nmenu_data &lt;- fromJSON('{\n  \"restaurant\": {\n    \"name\": \"Flavor Haven\",\n    \"location\": \"123 Main St\",\n    \"hours\": \"11am-10pm\",\n    \"menu\": {\n      \"appetizers\": [\n        {\"item\": \"Garlic Bread\", \"price\": 5.99, \"vegetarian\": true},\n        {\"item\": \"Buffalo Wings\", \"price\": 9.99, \"vegetarian\": false},\n        {\"item\": \"Mozzarella Sticks\", \"price\": 7.99, \"vegetarian\": true}\n      ],\n      \"main_courses\": [\n        {\"item\": \"Margherita Pizza\", \"price\": 14.99, \"vegetarian\": true, \"sizes\": [\"small\", \"medium\", \"large\"]},\n        {\"item\": \"Beef Burger\", \"price\": 12.99, \"vegetarian\": false, \"sides\": [\"fries\", \"salad\"]},\n        {\"item\": \"Fettuccine Alfredo\", \"price\": 13.99, \"vegetarian\": true, \"options\": [\"add chicken\"]}\n      ],\n      \"desserts\": [\n        {\"item\": \"Chocolate Cake\", \"price\": 6.99, \"vegetarian\": true},\n        {\"item\": \"Ice Cream\", \"price\": 4.99, \"vegetarian\": true, \"flavors\": [\"vanilla\", \"chocolate\", \"strawberry\"]}\n      ]\n    }\n  }\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(menu_data)\n\nList of 1\n $ restaurant:List of 4\n  ..$ name    : chr \"Flavor Haven\"\n  ..$ location: chr \"123 Main St\"\n  ..$ hours   : chr \"11am-10pm\"\n  ..$ menu    :List of 3\n  .. ..$ appetizers  :'data.frame': 3 obs. of  3 variables:\n  .. .. ..$ item      : chr [1:3] \"Garlic Bread\" \"Buffalo Wings\" \"Mozzarella Sticks\"\n  .. .. ..$ price     : num [1:3] 5.99 9.99 7.99\n  .. .. ..$ vegetarian: logi [1:3] TRUE FALSE TRUE\n  .. ..$ main_courses:'data.frame': 3 obs. of  6 variables:\n  .. .. ..$ item      : chr [1:3] \"Margherita Pizza\" \"Beef Burger\" \"Fettuccine Alfredo\"\n  .. .. ..$ price     : num [1:3] 15 13 14\n  .. .. ..$ vegetarian: logi [1:3] TRUE FALSE TRUE\n  .. .. ..$ sizes     :List of 3\n  .. .. .. ..$ : chr [1:3] \"small\" \"medium\" \"large\"\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : NULL\n  .. .. ..$ sides     :List of 3\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : chr [1:2] \"fries\" \"salad\"\n  .. .. .. ..$ : NULL\n  .. .. ..$ options   :List of 3\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : chr \"add chicken\"\n  .. ..$ desserts    :'data.frame': 2 obs. of  4 variables:\n  .. .. ..$ item      : chr [1:2] \"Chocolate Cake\" \"Ice Cream\"\n  .. .. ..$ price     : num [1:2] 6.99 4.99\n  .. .. ..$ vegetarian: logi [1:2] TRUE TRUE\n  .. .. ..$ flavors   :List of 2\n  .. .. .. ..$ : NULL\n  .. .. .. ..$ : chr [1:3] \"vanilla\" \"chocolate\" \"strawberry\"\n\n# THE DATA\nmenu_data\n\n$restaurant\n$restaurant$name\n[1] \"Flavor Haven\"\n\n$restaurant$location\n[1] \"123 Main St\"\n\n$restaurant$hours\n[1] \"11am-10pm\"\n\n$restaurant$menu\n$restaurant$menu$appetizers\n               item price vegetarian\n1      Garlic Bread  5.99       TRUE\n2     Buffalo Wings  9.99      FALSE\n3 Mozzarella Sticks  7.99       TRUE\n\n$restaurant$menu$main_courses\n                item price vegetarian                sizes        sides\n1   Margherita Pizza 14.99       TRUE small, medium, large         NULL\n2        Beef Burger 12.99      FALSE                 NULL fries, salad\n3 Fettuccine Alfredo 13.99       TRUE                 NULL         NULL\n      options\n1        NULL\n2        NULL\n3 add chicken\n\n$restaurant$menu$desserts\n            item price vegetarian                        flavors\n1 Chocolate Cake  6.99       TRUE                           NULL\n2      Ice Cream  4.99       TRUE vanilla, chocolate, strawberry\n\n\n\n\n17.3.1 Question (easy): Write R code to extract the name and location of the restaurant.\n\n\nClick for answer\n\n\n# Extract name\nmenu_data$restaurant$name\n\n[1] \"Flavor Haven\"\n\n# Extract location\nmenu_data$restaurant$location\n\n[1] \"123 Main St\"\n\n# Combined as a named vector\nc(name = menu_data$restaurant$name, location = menu_data$restaurant$location)\n\n          name       location \n\"Flavor Haven\"  \"123 Main St\" \n\n\n\n\n\n17.3.2 Question (easy): Write R code to extract all vegetarian items from the appetizers section.\n\n\nClick for answer\n\n\n# Filter the appetizers data frame for vegetarian items\nvegetarian_apps &lt;- menu_data$restaurant$menu$appetizers[menu_data$restaurant$menu$appetizers$vegetarian == TRUE, ]\n\n# Just the names of vegetarian appetizers\nvegetarian_app_names &lt;- menu_data$restaurant$menu$appetizers$item[menu_data$restaurant$menu$appetizers$vegetarian == TRUE]\n\n# Show the result\nvegetarian_apps\n\n               item price vegetarian\n1      Garlic Bread  5.99       TRUE\n3 Mozzarella Sticks  7.99       TRUE\n\n# or\nvegetarian_app_names\n\n[1] \"Garlic Bread\"      \"Mozzarella Sticks\"\n\n\n\n\n\n17.3.3 Question (easy): Write R code to calculate the average price of all main courses.\n\n\nClick for answer\n\n\nmean(menu_data$restaurant$menu$main_courses$price)\n\n[1] 13.99",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-nested-arrays",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-nested-arrays",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "17.4 Exercise: Nested Arrays",
    "text": "17.4 Exercise: Nested Arrays\nAnswer the questions below by referring to the following JSON response from an API.\n\n\nClick to see JSON\n\n\n# API response stored in variable 'library_data'\nlibrary_data &lt;- fromJSON('{\n  \"library\": {\n    \"name\": \"Central Library\",\n    \"books\": [\n      {\n        \"title\": \"Data Science Basics\",\n        \"authors\": [\"Smith, J.\", \"Jones, K.\"],\n        \"categories\": [\"programming\", \"statistics\"],\n        \"ratings\": [4.5, 4.8, 4.2]\n      },\n      {\n        \"title\": \"Statistics Using R\",\n        \"authors\": [\"Smith, J.\", \"Jones, K.\"],\n        \"categories\": [\"statistics\", \"R\"],\n        \"ratings\": [4.5, 4.8, 4.2]\n      },\n      {\n        \"title\": \"R Programming Guide\",\n        \"authors\": [\"Wilson, M.\"],\n        \"categories\": [\"programming\", \"R\"],\n        \"ratings\": [4.7, 4.6, 4.9, 4.5]\n      }\n    ]\n  }\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(library_data)\n\nList of 1\n $ library:List of 2\n  ..$ name : chr \"Central Library\"\n  ..$ books:'data.frame':   3 obs. of  4 variables:\n  .. ..$ title     : chr [1:3] \"Data Science Basics\" \"Statistics Using R\" \"R Programming Guide\"\n  .. ..$ authors   :List of 3\n  .. .. ..$ : chr [1:2] \"Smith, J.\" \"Jones, K.\"\n  .. .. ..$ : chr [1:2] \"Smith, J.\" \"Jones, K.\"\n  .. .. ..$ : chr \"Wilson, M.\"\n  .. ..$ categories:List of 3\n  .. .. ..$ : chr [1:2] \"programming\" \"statistics\"\n  .. .. ..$ : chr [1:2] \"statistics\" \"R\"\n  .. .. ..$ : chr [1:2] \"programming\" \"R\"\n  .. ..$ ratings   :List of 3\n  .. .. ..$ : num [1:3] 4.5 4.8 4.2\n  .. .. ..$ : num [1:3] 4.5 4.8 4.2\n  .. .. ..$ : num [1:4] 4.7 4.6 4.9 4.5\n\n# THE DATA\nlibrary_data\n\n$library\n$library$name\n[1] \"Central Library\"\n\n$library$books\n                title              authors              categories\n1 Data Science Basics Smith, J., Jones, K. programming, statistics\n2  Statistics Using R Smith, J., Jones, K.           statistics, R\n3 R Programming Guide           Wilson, M.          programming, R\n             ratings\n1      4.5, 4.8, 4.2\n2      4.5, 4.8, 4.2\n3 4.7, 4.6, 4.9, 4.5\n\n\n\n\n17.4.1 Question (medium): Write code to create a vector of all unique book categories\nacross all books.\n\n\nClick for answer\n\n\nunique(unlist(library_data$library$books$categories))\n\n[1] \"programming\" \"statistics\"  \"R\"          \n\n\n\n\n\n17.4.2 Question (harder): get named vector of ratings for each book\nCalculate the average rating for each book and store it in a named vector where names are book titles.\n\n\nClick for answer\n\n\nbookTitles = sapply(library_data$library$books$ratings, mean)\nnames(bookTitles) = library_data$library$books$title\nbookTitles\n\nData Science Basics  Statistics Using R R Programming Guide \n              4.500               4.500               4.675",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-complex-nested-structures",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-complex-nested-structures",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "17.5 Exercise: Complex Nested Structures",
    "text": "17.5 Exercise: Complex Nested Structures\nAnswer the questions below by referring to the following JSON response from a social media API.\nThe API was called with the following URL: https://api.somesite.com/userinfo/Alex Chen\n\n\nClick to see JSON\n\n\n# API response stored in variable 'social_data'\n# The response includes info about a single user \"Alex Chen\"\nsocial_data &lt;- fromJSON('{\n  \"user\": {\n    \"id\": \"u123\",\n    \"name\": \"Alex Chen\",\n    \"posts\": [\n      {\n        \"id\": \"p1\",\n        \"content\": \"Learning R!\",\n        \"timestamp\": \"2024-02-15\",\n        \"comments\": [\n          {\n            \"user\": \"Carol Wu\",\n            \"text\": \"Check out tidyverse\",\n            \"likes\": 5\n          },\n          {\n            \"user\": \"Bob Smith\",\n            \"text\": \"Great choice!\",\n            \"likes\": 3\n          },\n          {\n            \"user\": \"Carol Wu\",\n            \"text\": \"tidyverse makes many things easier\",\n            \"likes\": 2\n          }\n        ]\n      },\n      {\n        \"id\": \"p2\",\n        \"content\": \"JSON parsing is fun\",\n        \"timestamp\": \"2024-02-16\",\n        \"comments\": [\n          {\n            \"user\": \"Mike Jones\",\n            \"text\": \"Try jsonlite\",\n            \"likes\": 4\n          }\n        ]\n      },\n      {\n        \"id\": \"p3\",\n        \"content\": \"Coffee is key :)\",\n        \"timestamp\": \"2024-02-16\",\n        \"comments\": [\n          {\n            \"user\": \"Carol Wu\",\n            \"text\": \"I totally agree!\",\n            \"likes\": 1\n          },\n          {\n            \"user\": \"Mike Jones\",\n            \"text\": \"yessss\",\n            \"likes\": 0\n          }\n        ]\n      }\n\n    ]\n  }\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(social_data)\n\nList of 1\n $ user:List of 3\n  ..$ id   : chr \"u123\"\n  ..$ name : chr \"Alex Chen\"\n  ..$ posts:'data.frame':   3 obs. of  4 variables:\n  .. ..$ id       : chr [1:3] \"p1\" \"p2\" \"p3\"\n  .. ..$ content  : chr [1:3] \"Learning R!\" \"JSON parsing is fun\" \"Coffee is key :)\"\n  .. ..$ timestamp: chr [1:3] \"2024-02-15\" \"2024-02-16\" \"2024-02-16\"\n  .. ..$ comments :List of 3\n  .. .. ..$ :'data.frame':  3 obs. of  3 variables:\n  .. .. .. ..$ user : chr [1:3] \"Carol Wu\" \"Bob Smith\" \"Carol Wu\"\n  .. .. .. ..$ text : chr [1:3] \"Check out tidyverse\" \"Great choice!\" \"tidyverse makes many things easier\"\n  .. .. .. ..$ likes: int [1:3] 5 3 2\n  .. .. ..$ :'data.frame':  1 obs. of  3 variables:\n  .. .. .. ..$ user : chr \"Mike Jones\"\n  .. .. .. ..$ text : chr \"Try jsonlite\"\n  .. .. .. ..$ likes: int 4\n  .. .. ..$ :'data.frame':  2 obs. of  3 variables:\n  .. .. .. ..$ user : chr [1:2] \"Carol Wu\" \"Mike Jones\"\n  .. .. .. ..$ text : chr [1:2] \"I totally agree!\" \"yessss\"\n  .. .. .. ..$ likes: int [1:2] 1 0\n\n# THE DATA\nsocial_data\n\n$user\n$user$id\n[1] \"u123\"\n\n$user$name\n[1] \"Alex Chen\"\n\n$user$posts\n  id             content  timestamp\n1 p1         Learning R! 2024-02-15\n2 p2 JSON parsing is fun 2024-02-16\n3 p3    Coffee is key :) 2024-02-16\n                                                                                                        comments\n1 Carol Wu, Bob Smith, Carol Wu, Check out tidyverse, Great choice!, tidyverse makes many things easier, 5, 3, 2\n2                                                                                    Mike Jones, Try jsonlite, 4\n3                                                           Carol Wu, Mike Jones, I totally agree!, yessss, 1, 0\n\n\n\n\n17.5.1 Question (harder): Create a data frame containing each comment’s user and number of likes.\n\n\nClick for answer\n\n\n# Method 1 - extract vectors separately and combine into a dataframe\nusers = unlist(sapply(social_data$user$posts$comments, function(x) x$user))\nlikes = unlist(sapply(social_data$user$posts$comments, function(x) x$likes))\ndata.frame(user=users, likes=likes)\n\n        user likes\n1   Carol Wu     5\n2  Bob Smith     3\n3   Carol Wu     2\n4 Mike Jones     4\n5   Carol Wu     1\n6 Mike Jones     0\n\n# Method 2: Using do.call to combine list of data frames\ndo.call(rbind, lapply(social_data$user$posts$comments, function(comments) {\n  comments\n}))[, c(\"user\", \"likes\")]\n\n        user likes\n1   Carol Wu     5\n2  Bob Smith     3\n3   Carol Wu     2\n4 Mike Jones     4\n5   Carol Wu     1\n6 Mike Jones     0\n\n# Method 3: Using tidyverse if available\nlibrary(tidyverse)\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nbind_rows(social_data$user$posts$comments)[,c(\"user\",\"likes\")]\n\n        user likes\n1   Carol Wu     5\n2  Bob Smith     3\n3   Carol Wu     2\n4 Mike Jones     4\n5   Carol Wu     1\n6 Mike Jones     0\n\n\n\n\n\n17.5.2 Question (harder): function returning all comments by specific user\nCreate a function that takes the social_data object and a username as parameters, then returns all comments made by that user.\n\n\nClick for answer\n\n\nfind_user_comments &lt;- function(data, username) {\n  \n  # CommentsListDf is a list of dataframes. Each dataframe in the\n  # list contains the columns: user, text, likes\n  commentsListDf = data$user$posts$comments\n  \n  # commentsList is a list of vectors. The vectors contain just the comments for \n  # the specified username. There is one vector in the list from each of\n  # the original dataframes.\n  commentsList = lapply(commentsListDf, function(df){ df[df$user==username, \"text\"]})\n  \n  # Combine all of the vectors in the list into a single vector.\n  return( unlist(commentsList) )\n}\n\n# Test the function\nfind_user_comments(social_data, \"Carol Wu\")\n\n[1] \"Check out tidyverse\"                \"tidyverse makes many things easier\"\n[3] \"I totally agree!\"",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "0000093-jsonParsingPractice-v001.html#exercise-working-with-lists-of-data-frames",
    "href": "0000093-jsonParsingPractice-v001.html#exercise-working-with-lists-of-data-frames",
    "title": "17  17.  JSON Parsing Practice in R",
    "section": "17.6 Exercise: Working with Lists of Data Frames",
    "text": "17.6 Exercise: Working with Lists of Data Frames\nAnswer the questions below by referring to the following JSON response from a fitness tracking API:\n\n\nClick to see JSON\n\n\n# API response stored in variable 'fitness_data'\nfitness_data &lt;- fromJSON('{\n  \"user_id\": \"f789\",\n  \"tracking_data\": {\n    \"daily_steps\": [\n      {\"date\": \"2024-02-14\", \"steps\": 8432, \"active_minutes\": 45},\n      {\"date\": \"2024-02-15\", \"steps\": 10234, \"active_minutes\": 62},\n      {\"date\": \"2024-02-16\", \"steps\": 7321, \"active_minutes\": 38}\n    ],\n    \"workouts\": [\n      {\n        \"type\": \"running\",\n        \"sessions\": [\n          {\"date\": \"2024-02-14\", \"duration\": 30, \"distance\": 5.2},\n          {\"date\": \"2024-02-14\", \"duration\": 25, \"distance\": 3.9},\n          {\"date\": \"2024-02-16\", \"duration\": 25, \"distance\": 4.1}\n        ]\n      },\n      {\n        \"type\": \"cycling\",\n        \"sessions\": [\n          {\"date\": \"2024-02-14\", \"duration\": 30, \"distance\": 9.1},\n          {\"date\": \"2024-02-15\", \"duration\": 45, \"distance\": 15.3}\n        ]\n      }\n    ]\n  }\n}')\n\n\n\n\nClick to see fromJSON() output\n\n\n# THE STRUCTURE\nstr(fitness_data)\n\nList of 2\n $ user_id      : chr \"f789\"\n $ tracking_data:List of 2\n  ..$ daily_steps:'data.frame': 3 obs. of  3 variables:\n  .. ..$ date          : chr [1:3] \"2024-02-14\" \"2024-02-15\" \"2024-02-16\"\n  .. ..$ steps         : int [1:3] 8432 10234 7321\n  .. ..$ active_minutes: int [1:3] 45 62 38\n  ..$ workouts   :'data.frame': 2 obs. of  2 variables:\n  .. ..$ type    : chr [1:2] \"running\" \"cycling\"\n  .. ..$ sessions:List of 2\n  .. .. ..$ :'data.frame':  3 obs. of  3 variables:\n  .. .. .. ..$ date    : chr [1:3] \"2024-02-14\" \"2024-02-14\" \"2024-02-16\"\n  .. .. .. ..$ duration: int [1:3] 30 25 25\n  .. .. .. ..$ distance: num [1:3] 5.2 3.9 4.1\n  .. .. ..$ :'data.frame':  2 obs. of  3 variables:\n  .. .. .. ..$ date    : chr [1:2] \"2024-02-14\" \"2024-02-15\"\n  .. .. .. ..$ duration: int [1:2] 30 45\n  .. .. .. ..$ distance: num [1:2] 9.1 15.3\n\n# THE DATA\nfitness_data\n\n$user_id\n[1] \"f789\"\n\n$tracking_data\n$tracking_data$daily_steps\n        date steps active_minutes\n1 2024-02-14  8432             45\n2 2024-02-15 10234             62\n3 2024-02-16  7321             38\n\n$tracking_data$workouts\n     type                                                      sessions\n1 running 2024-02-14, 2024-02-14, 2024-02-16, 30, 25, 25, 5.2, 3.9, 4.1\n2 cycling                     2024-02-14, 2024-02-15, 30, 45, 9.1, 15.3\n\n\n\n\n17.6.1 Question (harder): Create a data frame with total distance covered for each type of workout.\n\n\nClick for answer\n\n\nworkout_summary &lt;- data.frame(\n  type = fitness_data$tracking_data$workouts$type,\n  total_distance = sapply(fitness_data$tracking_data$workouts$sessions, \n                         function(x) sum(x$distance))\n)\n\nworkout_summary\n\n     type total_distance\n1 running           13.2\n2 cycling           24.4",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'><yrChapterNumber>17. </yrChapterNumber> JSON Parsing Practice in R</span>"
    ]
  },
  {
    "objectID": "assignment-api-0000100-weatherDataApi-v002.html",
    "href": "assignment-api-0000100-weatherDataApi-v002.html",
    "title": "18  18.  assignment - weather API",
    "section": "",
    "text": "18.1 The Weather API\nAll URLs used to call the weather api start with https://api.weather.gov/. This is known as the “base URL” for the API.\nThe following describes how to use the weather API.",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'><yrChapterNumber>18. </yrChapterNumber> assignment - weather API</span>"
    ]
  },
  {
    "objectID": "assignment-api-0000100-weatherDataApi-v002.html#the-weather-api",
    "href": "assignment-api-0000100-weatherDataApi-v002.html#the-weather-api",
    "title": "18  18.  assignment - weather API",
    "section": "",
    "text": "18.1.1 https://api.weather.gov/points/{latitude},{longitude}\nThe first “endpoint” we will use has the following format:\n\nhttps://api.weather.gov/points/{latitude},{longitude}\n\nwhere {latitude} and {longitude} are replaced with specific numeric values for latitude and longitude. For example, the following URL returns data for info about latitude 38.8894 and longitude -77.0352 (make sure to notice the comma “,” that appears between the latitude and longitude numbers in the URL) To make a similar call for other locations, simply replace the values with the appropriate latitude,longitude numbers.\n\nhttps://api.weather.gov/points/38.8894,-77.0352\n\nThis “API call” doesn’t return the actual weather data. Rather, the JSON it returns includes references to other API URLs that can be used to get the actual weather data (see below). Specifically, the URL that we will be interested in is identified in the “forecastHourly” entry in the JSON. Below is an outline of the JSON that is returned from this API call. We omitted most of the data so that we can clearly highlight where the “forecastHourly” URL is in the JSON.\n\n// NOTE - this is a comment. Classic JSON does NOT allow for comments, \n// but JSONC (i.e. JSON with Comments) is recognized by some JSON parsers.\n// A comment in JSONC is introduced with \"//\" (similar to how a comment\n// in R is introduced with \"#\").\n//\n// The actual API does not include comments in the JSON that is returned. \n// I added this comment and the comments in the JSON below in order\n// to highlight just the portion of the JSON I wanted you to focus on.\n{\n  \"@context\":[\n    \n    // SOME STUFF REMOVED FOR BREVITY\n    \n  ],\n  \"id\":\"https://api.weather.gov/points/38.8894,-77.0352\",\n  \"type\":\"Feature\",\n  \"geometry\":{\n    \n    // SOME STUFF REMOVED FOR BREVITY\n    \n  },\n  \"properties\":{\n    \n    // SOME STUFF REMOVED FOR BREVITY\n    \n    // THIS IS THE LINE WE ARE INTERESTED IN\n    \"forecastHourly\":\"https://api.weather.gov/gridpoints/LWX/97,71/forecast/hourly\"\n    \n    // SOME STUFF REMOVED FOR BREVITY\n\n  }\n}\n\n\n18.1.2 getting the hourly forecasts\nThe “forecastHourly” URL is obtained from the API call described in the previous section (see above). That JSON included the following line. It is this URL that we use to get the actual hour by hour weather forecast data.\n\n“forecastHourly”:“https://api.weather.gov/gridpoints/LWX/97,71/forecast/hourly”,\n\nAmong other data, the JSON returned by this URL includes an array of time “periods”. Each period describes the forecast for a single hour of time in the specified location. For this specific URL there are 156 such periods in the data (the exact number of periods may change each time you visit this URL). Below we show an example of the first 2 periods from a specific call to this API. We removed much of the other data from the JSON shown below. (NOTE that the forecast data could potentially change every time you call the API)\n\n{\n  \"@context\":[\n    \n    // SOME STUFF REMOVED FOR BREVITY\n    \n  ],\n  \"type\":\"Feature\",\n  \"geometry\":{\n    \n    // SOME STUFF REMOVED FOR BREVITY\n    \n  },\n  \"properties\":{\n    \"units\":\"us\",\n    \"forecastGenerator\":\"HourlyForecastGenerator\",\n    \"generatedAt\":\"2025-02-12T17:34:21+00:00\",\n    \"updateTime\":\"2025-02-12T17:30:41+00:00\",\n    \"validTimes\":\"2025-02-12T11:00:00+00:00/P7DT14H\",\n    \"elevation\":{\n    \n      // SOME STUFF REMOVED FOR BREVITY\n    \n     },\n    \"periods\":[\n      {\n        \"number\":1,\n        \"name\":\"\",\n        \"startTime\":\"2025-02-12T12:00:00-05:00\",\n        \"endTime\":\"2025-02-12T13:00:00-05:00\",\n        \"isDaytime\":true,\n        \"temperature\":34,\n        \"temperatureUnit\":\"F\",\n        \"temperatureTrend\":\"\",\n        \"probabilityOfPrecipitation\":{\n          \"unitCode\":\"wmoUnit:percent\",\n          \"value\":12\n        },\n        \"dewpoint\":{\n          \"unitCode\":\"wmoUnit:degC\",\n          \"value\":-0.5555555555555556\n        },\n        \"relativeHumidity\":{\n          \"unitCode\":\"wmoUnit:percent\",\n          \"value\":89\n        },\n        \"windSpeed\":\"6 mph\",\n        \"windDirection\":\"E\",\n        \"icon\":\"https://api.weather.gov/icons/land/day/ovc?size=small\",\n        \"shortForecast\":\"Cloudy\",\n        \"detailedForecast\":\"\"\n      },\n      {\n        \"number\":2,\n        \"name\":\"\",\n        \"startTime\":\"2025-02-12T13:00:00-05:00\",\n        \"endTime\":\"2025-02-12T14:00:00-05:00\",\n        \"isDaytime\":true,\n        \"temperature\":34,\n        \"temperatureUnit\":\"F\",\n        \"temperatureTrend\":\"\",\n        \"probabilityOfPrecipitation\":{\n          \"unitCode\":\"wmoUnit:percent\",\n          \"value\":13\n        },\n        \"dewpoint\":{\n          \"unitCode\":\"wmoUnit:degC\",\n          \"value\":-0.5555555555555556\n        },\n        \"relativeHumidity\":{\n          \"unitCode\":\"wmoUnit:percent\",\n          \"value\":89\n        },\n        \"windSpeed\":\"6 mph\",\n        \"windDirection\":\"E\",\n        \"icon\":\"https://api.weather.gov/icons/land/day/ovc?size=small\",\n        \"shortForecast\":\"Cloudy\",\n        \"detailedForecast\":\"\"\n      },\n      \n      // THE FIRST 2 PERIODS ARE SHOWN ABOVE.\n      // THE API CALL RETURNED 156 PERIODS. \n      // THE REMAINDER OF THE PERIODS WERE REMOVED FOR BREVITY.\n      \n    ]\n  }\n}\n\n\n18.1.3 More info about the weather API\nThe info above should cover all that you need for this assignment. However, if you want to know more about the API see the following URLs.\nHow to use the API\n\nhttps://weather-gov.github.io/api/general-faqs\n\nOther information about the API\n\nhttps://www.weather.gov/documentation/services-web-api\nhttps://weather-gov.github.io/api/",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'><yrChapterNumber>18. </yrChapterNumber> assignment - weather API</span>"
    ]
  },
  {
    "objectID": "assignment-api-0000100-weatherDataApi-v002.html#what-you-need-to-do",
    "href": "assignment-api-0000100-weatherDataApi-v002.html#what-you-need-to-do",
    "title": "18  18.  assignment - weather API",
    "section": "18.2 What you need to do",
    "text": "18.2 What you need to do\n\n18.2.1 Use of AI\nYou MAY use AI to help you with this. However, you MUST UNDERSTAND all the code that you submit. If you don’t understand the code, don’t submit it.\nYou may only use Base R, and the packages that we have learned about in class. This includes the following packages:\n\ntibble\nmagrittr\nfs\njsonlite\nstringr\n\nYou should inform the AI of these instructions so that it knows not to use functions from other packages. However, keep in mind that even if you do tell the AI to only use functions from these packages, it might use other packages anyway. Ultimately, you are responsible for reviewing the code and ensuring that it adheres to these directions.\nNote that we did not cover all the functions in these packages. Nevertheless, you MAY use functions that we didn’t cover from these packages, but you must understand the code before submitting it (you should do your own research about any functions that we didn’t cover in class).\nYou MUST include a copy of all of your conversations with the AI in your submission. (You may have multiple conversations with the AI). Use the yrWebSnap chrome extension to take a snapshot of the HTML page of the conversations you had with the AI. Do NOT submit images (i.e. screenshots) of the AI conversation. Rather, I want an HTML file that was produced by the yrWebSnap Chrome extension, so that I can copy/paste the text from your conversations. You can’t easily copy/paste text from an image.\nYou may be tempted to propmt the AI with a complete copy of the instructions in this file. That is fine. You can try that. However, sometimes you have better succes with an AI if you keep you focus on one particular part of the problem at a time. Experiment and see what works. Write up your experiences about what worked and what didn’t in the Quarto file you submit.\n\n\n18.2.2 summary\nThis section summarizes what you need to do. The next section goes into more detail about each step.\n\nCreate the following R functions\n\ngetForecastDf() -\nCreate an R function that calls the API to get the weather forecasts for a specific latitude and longitude. The latitude and longitude should be passed to this function as arguments. The function should return a dataframe\nthat contains the weather data.\n(see next section for details)\nfunction createForecastFiles() - Create a second R function that calls the first function in a loop. Each time the first function is called the data that is returned should be saved in a new csv file. All of the CSV files should be placed in the same folder\n(see next section for details)\ncombineCsvFiles() - Create a third R function that takes an entire folder of these csv files and returns a single dataframe that contains all of the rows from all of the csv files.\n(see next section for details)\n\nWriteup your results in a Quarto document.\n(see next section for details)\n\n\n\n18.2.3 detailed instructions\n\nfunction getForecastDf()\nCreate the following function. See below for an explanation of each of the arguments. Below is a list of what the function needs to do.\ngetForecastDf = function(latitude , longitude){\n  # your code goes here\n}\n\n(a) Check all arguments\nYour function should check that the arguments make sense. Both latitude and longitude should be numeric vectors of length 1 (i.e. just one number for each argument). If the arguments don’t follow these rules then the function should stop with an appropriate error message - i.e. call stop( SOME_ERROR_MESSAGE )\n\n\n(b) Call the API and return a dataframe with the data\nThe function should call the API to get the weather data. The function should return a non-nested dataframe (i.e. a “normal” dataframe in which each entry is a single number, TRUE/FALSE, character value or NA). The dataframe should include all of the data from each period from the hourly weather forecast as shown in the image below. There should be one row in the dataframe for each time period from the JSON.\nIn addition, to the data that is in the JSON, your dataframe should include the following columns. These extra columns will have the exact same data for each row in the dataframe and should be the first three columns in the dataframe:\n\nlatitude - ie. the latitude\nlongitude - ie. the longitude\napiCallTime - ie. the date and time that the API was called which included the info in the dataframe. The date/time/timezone should be formatted in ISO 8601 format (see the appendix at the bottom of this file). You can get this info using the following R code:\napiCallTime = format(Sys.time(), \"%Y-%m-%dT%H:%M:%S-%z\")\n\nBelow is an image of an example of the first few rows of a dataframe returned from this function.\n\n\n\n\n\n\n\nfunction createForecastFiles()\nCreate the following function. See below for an explanation of each of the arguments. Below is a list of what the function needs to do.\ncreateForecastFiles = function(latitude , \n                               longitude , \n                               sleepSeconds, \n                               outputFolder = \".\" , \n                               createFolder = FALSE ,\n                               numberOfApiCalls = Inf){\n  # your code goes here\n}\n\n(a) Check all arguments\nYour function should check that the arguments adhere to the following rules. If they don’t, the function should stop with an appropriate error message - i.e. call stop( SOME_ERROR_MESSAGE )\n\nlatitude, longitude, sleepSeconds and numberOfApiCalls should all be numeric vectors whose length is 1.\ncreateFolder should be a logical vector whose length is 1.\n\n\n\n(b) possibly create the output folder\nIf the outputFolder doesn’t exist then\n\nIf the createFolder argument is FALSE then the function should stop with an appropriate error message - i.e. call the function, stop(ERROR_MESSAGE).\nIf the createFolder argument is TRUE then the function should create the folder To do this, use the dir_create function in the fs package.\n\n\n\n(c) call the getForecastDf function in a loop\nThe createForecastFiles function should continuously call your getForecastDf function (that was described above) in a loop. The loop should iterate (i.e. “go around”) as many times as is specified in numberOfApiCalls argument. As part of this loop the function should call Sys.sleep(sleepSeconds) to ensure that there are at least sleepSeconds seconds between every call to the API.\n\n\n(d) save each dataframe in a separate .csv file\nEach time getForecastsDf() is called, the dataframe that is returned should be saved in a separate csv file. The name of each csv file should contain the timestamp that the API was called in the following format:\n\nweatherData_lat_long_YYYY-MM-DDTHH-MM-SS-TZ.csv\n\nNOTES\n\nhourlyData is always the first part of the filename\nlat is the latitude (e.g. 38.8894)\nlong in the longitude (e.g. -77.0352)\nYYYY-MM-DD-HH-MM-SS-TZ is the date/time/timezone for when the API was called. This should be the same date/time/timezone that is returned in the first column of the dataframe - see (1) above. However, note that the colons ( : ) from the date/time/timezone in dataframe should be changed to dashes ( - ) for the filename since filenames on both Mac and Windows may not contain colons. You can use the following code to make this change:\nTIMESTAMP_WITHOUT_COLONS = gsub(':', '-', TIMESTAMP_WITH_COLONS)\nThe different parts of the filename described above are separated from each other with underscores ( _ ).\n\nFor example - the filename for the the dataframe shown in the image in the previous section, should be:\n\nhourlyData_38.8894_-77.0352_2025-02-13T03-56-55-0500.csv\n\n\n\n(e) Use cat to display info as the function is running\nThis function could run for a long time. To make sure that the user knows that the function is running, the function should display a period (i.e. ” .”) every 15 seconds or so. This alerts the user to the fact that the function is indeed working. You can accomplish this by using a loop with code similar to the following:\nwhile( SOME_CONDITION ){\n  Sys.sleep( 15 )\n  cat(\".\")\n}\nIn addition, each time the API is called or a CSV file is written, your code should display a message to the user with the date/time and an appropriate message, for example:\n...........................................................................\n...........................................\n2025-02-17T13:00:01-05:00  Called API - https://api.weather.gov/gridpoints/LWX/97,71/forecast/hourly\n2025-02-17T13:00:03-05:00  Created file: hourlyData_38.8894_-77.0352_2025-02-17T13:00:03-05:00.csv\n...........................................................................\n...........................................\n2025-02-17T14:00:01-05:00  Called API - https://api.weather.gov/gridpoints/LWX/97,71/forecast/hourly\n2025-02-17T14:00:03-05:00  Created file: hourlyData_38.8894_-77.0352_2025-02-17T14:00:03-05:00.csv\n\n\n(f) return a vector with filenames that were created\n\nWhen the function is finished, it should return a vector that contains the names of all of the files that were created. (Note that if )\n\n\n\n\nfunction combineCsvFiles( folder=“.”, pattern=“.*\\.[cC][sS][vV]“)\nThis function should return a single dataframe. The dataframe should contain the rows from all of the csv files in the specified folder whose names match the specified regular expression pattern.\nHINT: Use the Base-R dir function (AKA list.files) with the pattern argument.\n\n\n\n\n18.2.4 Create a Quarto file\nInclude all of the following in the Quarto .qmd file:\n\nAn brief description of the problem - include a link to this webpage.\nDescribe your experience using AI. (If you didn’t use AI at all, just say so). Describe a little about your experience - which AI tools you used - what worked - what didn’t work - did you need to debug the code yourself or did the AI debug the code for you - etc.\nInclude links to the yrWebSnips .html files that contain a copy of your AI conversations.\nYour .R code file should include not only the funcitons but also examples of the functions being used. For example\nInclude links to your .R file(s) - e.g. &lt;myProgram.R&gt; - so that anyone viewing the Quarto html file in a browser can “download” your R file by clicking on the link.\nInclude the code from your .R file(s) in the Quarto by specifying the filename(s) in the code chunk as shown below. The code-line-numbers: true option displays line numbers next to the code.\n#| file: YOUR_FILENAME.R\n#| code-line-numbers: true  \nInclude R code (displayed in Quarto code chunks) that demonstrates your code working.\nThe code chunks should include the functions you wrote and examples of them working. When running the code, do NOT show entire dataframe(s). Rather just show the first 10 or so rows from each dataframe by using, for example, the function call head(SOME_DATAFRAME, 10).\n\n\n\n18.2.5 What to submit\n\nCreate a folder that contains all of the following:\n\nA copy of the .qmd file.\nA copy of the rendered qmd file (ie. the .html file) as well as the folder that is created when you render the .qmd file.\nA sample of at least 3 copies of the .csv files that are created when you run your code.\nThe .html files that contain the output from the yrWebSnip of your conversations with the AI\n\nCreate a ZIP file of this folder and submit it",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'><yrChapterNumber>18. </yrChapterNumber> assignment - weather API</span>"
    ]
  },
  {
    "objectID": "assignment-api-0000100-weatherDataApi-v002.html#appendix---explanation-of-standard-datetimetimezone-format",
    "href": "assignment-api-0000100-weatherDataApi-v002.html#appendix---explanation-of-standard-datetimetimezone-format",
    "title": "18  18.  assignment - weather API",
    "section": "18.3 Appendix - Explanation of standard date/time/timezone format",
    "text": "18.3 Appendix - Explanation of standard date/time/timezone format\nDifferent countries and locales have different standards for how to represent dates and times (e.g. month/day/year in USA and day/month/year in Europe). The following format has become well accepted in computing circles all over the world.\n\nYYYY-MM-DDTHH:MM:SS±HH:MM”\n\nFor example 2023-10-26T14:30:00-05:00 represents October 26, 2023 at 2:30 PM with a 5-hour time zone offset (e.g., Eastern Standard Time).\nComments:\n\n“YYYY” is the 4 digit year\n“MM” is the 2 digit month\n“DD” is the 2 digit day of the month\n“HH” is the 2 digit hour\n“MM” is the 2 digit minute\n“SS” is the 2 digit second\n“±HH:MM” represents the time zone offset from UTC (Coordinated Universal Time). This incluedes a plus (+) or minus (-) sign followed by the hours and minutes offset from UTC.\n\nAdditional comments:\n\nThere are dashes (i.e. “-”) between different parts of the date\nThe “T” character separates the date from the time.\nThere are colons (i.e. “:”) between different parts of the time\nIf the time is in UTC, you can use a “Z” instead of a time zone offset.\n\nThis format is described in the official RFC3339 standard which in turn was derived from the ISO 8601 standard.\n\nIt’s not necessary for this project, but if you’re interested and want to learn more about these standards see these links:\n\nWhat is an RFC? - videos\n\nShort to the point (4 minutes)\nMore info (10.5 minutes)\n\nUnderstanding RFC3339\nThe actual RFC3339 (very technical) - https://www.ietf.org/rfc/rfc3339.txt\nDifference between RFC3339 and ISO 8601\nInteresting graphic highlighting different date/time formats",
    "crumbs": [
      "JSON data and APIs",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'><yrChapterNumber>18. </yrChapterNumber> assignment - weather API</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html",
    "href": "0200200-a-introToRegularExpressions-v036.html",
    "title": "19  19.  regular expressions (regex)",
    "section": "",
    "text": "19.1 Intro to Regular Expressions (Also Known As “regex”)\nRegular expressions (AKA “regex”) are patterns that are used to identify specific types of data. They can be used to search through and slice and dice character (i.e. textual) data in a variety of ways. The best way to understand is just to dive right in, which we will do below.\nBefore we get started it’s important to note that regex is not an “R” thing. Rather regular expressions are a concept that are available to use in most programming languages (e.g. Python, Java, etc.) as well as in many text editors and in other technical environments (e.g. Bash). There are slightly different versions of regular expression syntax found in different environments. We will be focusing mainly on how to use regular expressions in R. However, this knowledge carries over directly to using regex in other languages and technical environments.",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#some-resources-for-learning-regular-expressions",
    "href": "0200200-a-introToRegularExpressions-v036.html#some-resources-for-learning-regular-expressions",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.2 Some resources for learning regular expressions",
    "text": "19.2 Some resources for learning regular expressions\nThe following is one of the many online tutorials.\n\nhttps://ryanstutorials.net/regular-expressions-tutorial/\n\nThe following R help pages are relevant to regular expressions. However, they are hard to understand without a little intro first. I recommend that you go through the material in this book first or in other tutorilas. Then you can refer to the help pages when necessary.\nRelevant R help pages\n\n?regex     \n?grep      \n?strsplit  \n\nR’s stringr package also includes several functions that can be used with regex. For now though, we will focus mainly on the “Base R” regex functions. Once you understand those, you will be very prepared to understand functions in the stringr package that utilize regular expressions.",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#data-for-examples",
    "href": "0200200-a-introToRegularExpressions-v036.html#data-for-examples",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.3 Data for examples",
    "text": "19.3 Data for examples\nBefore we start, let’s define some data to be used with examples in this file. (NOTE: I made up N. American apple and S. Korean Fig so that I can use *them in some examples.)\n\n19.3.1 fruit vector\n\nfruit = c(\"apple\", \"N. American apple\", \"S. Korean Fig\", \"fig\", \n          \"star fruit\", \"pear\", \"prickly pear\", \n          \"Beurre Hardy pear\", \"cherry\", \"black cherry\", \"peach\", \"plum\",\n          \"kumquat\", \"banana\", \"blueberry\", \"strawberry\", \"honeydew\", \n          \"strawberries\", \"yumberry\")\n\n\nfruit\n\n [1] \"apple\"             \"N. American apple\" \"S. Korean Fig\"    \n [4] \"fig\"               \"star fruit\"        \"pear\"             \n [7] \"prickly pear\"      \"Beurre Hardy pear\" \"cherry\"           \n[10] \"black cherry\"      \"peach\"             \"plum\"             \n[13] \"kumquat\"           \"banana\"            \"blueberry\"        \n[16] \"strawberry\"        \"honeydew\"          \"strawberries\"     \n[19] \"yumberry\"         \n\n\n\n\n19.3.2 addresses vector\n\naddresses = c(\"12345 Sesame Street\", \n              \"One Micro$oft Way\",                 # notice the $ sign\n              \"3 Olive St.\", \n              \"Two 1st Ave.\",\n              \"5678 Park Place\",\n              \"Forty Five 2nd Street\",\n              \"Ninety Nine Cone St. apartment 7\",\n              \"9 Main St. apt. 623\", \n              \"Five Google Drive\", \n              \"4\\\\2 Rechov Yafo\",                   # note the backslash (\\)\n              \"Fifteen Watchamacallit Boulevard\",   # some long words\n              \"Nineteen Watchamacallit Boulevard\",  # some longer words\n              \"One Main Street Apt 12b\",\n              \"Two Main Street Apt 123c\",\n              \"Three Main Street Apt 12343\",\n              \"City Hall Lockport, NY\")\n\n\naddresses\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\n# show each address, one per line\ncat(addresses, sep=\"\\n\")\n\n12345 Sesame Street\nOne Micro$oft Way\n3 Olive St.\nTwo 1st Ave.\n5678 Park Place\nForty Five 2nd Street\nNinety Nine Cone St. apartment 7\n9 Main St. apt. 623\nFive Google Drive\n4\\2 Rechov Yafo\nFifteen Watchamacallit Boulevard\nNineteen Watchamacallit Boulevard\nOne Main Street Apt 12b\nTwo Main Street Apt 123c\nThree Main Street Apt 12343\nCity Hall Lockport, NY\n\n\n\n\n19.3.3 famousQuotes dataframe\n\n# We will use the \"tribble\" function that is part of the \"tibble\" package\n# to create the data. That makes the code much easier to read then the \n# way you would normally create a dataframe (or tibble) in R.\n#\n# We will convert the data to a regular data.frame below so that you \n# don't need to understand tibbles to use this code.\n\nlibrary(tibble)\n\nfamousQuotesTbl &lt;- tribble(\n  ~Year,     ~Speaker,                                  ~Quote,\n  -47,     \"Julius Caesar\",                          \"I came, I saw, I conquered\",\n  -399,     \"Socrates\",                               \"I know that I know nothing\",\n  -399,     \"Socrates\",                               \"The unexamined life is not worth living\",\n  1597,     \"Francis Bacon\",                          \"Knowledge is power\",\n  1603,     \"William Shakespeare\",                    \"To be or not to be, that is the question\",\n  1637,     \"René Descartes\",                         \"I think, therefore I am\",\n  1759,     \"Voltaire\",                               \"I disapprove of what you say, but I will defend to the death your right to say it\",\n  1775,     \"Patrick Henry\",                          \"Give me liberty, or give me death\",\n  1789,     \"Marie Antoinette\",                       \"Let them eat cake\",\n  1843,     \"Karl Marx\",                              \"Religion is the opium of the people\",\n  1848,     \"Karl Marx\",                              \"Workers of the world, unite!\",\n  1858,     \"Abraham Lincoln\",                        \"A house divided against itself cannot stand\",\n  1863,     \"Abraham Lincoln\",                        \"Government of the people, by the people, for the people\",\n  1887,     \"Lord Acton\",                             \"Power tends to corrupt, and absolute power corrupts absolutely\",\n  1901,     \"Theodore Roosevelt\",                     \"Speak softly and carry a big stick\",\n  1929,     \"Sherlock Holmes (Arthur Conan Doyle)\",   \"Elementary, my dear Watson\",\n  1929,     \"Albert Einstein\",                        \"Imagination is more important than knowledge\",\n  1933,     \"Franklin D. Roosevelt\",                  \"The only thing we have to fear is fear itself\",\n  1940,     \"Winston Churchill\",                      \"We shall fight on the beaches\",\n  1940,     \"Eleanor Roosevelt\",                      \"The future belongs to those who believe in the beauty of their dreams\",\n  1941,     \"Winston Churchill\",                      \"Never, never, never give up\",\n  1942,     \"Douglas MacArthur\",                      \"I shall return\",\n  1945,     \"Harry S. Truman\",                        \"The buck stops here\",\n  1947,     \"Mahatma Gandhi\",                         \"Be the change you wish to see in the world\",\n  1949,     \"George Orwell\",                          \"War is peace. Freedom is slavery. Ignorance is strength\",\n  1951,     \"Albert Camus\",                           \"The only way to deal with an unfree world is to become so absolutely free that your very existence is an act of rebellion\",\n  1961,     \"John F. Kennedy\",                        \"Ask not what your country can do for you, ask what you can do for your country\",\n  1963,     \"Martin Luther King Jr.\",                 \"I have a dream\",\n  1964,     \"Muhammad Ali\",                           \"Float like a butterfly, sting like a bee\",\n  1964,     \"Marshall McLuhan\",                       \"The medium is the message\",\n  1968,     \"Martin Luther King Jr.\",                 \"I've been to the mountaintop\",\n  1969,     \"Neil Armstrong\",                         \"That's one small step for man, one giant leap for mankind\",\n  1970,     \"Irina Dunn\",                             \"A woman needs a man like a fish needs a bicycle\",\n  1980,     \"John Lennon\",                            \"Life is what happens while you're busy making other plans\",\n  1980,     \"Margaret Thatcher\",                      \"There is no alternative\",\n  1987,     \"Ronald Reagan\",                          \"Mr. Gorbachev, tear down this wall\",\n  1988,     \"George H. W. Bush\",                      \"Read my lips: no new taxes\",\n  2008,     \"Barack Obama\",                           \"Yes we can\"\n)\n\n# Convert to a regular data.frame\nfamousQuotesDf = as.data.frame(famousQuotesTbl)",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#the-stringr-package",
    "href": "0200200-a-introToRegularExpressions-v036.html#the-stringr-package",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.4 The stringr package",
    "text": "19.4 The stringr package\n\n19.4.1 stringr::words vector\nThe stringr package includes some data that we will use in the examples.\n\n#-------------------------------------------------------------------------.\n# NOTE - most of the examples in this file were created using the data above.\n# The stringr package also contains some data that can be used to experiment\n# with these functions. \n#-------------------------------------------------------------------------.\n\nif(!require(stringr)){install.packages(\"stringr\");require(stringr)}\n\nLoading required package: stringr\n\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\n\nAttaching package: 'stringr'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    fruit\n\nstringr::words\n\n  [1] \"a\"           \"able\"        \"about\"       \"absolute\"    \"accept\"     \n  [6] \"account\"     \"achieve\"     \"across\"      \"act\"         \"active\"     \n [11] \"actual\"      \"add\"         \"address\"     \"admit\"       \"advertise\"  \n [16] \"affect\"      \"afford\"      \"after\"       \"afternoon\"   \"again\"      \n [21] \"against\"     \"age\"         \"agent\"       \"ago\"         \"agree\"      \n [26] \"air\"         \"all\"         \"allow\"       \"almost\"      \"along\"      \n [31] \"already\"     \"alright\"     \"also\"        \"although\"    \"always\"     \n [36] \"america\"     \"amount\"      \"and\"         \"another\"     \"answer\"     \n [41] \"any\"         \"apart\"       \"apparent\"    \"appear\"      \"apply\"      \n [46] \"appoint\"     \"approach\"    \"appropriate\" \"area\"        \"argue\"      \n [51] \"arm\"         \"around\"      \"arrange\"     \"art\"         \"as\"         \n [56] \"ask\"         \"associate\"   \"assume\"      \"at\"          \"attend\"     \n [61] \"authority\"   \"available\"   \"aware\"       \"away\"        \"awful\"      \n [66] \"baby\"        \"back\"        \"bad\"         \"bag\"         \"balance\"    \n [71] \"ball\"        \"bank\"        \"bar\"         \"base\"        \"basis\"      \n [76] \"be\"          \"bear\"        \"beat\"        \"beauty\"      \"because\"    \n [81] \"become\"      \"bed\"         \"before\"      \"begin\"       \"behind\"     \n [86] \"believe\"     \"benefit\"     \"best\"        \"bet\"         \"between\"    \n [91] \"big\"         \"bill\"        \"birth\"       \"bit\"         \"black\"      \n [96] \"bloke\"       \"blood\"       \"blow\"        \"blue\"        \"board\"      \n[101] \"boat\"        \"body\"        \"book\"        \"both\"        \"bother\"     \n[106] \"bottle\"      \"bottom\"      \"box\"         \"boy\"         \"break\"      \n[111] \"brief\"       \"brilliant\"   \"bring\"       \"britain\"     \"brother\"    \n[116] \"budget\"      \"build\"       \"bus\"         \"business\"    \"busy\"       \n[121] \"but\"         \"buy\"         \"by\"          \"cake\"        \"call\"       \n[126] \"can\"         \"car\"         \"card\"        \"care\"        \"carry\"      \n[131] \"case\"        \"cat\"         \"catch\"       \"cause\"       \"cent\"       \n[136] \"centre\"      \"certain\"     \"chair\"       \"chairman\"    \"chance\"     \n[141] \"change\"      \"chap\"        \"character\"   \"charge\"      \"cheap\"      \n[146] \"check\"       \"child\"       \"choice\"      \"choose\"      \"Christ\"     \n[151] \"Christmas\"   \"church\"      \"city\"        \"claim\"       \"class\"      \n[156] \"clean\"       \"clear\"       \"client\"      \"clock\"       \"close\"      \n[161] \"closes\"      \"clothe\"      \"club\"        \"coffee\"      \"cold\"       \n[166] \"colleague\"   \"collect\"     \"college\"     \"colour\"      \"come\"       \n[171] \"comment\"     \"commit\"      \"committee\"   \"common\"      \"community\"  \n[176] \"company\"     \"compare\"     \"complete\"    \"compute\"     \"concern\"    \n[181] \"condition\"   \"confer\"      \"consider\"    \"consult\"     \"contact\"    \n[186] \"continue\"    \"contract\"    \"control\"     \"converse\"    \"cook\"       \n[191] \"copy\"        \"corner\"      \"correct\"     \"cost\"        \"could\"      \n[196] \"council\"     \"count\"       \"country\"     \"county\"      \"couple\"     \n[201] \"course\"      \"court\"       \"cover\"       \"create\"      \"cross\"      \n[206] \"cup\"         \"current\"     \"cut\"         \"dad\"         \"danger\"     \n[211] \"date\"        \"day\"         \"dead\"        \"deal\"        \"dear\"       \n[216] \"debate\"      \"decide\"      \"decision\"    \"deep\"        \"definite\"   \n[221] \"degree\"      \"department\"  \"depend\"      \"describe\"    \"design\"     \n[226] \"detail\"      \"develop\"     \"die\"         \"difference\"  \"difficult\"  \n[231] \"dinner\"      \"direct\"      \"discuss\"     \"district\"    \"divide\"     \n[236] \"do\"          \"doctor\"      \"document\"    \"dog\"         \"door\"       \n[241] \"double\"      \"doubt\"       \"down\"        \"draw\"        \"dress\"      \n[246] \"drink\"       \"drive\"       \"drop\"        \"dry\"         \"due\"        \n[251] \"during\"      \"each\"        \"early\"       \"east\"        \"easy\"       \n[256] \"eat\"         \"economy\"     \"educate\"     \"effect\"      \"egg\"        \n[261] \"eight\"       \"either\"      \"elect\"       \"electric\"    \"eleven\"     \n[266] \"else\"        \"employ\"      \"encourage\"   \"end\"         \"engine\"     \n[271] \"english\"     \"enjoy\"       \"enough\"      \"enter\"       \"environment\"\n[276] \"equal\"       \"especial\"    \"europe\"      \"even\"        \"evening\"    \n[281] \"ever\"        \"every\"       \"evidence\"    \"exact\"       \"example\"    \n[286] \"except\"      \"excuse\"      \"exercise\"    \"exist\"       \"expect\"     \n[291] \"expense\"     \"experience\"  \"explain\"     \"express\"     \"extra\"      \n[296] \"eye\"         \"face\"        \"fact\"        \"fair\"        \"fall\"       \n[301] \"family\"      \"far\"         \"farm\"        \"fast\"        \"father\"     \n[306] \"favour\"      \"feed\"        \"feel\"        \"few\"         \"field\"      \n[311] \"fight\"       \"figure\"      \"file\"        \"fill\"        \"film\"       \n[316] \"final\"       \"finance\"     \"find\"        \"fine\"        \"finish\"     \n[321] \"fire\"        \"first\"       \"fish\"        \"fit\"         \"five\"       \n[326] \"flat\"        \"floor\"       \"fly\"         \"follow\"      \"food\"       \n[331] \"foot\"        \"for\"         \"force\"       \"forget\"      \"form\"       \n[336] \"fortune\"     \"forward\"     \"four\"        \"france\"      \"free\"       \n[341] \"friday\"      \"friend\"      \"from\"        \"front\"       \"full\"       \n[346] \"fun\"         \"function\"    \"fund\"        \"further\"     \"future\"     \n[351] \"game\"        \"garden\"      \"gas\"         \"general\"     \"germany\"    \n[356] \"get\"         \"girl\"        \"give\"        \"glass\"       \"go\"         \n[361] \"god\"         \"good\"        \"goodbye\"     \"govern\"      \"grand\"      \n[366] \"grant\"       \"great\"       \"green\"       \"ground\"      \"group\"      \n[371] \"grow\"        \"guess\"       \"guy\"         \"hair\"        \"half\"       \n[376] \"hall\"        \"hand\"        \"hang\"        \"happen\"      \"happy\"      \n[381] \"hard\"        \"hate\"        \"have\"        \"he\"          \"head\"       \n[386] \"health\"      \"hear\"        \"heart\"       \"heat\"        \"heavy\"      \n[391] \"hell\"        \"help\"        \"here\"        \"high\"        \"history\"    \n[396] \"hit\"         \"hold\"        \"holiday\"     \"home\"        \"honest\"     \n[401] \"hope\"        \"horse\"       \"hospital\"    \"hot\"         \"hour\"       \n[406] \"house\"       \"how\"         \"however\"     \"hullo\"       \"hundred\"    \n[411] \"husband\"     \"idea\"        \"identify\"    \"if\"          \"imagine\"    \n[416] \"important\"   \"improve\"     \"in\"          \"include\"     \"income\"     \n[421] \"increase\"    \"indeed\"      \"individual\"  \"industry\"    \"inform\"     \n[426] \"inside\"      \"instead\"     \"insure\"      \"interest\"    \"into\"       \n[431] \"introduce\"   \"invest\"      \"involve\"     \"issue\"       \"it\"         \n[436] \"item\"        \"jesus\"       \"job\"         \"join\"        \"judge\"      \n[441] \"jump\"        \"just\"        \"keep\"        \"key\"         \"kid\"        \n[446] \"kill\"        \"kind\"        \"king\"        \"kitchen\"     \"knock\"      \n[451] \"know\"        \"labour\"      \"lad\"         \"lady\"        \"land\"       \n[456] \"language\"    \"large\"       \"last\"        \"late\"        \"laugh\"      \n[461] \"law\"         \"lay\"         \"lead\"        \"learn\"       \"leave\"      \n[466] \"left\"        \"leg\"         \"less\"        \"let\"         \"letter\"     \n[471] \"level\"       \"lie\"         \"life\"        \"light\"       \"like\"       \n[476] \"likely\"      \"limit\"       \"line\"        \"link\"        \"list\"       \n[481] \"listen\"      \"little\"      \"live\"        \"load\"        \"local\"      \n[486] \"lock\"        \"london\"      \"long\"        \"look\"        \"lord\"       \n[491] \"lose\"        \"lot\"         \"love\"        \"low\"         \"luck\"       \n[496] \"lunch\"       \"machine\"     \"main\"        \"major\"       \"make\"       \n[501] \"man\"         \"manage\"      \"many\"        \"mark\"        \"market\"     \n[506] \"marry\"       \"match\"       \"matter\"      \"may\"         \"maybe\"      \n[511] \"mean\"        \"meaning\"     \"measure\"     \"meet\"        \"member\"     \n[516] \"mention\"     \"middle\"      \"might\"       \"mile\"        \"milk\"       \n[521] \"million\"     \"mind\"        \"minister\"    \"minus\"       \"minute\"     \n[526] \"miss\"        \"mister\"      \"moment\"      \"monday\"      \"money\"      \n[531] \"month\"       \"more\"        \"morning\"     \"most\"        \"mother\"     \n[536] \"motion\"      \"move\"        \"mrs\"         \"much\"        \"music\"      \n[541] \"must\"        \"name\"        \"nation\"      \"nature\"      \"near\"       \n[546] \"necessary\"   \"need\"        \"never\"       \"new\"         \"news\"       \n[551] \"next\"        \"nice\"        \"night\"       \"nine\"        \"no\"         \n[556] \"non\"         \"none\"        \"normal\"      \"north\"       \"not\"        \n[561] \"note\"        \"notice\"      \"now\"         \"number\"      \"obvious\"    \n[566] \"occasion\"    \"odd\"         \"of\"          \"off\"         \"offer\"      \n[571] \"office\"      \"often\"       \"okay\"        \"old\"         \"on\"         \n[576] \"once\"        \"one\"         \"only\"        \"open\"        \"operate\"    \n[581] \"opportunity\" \"oppose\"      \"or\"          \"order\"       \"organize\"   \n[586] \"original\"    \"other\"       \"otherwise\"   \"ought\"       \"out\"        \n[591] \"over\"        \"own\"         \"pack\"        \"page\"        \"paint\"      \n[596] \"pair\"        \"paper\"       \"paragraph\"   \"pardon\"      \"parent\"     \n[601] \"park\"        \"part\"        \"particular\"  \"party\"       \"pass\"       \n[606] \"past\"        \"pay\"         \"pence\"       \"pension\"     \"people\"     \n[611] \"per\"         \"percent\"     \"perfect\"     \"perhaps\"     \"period\"     \n[616] \"person\"      \"photograph\"  \"pick\"        \"picture\"     \"piece\"      \n[621] \"place\"       \"plan\"        \"play\"        \"please\"      \"plus\"       \n[626] \"point\"       \"police\"      \"policy\"      \"politic\"     \"poor\"       \n[631] \"position\"    \"positive\"    \"possible\"    \"post\"        \"pound\"      \n[636] \"power\"       \"practise\"    \"prepare\"     \"present\"     \"press\"      \n[641] \"pressure\"    \"presume\"     \"pretty\"      \"previous\"    \"price\"      \n[646] \"print\"       \"private\"     \"probable\"    \"problem\"     \"proceed\"    \n[651] \"process\"     \"produce\"     \"product\"     \"programme\"   \"project\"    \n[656] \"proper\"      \"propose\"     \"protect\"     \"provide\"     \"public\"     \n[661] \"pull\"        \"purpose\"     \"push\"        \"put\"         \"quality\"    \n[666] \"quarter\"     \"question\"    \"quick\"       \"quid\"        \"quiet\"      \n[671] \"quite\"       \"radio\"       \"rail\"        \"raise\"       \"range\"      \n[676] \"rate\"        \"rather\"      \"read\"        \"ready\"       \"real\"       \n[681] \"realise\"     \"really\"      \"reason\"      \"receive\"     \"recent\"     \n[686] \"reckon\"      \"recognize\"   \"recommend\"   \"record\"      \"red\"        \n[691] \"reduce\"      \"refer\"       \"regard\"      \"region\"      \"relation\"   \n[696] \"remember\"    \"report\"      \"represent\"   \"require\"     \"research\"   \n[701] \"resource\"    \"respect\"     \"responsible\" \"rest\"        \"result\"     \n[706] \"return\"      \"rid\"         \"right\"       \"ring\"        \"rise\"       \n[711] \"road\"        \"role\"        \"roll\"        \"room\"        \"round\"      \n[716] \"rule\"        \"run\"         \"safe\"        \"sale\"        \"same\"       \n[721] \"saturday\"    \"save\"        \"say\"         \"scheme\"      \"school\"     \n[726] \"science\"     \"score\"       \"scotland\"    \"seat\"        \"second\"     \n[731] \"secretary\"   \"section\"     \"secure\"      \"see\"         \"seem\"       \n[736] \"self\"        \"sell\"        \"send\"        \"sense\"       \"separate\"   \n[741] \"serious\"     \"serve\"       \"service\"     \"set\"         \"settle\"     \n[746] \"seven\"       \"sex\"         \"shall\"       \"share\"       \"she\"        \n[751] \"sheet\"       \"shoe\"        \"shoot\"       \"shop\"        \"short\"      \n[756] \"should\"      \"show\"        \"shut\"        \"sick\"        \"side\"       \n[761] \"sign\"        \"similar\"     \"simple\"      \"since\"       \"sing\"       \n[766] \"single\"      \"sir\"         \"sister\"      \"sit\"         \"site\"       \n[771] \"situate\"     \"six\"         \"size\"        \"sleep\"       \"slight\"     \n[776] \"slow\"        \"small\"       \"smoke\"       \"so\"          \"social\"     \n[781] \"society\"     \"some\"        \"son\"         \"soon\"        \"sorry\"      \n[786] \"sort\"        \"sound\"       \"south\"       \"space\"       \"speak\"      \n[791] \"special\"     \"specific\"    \"speed\"       \"spell\"       \"spend\"      \n[796] \"square\"      \"staff\"       \"stage\"       \"stairs\"      \"stand\"      \n[801] \"standard\"    \"start\"       \"state\"       \"station\"     \"stay\"       \n[806] \"step\"        \"stick\"       \"still\"       \"stop\"        \"story\"      \n[811] \"straight\"    \"strategy\"    \"street\"      \"strike\"      \"strong\"     \n[816] \"structure\"   \"student\"     \"study\"       \"stuff\"       \"stupid\"     \n[821] \"subject\"     \"succeed\"     \"such\"        \"sudden\"      \"suggest\"    \n[826] \"suit\"        \"summer\"      \"sun\"         \"sunday\"      \"supply\"     \n[831] \"support\"     \"suppose\"     \"sure\"        \"surprise\"    \"switch\"     \n[836] \"system\"      \"table\"       \"take\"        \"talk\"        \"tape\"       \n[841] \"tax\"         \"tea\"         \"teach\"       \"team\"        \"telephone\"  \n[846] \"television\"  \"tell\"        \"ten\"         \"tend\"        \"term\"       \n[851] \"terrible\"    \"test\"        \"than\"        \"thank\"       \"the\"        \n[856] \"then\"        \"there\"       \"therefore\"   \"they\"        \"thing\"      \n[861] \"think\"       \"thirteen\"    \"thirty\"      \"this\"        \"thou\"       \n[866] \"though\"      \"thousand\"    \"three\"       \"through\"     \"throw\"      \n[871] \"thursday\"    \"tie\"         \"time\"        \"to\"          \"today\"      \n[876] \"together\"    \"tomorrow\"    \"tonight\"     \"too\"         \"top\"        \n[881] \"total\"       \"touch\"       \"toward\"      \"town\"        \"trade\"      \n[886] \"traffic\"     \"train\"       \"transport\"   \"travel\"      \"treat\"      \n[891] \"tree\"        \"trouble\"     \"true\"        \"trust\"       \"try\"        \n[896] \"tuesday\"     \"turn\"        \"twelve\"      \"twenty\"      \"two\"        \n[901] \"type\"        \"under\"       \"understand\"  \"union\"       \"unit\"       \n[906] \"unite\"       \"university\"  \"unless\"      \"until\"       \"up\"         \n[911] \"upon\"        \"use\"         \"usual\"       \"value\"       \"various\"    \n[916] \"very\"        \"video\"       \"view\"        \"village\"     \"visit\"      \n[921] \"vote\"        \"wage\"        \"wait\"        \"walk\"        \"wall\"       \n[926] \"want\"        \"war\"         \"warm\"        \"wash\"        \"waste\"      \n[931] \"watch\"       \"water\"       \"way\"         \"we\"          \"wear\"       \n[936] \"wednesday\"   \"wee\"         \"week\"        \"weigh\"       \"welcome\"    \n[941] \"well\"        \"west\"        \"what\"        \"when\"        \"where\"      \n[946] \"whether\"     \"which\"       \"while\"       \"white\"       \"who\"        \n[951] \"whole\"       \"why\"         \"wide\"        \"wife\"        \"will\"       \n[956] \"win\"         \"wind\"        \"window\"      \"wish\"        \"with\"       \n[961] \"within\"      \"without\"     \"woman\"       \"wonder\"      \"wood\"       \n[966] \"word\"        \"work\"        \"world\"       \"worry\"       \"worse\"      \n[971] \"worth\"       \"would\"       \"write\"       \"wrong\"       \"year\"       \n[976] \"yes\"         \"yesterday\"   \"yet\"         \"you\"         \"young\"      \n\nhead(stringr::words, 100)\n\n  [1] \"a\"           \"able\"        \"about\"       \"absolute\"    \"accept\"     \n  [6] \"account\"     \"achieve\"     \"across\"      \"act\"         \"active\"     \n [11] \"actual\"      \"add\"         \"address\"     \"admit\"       \"advertise\"  \n [16] \"affect\"      \"afford\"      \"after\"       \"afternoon\"   \"again\"      \n [21] \"against\"     \"age\"         \"agent\"       \"ago\"         \"agree\"      \n [26] \"air\"         \"all\"         \"allow\"       \"almost\"      \"along\"      \n [31] \"already\"     \"alright\"     \"also\"        \"although\"    \"always\"     \n [36] \"america\"     \"amount\"      \"and\"         \"another\"     \"answer\"     \n [41] \"any\"         \"apart\"       \"apparent\"    \"appear\"      \"apply\"      \n [46] \"appoint\"     \"approach\"    \"appropriate\" \"area\"        \"argue\"      \n [51] \"arm\"         \"around\"      \"arrange\"     \"art\"         \"as\"         \n [56] \"ask\"         \"associate\"   \"assume\"      \"at\"          \"attend\"     \n [61] \"authority\"   \"available\"   \"aware\"       \"away\"        \"awful\"      \n [66] \"baby\"        \"back\"        \"bad\"         \"bag\"         \"balance\"    \n [71] \"ball\"        \"bank\"        \"bar\"         \"base\"        \"basis\"      \n [76] \"be\"          \"bear\"        \"beat\"        \"beauty\"      \"because\"    \n [81] \"become\"      \"bed\"         \"before\"      \"begin\"       \"behind\"     \n [86] \"believe\"     \"benefit\"     \"best\"        \"bet\"         \"between\"    \n [91] \"big\"         \"bill\"        \"birth\"       \"bit\"         \"black\"      \n [96] \"bloke\"       \"blood\"       \"blow\"        \"blue\"        \"board\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#grep-function",
    "href": "0200200-a-introToRegularExpressions-v036.html#grep-function",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.5 grep function",
    "text": "19.5 grep function\n\n# Show all words that\n# \"start with a p, end with a y (with anything in the middle)\"\ngrep(stringr::words, pattern=\"^p.*y$\", value=TRUE)\n\n[1] \"party\"  \"pay\"    \"play\"   \"policy\" \"pretty\"\n\n# Starts with a p, ends with a y, nothing in the middle.\n# Only matches \"py\". \n# There are no words that match.\ngrep(stringr::words, pattern=\"^py$\", value=TRUE)\n\ncharacter(0)\n\n# match any word that start with p, ends with y and has a single\n# character between them\ngrep(stringr::words, pattern=\"^p.y$\", value=TRUE)\n\n[1] \"pay\"\n\n# match any word that start with p, ends with y and\n# has exactly two characters between them.\ngrep(stringr::words, pattern=\"^p..y$\", value=TRUE)\n\n[1] \"play\"\n\n# match any word that start with p, ends with y and\n# has exactly four characters between them.\ngrep(stringr::words, pattern=\"^p....y$\", value=TRUE)\n\n[1] \"policy\" \"pretty\"\n\n# match any sequence of characters between the p and the y\ngrep(stringr::words, pattern=\"^p.*y$\", value=TRUE)\n\n[1] \"party\"  \"pay\"    \"play\"   \"policy\" \"pretty\"\n\n# starts with a p\ngrep(stringr::words, pattern=\"^p\", value=TRUE)\n\n [1] \"pack\"       \"page\"       \"paint\"      \"pair\"       \"paper\"     \n [6] \"paragraph\"  \"pardon\"     \"parent\"     \"park\"       \"part\"      \n[11] \"particular\" \"party\"      \"pass\"       \"past\"       \"pay\"       \n[16] \"pence\"      \"pension\"    \"people\"     \"per\"        \"percent\"   \n[21] \"perfect\"    \"perhaps\"    \"period\"     \"person\"     \"photograph\"\n[26] \"pick\"       \"picture\"    \"piece\"      \"place\"      \"plan\"      \n[31] \"play\"       \"please\"     \"plus\"       \"point\"      \"police\"    \n[36] \"policy\"     \"politic\"    \"poor\"       \"position\"   \"positive\"  \n[41] \"possible\"   \"post\"       \"pound\"      \"power\"      \"practise\"  \n[46] \"prepare\"    \"present\"    \"press\"      \"pressure\"   \"presume\"   \n[51] \"pretty\"     \"previous\"   \"price\"      \"print\"      \"private\"   \n[56] \"probable\"   \"problem\"    \"proceed\"    \"process\"    \"produce\"   \n[61] \"product\"    \"programme\"  \"project\"    \"proper\"     \"propose\"   \n[66] \"protect\"    \"provide\"    \"public\"     \"pull\"       \"purpose\"   \n[71] \"push\"       \"put\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#grep-and-grepl",
    "href": "0200200-a-introToRegularExpressions-v036.html#grep-and-grepl",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.6 grep and grepl",
    "text": "19.6 grep and grepl\n\n# When value=FALSE grep returns the positions in the vector of \n# values that matched\ngrep(stringr::words, pattern=\"^p.*y$\", value=TRUE)\n\n[1] \"party\"  \"pay\"    \"play\"   \"policy\" \"pretty\"\n\ngrep(stringr::words, pattern=\"^p.*y$\", value=FALSE)\n\n[1] 604 607 623 628 643\n\n# default is value=FALSE\ngrep(stringr::words, pattern=\"^p.*y$\", value=FALSE)\n\n[1] 604 607 623 628 643\n\n\n\n#@ grep and grepl\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#@ grep stands for \"Globally search for a Regular Expression and Print the result\"\n#@\n#@ Grep will search through the entries in a character vector and display those\n#@ entries that match a specified pattern (see examples below). These patterns\n#@ are known as regular expressions or \"regex\".\n#@\n#@ The history of grep started with a a command that was used on the Unix operating\n#@ system. It has been adapted for use with many programming environments. R has\n#@ its own version.\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n\n# grep ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# grep returns character values or the indexes (i.e. position numbers) \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Find all fruit whose name contains the letter \"h\"\ngrep(pattern=\"h\", x=fruit, value=TRUE)   # value=TRUE, show the acutal values that match the pattern \n\n[1] \"cherry\"       \"black cherry\" \"peach\"        \"honeydew\"    \n\ngrep(pattern=\"h\", x=fruit, value=FALSE)  # value=FALSE, show the index (ie. position) of the values that match \n\n[1]  9 10 11 17\n\n# grepl ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# grepl returns logical values (i.e. TRUE/FALSE vectors)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrepl(pattern=\"h\", x=fruit)    # find which values include an \"h\"\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE\n[13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#summary-3-ways-to-use-grep-or-grepl",
    "href": "0200200-a-introToRegularExpressions-v036.html#summary-3-ways-to-use-grep-or-grepl",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.7 Summary: 3 ways to use grep or grepl",
    "text": "19.7 Summary: 3 ways to use grep or grepl\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# - grep ( regexPattern , value=TRUE)  # returns the actual values that match  \n# - grep ( regexPattern , value=FALSE) # returns the index numbers of the values that match  \n# - grepl ( regexPattern )              # returns a logical vector that indicate which values match  \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# For now, let's focus on grep(... , value=TRUE) as it is easier to understand the results. \n\n\n# The pattern is searched for in the entire entry ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The pattern is considered \"matched\" if it appears anywhere in the data value.\n# For example:   grep(\"h\", fruit)\n#\n# returns all fruit that contain an \"h\", no matter whether the h is at the \n# beginning, end or middle of the word.\n#\n# You can change this behavior with the ^ and $ metacharacters (see below)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#spaces-are-not-ignored.",
    "href": "0200200-a-introToRegularExpressions-v036.html#spaces-are-not-ignored.",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.8 Spaces are NOT ignored.",
    "text": "19.8 Spaces are NOT ignored.\nSpaces count as part of the pattern. They are NOT ignored.\n\n# fruits that contain a space\ngrep(pattern=\" \", x=fruit, value=TRUE) # all fruit that contain a space\n\n[1] \"N. American apple\" \"S. Korean Fig\"     \"star fruit\"       \n[4] \"prickly pear\"      \"Beurre Hardy pear\" \"black cherry\"     \n\n\n\n# search for \"k \"  (i.e. k followed by a space)\ngrep(\"k \", fruit, value=TRUE) # \"black cherry\"\n\n[1] \"black cherry\"\n\n# search for \"k\"  (i.e. without a space - JUST a \"k\")\ngrep(\"k\", fruit, value=TRUE) # \"prickly pear\"  \"black cherry\"  \"kumquat\"\n\n[1] \"prickly pear\" \"black cherry\" \"kumquat\"     \n\n# search for \"ck\" \ngrep(\"ck\", fruit, value=TRUE) # \"prickly pear\"  \"black cherry\"  \"kumquat\"\n\n[1] \"prickly pear\" \"black cherry\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#regex-patterns-do-not-understand-numbers",
    "href": "0200200-a-introToRegularExpressions-v036.html#regex-patterns-do-not-understand-numbers",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.9 regex patterns do NOT understand “numbers”",
    "text": "19.9 regex patterns do NOT understand “numbers”\n\n# regex patterns do NOT understand \"numbers\" ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Digits are NOT treated as numbers. They are treated the same as any\n# other character. Therefore grep(\"12\", SOME_VECTOR) will match any value\n# that contains a 1 followed by a 2, including \"123\" and \"34321234\".\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\naddresses # show all the addresses\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\ngrep(\"23\", addresses, value=TRUE)  # matches anything that contains 23\n\n[1] \"12345 Sesame Street\"         \"9 Main St. apt. 623\"        \n[3] \"Two Main Street Apt 123c\"    \"Three Main Street Apt 12343\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#case-sensitivity",
    "href": "0200200-a-introToRegularExpressions-v036.html#case-sensitivity",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.10 case sensitivity",
    "text": "19.10 case sensitivity\nBy default, R’s version of grep is case sensitive.\nThere are a few different approaches for changing the default behavior to instead search case-INsensitively.\n\n19.10.1 (a) case INsensitive searches - use ignore.case argument\nThe first way - use ignore.case = TRUE. See the code below.\n\ngrep(\"H\",fruit, value=TRUE)  # contains a capital \"H\"\n\n[1] \"Beurre Hardy pear\"\n\ngrep(\"h\",fruit, value=TRUE)  # contains a lowercase \"h\"\n\n[1] \"cherry\"       \"black cherry\" \"peach\"        \"honeydew\"    \n\ngrep(\"h\", fruit, value=TRUE, ignore.case=TRUE) # contains AnY h\n\n[1] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n[4] \"peach\"             \"honeydew\"         \n\ngrep(\"H\", fruit, value=TRUE, ignore.case=TRUE) # same thing\n\n[1] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n[4] \"peach\"             \"honeydew\"         \n\n\n\n\n19.10.2 (b) case INsensitive searches - character classes - e.g. [aA]\nAnother way to search for for both CAPITAL and lowercase characters, e.g. [Hh] For example, [hH] indicates that h or H is valid to be matched. We will describe the exact meaning of the [square brackets] in a lot more detail below.\n\ngrep(\"[hH]\", fruit, value=TRUE)\n\n[1] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n[4] \"peach\"             \"honeydew\"         \n\n\n\n\n19.10.3 (c) case INsensitive searches - use toupper() and tolower()\nanother way using R’s toupper or tolower functions\n\nmsg = \"She said 'Hello' to Joe.\"\nmsg\n\n[1] \"She said 'Hello' to Joe.\"\n\ntoupper(msg)\n\n[1] \"SHE SAID 'HELLO' TO JOE.\"\n\ntolower(msg)\n\n[1] \"she said 'hello' to joe.\"\n\ngrep(\"h\", tolower(fruit), value=TRUE)\n\n[1] \"beurre hardy pear\" \"cherry\"            \"black cherry\"     \n[4] \"peach\"             \"honeydew\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#str_view-from-the-stringr-package",
    "href": "0200200-a-introToRegularExpressions-v036.html#str_view-from-the-stringr-package",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.11 str_view() from the stringr package",
    "text": "19.11 str_view() from the stringr package\nThe str_view function from the stringr package can be very helpful when you’re trying to understand a regular expression. str_view shows exactly what parts of a string match the pattern. See the example below.\n\n# str_view is part of the stringr package\nlibrary(stringr)\n\ngreetings = c(\"hi there\", \"yo dude\", \"shalom\", \"bon jour\")\ncat(greetings, sep=\"\\n\")\n\nhi there\nyo dude\nshalom\nbon jour\n\n# match the letter h in each greeting\nstr_view(greetings, \"h\")\n\n[1] │ &lt;h&gt;i t&lt;h&gt;ere\n[3] │ s&lt;h&gt;alom",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#sub-and-gsub-functions",
    "href": "0200200-a-introToRegularExpressions-v036.html#sub-and-gsub-functions",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.12 sub and gsub functions",
    "text": "19.12 sub and gsub functions\nsub and gsub functions are used to “substitute” the text that was matched by a regular expression with other text.\nThe difference between sub() and gsub() is that in a single character value, sub() function only substitutes the first part of the character value that matched the regex. By contrast, the gsub() function replaces EVERY part of the character value that matched the regex. (the “g” in “gsub” stands for “global”). See the examples below.\nIMPORTANT - both sub and gsub return the ENTIRE vector with only the values matched the regex being changed. This is different from the grep and grepl functions that returned only those entries in the vector that matched the regular expression.\n\n#@ sub and gsub functions ####\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#@\n#@ sub (SOME_REGEX_PATTERN, REPLACMENT, SOME_VECTOR)\n#@    sub returns a new vector. The return value is the same as SOME_VECTOR\n#@    except that the FIRST match of the pattern in each entry of SOME_VECTOR\n#@    is replaced with REPLACEMENT - see the examples below.\n#@\n#@ gsub (SOME_REGEX_PATTERN, REPLACMENT, SOME_VECTOR)\n#@    same as sub but ALL matches of the pattern are replaced (not just the\n#@    first in each entry of the the vector - see the exmaples below\n#@\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# replace the first letter \"e\" that appears in any fruit with the letter \"X\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\nsub(pattern=\"e\", replacement=\"X\", x=fruit)  # \"applX\"    \"N. AmXrican apple\"     etc\n\n [1] \"applX\"             \"N. AmXrican apple\" \"S. KorXan Fig\"    \n [4] \"fig\"               \"star fruit\"        \"pXar\"             \n [7] \"prickly pXar\"      \"BXurre Hardy pear\" \"chXrry\"           \n[10] \"black chXrry\"      \"pXach\"             \"plum\"             \n[13] \"kumquat\"           \"banana\"            \"bluXberry\"        \n[16] \"strawbXrry\"        \"honXydew\"          \"strawbXrries\"     \n[19] \"yumbXrry\"         \n\n\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# replace ALL of the \"e\"s that appears in any fruit with the letter \"x\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\ngsub(pattern=\"e\", replacement=\"X\", fruit)   # \"applX\"    \"N. AmXrican applX\"     etc\n\n [1] \"applX\"             \"N. AmXrican applX\" \"S. KorXan Fig\"    \n [4] \"fig\"               \"star fruit\"        \"pXar\"             \n [7] \"prickly pXar\"      \"BXurrX Hardy pXar\" \"chXrry\"           \n[10] \"black chXrry\"      \"pXach\"             \"plum\"             \n[13] \"kumquat\"           \"banana\"            \"bluXbXrry\"        \n[16] \"strawbXrry\"        \"honXydXw\"          \"strawbXrriXs\"     \n[19] \"yumbXrry\"         \n\n\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# remove all spaces from the addresses\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\ngsub(pattern=\" \", replacement=\"\", addresses)   # \"12345SesameStreet\"   \"OneMicro$oftWay\"   etc.\n\n [1] \"12345SesameStreet\"               \"OneMicro$oftWay\"                \n [3] \"3OliveSt.\"                       \"Two1stAve.\"                     \n [5] \"5678ParkPlace\"                   \"FortyFive2ndStreet\"             \n [7] \"NinetyNineConeSt.apartment7\"     \"9MainSt.apt.623\"                \n [9] \"FiveGoogleDrive\"                 \"4\\\\2RechovYafo\"                 \n[11] \"FifteenWatchamacallitBoulevard\"  \"NineteenWatchamacallitBoulevard\"\n[13] \"OneMainStreetApt12b\"             \"TwoMainStreetApt123c\"           \n[15] \"ThreeMainStreetApt12343\"         \"CityHallLockport,NY\"            \n\n\nWe will revisit sub and gsub later with more complex examples …",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#strsplit-function",
    "href": "0200200-a-introToRegularExpressions-v036.html#strsplit-function",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.13 strsplit function",
    "text": "19.13 strsplit function\nstrsplit() is used to split a string based on a “delimeter” that appears between the different values. This “delimeter” can be a regular expression. We’ll come back to strsplit later, but let’s introduce it here.\n\nsentences = c(\"He said hi. She said bye. We went to the park.\",\n              \"I like ice cream! Do you? Sue likes pizza.\")\nsentences\n\n[1] \"He said hi. She said bye. We went to the park.\"\n[2] \"I like ice cream! Do you? Sue likes pizza.\"    \n\n\n\n#------------------------------------------------------------------------.\n# QUESTION - \n# Use strsplit to split the values in the sentences vector by \n# splitting based on spaces. Assign the result to the varible \"sentenceWords\".\n#\n# Write code to get the 3rd \"word\" from the 1st entry in the sentences \n# vector.\n#------------------------------------------------------------------------.\n\n# ANSWER\nsentenceWords = strsplit(sentences, split=\" \")\nsentenceWords\n\n[[1]]\n [1] \"He\"    \"said\"  \"hi.\"   \"She\"   \"said\"  \"bye.\"  \"We\"    \"went\"  \"to\"   \n[10] \"the\"   \"park.\"\n\n[[2]]\n[1] \"I\"      \"like\"   \"ice\"    \"cream!\" \"Do\"     \"you?\"   \"Sue\"    \"likes\" \n[9] \"pizza.\"\n\n# Notice that the result is a LIST:\nstr(sentenceWords)\n\nList of 2\n $ : chr [1:11] \"He\" \"said\" \"hi.\" \"She\" ...\n $ : chr [1:9] \"I\" \"like\" \"ice\" \"cream!\" ...\n\n# Show the 3rd word in the 1st sentence\nsentenceWords[[1]][3]\n\n[1] \"hi.\"\n\n\n\n— practice —\n\n#------------------------------------------------------------------------.\n# QUESTION - split each entry in the sentences variable into individual \n# sententces. \n# \n# WARNING - the value of the split argument is interpreted as a\n# regular expression pattern. Be careful.\n#------------------------------------------------------------------------.\n\n# ANSWER\n\n# 1st attempt - doesn't work. \nstrsplit(sentences, \".\")\n\n[[1]]\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[26] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n[[2]]\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n[26] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n# This doesn't work since the 2nd argument is a regular expression.\n# The following will split based on periods.\nsentences\n\n[1] \"He said hi. She said bye. We went to the park.\"\n[2] \"I like ice cream! Do you? Sue likes pizza.\"    \n\nstrsplit(sentences, \"\\\\.\")\n\n[[1]]\n[1] \"He said hi\"           \" She said bye\"        \" We went to the park\"\n\n[[2]]\n[1] \"I like ice cream! Do you? Sue likes pizza\"\n\n# Use a \"regular expression\" to instead split on any of a period, \n# question mark, or exclamation point.\nsentences\n\n[1] \"He said hi. She said bye. We went to the park.\"\n[2] \"I like ice cream! Do you? Sue likes pizza.\"    \n\nstrsplit(sentences, \"[.?!]\")  # split on any one of .?!\n\n[[1]]\n[1] \"He said hi\"           \" She said bye\"        \" We went to the park\"\n\n[[2]]\n[1] \"I like ice cream\" \" Do you\"          \" Sue likes pizza\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#other-functions-regmatches-regexec-regexpr-gregexpr",
    "href": "0200200-a-introToRegularExpressions-v036.html#other-functions-regmatches-regexec-regexpr-gregexpr",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.14 Other functions: regmatches, regexec, regexpr, gregexpr",
    "text": "19.14 Other functions: regmatches, regexec, regexpr, gregexpr\nThe following are other function in Base R that use regex. These are a little more advanced. It’s probably better to try researching these functions after first understanding the material presented in this section.\nYou can search online or see the R documentation for more info about these functions.\n\nregmatches\nregexec\nregexpr\ngregexpr",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#stringr-functions",
    "href": "0200200-a-introToRegularExpressions-v036.html#stringr-functions",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.15 stringr functions",
    "text": "19.15 stringr functions\nThe stringr package includes many functions for use with character vectors. One example is str_length, which is very similar to the nchar() function in Base R.\n\n# The str_length function is part of the stringr package.\n# To use it you must install stringr (or install tidyverse, which is a \n# collection of packages one of which is stringr)\nstr_length(c(\"abc\", \"hello\", \"I like ice cream!\"))\n\n[1]  3  5 17\n\n# This function is very similar to the\n# nchar function that is built into base R.\nnchar(c(\"abc\", \"hello\", \"I like ice cream!\"))  \n\n[1]  3  5 17\n\n\nThe stringr package also includes numerous functions that make use of regular expressions. The following is a table of the stringr functions and the Base R functions that can be used to accomplish similar things.\n\n\n\n\n\n\n\n\nstringr\nBase R\nDescription\n\n\n\n\nstr_detect()\ngrepl()\nReturns TRUE/FALSE if pattern is found\n\n\nstr_extract(), str_extract_all()\nregmatches()\nExtract matching patterns\n\n\nstr_match(), str_match_all()\nregexec(), regmatches()\nExtract matched groups\n\n\nstr_replace(), str_replace_all()\nsub(), gsub()\nReplace matched patterns\n\n\nstr_split()\nstrsplit()\nSplit string on pattern\n\n\nstr_subset()\ngrep(value = TRUE)\nKeep strings matching pattern\n\n\nstr_locate(), str_locate_all()\nregexpr(), gregexpr()\nFind positions of matches\n\n\nstr_count()\nlengths(regmatches())\nCount pattern occurrences",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#anchors-and",
    "href": "0200200-a-introToRegularExpressions-v036.html#anchors-and",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.16 “anchors” ^ and $",
    "text": "19.16 “anchors” ^ and $\n\n# ^ and $    ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Regular Expressions use special characters to control what is matched.\n# These characters are called \"meta-characters\".\n#\n#   ^  \"matches\" the start of the character value\n#   $  \"matches\" the end of the character value\n#\n#   The [square brackets] described above are also \"meta characters\" in\n#   regular expressions. (We will describe those in more detail next)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(pattern=\"^a\", x=fruit, value=TRUE)  # find fruits that START with an \"a\"\n\n[1] \"apple\"\n\ngrep(\"a$\", fruit, value=TRUE)  # find fruits that END with an \"a\"\n\n[1] \"banana\"\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a command using grep to display all the fruits that start with\n# with a c or an s. Make your search case insensitive.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#         ANSWERS ####\ngrep(\"^[csCS]\", fruit, value=TRUE)\n\n[1] \"S. Korean Fig\" \"star fruit\"    \"cherry\"        \"strawberry\"   \n[5] \"strawberries\" \n\ngrep(\"^[cs]\", fruit, value=TRUE, ignore.case = TRUE)\n\n[1] \"S. Korean Fig\" \"star fruit\"    \"cherry\"        \"strawberry\"   \n[5] \"strawberries\" \n\ngrep(\"^[cs]\", tolower(fruit), value=TRUE)\n\n[1] \"s. korean fig\" \"star fruit\"    \"cherry\"        \"strawberry\"   \n[5] \"strawberries\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#periods",
    "href": "0200200-a-introToRegularExpressions-v036.html#periods",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.17 periods",
    "text": "19.17 periods\n\n# . (i.e. a period) ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# a period (ie  .  ) \"matches\" any single character.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(\"^.a\", fruit, value=TRUE)  # 2nd letter is a\n\n[1] \"banana\"\n\ngrep(\"^..a\", fruit, value=TRUE)  # 3rd letter is a\n\n[1] \"star fruit\"   \"pear\"         \"black cherry\" \"peach\"       \n\ngrep(\"a.$\", fruit, value=TRUE)  # 2nd to last letter is an a\n\n[1] \"pear\"              \"prickly pear\"      \"Beurre Hardy pear\"\n[4] \"kumquat\"          \n\ngrep(\"....\", fruit, value=TRUE) # all fruit that are AT LEAST 4 characters long\n\n [1] \"apple\"             \"N. American apple\" \"S. Korean Fig\"    \n [4] \"star fruit\"        \"pear\"              \"prickly pear\"     \n [7] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n[10] \"peach\"             \"plum\"              \"kumquat\"          \n[13] \"banana\"            \"blueberry\"         \"strawberry\"       \n[16] \"honeydew\"          \"strawberries\"      \"yumberry\"         \n\ngrep(\"^....$\", fruit, value=TRUE) # all fruit that are EXACTLY 4 characters long\n\n[1] \"pear\" \"plum\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#character-classes-eg.-abc",
    "href": "0200200-a-introToRegularExpressions-v036.html#character-classes-eg.-abc",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.18 “character classes” eg. [abc]",
    "text": "19.18 “character classes” eg. [abc]\n\n# [abc] matches a SINGLE \"a\", \"b\" or \"c\".    ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# characters in [square brackets] match a single copy of\n# any of those characters, e.g.\n#  [abc]    matches exactly one of a,b or c\n#\n# DEFINITION: \n# The [square brackets] with the characters in them are often referred to as \n# \"character classes\" or \"character sets\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n19.18.1 Examples\n\ngrep(\"[qbxz]\", fruit, value=TRUE)  # find fruit that contain q,b,x or z\n\n[1] \"black cherry\" \"kumquat\"      \"banana\"       \"blueberry\"    \"strawberry\"  \n[6] \"strawberries\" \"yumberry\"    \n\ngrep(\"[aeiou][aeiou]\", fruit, value=TRUE)  # two vowels in a row\n\n[1] \"S. Korean Fig\"     \"star fruit\"        \"pear\"             \n[4] \"prickly pear\"      \"Beurre Hardy pear\" \"peach\"            \n[7] \"kumquat\"           \"blueberry\"         \"strawberries\"     \n\ngrep(\"^.[aeiou][aeiou]\", fruit, value=TRUE)  # vowels in the 2nd and 3rd positions\n\n[1] \"pear\"              \"Beurre Hardy pear\" \"peach\"            \n\ngrep(\"[aeiou][aeiou].$\", fruit, value=TRUE)  # 2nd & 3rd to last characters are vowels\n\n[1] \"star fruit\"        \"pear\"              \"prickly pear\"     \n[4] \"Beurre Hardy pear\" \"kumquat\"           \"strawberries\"     \n\ngrep(\"[aeiou][aeiou][aeiou]\", fruit, value=TRUE)  # 3 vowels in a row\n\ncharacter(0)\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# replace the FIRST vowel that appears in any fruit with the letter \"X\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# replace ALL vowels that appears in any fruit with the letter \"X\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\ngsub(\"[aeiouAEIOU]\", \"X\", fruit)\n\n [1] \"XpplX\"             \"N. XmXrXcXn XpplX\" \"S. KXrXXn FXg\"    \n [4] \"fXg\"               \"stXr frXXt\"        \"pXXr\"             \n [7] \"prXckly pXXr\"      \"BXXrrX HXrdy pXXr\" \"chXrry\"           \n[10] \"blXck chXrry\"      \"pXXch\"             \"plXm\"             \n[13] \"kXmqXXt\"           \"bXnXnX\"            \"blXXbXrry\"        \n[16] \"strXwbXrry\"        \"hXnXydXw\"          \"strXwbXrrXXs\"     \n[19] \"yXmbXrry\"         \n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# remove ALL of the vowels that appear in any fruit\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\ngsub(\"[aeiouAEIOU]\", \"\", fruit)\n\n [1] \"ppl\"         \"N. mrcn ppl\" \"S. Krn Fg\"   \"fg\"          \"str frt\"    \n [6] \"pr\"          \"prckly pr\"   \"Brr Hrdy pr\" \"chrry\"       \"blck chrry\" \n[11] \"pch\"         \"plm\"         \"kmqt\"        \"bnn\"         \"blbrry\"     \n[16] \"strwbrry\"    \"hnydw\"       \"strwbrrs\"    \"ymbrry\"     \n\n\n\n\n19.18.2 dashes, e.g. [a-d]\n\n# Specify ranges with dash, e.g. [a-d] or [0-3] or [a-d0-3], etc ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Specify ranges with dash, e.g. [a-d] is same as [abcd], [0-3] is same as [0123] ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n19.18.3 Examples\n\ngrep(\"[j-m]\", fruit, value=TRUE)  # fruits that contain any of j,k,l,m\n\n[1] \"apple\"             \"N. American apple\" \"prickly pear\"     \n[4] \"black cherry\"      \"plum\"              \"kumquat\"          \n[7] \"blueberry\"         \"yumberry\"         \n\naddresses  # this was defined above\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\ngrep(\"[0-9]\", addresses, value=TRUE)  # contains a digit\n\n [1] \"12345 Sesame Street\"              \"3 Olive St.\"                     \n [3] \"Two 1st Ave.\"                     \"5678 Park Place\"                 \n [5] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n [7] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n [9] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n[11] \"Three Main Street Apt 12343\"     \n\ngrep(\"[6-9]\", addresses, value=TRUE)  # contains 6,7,8 or 9\n\n[1] \"5678 Park Place\"                  \"Ninety Nine Cone St. apartment 7\"\n[3] \"9 Main St. apt. 623\"             \n\ngrep(\"[0-9][0-9]\", addresses, value=TRUE)  # contains a number with at least 2 digits\n\n[1] \"12345 Sesame Street\"         \"5678 Park Place\"            \n[3] \"9 Main St. apt. 623\"         \"One Main Street Apt 12b\"    \n[5] \"Two Main Street Apt 123c\"    \"Three Main Street Apt 12343\"\n\ngrep(\"[0-9][0-9][0-9][0-9]\", addresses, value=TRUE)  # contains a # with at least 4 digits \n\n[1] \"12345 Sesame Street\"         \"5678 Park Place\"            \n[3] \"Three Main Street Apt 12343\"\n\ngrep(\"^[6-9]\", addresses, value=TRUE)  # contains 6,7,8 or 9 as the first character\n\n[1] \"9 Main St. apt. 623\"\n\ngrep(\"^.[6-9]\", addresses, value=TRUE) # 6,7,8 or 9 is second character\n\n[1] \"5678 Park Place\"\n\ngrep(\"[0-9]$\", addresses, value=TRUE) # last character is a digit\n\n[1] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n[3] \"Three Main Street Apt 12343\"     \n\ngrep(\"[0-9][0-9][0-9]$\", addresses, value=TRUE) # ends with at least 3 digits\n\n[1] \"9 Main St. apt. 623\"         \"Three Main Street Apt 12343\"\n\ngrep(\"[0-9][0-9][0-9][0-9]$\", addresses, value=TRUE) # ends with at least 4 digits\n\n[1] \"Three Main Street Apt 12343\"\n\n\n\n\n19.18.4 combine multiple ranges one [brackets]\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You can combine multiple ranges and values in a single [brackets]\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(\"[1-3x-z]$\", addresses, value=TRUE) # ends with 1,2,3,x,y or z\n\n[1] \"One Micro$oft Way\"           \"9 Main St. apt. 623\"        \n[3] \"Three Main Street Apt 12343\"\n\ngrep(\"[of-hq]\", fruit, value=TRUE)  # seraches for any of o,f,g,h,q\n\n[1] \"S. Korean Fig\" \"fig\"           \"star fruit\"    \"cherry\"       \n[5] \"black cherry\"  \"peach\"         \"kumquat\"       \"honeydew\"     \n\n# REMEMBER THE [BRACKETS]!!!\ngrep(\"of-hq\", fruit, value=TRUE)  # searches for exactly :  \"of-hq\"\n\ncharacter(0)\n\n\n\n\n19.18.5 [^abc]\n\n# [^abc] matches a single character that is NOT a,b or c ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# When ^ is the FIRST character in the [^brackets] it means to match a \n# single character that is NOT one of the characters in the brackets\n#\n# [^abc]  - i.e. a single character that is NOT a,b or c\n# [^0-9]  - a non-digit\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(\"^[^0-9]\", addresses, value=TRUE)  # does NOT start with a digit\n\n [1] \"One Micro$oft Way\"                 \"Two 1st Ave.\"                     \n [3] \"Forty Five 2nd Street\"             \"Ninety Nine Cone St. apartment 7\" \n [5] \"Five Google Drive\"                 \"Fifteen Watchamacallit Boulevard\" \n [7] \"Nineteen Watchamacallit Boulevard\" \"One Main Street Apt 12b\"          \n [9] \"Two Main Street Apt 123c\"          \"Three Main Street Apt 12343\"      \n[11] \"City Hall Lockport, NY\"           \n\ngrep(\"[^0-9]$\", addresses, value=TRUE)  # does NOT end with a digit\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n [9] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[11] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[13] \"City Hall Lockport, NY\"           \n\ngrep(\"[^0-9]\", addresses, value=TRUE)\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\n# Contains 5 non-vowels in a row (notice that space counts as a non-vowel)\ngrep(\"[^aeiou][^aeiou][^aeiou][^aeiou][^aeiou]\", fruit, value=TRUE, ignore.case=TRUE)\n\n[1] \"prickly pear\"      \"Beurre Hardy pear\" \"black cherry\"     \n\n\n\n\n19.18.6 metachars lose special meaning in brackets eg. [.$*]\n\n# meta characters in [brackets] other than ^ - and ] lose their special meaning ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Most meta characters inside of [brackets] are treated like any other character.\n# They do NOT have any special meaning in the brackets. Therefore you can use\n# them without any problem inside a character class. For example [.$]\n# matches either a period or a dollar sign (see exmaples below).\n#\n# The only exceptions are ^ - and ] which DO have a special meaning inside of\n# the [square brackets] - see more info below.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nstuff = c(\"This is a period: .\", \"apple\", \"45 oranges\", \"$2\", \"This is an open bracket: [\")\nstuff\n\n[1] \"This is a period: .\"        \"apple\"                     \n[3] \"45 oranges\"                 \"$2\"                        \n[5] \"This is an open bracket: [\"\n\n# EXAMPLE\n#\n# Match a period, left-square-bracket, or a dollar sign\n# You do NOT need backslashes inside of the [brackets]\ngrep(\"[.[$]\", stuff, value=TRUE)\n\n[1] \"This is a period: .\"        \"$2\"                        \n[3] \"This is an open bracket: [\"\n\n# ANOTHER EXAMPLE\n#\n# The following matches either a period or a digit.\n# You do NOT need to use a backslash before the period.\ngrep(\"[.1-9]\", stuff, value=TRUE)\n\n[1] \"This is a period: .\" \"45 oranges\"          \"$2\"                 \n\n# The backslash will not hurt (but it isn't necessary inside the character class)\n# (below we will explain why there are TWO backslashes - for now you can leave\n# off both of the backslashes)\ngrep(\"[\\\\.1-9]\", stuff, value=TRUE)\n\n[1] \"This is a period: .\" \"45 oranges\"          \"$2\"                 \n\n\n\n\n19.18.7 Special cases: ^ - ]\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Special cases:    ^    -    ]\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The following characters have to be addressed in a special way inside\n# of a character class. \n#\n# ^   As we saw above, if the first character in the brackets is ^ the regex will\n#     look for characters that are NOT in the brackets. If ^ appears anywhere else\n#     inside the brackets it has no special meaning.\n#\n# -   As we saw above, [a-d] is the same as [abcd]. Therefore the - has a special\n#     meaning inside of a character class. If you want to actually search\n#     for a -, it must be the first, e.g. [-abc] or last character, eg. [abc-]\n#     in the class.\n#\n# ]   has a special meaning - i.e. to end the character class. Therefore if \n#     you want to seach for an actual \"]\", the \"]\" should be specified\n#     as the very FIRST character in the class, e.g. []abc]\n#-------------------------------------------------------------------\n\n# Examples:\n\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n# The \".\" inside of [brackets] simply means an actual period.\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\nstuff = c(\"...\", \"def\", \"...bbbzzzbz.bzz...z.b\", \"^^^\")\nstuff\n\n[1] \"...\"                   \"def\"                   \"...bbbzzzbz.bzz...z.b\"\n[4] \"^^^\"                  \n\ngrep (\"[.x]\", stuff, value=TRUE)\n\n[1] \"...\"                   \"...bbbzzzbz.bzz...z.b\"\n\n\n\n\n19.18.8 matching ^ inside [square brackets]\n\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n# The \"^\" inside of [brackets] has a different meaning if it is in the \n# first position or if it is anywhere else. For example:\n#\n# [^abc]   matches anything that is NOT an \"a\",\"b\" or \"c\"\n#\n# [a^bc]   \n# [abc^]   both of these examples matches one \"a\",\"b\",\"c\" or \"^\" character\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n\ncaretStuff = c(\"^^^\", \"hello\", \"???\")\ncaretStuff\n\n[1] \"^^^\"   \"hello\" \"???\"  \n\ngrep (\"[^a-z]\", caretStuff, value=TRUE)   # \"^^^\" \"???\"\n\n[1] \"^^^\" \"???\"\n\ngrep (\"[a-z^]\", caretStuff, value=TRUE)   # \"^^^\" \"hello\"\n\n[1] \"^^^\"   \"hello\"\n\ngrep (\"[^^]\", caretStuff, value=TRUE)     # \"hello\" \"???\"\n\n[1] \"hello\" \"???\"  \n\n# find all entries that have any symbol that is not a \".\", \"b\" or \"z\"\n\nstuff = c(\"...\", \"def\", \"...bbbzzzbz.bzz...z.b\", \"^^^\")\nstuff\n\n[1] \"...\"                   \"def\"                   \"...bbbzzzbz.bzz...z.b\"\n[4] \"^^^\"                  \n\ngrep (\"[^.bz]\", stuff, value=TRUE)\n\n[1] \"def\" \"^^^\"\n\n#grep(\"[a-z.]\", c())\n\n\n\n19.18.9 Matching a - inside [square brackets]\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Matching a dash (i.e. - ) inside a character class\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# As we saw earlier, inside of [bracketes] a dash special meaning (to indicate a range).\n# To actually match a dash as one of the characters place the dash as either the \n# first or last character in the brackets.\n\n\ndashStuff = c(\"---\", \"hello\", \"xxx\")\ndashStuff\n\n[1] \"---\"   \"hello\" \"xxx\"  \n\ngrep (\"[-xyz]\", dashStuff, value=TRUE)   # \"---\" \"xxx\"\n\n[1] \"---\" \"xxx\"\n\ngrep (\"[xyz-]\", dashStuff, value=TRUE)   # \"---\" \"xxx\"   (same thing)\n\n[1] \"---\" \"xxx\"\n\ngrep (\"[a-f]\", dashStuff, value=TRUE)   # \"hello\"\n\n[1] \"hello\"\n\ngrep (\"[-a-f]\", dashStuff, value=TRUE)   # \"---\" \"hello\"\n\n[1] \"---\"   \"hello\"\n\n\n\n\n19.18.10 Matching a ] inside [square brackets]\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# To match a closing-square-bracket \"]\" inside a character class you must\n# specify the ] as the very FIRST symbol in the character class.\n#\n# NOTE - there are no special rules for matching an open-square-bracket, \"[\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nstuff = c(\"]\", \"apple\", \"zzz\" )\nstuff\n\n[1] \"]\"     \"apple\" \"zzz\"  \n\ngrep(\"]\", stuff, value=TRUE)    #  \"]\"\n\n[1] \"]\"\n\n# The pattern \"[]a]\" matches a single character that is either \"]\" or \"a\".\n# It finds \"]\" and also \"apple\" (since \"apple\" it contains an \"a\").\n#\n# This works since \"]\" is placed as the very first character in the \n# [brackets] so it is simply one of the characters that is searched for.\n\ngrep(\"[]a]\", stuff, value=TRUE) #  \"]\"  \"apple\"\n\n[1] \"]\"     \"apple\"\n\n# This is VERY different for the pattern \"[a]]\"\n#\n# The following example shows what happens if you put the \"]\" in any\n# position other than the first. The pattern \"[a]]\" is broken down as follows:\n#\n#   [a]   This is the single character \"a\". Note\n#\n#   ]     This does NOT signify the end of the character class, but is \n#         rather just a regular character that must be part of the text to be \n#         matched.\n#\n# Therefore [a]] is looking for the EXACT text \"a]\" somewhere in the text\n# being searched.\n\ngrep(\"[a]]\", stuff, value=TRUE) # No matches - looking for \"a]\" in the text\n\ncharacter(0)",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#searching-with-regular-expressions-in-a-text-editor",
    "href": "0200200-a-introToRegularExpressions-v036.html#searching-with-regular-expressions-in-a-text-editor",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.19 Searching with regular expressions in a “text editor”",
    "text": "19.19 Searching with regular expressions in a “text editor”\nA “text editor” is a program that is used to edit “text files”. A text file can only contain “plain text” - i.e. no pictures, no music, only one font,\nRStudio’s text editor\nThe text editor that is in RStudio can be used to create many different types of files. For example, it can be used to create both “R Script files” (i.e. .R files) and “Quarto Documents” (i.e. .qmd files). In addition, it can be used to create “plain text files”. To do so, choose the following menu choices from RStudio’s menu: “File | New File | Text File”\n\n# NOTE: regular expressions are used in many different languages and environments.\n# In general in regular expression in OTHER environments, \n# if you want to actually match a metacharacter (e.g. period, parentheses,\n# caret, dollar sign, etc) you precede the metacharacter with a backslash.\n#\n# For example, you can do this in RStudios text editor - just type ctrl-f or cmd-f\n# and click \"regex\" checkbox. Then type your regular expression into the \n# search box.\n# For example\n# Try searching the addresses.txt file for the following in the RStudio text\n# editor:\n#\n#   one|1\n#   .\n#   \\.\n#   $\n#   \\$\n#\n# Matching meta characters requires that you \"escape\" the meta-character\n# by preceding it with a backslash e.g.  \\.",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#matching-meta-characters",
    "href": "0200200-a-introToRegularExpressions-v036.html#matching-meta-characters",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.20 Matching meta-characters",
    "text": "19.20 Matching meta-characters\n\n# Matching meta-characters ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# NOTE: regular expressions are used in many different languages and environments.\n# In general in regular expression in OTHER environments, \n# if you want to actually match a metacharacter (e.g. period, parentheses,\n# caret, dollar sign, etc) you precede the metacharacter with a backslash.\n#\n# For example, you can do this in RStudios text editor - just type ctrl-f or cmd-f\n# and click \"regex\" checkbox. Then type your regular expression into the \n# search box.\n#\n# For example\n# Try searching the addresses.txt file for the following in the RStudio text\n# editor:\n#\n#   one|1\n#   .\n#   \\.\n#   $\n#   \\$\n#\n# Matching meta characters requires that you \"escape\" the meta-character\n# by preceding it with a backslash e.g.  \\.\n#\n#\n#\n# When writing regular expression patterns in R you must use TWO \\\\'s to escape a metacharacter ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# HOWEVER, in R, since character values already use a backslash\n# such as \\n for a new line, you must use TWO backslashes in the regex\n# pattern. The first backslash escapes the 2nd backslash from R\n# so that R's character values don't interpret it in a special way.\n# See the examples below.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n19.20.1 Reminder of how backslashes () are used in R\n\n# Reminder of how backslashes (\\) are used in R ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Remember that R uses backslashes to change the meaning (or to \"escape\" \n# the meaning) of the character that follows the backslash. \n# For example in the following cat command, \\n, is displayed as a \n# \"newline character\" and \\t is displayed as a tab.\n\ncat(\"Hello\\nJoe\\thow are you\\n\\ndoing?\\n\\tI'm fine.\")\n\nHello\nJoe how are you\n\ndoing?\n    I'm fine.\n\n# Similarly in the following cat command the \\\" escapes the meaning\n# of the quote. It no longer implies the end of the quotation. The \n# meaning of \\\" is simply to include a quotation mark as part of the \n# text.\n\ncat(\"Lincoln said \\\"Four score and seven years ago today...\\\"\")\n\nLincoln said \"Four score and seven years ago today...\"\n\n# If the following line were not commented it would cause an error\n# because the quotation is not actually closed due to the \\ before the\n# final quotation mark.\n#\n#cat(\"This is a backslash: \\\")   # ERROR\n\n# The following works correctly. Note that \\\\ is needed to escape\n# the normal meaning of the backslash character!\n#cat(\"This is a backslash: \\\\\")   # ERROR\n\n#cat(\"This is a period \\.\")   # ERROR \\. is NOT an R escape sequence\n\n# You must use TWO backslashes in R's regular expressions ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# In R you must use two backslashes in a regex pattern to \n# escape a metacharacter.\n\n\ngrep(\"\\\\.\", addresses, value=TRUE)  # all addresses that contain a period\n\n[1] \"3 Olive St.\"                      \"Two 1st Ave.\"                    \n[3] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n\ngrep(\".\", addresses, value=TRUE)  # All the addresses\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\nstuff = c(\"\", \"apple\", \"\", \"banana\")\nstuff\n\n[1] \"\"       \"apple\"  \"\"       \"banana\"\n\ngrep(\".\", stuff, value=FALSE)\n\n[1] 2 4\n\ngrep(\".\", stuff, value=TRUE)\n\n[1] \"apple\"  \"banana\"\n\n# This is an ERROR in R but would be correct in other \n# languages or environment that use regular expressions\n#grep(\"\\.\", addresses, value=\"TRUE\")  # ERROR - R doesn't recognize \\.\n\n# Without the backslash you will find all addresses that contain\n# at least a single character (i.e. all the addresses)\ngrep(\".\", addresses, value=\"TRUE\")  \n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\ngrep(\"\\\\$\", addresses, value=TRUE)   # addresses that contain a dollar sign\n\n[1] \"One Micro$oft Way\"\n\ngrep(\"$\", addresses, value=TRUE)    # all addresses - why?? - they all have an ending\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"5678 Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\n\n\n\n19.20.2 To search for an actual backslash you must use 4 backslashes in the pattern\n\n# To search for an actual backslash you must use 4 backslashes in the pattern ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Searching for an actual backslash in the data can be tricky. \n# Remember, one of our addresses had a backslash in it. Let's find it.\n#\n# To look for a single backslash in the data you must use FOUR backslashes.\n# Just as R character values need to \"escape\" a backslash with a 2nd backslash,\n# so too do regular expressions need to escape a backslash with a 2nd backslash.\n# Therefore if you want to write a regular expression in R that searches for\n# a backslash, you must write FOUR backslashes in a row. The first two resolve\n# to a single backslash. The 3rd and 4th resolve to a single backslash. Then finally\n# the two single backslashes are used in the regular expression to match a \n# single actual backslash in the data.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nanswer = grep(\"\\\\\\\\\", addresses, value=TRUE)  # look for a single backslash in the data\n\nanswer\n\n[1] \"4\\\\2 Rechov Yafo\"\n\ncat(answer)\n\n4\\2 Rechov Yafo\n\nstuff=\"\\\\\\\\\"\nstuff\n\n[1] \"\\\\\\\\\"\n\ncat(stuff)\n\n\\\\",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#matching-quotes",
    "href": "0200200-a-introToRegularExpressions-v036.html#matching-quotes",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.21 Matching QUOTES",
    "text": "19.21 Matching QUOTES\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Matching QUOTES\n#\n# \"Quotation marks\" are NOT meta-characters in regular expressions. They\n# have no special meaning in a regular expression. However, as with all \n# R code you must make sure to use a single backslash if the quotation mark\n# is inside of quotation marks (e.g.  \"\\\"\" ) - see the example below.\n#\n# Note that when using R's regular expression functions, regex\n# meta-characters, such as the period or ^ for which you want to remove \n# the special meaning require a DOUBLE backslash (as explained above).\n#\n# A regex pattern in VS Code (or a similar editor) that includes \" or ' \n# would not need any backslashes since these aren't regex meta characters.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nquoteStuff = c(\"Joe says great stuff.\", \"Franklin said \\\"a penny saved ...\\\"\")\nquoteStuff\n\n[1] \"Joe says great stuff.\"               \"Franklin said \\\"a penny saved ...\\\"\"\n\ncat(quoteStuff, sep=\"\\n\")\n\nJoe says great stuff.\nFranklin said \"a penny saved ...\"\n\ngrep (\"\\\"\", quoteStuff, value=TRUE) #  \"Franklin said \\\"a penny saved ...\\\"\"\n\n[1] \"Franklin said \\\"a penny saved ...\\\"\"\n\ngrep (\"\\\\.\", quoteStuff, value=TRUE) #  \"Franklin said \\\"a penny saved ...\\\"\"\n\n[1] \"Joe says great stuff.\"               \"Franklin said \\\"a penny saved ...\\\"\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#different-flavors-or-dialects-of-regular-expressions.",
    "href": "0200200-a-introToRegularExpressions-v036.html#different-flavors-or-dialects-of-regular-expressions.",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.22 Different “flavors” or “dialects” of regular expressions.",
    "text": "19.22 Different “flavors” or “dialects” of regular expressions.\n\n# Different \"flavors\" or \"dialects\" of regular expressions. ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Regular expressions have been around for a long time. Different \"dialects\"\n# of regular expressions have popped up over the years. \n#\n# Some programming languages and tools use slightly different \"rules\" \n# for regular expressions. This can be frustrating. However, the basic set\n# of regular expresion rules remains the same for most programming languages \n# and tools. \n#\n# Regular expressions first became popular with the Unix operating system in \n# the 1970s. There were many different versions of Unix being marketed by \n# different companies, each with slight differences. POSIX is a standard that\n# defines how things should be done in a standard way across all the different\n# versions of Unix. POSIX addresses regular expressions too. \n#\n# POSIX introduced \"named character classes\" as described below. R will \n# recognize these. \n#\n# Other additions to the regular expression notation were introduced by \n# the once very popular Perl programming language. You can get these features\n# to work in R by specifying perl=TRUE as one of the arguments for grep\n# and other functions in R that work with regular expressions. \n# For more details about perl regular expressions, see ?regex.\n#\n# As we said above, regular expressions are NOT totally standardized across all \n# languages and environments. For example (as of Feb 10, 2022)\n# there are subtle differences between the rules for regular expressions\n# that are used in R and those that are used in the \n# Visual Studio Code (VS Code) text editor. You can see a summary of the \n# rules used by VS Code here:\n#   https://docs.microsoft.com/en-us/visualstudio/ide/using-regular-expressions-in-visual-studio?view=vs-2022\n#\n# Although there may be some differences between different languaes and \n# environments, the vast majority of regular expression meta characters\n# work the same across the different environments. \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n19.22.1 [[:digit:]] vs \\d - different shorthand notations for character classes\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Two different shorthand notations for character classes\n# - POSIX named character classes , e.g. [[:alnum:]]  [[:digit:]]   etc. \n# - backslash shortcuts , e.g. \\s \\S \\d \\D  etc.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n“POSIX” named character classes, e.g. [[:digit:]]\n\n# \"POSIX\" named character classes, e.g. [[:alnum:]] ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# NOTE: These are available in R. \n#\n#       They currently are NOT available in VSCode\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Bracket notation in regular expressions (e.g. [aeiou] or [0-9]) are known as \n# character classes. \n#\n# You can use several \"named character classes\" as shorthand for some common\n# character classes. These are shown below. Notice the [[double brackets]]\n# We'll explain more about the the [[double brackets]] below.\n#\n# [[:upper:]]   same as [A-Z]\n# [[:lower:]]   same as [a-z]\n# [[:space:]]   same as [ \\r\\n\\t]\n# [[:punct:]]   all \"special\" characters, eg. !@#$% etc...\n# [[:digit:]]   same as [0-9]\n# [[:alpha:]]   same as [a-zA-Z]\n# [[:alnum:]]   same as [a-zA-Z0-9]\n#\n# The [[double brackets]] shown above are necessary since these\n# \"named character classes\" must actually be placed inside a pair of \n# [square brackets]. For example, you can also use the named\n# character classes inside a larger character class.\n#\n# For example the following will match any single character\n# from the following list:    -,+,*,/,(,),0,1,2,3,4,5,6,7,8,9\n#\n#   [-+*/()[:digit:]]    is the same as [-+*/()0-9]\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n\n19.22.2 — practice —\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Search for addresses that contain at least one digit. Use a POSIX\n# named character class.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER \ngrep(\"[[:digit:]]\", addresses, value=TRUE)   # uses POSIX named character classes\n\n [1] \"12345 Sesame Street\"              \"3 Olive St.\"                     \n [3] \"Two 1st Ave.\"                     \"5678 Park Place\"                 \n [5] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n [7] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n [9] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n[11] \"Three Main Street Apt 12343\"     \n\nstr_view(addresses, \"[[:digit:]]\")\n\n [1] │ &lt;1&gt;&lt;2&gt;&lt;3&gt;&lt;4&gt;&lt;5&gt; Sesame Street\n [3] │ &lt;3&gt; Olive St.\n [4] │ Two &lt;1&gt;st Ave.\n [5] │ &lt;5&gt;&lt;6&gt;&lt;7&gt;&lt;8&gt; Park Place\n [6] │ Forty Five &lt;2&gt;nd Street\n [7] │ Ninety Nine Cone St. apartment &lt;7&gt;\n [8] │ &lt;9&gt; Main St. apt. &lt;6&gt;&lt;2&gt;&lt;3&gt;\n[10] │ &lt;4&gt;\\&lt;2&gt; Rechov Yafo\n[13] │ One Main Street Apt &lt;1&gt;&lt;2&gt;b\n[14] │ Two Main Street Apt &lt;1&gt;&lt;2&gt;&lt;3&gt;c\n[15] │ Three Main Street Apt &lt;1&gt;&lt;2&gt;&lt;3&gt;&lt;4&gt;&lt;3&gt;\n\n# NOTE - the pattern \"[:digit:]\" with one set of [brackets] does NOT work.\n#\n# Since there is only one set of [brackets], the pattern matches any one of\n# the characters that are between the [brackets], i.e. match\n# one of the characters \":\", \"d\", \"i\", \"g\", \"i\", \"t\" or \":\"\n# This is equivalent to \"[:digt]\"  (I removed the 2nd \":\" and the 2nd \"i\" as\n# they are repetitive.)\n\n# THIS DOESN'T WORK! - see note above\ngrep(\"[:digit:]\", addresses, value=TRUE)  # looks for one of the following :,d,i,g,i,t,:\n\n [1] \"12345 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"Forty Five 2nd Street\"             \"Ninety Nine Cone St. apartment 7\" \n [7] \"9 Main St. apt. 623\"               \"Five Google Drive\"                \n [9] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[11] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[13] \"Three Main Street Apt 12343\"       \"City Hall Lockport, NY\"           \n\ngrep(\"^[N[:digit:]]\", addresses, value=TRUE)   # same as [N0-9]\n\n[1] \"12345 Sesame Street\"               \"3 Olive St.\"                      \n[3] \"5678 Park Place\"                   \"Ninety Nine Cone St. apartment 7\" \n[5] \"9 Main St. apt. 623\"               \"4\\\\2 Rechov Yafo\"                 \n[7] \"Nineteen Watchamacallit Boulevard\"\n\n\n\n# QUESTION:\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a command that replaces any sequence of digits or mathematical \n# operators with the text \"&lt;&lt;MATH-EXPRESSION&gt;&gt;\"\n#\n# You can use the following \"mathStuff\" variable to test your answer.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nmathStuff &lt;- c(\"What is 3+2 ? Do you know the answer?\",\n               \"99.5 desgrees in Farenheit is 99.5*(5/9)-32 degrees in Celcius\")\n\nmathStuff\n\n[1] \"What is 3+2 ? Do you know the answer?\"                         \n[2] \"99.5 desgrees in Farenheit is 99.5*(5/9)-32 degrees in Celcius\"\n\n\n\n# ANSWER\n\ngsub(\"[-+*/().[:digit:]]+\", \"&lt;&lt;MATH-EXPRESSION&gt;&gt;\", mathStuff)\n\n[1] \"What is &lt;&lt;MATH-EXPRESSION&gt;&gt; ? Do you know the answer?\"                              \n[2] \"&lt;&lt;MATH-EXPRESSION&gt;&gt; desgrees in Farenheit is &lt;&lt;MATH-EXPRESSION&gt;&gt; degrees in Celcius\"\n\n\n# QUESTION:\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a grep command that matches punctuation and letters, but not numbers.\n# You can use the following data to test your answer. Use POSIX named\n# character classes.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nstuff = c(\"1234\",  # This should NOT match since it doesn't contain letters or punctuation\n          \"12.34\", # This SHOULD match since it contains punctuation.\n          \".\",     # This SHOULD match since it contains punctuation.\n          \"hi\")    # This SHOULD match since it contains at least one letter\n\n# ANSWER\n\n# The following will match any punctuation or letters but not numbers\ngrep(\"[[:punct:][:alpha:]]\", stuff, value=TRUE)   #   \".\"    \"hi\"\n\n[1] \"\\\\\\\\\"\n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Search for fruits that contain spaces using the POSIX\n# named character classes for spaces\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ANSWER\ngrep(\"[[:space:]]\", fruit, value=TRUE)\n\n[1] \"N. American apple\" \"S. Korean Fig\"     \"star fruit\"       \n[4] \"prickly pear\"      \"Beurre Hardy pear\" \"black cherry\"     \n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Search fruit for those that contain punctuation (e.g. periods, commas, etc)\n# using the POSIX named character classes\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ANSWER\n\ngrep (\"[[:punct:]]\", fruit, value=TRUE)\n\n[1] \"N. American apple\" \"S. Korean Fig\"    \n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Show fruit that contain either an x,y,z or some punctuation.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ANSWER\n\n# NOTE that there are TWO sets of brackets. The POSIX named character\n# class, [:punct:], is itself inside a set of [brackets].\n\ngrep(\"[xyz[:punct:]]\", fruit, value=TRUE)\n\n [1] \"N. American apple\" \"S. Korean Fig\"     \"prickly pear\"     \n [4] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n [7] \"blueberry\"         \"strawberry\"        \"honeydew\"         \n[10] \"yumberry\"         \n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Search all ADDRESSES for those that contain punctuation (e.g. periods,\n# commas, etc) or actual digits (e.g. 0123456789) using POSIX named\n# character classes\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ANSWER\n\ngrep(\"[[:punct:][:digit:]]\", addresses, value=TRUE)\n\n [1] \"12345 Sesame Street\"              \"One Micro$oft Way\"               \n [3] \"3 Olive St.\"                      \"Two 1st Ave.\"                    \n [5] \"5678 Park Place\"                  \"Forty Five 2nd Street\"           \n [7] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n [9] \"4\\\\2 Rechov Yafo\"                 \"One Main Street Apt 12b\"         \n[11] \"Two Main Street Apt 123c\"         \"Three Main Street Apt 12343\"     \n[13] \"City Hall Lockport, NY\"          \n\n# This also works\ngrep(\"[[:punct:]0-9]\", addresses, value=TRUE)\n\n [1] \"12345 Sesame Street\"              \"One Micro$oft Way\"               \n [3] \"3 Olive St.\"                      \"Two 1st Ave.\"                    \n [5] \"5678 Park Place\"                  \"Forty Five 2nd Street\"           \n [7] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n [9] \"4\\\\2 Rechov Yafo\"                 \"One Main Street Apt 12b\"         \n[11] \"Two Main Street Apt 123c\"         \"Three Main Street Apt 12343\"     \n[13] \"City Hall Lockport, NY\"          \n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Search all ADDRESSES for those that contain some punctuation that \n# comes immediately after the letter t. Use POSIX named classes.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ANSWER\ngrep(\"[tT][[:punct:]]\", addresses, value=TRUE)\n\n[1] \"3 Olive St.\"                      \"Ninety Nine Cone St. apartment 7\"\n[3] \"9 Main St. apt. 623\"              \"City Hall Lockport, NY\"          \n\n# ANOTHER WAY\ngrep(\"[tT][^[:alnum:]]\", addresses, value=TRUE)\n\n [1] \"One Micro$oft Way\"                 \"3 Olive St.\"                      \n [3] \"Two 1st Ave.\"                      \"Ninety Nine Cone St. apartment 7\" \n [5] \"9 Main St. apt. 623\"               \"Fifteen Watchamacallit Boulevard\" \n [7] \"Nineteen Watchamacallit Boulevard\" \"One Main Street Apt 12b\"          \n [9] \"Two Main Street Apt 123c\"          \"Three Main Street Apt 12343\"      \n[11] \"City Hall Lockport, NY\"           \n\n\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# answer the previous question without using POSIX named classes.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(\"[tT][,.!?]\", addresses, value=TRUE)\n\n[1] \"3 Olive St.\"                      \"Ninety Nine Cone St. apartment 7\"\n[3] \"9 Main St. apt. 623\"              \"City Hall Lockport, NY\"          \n\n\n\nbackslash shortcuts for character classes, e.g. etc.\n\n# backslash shortcuts for character classes, e.g. \\s \\S \\d \\D etc. ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The following are also shorthand notation that you can use for some\n# characer classes.\n#\n# Note that in R you must use a double backslash, e.g. \\\\s instead of \\s\n#\n# \\s   is the same as [ \\n\\t\\r]  also same as [[:space:]]\n#      It matches anything which is considered whitespace.\n#      This could be a space, tab, line break etc.\n#\n# \\S   is the same as [^ \\n\\t\\r]\n#      It matches the opposite of \\s, that is anything which is not considered\n#      whitespace.\n#\n# \\d   is the same as [0-9]  (ie. it matches a single digit)  same as [[:digit:]]\n#\n# \\D   is the same as [^0-9] (i.e. it matches a single NON-digit)\n#\n# \\w - matches anything which is considered a word character. That is\n#      [A-Za-z0-9_]. Note the inclusion of the underscore character '_'. This is\n#      because in programming and other areas we regularly use the underscore as part\n#      of, say, a variable or function name.\n#\n# \\W - matches  [^A-Za-z0-9_] the opposite of \\w, that is anything which is not considered a\n#      word character.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#word-boundries",
    "href": "0200200-a-introToRegularExpressions-v036.html#word-boundries",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.23 WORD BOUNDRIES: ::",
    "text": "19.23 WORD BOUNDRIES: ::\n# WORD BOUNDRIES:  \\b ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# \\b matches a \"word boundary\"   (remember in R use \\\\b)\n# \n# You can use this before a pattern to mean that the pattern must\n# come at the beginning of a word - see examples below.\n#\n# You can use this after a pattern to mean that the pattern must\n# come at the end of a word - see examples below.\n#\n# Note that a \"word boundary\" is not a particular character such as a \n# space or comma, but rather is a position in the text. \n#\n# NOTE - a \"word\" may include letters, digits or underscores\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nheWords = c(\"he\", \"here\", \"there\", \"the\", \"who was she?\", \"headache\")\nheWords\n\n[1] \"he\"           \"here\"         \"there\"        \"the\"          \"who was she?\"\n[6] \"headache\"    \n\n# Example: word starts with \"he\"\ngrep (\"\\\\bhe\", heWords, value=TRUE)  # \"he\" \"here\" (NOT \"there\",\"the\",\"who was she?\")\n\n[1] \"he\"       \"here\"     \"headache\"\n\n# Example: word ends with \"he\"\ngrep (\"he\\\\b\", heWords, value=TRUE)  # \"he\" \"the\" \"who was she?\" (NOT \"here\",\"there\")\n\n[1] \"he\"           \"the\"          \"who was she?\" \"headache\"    \n\n# Example: match only the word \"he\"\ngrep (\"\\\\bhe\\\\b\", heWords, value=TRUE)  # \"he\"\n\n[1] \"he\"\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You can use \\b with more complicated patterns too.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Examples: \ngrep (\"\\\\b[Hh]\", fruit, value=TRUE)  # fruits that have a word that starts with H or h\n\n[1] \"Beurre Hardy pear\" \"honeydew\"         \n\ngrep (\"[aeiouAEIOU]\\\\b\", fruit, value=TRUE)  # fruits that end with a vowel\n\n[1] \"apple\"             \"N. American apple\" \"Beurre Hardy pear\"\n[4] \"banana\"           \n\n:::",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#pattern1pattern2-matches-pattern1-or-pattern2",
    "href": "0200200-a-introToRegularExpressions-v036.html#pattern1pattern2-matches-pattern1-or-pattern2",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.24 “pattern1|pattern2” matches pattern1 OR pattern2",
    "text": "19.24 “pattern1|pattern2” matches pattern1 OR pattern2\n\n# \"pattern1|pattern2\"  matches pattern1 OR pattern2 ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Combining patterns\n#\n#    PATTERN1|PATTERN2  matches if either PATTERN1 or PATTERN2 is found\n#\n#    (PATTERN)          you may surround patterns with (parentheses) if necessary\n#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngrep(\"black|blue|green\", fruit, value=TRUE) # contains black,blue or green\n\n[1] \"black cherry\" \"blueberry\"   \n\ngrep(\"^(1|One)\", addresses, value=TRUE, ignore.case=TRUE) # 1 or One at beginning \n\n[1] \"12345 Sesame Street\"     \"One Micro$oft Way\"      \n[3] \"One Main Street Apt 12b\"\n\ngrep(\"(^1|^One)\", addresses, value=TRUE) # same thing \n\n[1] \"12345 Sesame Street\"     \"One Micro$oft Way\"      \n[3] \"One Main Street Apt 12b\"\n\ngrep(\"^1|^One\", addresses, value=TRUE) # same thing \n\n[1] \"12345 Sesame Street\"     \"One Micro$oft Way\"      \n[3] \"One Main Street Apt 12b\"\n\ngrep(\"[0-9]\", addresses, value=TRUE)\n\n [1] \"12345 Sesame Street\"              \"3 Olive St.\"                     \n [3] \"Two 1st Ave.\"                     \"5678 Park Place\"                 \n [5] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n [7] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n [9] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n[11] \"Three Main Street Apt 12343\"     \n\ngrep(\"0|1|2|3|4|5|6|7|8|9\", addresses, value=TRUE)  # Same as [0-9]\n\n [1] \"12345 Sesame Street\"              \"3 Olive St.\"                     \n [3] \"Two 1st Ave.\"                     \"5678 Park Place\"                 \n [5] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n [7] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n [9] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n[11] \"Three Main Street Apt 12343\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#breaking-up-long-patterns-with-paste0",
    "href": "0200200-a-introToRegularExpressions-v036.html#breaking-up-long-patterns-with-paste0",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.25 breaking up long patterns with paste0",
    "text": "19.25 breaking up long patterns with paste0\n\n# breaking up long patterns with paste0 ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Do NOT include extra whitespace in patterns!!!\n# \n# For long patterns you can use paste0 to break up the pattern\n# so it is more readable in the code.\n#\n# See examples below.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n19.25.1 example\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Show those addresses that contain one of the numbers 1-9 spelled out in words,\n# e.g. \"one\", \"two\", etc\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER - note that writing the pattern using paste0 with collapse = \"\"\n# makes the pattern easy to understand. You can also comment on \n# individual portions of the pattern. \n\npattern = paste0(\"\\\\bone\\\\b|\",     # match the word \"one\"\n                 \"\\\\btwo\\\\b|\",     # match the word \"two\"\n                 \"\\\\bthree\\\\b|\",   # etc.\n                 \"\\\\bfour\\\\b|\",\n                 \"\\\\bfive\\\\b|\",\n                 \"\\\\bsix\\\\b|\",\n                 \"\\\\bseven\\\\b|\",\n                 \"\\\\beight\\\\b|\",\n                 \"\\\\bnine\\\\b\")\n\ngrep(pattern, addresses, value=TRUE, ignore.case = TRUE)\n\n[1] \"One Micro$oft Way\"                \"Two 1st Ave.\"                    \n[3] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n[5] \"Five Google Drive\"                \"One Main Street Apt 12b\"         \n[7] \"Two Main Street Apt 123c\"         \"Three Main Street Apt 12343\"     \n\n# Note that the following also works but is\n#   - MUCH harder to read \n#   - MUCH harder to check for errors and\n#   - cannot be commented on for different parts of the pattern\n\ngrep(\n  \"\\\\bone\\\\b|\\\\btwo\\\\b|\\\\bthree\\\\b|\\\\bfour\\\\b|\\\\bfive\\\\b|\\\\bsix\\\\b|\\\\bseven\\\\b|\\\\beight\\\\b|\\\\bnine\\\\b\",\n  addresses, value=TRUE, ignore.case = TRUE)\n\n[1] \"One Micro$oft Way\"                \"Two 1st Ave.\"                    \n[3] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n[5] \"Five Google Drive\"                \"One Main Street Apt 12b\"         \n[7] \"Two Main Street Apt 123c\"         \"Three Main Street Apt 12343\"     \n\n\n\n\n19.25.2 example\n\n# A more complex example ####\n\n# QUESTION \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Display those addresses that contain a number that is exactly one\n# digit long. For example: \n#\n#   addresses =\n#    c(\"3 Olive St.\",              # should match (because of 3)\n#      \"Forty Five 2nd Street\",    # should match (because of 2nd)\n#      \"Ninety Nine Cone St. apartment 7\",\n#                                  # should match (because of 7)\n#      \"7\",                        # should match\n#\n#      \"12345 Sesame Street\",      # should NOT match (12345 is five digits)\n#      \"One main Street Apt 12b\",  # should NOT match (12 is two digits)\n#      \"Two Main St. Apt 99\",      # should NOT match (99 is two digits) \n#      \"45\")                       # should NOT match\n#\n#   &gt; YOUR COMMAND GOES HERE\n#   [1] \"3 Olive St.\"\n#   [2] \"Forty Five 2nd Street\"\n#   [3] \"Ninety Nine Cone St. apartment 7\"\n#   [4] \"7\"\n#\n# NOTE: the pattern \"[0-9]\" will NOT work as it will match every one of \n# values above\n#\n# NOTE: the pattern \"\\\\b[0-9]\\\\b\" is a good try but will not match\n# \"Forty Five 2nd Street\" as the 2 in \"2nd\" is NOT followed by a word boundary.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n\n# Note - there are 4 possible ways for a single digit appear in the text:\n#\n# (a) The digit can appear at the very beginning of the text and be followed by\n#     a non-digit, e.g. \"3 Olive street\". \n#\n#     The pattern: \"^[0-9][^0-9]\"     \n#     matches \"3 Olive street\"\n#     but doesn't match \"Forty Five 2nd Street\" (since the 2 is not at the\n#                                                beginning of the text).\n#\n# Similarly, each of the following patterns will match a single digit for\n# some texts but not for others. \n#\n# (b) [^0-9][0-9][^0-9] : NONdigit digit NONdigit anywhere in the text\n# (c) [^0-9][0-9]$      : last two characters are a NONdigit followed by a single digit\n# (d) ^[0-9]$           : whole thing is JUST one digit\n#\n# For actual addresses you probably don't have to worry about the last\n# case, but for other types of data you might.\n#\n# You can write a pattern that deals with all of these cases by\n# separating the different \"sub-patterns\" from each other with \"|\" symbols.\n# For example, the following answers the question, but the pattern is VERY\n# hard to read. (see below for a better way to write this code.)\n\ngrep(\"^[0-9][^0-9]|[^0-9][0-9][^0-9]|[^0-9][0-9]$|^[0-9]$\", addresses, value=TRUE)\n\n[1] \"3 Olive St.\"                      \"Two 1st Ave.\"                    \n[3] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n[5] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n\n# we can use paste0 to make this easier to read\n\npattern &lt;- \n  paste0 ( \"^[0-9][^0-9]\" ,     # starts with digit followed by a NONdigit\n         \"|[^0-9][0-9][^0-9]\",  # NONdigit digit NONdigit anywhere in the text \n         \"|[^0-9][0-9]$\",       # ends with a NONdigit followed by a single digit\n         \"|^[0-9]$\")            # whole thing is JUST one digit \n\ngrep(pattern, addresses, value=TRUE)\n\n[1] \"3 Olive St.\"                      \"Two 1st Ave.\"                    \n[3] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n[5] \"9 Main St. apt. 623\"              \"4\\\\2 Rechov Yafo\"                \n\n\n\n\n19.25.3 example\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a command that displays all addresses that contain the\n# number \"one\" or 1.\n#\n# Notice that the following will NOT work. This gets \"Cone\" and 12345 too:\n#\n#   grep(\"one|1\", addresses, value=TRUE, ignore.case=TRUE) # NO - matches Cone and 12345\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n\n# The following is the way to do it. Regular expressions require \n# a very thoughtful approach!\n#\n# The word \"one\" or the number \"1\" (not including the number 123)\npattern = paste0(\"^one[^a-z]|\",       # one at beginning\n                 \"[^a-z]one[^a-z]|\",  # one in middle\n                 \"[^a-z]one$|\",       # one at end\n                 \"^one$|\",            # ONLY the word \"one\"\n                 \"^1[^0-9]|\",         # 1 at beginning\n                 \"[^0-9]1[^0-9]|\",    # 1 in middle\n                 \"[^0-9]1$|\",         # 1 at end\n                 \"^1$\")               # ONLY the number 1\n\n\npattern\n\n[1] \"^one[^a-z]|[^a-z]one[^a-z]|[^a-z]one$|^one$|^1[^0-9]|[^0-9]1[^0-9]|[^0-9]1$|^1$\"\n\ngrep(pattern, addresses, value=TRUE, ignore.case=TRUE)\n\n[1] \"One Micro$oft Way\"       \"Two 1st Ave.\"           \n[3] \"One Main Street Apt 12b\"\n\n# Same thing but MUCH harder to read!!!\n# You should break up long patterns with paste0 and comment them as shown above.\n\ngrep(\n  \"^one[^a-z]|[^a-z]one[^a-z]|[^a-z]one$|^one$|^1[^0-9]|[^0-9]1[^0-9]|[^0-9]1$|^1$\",\n  addresses, value=TRUE, ignore.case=TRUE)\n\n[1] \"One Micro$oft Way\"       \"Two 1st Ave.\"           \n[3] \"One Main Street Apt 12b\"\n\n\n\n\n19.25.4 example\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# df is a dataframe. Write a command that shows all rows from df\n# for which the 2nd character in the first column is \"x\". \n# \n# Hints: \n#   a. Access a dataframe as you normally would but use grep or lgrep to\n#      return either the row numbers or TRUE/FALSE\n#      values that identify the rows to be displayed.\n# \n#   b. Remember that you are NOT told what the column names are. Therefore you\n#      must use a number to stipulate the first column and NOT a column name.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Use this example data\ndf = data.frame( partNum = c(\"ax4321\", \"az12\", \"bx1234\", \"bw987\"),\n                 partName = c(\"widget\",\"thingie\",\"gadget\",\"gizmo\"),\n                 price =    c(0.50, 0.60, 1.70, 0.80),\n                 stringsAsFactors = FALSE)\ndf\n\n  partNum partName price\n1  ax4321   widget   0.5\n2    az12  thingie   0.6\n3  bx1234   gadget   1.7\n4   bw987    gizmo   0.8\n\n# Show the rows that contain \"x\" as the 2nd character in the partNum\n\n\n# One answer - using grep\ndf[ grep ( \"^.x\", df$partNum , ignore.case=TRUE ) ,   ]\n\n  partNum partName price\n1  ax4321   widget   0.5\n3  bx1234   gadget   1.7\n\n# Another answer - using grepl\ndf[ grepl ( \"^.x\", df$partNum , ignore.case=TRUE ) ,   ]\n\n  partNum partName price\n1  ax4321   widget   0.5\n3  bx1234   gadget   1.7\n\n\n\n\n19.25.5 example\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Using the same data as above, only show those rows that contain an \"x\"\n# in the 2nd character of the partNum whose price is also less than 1.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndf\n\n  partNum partName price\n1  ax4321   widget   0.5\n2    az12  thingie   0.6\n3  bx1234   gadget   1.7\n4   bw987    gizmo   0.8\n\ndf[ grepl ( \"^.x\", df$partNum , ignore.case=TRUE ) & df$price &lt; 1 ,   ]\n\n  partNum partName price\n1  ax4321   widget   0.5",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#quantifiers-nm-n-n",
    "href": "0200200-a-introToRegularExpressions-v036.html#quantifiers-nm-n-n",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.26 Quantifiers: {n,m} {n} {n,} + * ?",
    "text": "19.26 Quantifiers: {n,m} {n} {n,} + * ?\n\n#@ Quantifiers: {n,m}  {n}  {n,}  +  *  ?  ####\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#@\n#@ Quantifiers are symbols in the pattern that identify how many repetetitions\n#@ to look for of a particular sub-pattern. The quantifiers include\n#@\n#@   SOME_SUB_PATTERN{n,m}    (where n and m are numbers)\n#@   SOME_SUB_PATTERN{n}      (where n is a number)\n#@   SOME_SUB_PATTERN{n,}      (where n is a number)\n#@   SOME_SUB_PATTERN+\n#@   SOME_SUB_PATTERN*\n#@   SOME_SUB_PATTERN?\n#@\n#@ See below for an explanation of each type of quantifier.\n#@\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n\n19.26.1 Quantifiers with {curly braces} eg. {3} {2,5} {3,}\n\n# Quantifiers with {curly braces}   eg. {3}   {2,5}   {3,}   ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# \n#    SOME_PATTERN{3}       three matches in a row\n#    SOME_PATTERN{3,6}     between three and six matches in a row\n#    SOME_PATTERN{3,}      at least 3 matches in a row\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# EXAMPLE\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# \"[0-9]{4}\" matches 4 digits in a row, same as \"[0-9][0-9][0-9][0-9]\"\n#\n# Note that this will also return those entries that have more than 4 digits\n# in a row since these entries ALSO have 4 digits in a row (plus some extra\n# digits)\n\ngrep(\"[0-9]{4}\", addresses, value=TRUE)  # 4 digits , same as \"[0-9][0-9][0-9][0-9]\"\n\n[1] \"12345 Sesame Street\"         \"5678 Park Place\"            \n[3] \"Three Main Street Apt 12343\"\n\n# Note that if we use gsub, only the first 4 digits will be substituted.\ngsub(\"[0-9]{4}\", \"xxxx\", addresses)  # 4 digits , same as \"[0-9][0-9][0-9][0-9]\"\n\n [1] \"xxxx5 Sesame Street\"               \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"xxxx Park Place\"                   \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. 623\"              \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt 123c\"         \n[15] \"Three Main Street Apt xxxx3\"       \"City Hall Lockport, NY\"           \n\ngrep(\"[0-9]{3}\", addresses, value=TRUE)  # 3 digits \n\n[1] \"12345 Sesame Street\"         \"5678 Park Place\"            \n[3] \"9 Main St. apt. 623\"         \"Two Main Street Apt 123c\"   \n[5] \"Three Main Street Apt 12343\"\n\n# Note that if we use gsub, only the first 3 digits will be substituted.\ngsub(\"[0-9]{3}\", \"xxxx\", addresses)  # 4 digits , same as \"[0-9][0-9][0-9][0-9]\"\n\n [1] \"xxxx45 Sesame Street\"              \"One Micro$oft Way\"                \n [3] \"3 Olive St.\"                       \"Two 1st Ave.\"                     \n [5] \"xxxx8 Park Place\"                  \"Forty Five 2nd Street\"            \n [7] \"Ninety Nine Cone St. apartment 7\"  \"9 Main St. apt. xxxx\"             \n [9] \"Five Google Drive\"                 \"4\\\\2 Rechov Yafo\"                 \n[11] \"Fifteen Watchamacallit Boulevard\"  \"Nineteen Watchamacallit Boulevard\"\n[13] \"One Main Street Apt 12b\"           \"Two Main Street Apt xxxxc\"        \n[15] \"Three Main Street Apt xxxx43\"      \"City Hall Lockport, NY\"           \n\ngrep(\"[0-9]{3}\", addresses, value=TRUE)  # 3 digits \n\n[1] \"12345 Sesame Street\"         \"5678 Park Place\"            \n[3] \"9 Main St. apt. 623\"         \"Two Main Street Apt 123c\"   \n[5] \"Three Main Street Apt 12343\"\n\ngrep(\"\\\\b[0-9]{3}\\\\b\", addresses, value=TRUE)  # 3 digits \n\n[1] \"9 Main St. apt. 623\"\n\n# EXAMPLE\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# match a word that is exactly 4 letters long\n\npattern = \n  paste0 (\"^[a-zA-Z]{4}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{4}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{4}$\",\n          \"|^[a-zA-Z]{4}$\")\n\npattern =\n  paste0 (\"\\\\b[a-zA-Z]{4}\\\\b\")\n\n# use the same pattern for both addressess and fruit\n\ngrep(pattern, addresses, value=TRUE)\n\n [1] \"5678 Park Place\"                  \"Forty Five 2nd Street\"           \n [3] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n [5] \"Five Google Drive\"                \"4\\\\2 Rechov Yafo\"                \n [7] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n [9] \"Three Main Street Apt 12343\"      \"City Hall Lockport, NY\"          \n\ngrep(pattern, fruit, value=TRUE)\n\n[1] \"star fruit\"        \"pear\"              \"prickly pear\"     \n[4] \"Beurre Hardy pear\" \"plum\"             \n\n# Another way - using \\b\npattern = \"\\\\b[a-zA-Z]{4}\\\\b\"\ngrep(pattern, addresses, value=TRUE)\n\n [1] \"5678 Park Place\"                  \"Forty Five 2nd Street\"           \n [3] \"Ninety Nine Cone St. apartment 7\" \"9 Main St. apt. 623\"             \n [5] \"Five Google Drive\"                \"4\\\\2 Rechov Yafo\"                \n [7] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n [9] \"Three Main Street Apt 12343\"      \"City Hall Lockport, NY\"          \n\n# Use a text editor (e.g. the one in RStudio) to see how this works ...\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Paste the following in the text editor to see how this pattern works.\n#\n#  \\b[a-zA-Z]{4}\\b\n#\n# This will match any 4 character word. It will NOT match 3 or 5 character words.\n# Note that in VS Code you should use only one backslash (i.e. \\b ) but in R\n# you would use two backslashes (i.e. \\\\b ) as explained above.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# match a word that is at least 4 letters long\n# Use {4,} instead of {4}\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Paste this into VSCode regex search.\n#   ^[a-zA-Z]{4,}[^a-zA-Z]|[^a-zA-Z][a-zA-Z]{4,}[^a-zA-Z]|[^a-zA-Z][a-zA-Z]{4,}$|^[a-zA-Z]{4,}$\n\n\n# match a word that is at least 3,4 or 5 letters long\n# Use {3,5} instead of {4}\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Paste this into VSCode regex search.\n#   ^[a-zA-Z]{3,5}[^a-zA-Z]|[^a-zA-Z][a-zA-Z]{3,5}[^a-zA-Z]|[^a-zA-Z][a-zA-Z]{3,5}$|^[a-zA-Z]{3,5}$\n\n\n# ANOTHER EXAMPLE\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# match a word that is exactly 7 letters long\npattern = \n  paste0 (\"^[a-zA-Z]{7}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{7}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{7}$\",\n          \"|^[a-zA-Z]{7}$\")\n\n# use the same pattern for both addresses and fruit\ngrep(pattern, addresses, value=TRUE)\n\n[1] \"Fifteen Watchamacallit Boulevard\"\n\ngrep(pattern, fruit, value=TRUE)\n\n[1] \"prickly pear\" \"kumquat\"     \n\n# match any word that is between 4 and 7 letters long\npattern = \n  paste0 (\"^[a-zA-Z]{4,7}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{4,7}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{4,7}$\",\n          \"|^[a-zA-Z]{4,7}$\")\n\ngrep(pattern, addresses, value=TRUE)\n\n [1] \"12345 Sesame Street\"              \"One Micro$oft Way\"               \n [3] \"3 Olive St.\"                      \"5678 Park Place\"                 \n [5] \"Forty Five 2nd Street\"            \"Ninety Nine Cone St. apartment 7\"\n [7] \"9 Main St. apt. 623\"              \"Five Google Drive\"               \n [9] \"4\\\\2 Rechov Yafo\"                 \"Fifteen Watchamacallit Boulevard\"\n[11] \"One Main Street Apt 12b\"          \"Two Main Street Apt 123c\"        \n[13] \"Three Main Street Apt 12343\"      \"City Hall Lockport, NY\"          \n\ngrep(pattern, fruit, value=TRUE)      # ... \"apple\" ... \"peach\" ... (5 letter long words too)\n\n [1] \"apple\"             \"N. American apple\" \"S. Korean Fig\"    \n [4] \"star fruit\"        \"pear\"              \"prickly pear\"     \n [7] \"Beurre Hardy pear\" \"cherry\"            \"black cherry\"     \n[10] \"peach\"             \"plum\"              \"kumquat\"          \n[13] \"banana\"           \n\n# match a word that is at least 7 letters long\npattern = \n  paste0 (\"^[a-zA-Z]{7,}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{7,}[^a-zA-Z]\",\n          \"|[^a-zA-Z][a-zA-Z]{7,}$\",\n          \"|^[a-zA-Z]{7,}$\")\n\ngrep(pattern, addresses, value=TRUE)\n\n[1] \"Ninety Nine Cone St. apartment 7\"  \"Fifteen Watchamacallit Boulevard\" \n[3] \"Nineteen Watchamacallit Boulevard\" \"City Hall Lockport, NY\"           \n\ngrep(pattern, fruit, value=TRUE)\n\n[1] \"N. American apple\" \"prickly pear\"      \"kumquat\"          \n[4] \"blueberry\"         \"strawberry\"        \"honeydew\"         \n[7] \"strawberries\"      \"yumberry\"         \n\n# Exactly 3 digits (see example in last section of exactly one digit)\npattern = paste0(\n  \"^[0-9]{3}[^0-9]\",\n  \"|[^0-9][0-9]{3}[^0-9]\",\n  \"|[^0-9][0-9]{3}$\",\n  \"|^[0-9]{3}$\"\n)\ngrep(pattern, addresses, value=TRUE)\n\n[1] \"9 Main St. apt. 623\"      \"Two Main Street Apt 123c\"\n\ngrep(\"[^aeiou]{5}\", fruit, value=TRUE)  # at least 5 non vowels in a row\n\n[1] \"N. American apple\" \"prickly pear\"      \"Beurre Hardy pear\"\n[4] \"black cherry\"     \n\ngrep(\"[^aeiou]{6}\", fruit, value=TRUE)  # at least 6 non vowels in a row\n\n[1] \"prickly pear\"\n\ngrep(\"[^aeiou]{7}\", fruit, value=TRUE)  # at least 7 non vowels in a row\n\ncharacter(0)\n\ngrep(\"^.[aeiou]{2}\", fruit, value=TRUE)  # vowels in the 2nd and 3rd positions\n\n[1] \"pear\"              \"Beurre Hardy pear\" \"peach\"            \n\ngrep(\"[aeiou]{2}.$\", fruit, value=TRUE)  # 2nd & 3rd to last characters are vowels\n\n[1] \"star fruit\"        \"pear\"              \"prickly pear\"     \n[4] \"Beurre Hardy pear\" \"kumquat\"           \"strawberries\"     \n\n\n\n# QUESTION\n# Search for fruit that are 4 or 6 letters long.\n#\n\ngrep (\"^[a-zA-Z]{4}$|^[a-zA-Z]{6}$\", fruit, value=TRUE)\n\n[1] \"pear\"   \"cherry\" \"plum\"   \"banana\"\n\ngrep (\"(^[a-zA-Z]{4}$)|(^[a-zA-Z]{6}$)\", fruit, value=TRUE)\n\n[1] \"pear\"   \"cherry\" \"plum\"   \"banana\"\n\ngrep (\"^(([a-zA-Z]{4})|([a-zA-Z]{6}))$\", fruit, value=TRUE)\n\n[1] \"pear\"   \"cherry\" \"plum\"   \"banana\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#quantifiers-with-and",
    "href": "0200200-a-introToRegularExpressions-v036.html#quantifiers-with-and",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.27 Quantifiers with * + and ?",
    "text": "19.27 Quantifiers with * + and ?\n\n# Quantifiers with    *   +  and  ?  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#      PATTERN* is the same as PATTERN{0,}  i.e. zero or more repetitions\n# \n#      PATTERN+ is the same as PATTERN{1,}  i.e. one or more repetitions\n#\n#      PATTERN? is the same as PATTERN{0,1}\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ngrep(\"^[^aeiouAEIOU].*[^aeiouAEIOU]$\", fruit, value=TRUE) # start and end with non-vowel \n\n [1] \"S. Korean Fig\"     \"fig\"               \"star fruit\"       \n [4] \"pear\"              \"prickly pear\"      \"Beurre Hardy pear\"\n [7] \"cherry\"            \"black cherry\"      \"peach\"            \n[10] \"plum\"              \"kumquat\"           \"blueberry\"        \n[13] \"strawberry\"        \"honeydew\"          \"strawberries\"     \n[16] \"yumberry\"         \n\n# match at least two spaces in the text (including just two spaces)\npattern = \".* .* .*\"       \n\npattern = \".*e.*e.*\"       \ngrep(pattern, fruit, value=TRUE) \n\n[1] \"N. American apple\" \"Beurre Hardy pear\" \"blueberry\"        \n[4] \"honeydew\"          \"strawberries\"     \n\nspacesStuff = c(\"nospaces\",\n          \"this has three spaces\",\n          \"just two spaces\", \n          \"one space\", \n          \"two  spaces\", \n          \"three   spaces\", \n          \"\",\n          \" \", \n          \"  \", \n          \"   \")\nspacesStuff\n\n [1] \"nospaces\"              \"this has three spaces\" \"just two spaces\"      \n [4] \"one space\"             \"two  spaces\"           \"three   spaces\"       \n [7] \"\"                      \" \"                     \"  \"                   \n[10] \"   \"                  \n\ngrep(pattern, spacesStuff, value=TRUE) \n\n[1] \"this has three spaces\" \"one space\"             \"three   spaces\"       \n\n\n\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Find solutions for the questions at the website.\n# The website is free to use.\n#\n#         http://play.inginf.units.it/\n#\n# 1. When you get to this page, scroll all the way to the bottom.\n#\n# 2. You will be prompted for your \"regex skill level\" and other basic signup\n#    info. It, seems that you must fill in this info in order for the example\n#    questions to work correctly. However, the exact answers to these questions\n#    don't seem to matter (it seems that you get the same questions no \n#    matter what \"skill level\" you choose)\n# \n# 3. Press the \"Next\" button.\n#\n# 4. You will then be prompted with a list of regex metacharacters \n# Some of these are a little challenging: http://play.inginf.units.it/#/\n#\n# You can find sample answers here: https://avicoder.me/2019/01/21/regex-fun/\n# There could definitely be other valid answers.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWERS THAT WE DID TOGETHER IN CLASS\n\n# level 1\n#   \\d+\n\n\n# level 2\n#   [a-zA-Z0-9]{2}:[a-zA-Z0-9]{2}:[a-zA-Z0-9]{2}:[a-zA-Z0-9]{2}:[a-zA-Z0-9]{2}:[a-zA-Z0-9]{2}\n#   ([a-zA-Z0-9]{2}:){5}[a-zA-Z0-9]{2}\n#   ([0-9a-zA-Z][0-9a-zA-Z]:){5}[0-9a-zA-Z][0-9a-zA-Z]\n\n\n# level 3\n#\n#   ftp://ftp[^.]*\\.[a-zA-Z]+\\.[a-zA-Z]+(\\.[a-zA-Z]+)?/pub/FreeBSD/\n#   ftp://ftp[^.]*(\\.[a-zA-Z]+)+/pub/FreeBSD/\n#   ftp://ftp\\d*(\\.[a-zA-Z]+)+/pub/FreeBSD/\n\n\n# level 4\n#   \\$[^$]+\\$",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#gsub-works-with-patterns",
    "href": "0200200-a-introToRegularExpressions-v036.html#gsub-works-with-patterns",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.28 gsub works with patterns",
    "text": "19.28 gsub works with patterns\n\n# gsub ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# gsub works with patterns\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ngsub(\"[aeiou]\", \"x\", fruit)   # replace all vowels with x's\n\n [1] \"xpplx\"             \"N. Amxrxcxn xpplx\" \"S. Kxrxxn Fxg\"    \n [4] \"fxg\"               \"stxr frxxt\"        \"pxxr\"             \n [7] \"prxckly pxxr\"      \"Bxxrrx Hxrdy pxxr\" \"chxrry\"           \n[10] \"blxck chxrry\"      \"pxxch\"             \"plxm\"             \n[13] \"kxmqxxt\"           \"bxnxnx\"            \"blxxbxrry\"        \n[16] \"strxwbxrry\"        \"hxnxydxw\"          \"strxwbxrrxxs\"     \n[19] \"yxmbxrry\"         \n\ngsub(\"[^aeiou]\", \"x\", fruit)   # replace all non-vowels with x's\n\n [1] \"axxxe\"             \"xxxxxexixaxxaxxxe\" \"xxxxoxeaxxxix\"    \n [4] \"xix\"               \"xxaxxxxuix\"        \"xeax\"             \n [7] \"xxixxxxxxeax\"      \"xeuxxexxaxxxxxeax\" \"xxexxx\"           \n[10] \"xxaxxxxxexxx\"      \"xeaxx\"             \"xxux\"             \n[13] \"xuxxuax\"           \"xaxaxa\"            \"xxuexexxx\"        \n[16] \"xxxaxxexxx\"        \"xoxexxex\"          \"xxxaxxexxiex\"     \n[19] \"xuxxexxx\"         \n\ngsub(\"[^aeiou]+\", \"x\", fruit)   # replace one or more non-vowels with a single x\n\n [1] \"axe\"         \"xexixaxaxe\"  \"xoxeaxix\"    \"xix\"         \"xaxuix\"     \n [6] \"xeax\"        \"xixeax\"      \"xeuxexaxeax\" \"xex\"         \"xaxex\"      \n[11] \"xeax\"        \"xux\"         \"xuxuax\"      \"xaxaxa\"      \"xuexex\"     \n[16] \"xaxex\"       \"xoxexex\"     \"xaxexiex\"    \"xuxex\"      \n\ngsub(\"[^aeiou]*\", \"x\", fruit)\n\n [1] \"xaxex\"         \"xexixaxaxex\"   \"xoxexaxix\"     \"xix\"          \n [5] \"xaxuxix\"       \"xexax\"         \"xixexax\"       \"xexuxexaxexax\"\n [9] \"xex\"           \"xaxex\"         \"xexax\"         \"xux\"          \n[13] \"xuxuxax\"       \"xaxaxax\"       \"xuxexex\"       \"xaxex\"        \n[17] \"xoxexex\"       \"xaxexixex\"     \"xuxex\"        \n\ngsub(\"[^aeiou]*\", \"x\", \"apple\")\n\n[1] \"xaxex\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#backreferences-1-2-etc",
    "href": "0200200-a-introToRegularExpressions-v036.html#backreferences-1-2-etc",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.29 backreferences \\1 \\2 etc",
    "text": "19.29 backreferences \\1 \\2 etc\n\n# BACKREFERENCES  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Parenthesized expressions in a regex can be referred \"back\" to \n# with \\1, \\2 ... \n# (remember in R you need two backslashes - i.e. \\\\1, \\\\2, ...)\n#\n# The original regex standard only allowed for up to nine\n# backreferences, ie. \\1 \\2 \\3 ... \\9  \n# It did not allow for \\10. Some environments have ways to\n# allow you to reference \\10 and further but I personally\n# don't know how to do that in R ... I guess you could \n# research that if you need to but it usually doesn't\n# come up. If it becomes an issue, there is almost always\n# a simple way to workaround the situation\n# using loops and other coding approaches.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n# Find fruits that have 3 letters in the pattern xyx or aba\ngrep (\"([a-z])[a-z]\\\\1\", fruit, value=TRUE)\n\n[1] \"banana\"    \"blueberry\"\n\ngrep (\"([a-z])([a-z])\\\\1\\\\2\", fruit, value=TRUE)\n\n[1] \"banana\"\n\ngrep(\"([a-z][a-z])\\\\1\", fruit, value=TRUE)\n\n[1] \"banana\"\n\ngsub (\"([a-z])([a-z])\\\\1\", \"\\\\2\\\\1\\\\2\", fruit)\n\n [1] \"apple\"             \"N. American apple\" \"S. Korean Fig\"    \n [4] \"fig\"               \"star fruit\"        \"pear\"             \n [7] \"prickly pear\"      \"Beurre Hardy pear\" \"cherry\"           \n[10] \"black cherry\"      \"peach\"             \"plum\"             \n[13] \"kumquat\"           \"bnanna\"            \"blubebrry\"        \n[16] \"strawberry\"        \"honeydew\"          \"strawberries\"     \n[19] \"yumberry\"         \n\ngsub(\"^(.)(.)(.)\",   # reverse the first 3 characters  \n     \"\\\\3\\\\2\\\\1\", \n     fruit)\n\n [1] \"ppale\"             \" .NAmerican apple\" \" .SKorean Fig\"    \n [4] \"gif\"               \"atsr fruit\"        \"aepr\"             \n [7] \"irpckly pear\"      \"ueBrre Hardy pear\" \"ehcrry\"           \n[10] \"albck cherry\"      \"aepch\"             \"ulpm\"             \n[13] \"mukquat\"           \"nabana\"            \"ulbeberry\"        \n[16] \"rtsawberry\"        \"noheydew\"          \"rtsawberries\"     \n[19] \"muyberry\"         \n\n# QUESTION\n# Write a command to swap the first character and last character of\n# each fruit\n\ngsub( \"^(.)(.*)(.)$\" , \"\\\\3\\\\2\\\\1\" , fruit)\n\n [1] \"eppla\"             \"e. American applN\" \"g. Korean FiS\"    \n [4] \"gif\"               \"ttar fruis\"        \"reap\"             \n [7] \"rrickly peap\"      \"reurre Hardy peaB\" \"yherrc\"           \n[10] \"ylack cherrb\"      \"heacp\"             \"mlup\"             \n[13] \"tumquak\"           \"aananb\"            \"ylueberrb\"        \n[16] \"ytrawberrs\"        \"woneydeh\"          \"strawberries\"     \n[19] \"yumberry\"         \n\ngsub( \"^(.)(.*)(.)$\" , \"\\\\3-\\\\2-\\\\1\" , fruit)\n\n [1] \"e-ppl-a\"             \"e-. American appl-N\" \"g-. Korean Fi-S\"    \n [4] \"g-i-f\"               \"t-tar frui-s\"        \"r-ea-p\"             \n [7] \"r-rickly pea-p\"      \"r-eurre Hardy pea-B\" \"y-herr-c\"           \n[10] \"y-lack cherr-b\"      \"h-eac-p\"             \"m-lu-p\"             \n[13] \"t-umqua-k\"           \"a-anan-b\"            \"y-lueberr-b\"        \n[16] \"y-trawberr-s\"        \"w-oneyde-h\"          \"s-trawberrie-s\"     \n[19] \"y-umberr-y\"         \n\n# QUESTION \n# Find fruits that start and end with the same letter\n#\n\ngrep(\"^(.).*\\\\1$\", fruit, value=TRUE)\n\n[1] \"strawberries\" \"yumberry\"    \n\nshoppingList = c(\"35 yumberry pops\", \n                 \"four strawberries         \", \n                 \" five apples\",\n                 \"six yumberry and strawberries pops\")\nshoppingList\n\n[1] \"35 yumberry pops\"                   \"four strawberries         \"        \n[3] \" five apples\"                       \"six yumberry and strawberries pops\"\n\n# QUESTION\n# Use sub or gsub to replace words that start and end with the same letter\n# with the first letter then \"XXXX\" then the last letter of the word\n\ngsub(\"\\\\b(.).*\\\\1\\\\b\", \"\\\\1XXXX\\\\1\", shoppingList)\n\n[1] \"35 yumberry pops\"           \"four strawberries         \"\n[3] \" XXXX apples\"               \"sXXXXs\"                    \n\n# Make the * UN-greedy by following it with a ?\ngsub(\"\\\\b(.).*?\\\\1\\\\b\", \"\\\\1XXXX\\\\1\", shoppingList)\n\n[1] \"35 yumberry pops\"           \"four strawberries         \"\n[3] \" XXXX apples\"               \"sXXXXs\"                    \n\n#\ngsub(\"\\\\b([a-z])[a-z]*?\\\\1\\\\b\", \"\\\\1XXXX\\\\1\", shoppingList)\n\n[1] \"35 yXXXXy pops\"             \"four sXXXXs         \"      \n[3] \" five apples\"               \"six yXXXXy and sXXXXs pops\"",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#greedy-vs-non-greedy-quantifiers",
    "href": "0200200-a-introToRegularExpressions-v036.html#greedy-vs-non-greedy-quantifiers",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.30 “greedy” vs “non-greedy” quantifiers",
    "text": "19.30 “greedy” vs “non-greedy” quantifiers\n\n# \"greedy\" vs \"non-greedy\" quantifiers ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# By default, quantifiers (e.g. + * ?) are \"greedy\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# By **default** quantifiers are \"greedy\". In other words ...\n#\n# A \"greedy\" match works as follows:\n#\n#   1. The regex engine tries to start matching the regex at the beginning o\n#      of the text\n#\n#   2. If there are any quantifiers (e.g. * + ?) in the regex, the reular\n#      expression engine tries to match AS MUCH of the text as possible.\n#      (see the example in VSCode described below).\n#\n#   3. \n#   \n# by as much as it can. This behavior can be changed by using \n# \"non-greedy\" quantifiers as shown below. To make a quantifier non-greedy\n# just follow it with a question mark.\n#\n\n\n# Greedy quantifiers: match as MUCH as possible while still being able to\n# match rest of the pattern. The greedy quantifiers:\n#\n#   PATTERN{n,m}  minimum of n, maximum of m\n#   PATTERN{n,}   n or more\n#   PATTERN+      same as {1,} i.e. one or more \n#   PATTERN*      same as {0,} i.e. zero or more of the preceding pattern\n#   PATTERN?      same as {0,1} ie. zero or one (i.e. optional)\n#\n# UNgreedy (or stingy) quantifiers:\n# match as LITTLE as possible while still being able to\n# match rest of the pattern. The greedy quantifiers:\n#\n#   PATTERN{n,m}?  minimum of n, maximum of m\n#   PATTERN{n,}?   n or more\n#   PATTERN+?      same as {1,} i.e. one or more \n#   PATTERN*?      same as {0,} i.e. zero or more of the preceding pattern\n#   PATTERN??      same as {0,1} ie. zero or one (i.e. optional)\n#\n\n# EXAMPLES:\n\n# greedy   {n,m}\n# ungreedy {n,m}?\n\nsub(\"[0-9]{3,5}\", \"x\", \"123456 1234\")  # {3,5} greedy   \"x6 1234\"\n\n[1] \"x6 1234\"\n\nsub(\"[0-9]{3,5}?\", \"x\", \"123456 1234\") # {3,5}? UNgreedy \"x456 1234\"\n\n[1] \"x456 1234\"\n\ngsub(\"[0-9]{3,5}\", \"x\", \"123456 1234\")  # {3,5} greedy   \"x6 x\"\n\n[1] \"x6 x\"\n\ngsub(\"[0-9]{3,5}?\", \"x\", \"123456 1234\") # {3,5}? UNgreedy \"xx x4\"\n\n[1] \"xx x4\"\n\n# greedy   +\n# ungreedy +?\n\nsub(\"[0-9]+\", \"x\", \"123456 1234\")      # +  greedy   \"x 1234\"       \n\n[1] \"x 1234\"\n\nsub(\"[0-9]+?\", \"x\", \"123456 1234\")     # +? UNgreedy \"x23456 1234\"  \n\n[1] \"x23456 1234\"\n\ngsub(\"[0-9]+\", \"x\", \"123456 1234\")     # +     greedy   \"x x\"\n\n[1] \"x x\"\n\ngsub(\"[0-9]+?\", \"x\", \"123456 1234\")    # +?    UNgreedy \"xxxxxx xxxx\"\n\n[1] \"xxxxxx xxxx\"\n\n# greedy   *\n# ungreedy *?\n\nsub(\"[0-9]*\", \"x\", \"123456 1234\")      # *  greedy   \"x 1234\"\n\n[1] \"x 1234\"\n\nsub(\"[0-9]*?\", \"x\", \"123456 1234\")     # *? UNgreedy \"x123456 1234\"\n\n[1] \"x123456 1234\"\n\ngsub(\"[0-9]*\", \"x\", \"123456 1234\")     # *     greedy   \"x x\"\n\n[1] \"x x\"\n\ngsub(\"[0-9]*?\", \"x\", \"123456 1234\")    # *?    UNgreedy \"xxxxxx xxxx\"\n\n[1] \"x1x2x3x4x5x6x x1x2x3x4x\"\n\n# greedy   ?\n# ungreedy ??\n\nsub(\"[0-9]?\", \"x\", \"123456 1234\")      # ?  greedy   \"x23456 1234\"\n\n[1] \"x23456 1234\"\n\nsub(\"[0-9]??\", \"x\", \"123456 1234\")     # ?? UNgreedy \"x123456 1234\"\n\n[1] \"x123456 1234\"\n\ngsub(\"[0-9]?\", \"x\", \"123456 1234\")     # ?   greedy   \"xxxxxx xxxx\"\n\n[1] \"xxxxxx xxxx\"\n\ngsub(\"[0-9]??\", \"x\", \"123456 1234\")    # ??  UNgreedy  \"x1x2x3x4x5x6x x1x2x3x4x\"\n\n[1] \"x1x2x3x4x5x6x x1x2x3x4x\"\n\nsub(\"[a-z]*?\", \"x\", \"abcde\")\n\n[1] \"xabcde\"\n\n\n\n19.30.1 examples\n\n# Question\n# Extract JUST the first quotation from each of the following.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nquotations = c('Bill said \"hi\" to Jill. She replied \"bye\" to him and \"hello\" to the driver.',\n               'Tony said \"I love ice cream!\" to his mom. He then said \"I love mom:)\"')\n\n# ANSWER: use greedy and UNgreedy quantifiers as appropriate\nsub('(.*?)(\".*?\")(.*)', \"\\\\2\", quotations)\n\n[1] \"\\\"hi\\\"\"                \"\\\"I love ice cream!\\\"\"\n\n# ANOTHER ANSWER: This also works and perhaps is easier to understand (or perhaps not :)\nsub('([^\"]*)(\"[^\"]*\")(.*)', \"\\\\2\", quotations)\n\n[1] \"\\\"hi\\\"\"                \"\\\"I love ice cream!\\\"\"\n\n###################################.\n# The following are NOT answers\n###################################.\n\n# NOT an answer - Compare with the following - this one gets the last quotation\nsub('(.*)(\".*\")(.*)', \"\\\\2\", quotations)\n\n[1] \"\\\"hello\\\"\"        \"\\\"I love mom:)\\\"\"\n\n# It's helpful to see what happens with the str_view function in the following\n# cases\n\n# This matches everything from the first quotation mark to the last\nstr_view(quotations, '\".*\"')\n\n[1] │ Bill said &lt;\"hi\" to Jill. She replied \"bye\" to him and \"hello\"&gt; to the driver.\n[2] │ Tony said &lt;\"I love ice cream!\" to his mom. He then said \"I love mom:)\"&gt;\n\n# The following is more what we want. *? is now a non-greedy quantifier. \n# Therefore, it matches the first quoted info.\n# Then str_view continues by showing you the next match (i.e. the 2nd\n# quoted info), then the 3rd match, etc.\nstr_view(quotations, '\".*?\"')\n\n[1] │ Bill said &lt;\"hi\"&gt; to Jill. She replied &lt;\"bye\"&gt; to him and &lt;\"hello\"&gt; to the driver.\n[2] │ Tony said &lt;\"I love ice cream!\"&gt; to his mom. He then said &lt;\"I love mom:)\"&gt;\n\n\n\nAN ASIDE - the 1st question mark is not actually necessary in this case\n\n# This also works - see below for why the first ? is not necessary in this case.\nsub('(.*)(\".*?\")(.*)', \"\\\\2\", quotations)\n\n[1] \"\\\"hi\\\"\"                \"\\\"I love ice cream!\\\"\"\n\n\n\n# Use VSCode to understand greedy VS non-greedy quantifiers\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Use VSCode to open a text file that contains a lot of English text.\n#\n# Do a regex search in VSCode for\n#   e.*e\n#\n# This is a \"greedy\" search (since it uses * instead of *?).\n# It searches for \"e\" following by anything followed by another \"e\".\n# This will highlight on each line all the text starting from the \n# first e on the line until the last e on the line.\n#\n# Now search again using a non-greedy quantifier, i.e. .*?\n#   e.*?e\n#\n# The results will be potentially several matches on each line. \n# Each match starts with an \"e\" and extends to the next \"e\" but no further.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#the-complete-list-of-quantifiers",
    "href": "0200200-a-introToRegularExpressions-v036.html#the-complete-list-of-quantifiers",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.31 The complete list of quantifiers",
    "text": "19.31 The complete list of quantifiers\n\n# The complete list of quantifiers \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# In general you can use ? to turn any greedy quantifier into a non-greedy\n# quantifier\n#\n#   *  - zero or more (greedy)\n#   *? - zero or more (non-greedy)\n#\n#   +  - one or more (greedy)\n#   +? - one or more (non-greedy)\n#\n#   ?  - zero or one (greedy)\n#   ?? - zero or one (non-greedy)\n#\n# NOTE the following also allow for non-greedy ? modifier. However, these\n# are not really necessary - see the notes below.\n#\n#   {3,5}  - 3,4 or 5 repetitions (greedy - i.e. will match all 5 if they are there)\n#   {3,5}? - (non-greedy - will match 3 even if there are 5)\n#            Notice - you could just write {3} instead of {3,5}? (think about it)\n#\n#   {3,}   - (greedy)     3 or more matches in a row, matches as many as there are\n#   {3,}?  - (non-greedy) will always match first 3 even if there are more (non-greedy)\n#            Notice - you could just write {3} instead of {3,}? (think about it)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n19.31.1 examples\n\n# EXAMPLE\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nquotedCsv = c('\"a,b,c\",\"apple,orange\",\"watermellon\"')\ncat(quotedCsv)\n\n\"a,b,c\",\"apple,orange\",\"watermellon\"\n\n# The following gsub uses a greedy quantifier, ie. *.\n# It will match as much as it can.\n\n# greedy\ngsub('\".*\"', 'QUOTES', quotedCsv)   # \"QUOTES\"\n\n[1] \"QUOTES\"\n\n# non-greedy\ngsub('\".*?\"', 'QUOTES', quotedCsv) # \"QUOTES,QUOTES,QUOTES\"\n\n[1] \"QUOTES,QUOTES,QUOTES\"\n\n# A MORE COMPLEX EXAMPLE\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ntext = \"She sells sea shells by the sea shore.\"\n\nresult = gsub(\"(.*)(sea)(.*)\", \"1st \\\\1\\n2nd \\\\2\\n3rd \\\\3\", text)\n\ncat(result)\n\n1st She sells sea shells by the \n2nd sea\n3rd  shore.\n\n# Reults are \"greedy\", i.e. the .* in the beginning matches as much as \n# it can as long as the whole regex will work. The result is:\n\n# 1st part: She sells sea shells by the \n# 2nd part: sea\n# 3rd part: shore.\n\n# The following DOES NOT happen\n#\n# 1st part:    She sells \n# 2nd part:    sea\n# 3rd par:     shells by the sea shore.\n\n\n# we can make the regex UN-GREEDY by using a ? AFTER the *\n#\n\ntext = \"She sells sea shells by the sea shore.\"\n\n\nresult = gsub(\"(.*?)(sea)(.*)\", \"1st \\\\1\\n2nd \\\\2\\n3rd \\\\3\", text)\n\ncat(result)\n\n1st She sells \n2nd sea\n3rd  shells by the sea shore.",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#challenges",
    "href": "0200200-a-introToRegularExpressions-v036.html#challenges",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.32 Challenges",
    "text": "19.32 Challenges\n# Challenges ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Use the following for the challenges below\n\nstuff = c(\"His ssn is 876543890.\",\n          \"Call me at 212 950 3216 when you have time.\",\n          \"Please call Joe at 777-7777\",\n          \"Sue's number is (555)123   4567.\",\n          \"7182345678 is the number for the helpdesk.\",\n          \"Email Anne at anne@anneco.com and explain.\",\n          \"Meet me @ 10pm.\",\n          \"Mikes company is called mike@large\",\n          \"To work in Whatsapp internationally you need to enter his number as +1 555 555 5555.\")\nstuff\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a regular expression to find telephone numbers\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\npattern = paste0 ( \"(^|\\\\D)\",              # start with a non-digit or the start of the text\n                   \"(\\\\(?\\\\d{3}\\\\)?)?\",    # optional area code with optional (parentheses)\n                   \" *\",                   # zero or more spaces\n                   \"\\\\d{3}\",               # first 3 digits\n                   \" *-? *\",               # any number of spaces surrounding an optional dash\n                   \"\\\\d{4}\",\n                   \"(\\\\D|$)\")  # end with a non digit or a the end of the text\n\npattern\ngrep (pattern, stuff, value=TRUE)\n\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Challenge: \n# \n# Extract just the telephone numbers in a standard format.\n#\n# Make sure to use parentheses in the pattern.\n#\n# Substitute JUST the parts you want.\n#\n# Use grep (... value=FALSE ...) to get the positions that \n# matched and keep only those.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# Show how to extract just the area code, first 3 and last 4 digits\nstuff\nnewpattern = paste0 ( \"(^|.*\\\\D)\",  # tel# at beginning or after a non-digit\n                      \"(\\\\(?(\\\\d{3})\\\\)?)?\", # optional area code with optional (parentheses)\n                      \"( *)\",      # zero or more spaces\n                      \"(\\\\d{3})\",  # first 3 digits\n                      \"( *-? *)\",  # any number of spaces surrounding an optional dash\n                      \"(\\\\d{4})\",  # last 4 digits\n                      \"(\\\\D.*|$)\"  # tel # at end or before non-digit\n)\ngrep(newpattern, stuff, value=TRUE)\npositionsWithTelNums = grep(newpattern, stuff)\nreformatted = gsub(newpattern, \"\\\\2 \\\\5 \\\\7\", stuff) \n\nreformatted[positionsWithTelNums]\n\n\n\n\nresult = gsub (newpattern, \n               paste0(\"1stPart \\\\1\\n\",\n                      \"2ndPart \\\\2\\n\",\n                      \"3rdPart \\\\3\\n4thPart \\\\4\\n\",\n                      \"5thPart \\\\5\\n6thPart \\\\6\\n7thPart \\\\7\\n8thPart \\\\8\\n\"),\n               stuff)\n\nresult\n\ncat(result[1])\ncat(result[2])\ncat(result[3])\n\n# The following example shows a \"bug\" in the regex. \n# Since regular expressions are \"greedy\", the area code is matched\n# in the 1st part if it is indeed surrounded by parentheses. \n# We can fix this but it is tricky.\ncat(result[4])\n\ncat(result[5])\ncat(result[6])\ncat(result[7])\ncat(result[8])\ncat(result[9])\n\n# See the pattern used above\n\ntelNums = gsub(newpattern, \"\\\\3 \\\\5-\\\\7\", stuff)\ntelNums\ncat(telNums, sep=\"\\n\")\n\npositionsWithTelNums = grep(newpattern, stuff, value=FALSE)\npositionsWithTelNums\n\ntelNums\ntelNums [ positionsWithTelNums ]\n\n\nstuff\ngsub(newpattern, \"XXXXXXXXXX\", stuff)\n\n\n\n\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# CHALLENGE\n#\n# - write a regular expression to find email addresses and extract them \n#   from a character vector\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# CHALLENGE\n#\n# Get a vector of all the words from \"The adventures of Sherlock Holmes\".\n# You can find the UTF-8 encoding version here:\n#    https://www.gutenberg.org/files/1661/1661-0.txt\n#\n# HINT: use\n#  - readLines  with  url(\"https://www.gutenberg.org/files/1661/1661-0.txt\")\n#  - strsplit\n#  - unlist    (remember that strsplit returns a LIST)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nbook = readLines(url(\"https://www.gutenberg.org/files/1661/1661-0.txt\"),\n                 encoding=\"UTF-8\")\nhead(book)\n\nwords = strsplit(book, \" +\")\n\nhead(words)\n\nwords = unlist(words)\n\nhead(words, 100)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# CHALLENGE\n#\n# Get a vector of the 10 most common words in \"The Adventures of Sherlock Holmes\"\n# HINT: Use the table function.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nwordTable = table(words)\nhead(wordTable)\n\nsorted = sort(wordTable)\nhead(wordTable)\n\ntail(sorted)\n\n\n# Lookahead and lookbehind ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# Lookahead and lookbehind are used to match a portion of the text but NOT\n# consider it part of the match. To use this you MUST set the following\n# argument in grep, strsplit, gsub, etc:    perl=TRUE\n#\n#    Positive Lookahead (?=pattern)\n#    Negative Lookahead (?!pattern)\n#    Positive Lookbehind    (?&lt;= pattern)\n#    Negative Lookbehind    (?&lt;! pattern)\n# \n# See this page\n#  https://debuggingdata.com/post/r/regular-expressions-look-arounds/\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nstuff = c(\"a100\", \"200b\" , \"@300\", \"400@\")\nstuff\n\n# Lookbehind\ngsub(\"(?&lt;=[a-z])\\\\d+\", \"NUMBER\", stuff, perl=TRUE)\n\n\n# Lookbehind  (not equal to )\ngsub(\"(?&lt;![a-z0-9])\\\\d+\", \"NUMBER\", stuff, perl=TRUE)\n\n\n# Lookahead\ngsub(\"\\\\d+(?=[a-z])\", \"NUMBER\", stuff, perl=TRUE)\n\n# Lookahead   (not equal to )\ngsub(\"\\\\d+(?![a-z0-9])\", \"NUMBER\", stuff, perl=TRUE)\n\n\nquotations\n\n# Replace all letters between quotation marks with XXXX\ngsub('(?&lt;=\")[a-zA-Z!.? ]+(?=\")', \"XXXX\", quotations, perl=TRUE)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# CHALLENGE\n#\n# Get a vector of all the sentences from \"The adventures of Sherlock Holmes\".\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nbook = readLines(url(\"https://www.gutenberg.org/files/1661/1661-0.txt\"),\n                 encoding=\"UTF-8\")\n\nbook[100:110]\n\nnewBook = paste0(book, collapse=\" \")\n\nlength(newBook)\n\nstr(newBook)\n\nsentences = strsplit(newBook, \"[.?!]\")[[1]]\n\nstr(sentences)\n\nlength(sentences)\n\nsentences[3]\nhead(sentences)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# CHALLENGE\n#\n# Get a vector of all quotations from \"The adventures of Sherlock Holmes\".\n# HINT: use an un-greedy search\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n\n\n# Other arguments and functions ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# help pages\n\n?regex\n\n?grep\n\n?strsplit\n\n\n# grep(pattern, x, ignore.case = FALSE, perl = FALSE, value = FALSE,\n#      fixed = FALSE, useBytes = FALSE, invert = FALSE)\n#\n# grepl(pattern, x, ignore.case = FALSE, perl = FALSE,\n#       fixed = FALSE, useBytes = FALSE)\n#\n# sub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,\n#     fixed = FALSE, useBytes = FALSE)\n#\n# gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,\n#      fixed = FALSE, useBytes = FALSE)\n#\n\ntext = \"She sells sea shells by the sea shore.\"\n\nsub(\"sea\", \"xxxxx\", text)\n\ngsub(\"sea\", \"xxxxx\", text)\n\nsub(\"s\", \"x\", text)\n\ngsub(\"s\", \"x\", text)\n\ngsub(\".\", \"x\", text)\n\ngsub(\"\\\\.\", \"x\", text)\n\ngsub(\".\", \"x\", text, fixed=TRUE)\n\n\nwords = strsplit(text, \" \")\n\nwords\n\n# Get the 2nd word from the text:\nwords[[1]][2]\n\n\nwords = strsplit(addresses, \" \")\n\n\nwords\n\n# Get the 2nd word from the 3rd address\nwords[[3]][2]\n\n\n# Other functions - sub vs gsub, regexpr, gregexpr, regexec ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# See the documentation for the following functions: \n#\n# regexpr(pattern, text, ignore.case = FALSE, perl = FALSE,\n#         fixed = FALSE, useBytes = FALSE)\n#\n# gregexpr(pattern, text, ignore.case = FALSE, perl = FALSE,\n#          fixed = FALSE, useBytes = FALSE)\n#\n# regexec(pattern, text, ignore.case = FALSE, perl = FALSE,\n#         fixed = FALSE, useBytes = FALSE)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "0200200-a-introToRegularExpressions-v036.html#more-practice-with-regex---see-the-following-websites",
    "href": "0200200-a-introToRegularExpressions-v036.html#more-practice-with-regex---see-the-following-websites",
    "title": "19  19.  regular expressions (regex)",
    "section": "19.33 More practice with regex - see the following websites",
    "text": "19.33 More practice with regex - see the following websites\n# More practice with regex  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# See the following sites:\n\n# http://regextutorials.com/\n# https://regexone.com/\n# https://librarycarpentry.org/lc-data-intro/03-quiz/index.html\n# https://www.hackerrank.com/domains/regex\n# https://regex.sketchengine.co.uk/\n\n\n# Period comes between rest of line and first word\npattern = paste0(\"(^[A-Za-z]+)\",     # first word on line\n                 \"(\\\\s+)\",           # spaces after the first word\n                 \"(.*)\"              # rest of the line\n                 )\n\nstr_replace_all(sentences, pattern=pattern, replacement=\"\\\\3\\\\2\\\\1\")\n\n\n\n# Period comes between rest of line and first word\npattern = paste0(\"(^[A-Za-z]+)\",     # first word on line\n                 \"(\\\\s+)\",           # spaces after the first word\n                 \"(.*)\",              # rest of the line except for the final period\n                 \"([?!.])\"\n)\n\nmovedFirstWordToEnd = str_replace_all(sentences, pattern=pattern, replacement=\"\\\\3\\\\2\\\\1\\\\4\")\nstr_to_sentence(movedFirstWordToEnd)\n\n#########################\n\n# change the case on the first word to uppercase\n# change the case on the last word to lowercase\n\ntoupper(c(\"hello\", \"goodbye\"))\n\n\n\n#str_replace_all(sentences, pattern=\"(^[A-Za-z]+)(\\\\s+)(.*)\", \"\\\\3\\\\2\\\\1\")",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'><yrChapterNumber>19. </yrChapterNumber> regular expressions (regex)</span>"
    ]
  },
  {
    "objectID": "000260-PracticeWithRegulaExpressions-v003.html",
    "href": "000260-PracticeWithRegulaExpressions-v003.html",
    "title": "20  20.  Practice with Regular Expressions",
    "section": "",
    "text": "Please refer to the following websites for some practice with creating Regular Expressions (regex). Solutions are available online if you get stuck.\n\nThe following website has a tutorial and practice problems as you work through the tutorial: https://regexone.com/ ( also see: https://umar-yusuf.blogspot.com/2021/03/regexonecom-alternative-solution.html )\nThe following is another website with a short tutorial and practice problems: http://regextutorials.com/\nThe following website encourages you to practice regex concepts through puzzles that are very similar to typical “crossword” puzzles. Make sure to read “how to play”. There is a website and an app for your phone. They are both good (the phone app is much easier for using on a phone than the website): https://regexcrossword.com/",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'><yrChapterNumber>20. </yrChapterNumber> Practice with Regular Expressions</span>"
    ]
  },
  {
    "objectID": "0990100-cleaningData-v001.html",
    "href": "0990100-cleaningData-v001.html",
    "title": "21  21.  “Cleaning” data",
    "section": "",
    "text": "Often data comes to us in a “messy” format. One of the “less glamorous” but very important jobs of anyone working with data is to know how to “clean up” “messy” data. In other words to put the data into a common format.\nFor example, suppose that employees in a company submit expense reports every month for travel and other business expenses. It would be best if the data entry system (e.g. a website) enforced a standard format for the data. However, this doesn’t always happen. Sometimes the system tries to enforce a specific format but people “misuse” the system leading to inconsistencies between different entries. Sometimes data from different data entry systems are combined into a single data set.\nThe data below represents expenses from various employees. There are many inconsistencies between different entries. For example, some names are in “first last” format and some names are in “last, first” format. There are many other formatting inconsistencies that are listed below.\n\nlibrary(tibble)\nexpenses = tribble(\n  ~employee,           ~date,          ~amount,          ~category,         ~comments,\n  \"Sue Smith\",         \"1/19/2023\",    \"59.99\",         \"food\",            \"lunch with client\",\n  \"Schwartz, Joe\",     \"01/19/2023\",   \"$27.00\",        \"office supplies\", \"paper for printer\",\n  \"mike david harris\", \"2023-01-19\",   \"25\",            \"Office Stuff\",    NA,\n  \"Dr. A. Davis\",      \"19/1/2023\",    \"five hundred\",  \"FOOD\",            \"NA\",\n  \"Dr Jones\",          \"1/19/23\",      \"1,234.56\",      \"office suppl.\",   \"chairs\",\n  \"S. Jones Jr\",       \"19/1/23\",      \"1000\",          \"Office supplies\", \"desk\",\n  \"Conway, Ella Sr.\",  \"Jan 19, 2023\", \"$35.23\",        \"LUNCH\",           \"---\",\n)\n\nas.data.frame(expenses)\n\n           employee         date       amount        category          comments\n1         Sue Smith    1/19/2023        59.99            food lunch with client\n2     Schwartz, Joe   01/19/2023       $27.00 office supplies paper for printer\n3 mike david harris   2023-01-19           25    Office Stuff              &lt;NA&gt;\n4      Dr. A. Davis    19/1/2023 five hundred            FOOD                NA\n5          Dr Jones      1/19/23     1,234.56   office suppl.            chairs\n6       S. Jones Jr      19/1/23         1000 Office supplies              desk\n7  Conway, Ella Sr. Jan 19, 2023       $35.23           LUNCH               ---\n\n\nThe following are many of the inconsistencies\n\nInconsistencies with the name entries\n\nsome names are in “first last” format and some names are in “last, first” format.\nsome names are capitalized, some aren’t\nsome names have middle include middle names, some don’t\nsome names include titles, some don’t\nsome names include a title, e.g. Dr. , while some don’t\nsome of the titles contain a period after the title, some don’t\nsome names include a suffix (e.g. Jr), some don’t\n\nInconsistencies with dates\n\nsome dates are in mm/dd/yyyy format (USA style)\nsome dates are in dd/mm/yyyy format (European style)\nsome dates are in yyyy-mm-dd format (standard for many computer systems)\nsome dates use 4 digit years, some use 2 digit years\nsome dates have a leading zero for single digit days or months\n\nThere are additional inconsistencies with the amount, category and comments columns\n\n\n22 What you need to do\nWrite a function named cleanUp that takes the dataframe as an argument and returns a cleaned dataframe where all of the values in each column follow a consistent structure.",
    "crumbs": [
      "Regular Expressions",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'><yrChapterNumber>21. </yrChapterNumber> \"Cleaning\" data</span>"
    ]
  },
  {
    "objectID": "0000011-WhatIsACli-v010.html",
    "href": "0000011-WhatIsACli-v010.html",
    "title": "22  22.  Intro to Command Line Interfaces (CLIs)",
    "section": "",
    "text": "22.1 WHAT IS A COMMAND LINE INTERFACE (CLI)?\nModern computers and the programs that they run allow you to click and drag icons, use menus, open many programs at once, etc. It's hard to imagine a computer that doesn't have these features. However, a long time ago (pre 1990'ish) none of this was possible. In order to use a computer you had to memorize many different commands. You would type these commands into the computer in order to run programs and interact with the computer.\nToday the modern computer \"interface\" (the one that has icons, menus, etc.) is known as a Graphical User Interface or GUI (pronounced \"gooey\"). The old-style interface that required that you type commands to do ANYTHING is still able to be used. This type of interface is known as a \"Command Line Interface\" or CLI.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'><yrChapterNumber>22. </yrChapterNumber> Intro to Command Line Interfaces (CLIs)</span>"
    ]
  },
  {
    "objectID": "0000011-WhatIsACli-v010.html#why-should-i-care-about-a-cli",
    "href": "0000011-WhatIsACli-v010.html#why-should-i-care-about-a-cli",
    "title": "22  22.  Intro to Command Line Interfaces (CLIs)",
    "section": "22.2 WHY SHOULD I CARE ABOUT A CLI ?",
    "text": "22.2 WHY SHOULD I CARE ABOUT A CLI ?\nFor day to day casual computer work, it may be hard to understand why anyone would use a CLI instead of a modern GUI interface. Using a CLI requires memorizing many different seemingly arcane commands instead of just \"pointing and clicking\" on what you want. Why would anyone want to do that?\n\n22.2.1 Reason 1 - it allows you to do powerful things quickly\nMany of the CLI commands can be quite powerful and can cut down the amount of time it takes to do things by A LOT. For example, suppose a folder contains 1000 files. For example, \"expenseReport-Jan2020.xlsx\", \"meetingMinutes-Jan012020.docx\", etc. (1000 such files) You are told by your boss that you must add your name as part of the filename for each of these 1000 files. If you are using a GUI, that could take hours. However, for someone who is proficient with a using a CLI, it could take seconds to type in the correct commands to do that for all 1000 files.\n\n\n22.2.2 Reason 2: - you can create “scripts” to automate many things\nSuppose your boss told you that everyone in the company needs to make the same changes to their filenames. However, not everyone knows how to use a CLI. You could theoretically create a small \"script\" (ie. a program) using the CLI commands that would allow anyone to \"run the script\" to make the changes. Tasks that need to be done repeatedly are often best accomplished by creating a CLI script. The script can then be run anytime the task needs to be done. Note that you can create similar scripts or programs using another programming language but a CLI script is often the easiest and most appropriate method to create such scripts.\n\n\n22.2.3 Reason 3: Tech and data professionals are often expected to understand how to use a CLI.\nOften many advanced commands are only available to be run from the command line. For example, the wget program is a command line program that can be used to download the code for an entire website or just specific files that you’d like to get.\nMany basic concepts from the command line carry over directly to any programming language (including R) in relation to interacting with the operating system. Concepts such as absolute and relative paths, wildcards in filenames, etc. come up over and over again when you work with data.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'><yrChapterNumber>22. </yrChapterNumber> Intro to Command Line Interfaces (CLIs)</span>"
    ]
  },
  {
    "objectID": "0000011-WhatIsACli-v010.html#different-clis-for-different-computer-systems",
    "href": "0000011-WhatIsACli-v010.html#different-clis-for-different-computer-systems",
    "title": "22  22.  Intro to Command Line Interfaces (CLIs)",
    "section": "22.3 DIFFERENT CLIs FOR DIFFERENT COMPUTER SYSTEMS",
    "text": "22.3 DIFFERENT CLIs FOR DIFFERENT COMPUTER SYSTEMS\nJust as the Graphical User Interface (GUI) for the Mac is different from the GUI for Windows, the standard CLI available for the Mac is different from the CLI(s) that are available on Windows. This means that the commands that you need to type on Mac are different from the commands that you would type on Windows.\n\n22.3.1 Mac\nThe Mac CLI is used by running the Terminal program that comes with the Mac. The terminal program actual runs a form of the Bash shell (see more below).\n\n\n22.3.2 Windows\nWindows comes with two different CLIs built into the system:\n\nThe “Command Prompt” (cmd) : This was the original CLI for Windows. It is a little bit dated and not as powerful as other options. Nevertheless it is still very popular.\n“Powershell”: This is a newer CLI that Microsoft first introduced in 2006. It is more complicated to use than the “Command Prompt” but in many ways is more powerful. Today Windows comes with both CLIs.\nThere are also options for installing a version of the “Bash Shell” on Windows. See below for more info.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'><yrChapterNumber>22. </yrChapterNumber> Intro to Command Line Interfaces (CLIs)</span>"
    ]
  },
  {
    "objectID": "0000011-WhatIsACli-v010.html#what-is-the-bash-shell-program",
    "href": "0000011-WhatIsACli-v010.html#what-is-the-bash-shell-program",
    "title": "22  22.  Intro to Command Line Interfaces (CLIs)",
    "section": "22.4 What is the “Bash Shell” program?",
    "text": "22.4 What is the “Bash Shell” program?\nWhile most computer users today use either the Windows or Mac operating system, the Linux operating system is very popular for running “server” computers. These are the computers that support many of the “backoffice” tasks in companies today. These include computers that host company websites as well as company databases and many other critical corporate technology services.\nLinux is a derivative of the Unix operating system. These operating systems, (Linux and Unix), have many different “command line interface” (CLI) programs available to choose from. A very popular CLI on these computers is the “Bash Shell”. The word “shell” is a synonym for a “command line interface”. The word “shell” was first coined by the creators of Unix. The idea is that to interact with guts of the operating system you must “go through” the shell (i.e. use the CLI commands), similar to the way you must “go through the shell” of an egg in order to get to the inside.\nNote that the “Bash Shell” has become somewhat of a standard. Apple Mac is also derived from the original Unix operating system and by default, the Mac Terminal program also runs a version of the Bash Shell. While Windows doesn't natively come with the Bash Shell, you can download many different versions of a \"Bash\" CLI program that can be used on Windows. Some of these are listed below. Note that installing these go beyond the scope of what we are covering here. For more info search online for info about each of these options.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'><yrChapterNumber>22. </yrChapterNumber> Intro to Command Line Interfaces (CLIs)</span>"
    ]
  },
  {
    "objectID": "0000013-HowToRunTheBashShellCli-v011.html",
    "href": "0000013-HowToRunTheBashShellCli-v011.html",
    "title": "23  23.  How to run the Bash Shell",
    "section": "",
    "text": "23.0.1 How to run Bash on Microsoft Windows\nThe following are several options for installing Bash on Windows.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'><yrChapterNumber>23. </yrChapterNumber> How to run the Bash Shell</span>"
    ]
  },
  {
    "objectID": "0000013-HowToRunTheBashShellCli-v011.html#using-bash-in-rstudios-terminal-window",
    "href": "0000013-HowToRunTheBashShellCli-v011.html#using-bash-in-rstudios-terminal-window",
    "title": "23  23.  How to run the Bash Shell",
    "section": "23.1 Using Bash in RStudio’s “Terminal” window",
    "text": "23.1 Using Bash in RStudio’s “Terminal” window\nThe RStudio program has a “terminal” window in which you can run CLI commands. This is usually found as a “tab” in the lower left hand window “pane”. You can also choose the following menu choices to open the Terminal tab: Tools menu &gt; Terminal &gt; Move Focus to Terminal\nYou can configure RStudio to use whichever CLI you have on your computer. The posit.cloud version of RStudio as well as the locally installed version of RStudio that runs on a Mac is automatically configured to use the Bash Shell as the CLI so just opening the Terminal window is all you need to do.\n\n23.1.1 Make sure you are using Bash and not another CLI\nIf you are using a “local” install of RStudio on Windows, then you must first install a Bash environment (I recommend Git Bash). After installing Bash on your computer, you need to configure RStudio to use Bash in its “terminal” tab as described below). To do so, choose Bash from the following menu (this is covered in more detail in the following section of this document):\nTools menu &gt; Global Options &gt; Terminal &gt; New terminals open with (dropdown list) &gt;\n(choose Bash or Git Bash or whatever version of Bash you have installed)\n\n\n\n23.1.2 Problems running Bash in RStudio? Try changing some options in RStudio.\nThere are MANY ways to configure the RStudio environment. If you're having touble running Bash from the Terminal window in RStudio, follow the steps shown below. (To learn a lot more about how to use the Terminal window in RStudio, see the official documentation here:\nhttps://support.posit.co/hc/en-us/articles/115010737148-Using-the-RStudio-Terminal\n(You can also get to this page by clicking on the link entitled \"Using the RStudio terminal\" in the RStudio Terminal options window, as shown in the screen-shots below.)\nAre you having problems getting the \"Bash shell\" command line interface (CLI) to run in RStudio?\n\nIf you don't see the RStudio \"Terminal\" tab anywhere in the RStudio interface then choose, the following menu choices from RStudio's main menu: Tools &gt; Terminal &gt; Move Focus to Terminal\nIf you DO see the Terminal tab but you aren't running the \"Bash shell\" command line interface in the Terminal, then do the following:\n\n \nRStudio is highly configurable. To modify any of the plethora of options for RStudio - choose the \"tools\" menu in RStudio and select \"General Options\". That will open the window shown in the 2nd screenshot above. You can then choose from any of the categories on the left hand side of that window to access the options under that category. For example, to change the font used in the editor you can click on the \"Appearance\" category (on the left side of the Options window) - at which point you will see several options including those allowing you to specify the font, font-size, etc.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'><yrChapterNumber>23. </yrChapterNumber> How to run the Bash Shell</span>"
    ]
  },
  {
    "objectID": "0000013-HowToRunTheBashShellCli-v011.html#sec-bashPrompt",
    "href": "0000013-HowToRunTheBashShellCli-v011.html#sec-bashPrompt",
    "title": "23  23.  How to run the Bash Shell",
    "section": "23.2 What is the Bash \"prompt\" ?",
    "text": "23.2 What is the Bash \"prompt\" ?\nWhen you type commands in the R console, R \"prompts\" you to enter a new command every time you see the \"&gt; \" at the beginning of a line. For example, the following shows an R command being run in the R Console window. The \"&gt;\" at the beginning of the second line is the R \"prompt\", where you can enter your next command:\nThe \"R\" prompt below is \"&gt;\"\n \nIn a similar way, before you type a command in Bash, the CLI \"prompts\" you to enter a new command. However, in Bash the exact \"prompt\" can be different on different computers and for different users. Often the Bash prompt is something like this:\nusername@hostname:~$ (TYPE YOUR NEXT BASH COMMAND HERE)\nFor example, on my computer, the following shows when happens on my computer when I run the Bash command, \"expr 5 \\* 100\" , which simply multiplies 5*100 to get 500. The next line shows the prompt waiting for the user to type another command:\n\nThe Bash prompt below is: yrose@LAPTOP-IGVE3JIE:~$\n\nIn this example, \"yrose\" is my username on my computer and LAPTOP-IGVE3JIE is the \"name\" of my computer (many people don't realize that their computer has a name). This type of prompt became popular since power‑users of Bash often connect remotely to many different computers from their Bash window. By just looking at the Bash prompt a power‑user who might be connecting to different machines can know immediately which computer they are currently connected to.\nThe ~ is called a \"tilde\". The significance of the tilde, ~, will be discussed more later. Finally, the $ at the end of the prompt is there to show where you are to enter your next command. Historically, the Bash prompt was JUST a dollar sign, $ , similar to how the R prompt is just a greater than sign, &gt;. In subsequent years, Bash evolved to show more information in the prompt. The prompt can be configured to show a lot of different types of information (see the next section).\n\n23.2.1 How to change the Bash prompt\nAs noted above, the Bash prompt can be configured to include different types of information. Sometimes it is simplest to just see a simple '$' prompt. To do so, type the following command. There shouldn't be any spaces on either side of the equals sign:\nPS1='\\$ '\nAfter doing so your Bash prompt should become a single dollar sign, $. To change the prompt to the more typical version shown in the previous section type the following command:\nPS1='\\u@\\h:\\w\\$ '\nThe \\u stands for the username, \\h for the hostname and \\w for the \"working directory\" (we'll discuss what that means later). See the following page for more information about how to configure your prompt in all sorts of interesting ways,\nhttps://linuxhint.com/bash-ps1-customization/\nChanging your prompt to the \"working directory\" makes navigating the Bash environment much easier. Once you are running Bash, to see the \"working directory\" in the prompt, type the following Bash command. NOTE: there are no spaces ANYWHERE in this command other than the one space that appears before the final single quote (i.e. apostrophe):\n\nPS1='\\w\\$ '\nNow a user, joe, who runs the following commands will see the results shown below.\n\n$ cd # change the working directory to /Users/joe\n$ PS1='\\w\\$ ' # change the prompt to include the working directory\n~$ pwd # ~ is shorthand for /Users/joe\n/Users/joe\n~$ cd Documents # move to another directory\n~/Documents$ pwd # show the path for the current directory\n/Users/joe/Documents\nIf you want this to happen every time you start a new Terminal window, then run the following command. This command creates a file named .profile (there is a period at the very beginning of the filename) in your \"home directory\". This file contains options that control how Bash runs.\necho \"PS1='\\w\\$ '\" &gt;&gt; $HOME/.profile\n\nIn the rest of this document, I will use a simple $ prompt. This makes it easier to read the information in this document.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'><yrChapterNumber>23. </yrChapterNumber> How to run the Bash Shell</span>"
    ]
  },
  {
    "objectID": "0000013-HowToRunTheBashShellCli-v011.html#how-can-i-confirm-that-im-using-the-bash-command-line-interface-program",
    "href": "0000013-HowToRunTheBashShellCli-v011.html#how-can-i-confirm-that-im-using-the-bash-command-line-interface-program",
    "title": "23  23.  How to run the Bash Shell",
    "section": "23.3 How can I confirm that I'm using the \"Bash\" Command Line Interface program?",
    "text": "23.3 How can I confirm that I'm using the \"Bash\" Command Line Interface program?\nAs noted above, both Windows and Mac can be configured with different CLI programs. The RStudio Terminal window can be configured to use any CLI that is installed on your computer. The instructions to do so appear earlier in this document. To confirm that you are actually using the Bash CLI, you can run the following command:\necho $0\nIf you are running the Bash shell/CLI, then the output should include the word \"bash\" somewhere. For example the following is what I get when I run this command on my computer:\n\nIf you do not get a result showing the word \"bash\" somewhere, refer to the sections above in this document that describe how to configure RStudio to use Bash.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'><yrChapterNumber>23. </yrChapterNumber> How to run the Bash Shell</span>"
    ]
  },
  {
    "objectID": "0000013-HowToRunTheBashShellCli-v011.html#bash-commands-or-linux-commands",
    "href": "0000013-HowToRunTheBashShellCli-v011.html#bash-commands-or-linux-commands",
    "title": "23  23.  How to run the Bash Shell",
    "section": "23.4 \"Bash commands\" or \"Linux commands\" ?",
    "text": "23.4 \"Bash commands\" or \"Linux commands\" ?\nTechnically, many of the commands that you run in Bash are actually \"Linux\" commands (or Unix commands). Therefore when searching online for information, it is often more appropriate to use the words \"Linux command\" in your search instead of \"Bash command\". As we said above, the Bash shell, is only one of many CLIs that are available on Linux. However, the majority of commands we will learn are able to be used with ANY Linux CLI and not just Bash.\nIn addition to being able to use Bash on Apple Mac, you can also access many of the vast majority of these \"Linux\" commands on Mac. (Remember that Linux and Mac are both derived from the Unix operating system). Similarly, when you install a Bash environment on Windows, it will include many of these \"Linux\" programs too.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'><yrChapterNumber>23. </yrChapterNumber> How to run the Bash Shell</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html",
    "href": "0000025-introToBash-v007.html",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "",
    "text": "24.1 Ryans Tutorials\nThe following tutorial provides an intro to using Bash. The title of this tutorial is “Linux Tutorial” as Bash is commonly used on the Linux operating system. However, this tutorial also applies to using Bash on the Mac and on Windows.\nhttps://ryanstutorials.net/linuxtutorial/\nMost of the information that we covered before the midterm can be found in the first 8 sections of this tutorial. Links are below along with some of my own comments. Please use the notes on this website to study for the midterm.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#ryans-tutorials",
    "href": "0000025-introToBash-v007.html#ryans-tutorials",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "",
    "text": "The Command Line - What is it, how does it work and how do I get to one.\nBasic Navigation - An introduction to the Linux directory system and how to get around it.\nMore About Files - Find out some interesting characteristics of files and directories in a Linux environment.\nManual Pages - Learn how to make the most of the Linux commands you are learning.\nYou should know how to read the contents of a “man page”.\nNote that the “man” command isn’t installed by default on posit.cloud nor on Mac or Windows Bash environments. However, you can search online for any man page, (e.g. google for man ls).\nAlternatively , you can access the man pages here: https://man7.org/linux/man-pages/dir_section_1.html\nFile Manipulation - How to make, remove, rename, copy and move files and directories.\nYou are NOT responsible for the “vi” editor. “vi” is a text editor that is commonly found in most Bash environments. While it is useful to know, vi can be a bit confusing for new users. Vi Text Editor - Discover a powerful Linux based text editor.\nWildcards - Also referred to as globbing, this is a means to refer to several files in one go.\nWILF ONLY (we learned the basic concepts about how to understand the first column of info that is output by the “ls -l” command. You are NOT responsible for the chmod command that is described here.)\nPermissions - Learn to identify and change the permissions of files and directories and what the consequences of these are.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#download-files-for-use-in-these-lessons",
    "href": "0000025-introToBash-v007.html#download-files-for-use-in-these-lessons",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.2 Download files for use in these lessons",
    "text": "24.2 Download files for use in these lessons\nA “zip file” is a single file that contains a condensed version of one or more files or folders. Click on this link to download a “zip file” named myFiles.zip that contains files that we will be working with to demonstrate many of the commands available with the Bash CLI.\nTo “extract” (or unzip) all of the files that are in this zip file, run the following command:\nunzip myFiles.zip",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#working-with-zip-files-in-bash",
    "href": "0000025-introToBash-v007.html#working-with-zip-files-in-bash",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.3 Working with ZIP files in Bash",
    "text": "24.3 Working with ZIP files in Bash\nA “zip” file is a single file into which many different individual files have been placed. For example, if you want to mail several different files to someone, you can place all of those files into a single “zip” file and then email the single zip file. The recipient will need to “unzip” the file to retrieve the individual files. The single zip file is often smaller in size than the collection of individual files (i.e. the “zip” file is “compressed”).\nOn Mac’s the contents of a zip file is automatically extracted into a folder when you try to access the zip file by clicking on it. The Mac creates a new folder to place the contents of the zip file. On Windows, this doesn’t happen automatically. Rather you must “uncompress” the zip file. You can do so by right clicking on the file name and choosing “uncompress”. On posit.cloud, it seems like zip files are automatically unzipped into a folder when a zip file is uploaded.\nThe following commands can be used from Bash to zip or unzip a set of files. There are different types of zip files - however, the different types often use the same .zip filename extension. The zip/unzip commands work with one type of zip file and gzip/gunzip commands work with another type of zip file. To unzip a file you can try the unzip command and if that doesn’t work try the gunzip command.\n\nzip -r FILENAME.zip SOME_FOLDER - creates a new zip file\nunzip FILENAME.zip - extracts the contents of a zip file\ngzip / gunzip - zip/unzip Gnu types of zip files\n\n\n24.3.1 Vocabulary related to folders (also known as “directories”)\n\nA “directory” is a “folder” The word “directory” is a synonym for the word “folder”. They two words mean the EXACT SAME THING.\nThe “parent” of folder/direcotry, x, is the folder/directory that contains x.\n\n\n\n24.3.2 Important Bash commands that everyone should know\nThe following gives a brief description of how many commands work. In the descriptions below, any part of the command that appears in [square brackets] is optional.\n\npwd - “Print the Working Directory” (ie display the “path” for the current working directory (CWD)\ncd SOME_DIRECTORY - set the “working directory” to the specified directory\ncd - set the “working directory” to your “home directory”\ncd .. - set the “working directory” to the “parent” of the current working directory\ncd ../.. - set the “working directory” to the parent of the parent of the current working directory\ncd ../../.. (etc.) - set the “working directory” to 3 levels up in the directory hierarchy)\nmv SOME_FILE SOME_DIRECTORY - move SOME_FILE to SOME_DIRECTORY\nmv SOME_DIRECOTRY ANOTHER_DIRECTORY - move SOME_FILE to ANOTHER_DIRECTORY\nls [SOME_DIRECTORY] - list the files in the current directory or the specified [DIRECTORY]\nls -l [SOME_DIRECTORY] - show a “long” listing (i.e. more info about each file/folder)\nls -r [SOME_DIRECTORY] - reverse the order that the files are listed\nls -lt [SOME_DIRECOTRY] - show the contents in time order (with a long listing format)\nls -l -t [SOME_DIRECOTRY] - SAME THING - show the contents in time order (with a long listing format)\nls -a [SOME_DIRECTORY] - show all files (including files that start with a period ( . )\nls -R [SOME_DIRECTORY] - show the files that are in all folders below the CWD (or the specified directory)\nls SOME_FILES - list the names of the files\ncat SOME_FILES - display the contents of the files (only for “text” files)\nfile SOME_FILE - determine the type of file (e.g. ASCII, Excel, etc.)\ntree . - show directory hierarchy (tree is not installed on many systems)",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#creatingremoving-directories-aka-folders",
    "href": "0000025-introToBash-v007.html#creatingremoving-directories-aka-folders",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.4 creating/removing directories (aka folders)",
    "text": "24.4 creating/removing directories (aka folders)\n\nmkdir SOME_DIRECTORY - “make” the specified directory\nrmdir SOME_DIRECORY - “remove” directory (must be empty, NOTE also see rm command)",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#removing-files",
    "href": "0000025-introToBash-v007.html#removing-files",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.5 Removing files",
    "text": "24.5 Removing files\nBE CAREFUL WITH THE FOLLOWING COMMANDS!!!\nTHERE IS NO UNDO\nONCE YOU’VE REMOVED A FILE/FOLDER YOU CANNOT GET IT BACK!\n\nrm SOME_FILES - remvove the files (THERE IS NO WAY TO GET THEM BACK)\nif you are brave, you can research how to use rm to remove entire directories (that aren’t empty)",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#creating-files-using-and",
    "href": "0000025-introToBash-v007.html#creating-files-using-and",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.6 creating files using > and >>",
    "text": "24.6 creating files using &gt; and &gt;&gt;\n\necho SOME_MESSAGE - display the message\necho SOME_MESSAGE &gt; SOME_FILE - put the message in the file (erase current contents of file)\necho SOME_MESSAGE &gt;&gt; SOME_FILE - append the message to the file",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#working-with-bash-variables",
    "href": "0000025-introToBash-v007.html#working-with-bash-variables",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.7 Working with BASH variables",
    "text": "24.7 Working with BASH variables\n\nenv - display the variables that currently exist\nVARIABLE=VALUE - set the value of VARIABLE to VALUE (do NOT type any spaces next to the = sign)\necho $VARIABLE - show the value of a specific variable\nFor an example see see Section 23.2 for info about the PS1 variable",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#material-that-is-not-on-the-midterm",
    "href": "0000025-introToBash-v007.html#material-that-is-not-on-the-midterm",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.8 Material that is NOT on the midterm",
    "text": "24.8 Material that is NOT on the midterm\n\n24.8.1 Working with “tar” files\n“tar” files are similar to zip files but tar files are not “compressed”. “tar” files are popular mostly on Linux computers while “zip” files are popular on all computer platforms.\n\ntar - create or extract a tar archive\n\n\n\n24.8.2 SOME MORE COMMANDS - search online for info about these\n\nwc - count words, lines, symbols in a file\nhead - show first few lines of a file\ntail - show last few lines of a file\nsort\nuniq\ncut\npaste\ncurl - download files from the Internet\ntouch SOME_FILE - update the timestamp of the file OR create the file if it didn’t exist\nless (or “more” depending on your system) - display the contents of a file one page at a time\ngrep - search for a specific string of text in a file or multiple files\nfind - search for files and directories in a file system\nchmod - change the permissions of a file or directory\nchown - change the ownership of a file or directory\nhistory - display a list of previously executed commands",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0000025-introToBash-v007.html#launch-vscode-from-bash-on-windows",
    "href": "0000025-introToBash-v007.html#launch-vscode-from-bash-on-windows",
    "title": "24  24.  Working with the Bash Shell.",
    "section": "24.9 Launch VSCode from Bash (on Windows)",
    "text": "24.9 Launch VSCode from Bash (on Windows)\nVSCode is a popular text editor. To launch VSCode directly from Bash on Windows, you can use the following commands directly from Bash. While VSCode is also popular on Mac, unfortunately the following commands do now work on Macs.\n\ncode - launch the VSCode text editor program (if you’ve installed it)\ncode [SOME_FILE] - launch VSCode and open SOME_FILE\ncode [SOME_DIRECTORY] - launch VSCode with the “explorer” window set to SOME_DIRECTORY.\ncode . (there is a period there) - launch VSCode with the “explorer” window set to the current working directory.",
    "crumbs": [
      "Bash",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'><yrChapterNumber>24. </yrChapterNumber> Working with the Bash Shell.</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html",
    "href": "0500100-a-tdyvrsDplyr-v005.html",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "",
    "text": "25.1 Get the data\nThe data we are using in this section contains information about salespeople who are employees of a company. Each row of the data contains info about one salesperson. The salespeople get paid some “base pay” as well as a commission that is a percent of the total dollar amount of sales they make.",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#get-the-data",
    "href": "0500100-a-tdyvrsDplyr-v005.html#get-the-data",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "",
    "text": "25.1.1 Load the R packages we’ll neeed\n\nif(!require(tibble)){install.packages(\"tibble\");require(tibble);}\nif(!require(dplyr)){install.packages(\"dplyr\");require(dplyr);}\nif(!require(readr)){install.packages(\"readr\");require(readr);}\n\n\n\n25.1.2 Download the file\nThe data is contained in a csv file. If you’d like to follow along with this tutorial on your own computer you can download the .csv file we are using by clicking here\n\n\n\n25.1.3 dplyr is best used with tibbles\nThe majority of the dplyr functions work with tibbles or dataframes. However, the tidyverse in general and dplyr specifically is designed to work better with tibbles.\nIf you need to, you can always convert a tibble to a dataframe or a dataframe to a tibble.\n\n\n25.1.4 Import the data by clicking on some buttons …\n\nThe code in the next section below uses read_csv function to read the data into R. If you are not comfortable with R, I recommend that you instead, follow the instructions starting in the next bullet to import the data into R.\nIf you are not familiar with R, you may have some trouble running the read_csv() code shown below. Instead, I recommend that you follow the following instructions to import the file into R.\nTo do so, click on “Import Dataset” button on the “Environment” tab (usually found in the upper-right-hand window pane in RStudio).\nChoose “From Text (base)” and locate your file. You should see something like this:\n\nMake sure to change the “Name” portion (see circled section in picture) to read “sales”, then press the “import” button. This will open up a new tab in RStudio that shows the contents of the file. You can safely navigate away from this tab or close the tab and the data will remain imported and can be seen by typing “sales” (without the “quotes”).\nFinally, in order to ensure that you are working with a tibble and not with a data.frame you can run the following R code:\nsales = as_tibble(sales)\n\n\n\n25.1.5 Import the data by typing some R code …\nThe following code reads the data into R. Alternatively, you can follow the instructions above to click on some buttons to import the data.\n\n# Read in the data into a tibble\n#\n# Note that the following code uses the readr::read_csv function from the readr\n# package which is part of the tidyverse collection of packages. \n# This function is similar to the base-R read.csv function.\n# \n# read_csv returns a tibble, which is the data structure that the \n# tidyverse packages use in lieu of dataframes. A tibble is basically\n# a dataframe with extra features.\n# By contrast, the base-r read.csv function returns a dataframe.\n \nsales = read_csv(\"salespeople-v002.csv\", na=c(\"\",\"NULL\"), show_col_types=FALSE)",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#display-the-data",
    "href": "0500100-a-tdyvrsDplyr-v005.html#display-the-data",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "25.2 Display the data",
    "text": "25.2 Display the data\nSince the data is in a tibble, by default, only the first 10 rows are displayed. In addition, only the columns that fit on the screen will be displayed. If the rows are too wide for the screen, then some columns may not be displayed and/or the contents of some columns may be shortened.\n\n# Display the first few rows and columns of the sales data\nsales\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith   Mana… usa                 40              100               10\n 2 Sam    Lee     Sale… usa                 40              200                6\n 3 Sue    Sallin… Sale… asia                30              150               10\n 4 Barb   Brown   Sr. … asia                35               79                4\n 5 Jack   Martin  Mana… europe              20               40                3\n 6 Kate   Carey   Sr. … europe              20               69                2\n 7 Emy    Parsons Sale… africa              25               87                3\n 8 Monica Oja     Sale… africa              35               98               10\n 9 Jim    Kuruzo… Sr. … usa                 35              140                3\n10 Zeke   Cohen   Mana… usa                 30              250               10\n# ℹ 14 more rows",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#display-more-rowscolumns-of-the-tibble",
    "href": "0500100-a-tdyvrsDplyr-v005.html#display-more-rowscolumns-of-the-tibble",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "25.3 Display more rows/columns of the tibble",
    "text": "25.3 Display more rows/columns of the tibble\n\n25.3.1 print( SOME_TIBBLE, n=NUMBER_OF_ROWS)\n\n# Use the print function to modify the output. Use the following arguments:\n#    n - number of rows to display (Inf, i.e. infinity, for all rows)\n#    width - maximum width of a row to display (might wrap if it's too long for your screen)\nprint(sales, n=3)\n\n# A tibble: 24 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n# ℹ 21 more rows\n\n\n\n\n25.3.2 print( SOME_TIBBLE, width=WIDTH_OF_DATA)\n\n\n# Display all rows and all columns\nprint(sales, n=Inf, width=Inf)\n\n# A tibble: 24 × 7\n   first  last       title           region baseInThousands salesInThousands\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith      Manager         usa                 40              100\n 2 Sam    Lee        Salesperson     usa                 40              200\n 3 Sue    Sallinger  Salesperson     asia                30              150\n 4 Barb   Brown      Sr. Salesperson asia                35               79\n 5 Jack   Martin     Manager         europe              20               40\n 6 Kate   Carey      Sr. Salesperson europe              20               69\n 7 Emy    Parsons    Salesperson     africa              25               87\n 8 Monica Oja        Salesperson     africa              35               98\n 9 Jim    Kuruzovich Sr. Salesperson usa                 35              140\n10 Zeke   Cohen      Manager         usa                 30              250\n11 Zeke   Smith      Salesperson     asia                20              120\n12 Amy    Markovic   Salesperson     asia                46              340\n13 Joe    Washington Sr. Salesperson europe              33              370\n14 Sam    Lincoln    Manager         europe              30              500\n15 Sue    Aames      Sr. Salesperson africa              35              600\n16 Barb   Aames      Salesperson     usa                 21              255\n17 Jack   Aames      Salesperson     usa                 43              105\n18 Kate   Zeitchik   Sr. Salesperson usa                 50              187\n19 Emy    Zeitchik   Manager         asia                34              166\n20 Monica Zeitchik   Salesperson     asia                23              184\n21 Jim    Brown      Salesperson     europe              50              167\n22 Larry  Green      Sr. Salesperson europe              20              113\n23 Laura  White      Manager         africa              20              281\n24 Hugh   Black      Sr. Salesperson africa              40              261\n   yearsWithCompany\n              &lt;dbl&gt;\n 1               10\n 2                6\n 3               10\n 4                4\n 5                3\n 6                2\n 7                3\n 8               10\n 9                3\n10               10\n11                3\n12                3\n13                2\n14                2\n15               10\n16                7\n17                4\n18                4\n19                4\n20                1\n21                2\n22                4\n23                8\n24                9\n\n\n\n\n25.3.3 print doesn’t change the tibble\nThe print function doesn’t change the tibble. print just controls what is displayed to the screen. Therefore if you save the results to a variable, the variable will still contain the entire tibble.\n\n# display first 3 rows\nprint(sales, n=3)\n\n# A tibble: 24 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n# ℹ 21 more rows\n\n# capture in a variable\nx = print(sales, n=3)\n\n# A tibble: 24 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n# ℹ 21 more rows\n\n# x still contains ALL the rows\nx\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith   Mana… usa                 40              100               10\n 2 Sam    Lee     Sale… usa                 40              200                6\n 3 Sue    Sallin… Sale… asia                30              150               10\n 4 Barb   Brown   Sr. … asia                35               79                4\n 5 Jack   Martin  Mana… europe              20               40                3\n 6 Kate   Carey   Sr. … europe              20               69                2\n 7 Emy    Parsons Sale… africa              25               87                3\n 8 Monica Oja     Sale… africa              35               98               10\n 9 Jim    Kuruzo… Sr. … usa                 35              140                3\n10 Zeke   Cohen   Mana… usa                 30              250               10\n# ℹ 14 more rows",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#convert-between-tibbles-and-dataframes",
    "href": "0500100-a-tdyvrsDplyr-v005.html#convert-between-tibbles-and-dataframes",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "25.4 Convert between tibbles and dataframes",
    "text": "25.4 Convert between tibbles and dataframes\nThe majority of the dplyr functions work with tibbles or dataframes. However, the tidyverse in general and dplyr specifically is designed to work better with tibbles.\nIf you need to, you can always convert a tibble to a dataframe or a dataframe to a tibble.\n\n# Convert the tibble to a data.frame\ndfSales = as.data.frame(sales)\n\n# Convert a dataframe to a tibble\ntblSales = as_tibble(dfSales)\n\n# A tibble is also a dataframe.\n# This is similar to how a square is also a rectangle.\n# You can see that from the class of the variable. \n# The class of a tibble contains \"tbl_df\" \"tbl\" as well as \"data.frame\"\nclass(tblSales)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# However, a dataframe is not necessarily a tibble.\n# This is similar ot how a rectangle is not necessarily a square.\n# Notice how the class of dfSales is only \"data.frame\"\nclass(dfSales)\n\n[1] \"data.frame\"\n\n# The head function is designed to work with dataframes\nhead(dfSales, 3)\n\n  first      last       title region baseInThousands salesInThousands\n1   Joe     Smith     Manager    usa              40              100\n2   Sam       Lee Salesperson    usa              40              200\n3   Sue Sallinger Salesperson   asia              30              150\n  yearsWithCompany\n1               10\n2                6\n3               10\n\n# Therefore the head function also works with tibbles\nhead(tblSales, 3)\n\n# A tibble: 3 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n\n\n\n#######################################################################\n# However, features of some functions that are designed to\n# work with tibbles might not work with data.frames.\n#######################################################################\n\n\n# This works fine - display the first 3 rows of the tibble.\nprint(tblSales, n=3) \n\n# A tibble: 24 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n# ℹ 21 more rows\n\n# ERROR - n argument is not designed to work with dataframes that aren't tibbles\nprint(dfSales, n=3) \n\nError in print.default(m, ..., quote = quote, right = right, max = max): invalid 'na.print' specification",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#getting-slices-of-the-rows",
    "href": "0500100-a-tdyvrsDplyr-v005.html#getting-slices-of-the-rows",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "25.5 getting “slices” of the rows",
    "text": "25.5 getting “slices” of the rows\nAmong some of the many functions included in dplyr are the various “slice_…” functions. These functions return a new tibble with just the specified rows. The new tibble can be saved in a variable. This is different from how the print function works (see above).\nThe functions can take several arguments that modify their behavior. The following are just a few examples. See the ?slice for more info about these functions and their arguments.\n\n25.5.1 slice_sample()\nslice_sample() chooses a random sample of the rows.\n\n# see 3 randomly chosen rows\nsales |&gt; slice_sample(n=3)  \n\n# A tibble: 3 × 7\n  first  last     title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Emy    Parsons  Sale… africa              25               87                3\n2 Jack   Martin   Mana… europe              20               40                3\n3 Monica Zeitchik Sale… asia                23              184                1\n\n# randomly chose 10% of the rows to view\nsales |&gt; slice_sample(prop=0.10)  \n\n# A tibble: 2 × 7\n  first last    title   region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith   Manager usa                 40              100               10\n2 Sam   Lincoln Manager europe              30              500                2\n\n\n\n\n25.5.2 slice_min() and slice_max()\nTo see the rows that correspond to the min or max values of a specified column use slice_min()/slice_max().\n\n# see rows with 3 smallest salesInThousands\nsales |&gt; slice_min(order_by=salesInThousands, n=3)  \n\n# A tibble: 3 × 7\n  first last   title    region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Jack  Martin Manager  europe              20               40                3\n2 Kate  Carey  Sr. Sal… europe              20               69                2\n3 Barb  Brown  Sr. Sal… asia                35               79                4\n\n# see rows with 3 smallest salesInThousands\nsales |&gt; slice_max(order_by=salesInThousands, n=3)  \n\n# A tibble: 3 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Sue   Aames     Sr. … africa              35              600               10\n2 Sam   Lincoln   Mana… europe              30              500                2\n3 Joe   Washingt… Sr. … europe              33              370                2\n\n\n\n\n25.5.3 slice() to see specific rows\nYou don’t need c() to specify the rows. This is typical of the style of functions in the tidyverse.\n\n# get rows 1,5,7 in a new dataframe\nx |&gt; slice(1,5,7)\n\n# A tibble: 3 × 7\n  first last    title   region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith   Manager usa                 40              100               10\n2 Jack  Martin  Manager europe              20               40                3\n3 Emy   Parsons Salesp… africa              25               87                3\n\n# get all rows except for rows 5 through 22\nx |&gt; slice(-(5:22))\n\n# A tibble: 6 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n4 Barb  Brown     Sr. … asia                35               79                4\n5 Laura White     Mana… africa              20              281                8\n6 Hugh  Black     Sr. … africa              40              261                9\n\n\n\n\n25.5.4 slice_head() and slice_tail()\nThe slice_head() and slice_tail() are very similar to the base-R head() and tail() functions. They return the first few (slice_head) or last few (slice_tail) rows from a tibble.\n\n# get just first 3 rows\nsales |&gt; slice_head(n=3)\n\n# A tibble: 3 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n\n\n\nprint(n=3) vs slice_head(3)\n\n# get just first 3 rows\nsales |&gt; slice_head(n=3)\n\n# A tibble: 3 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n\n# save that in a variable\nx = sales |&gt; slice_head(n=3)\n\n# x only contains 3 rows\nx\n\n# A tibble: 3 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n\n# print appears to do the same thing\n# but print only affects what is displayed to the screen, not \n# what is \"returned\" and can be saved in a variable.\ny = sales |&gt; print(n=3)\n\n# A tibble: 24 × 7\n  first last      title region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Joe   Smith     Mana… usa                 40              100               10\n2 Sam   Lee       Sale… usa                 40              200                6\n3 Sue   Sallinger Sale… asia                30              150               10\n# ℹ 21 more rows\n\n# y still has all of the rows\ny\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith   Mana… usa                 40              100               10\n 2 Sam    Lee     Sale… usa                 40              200                6\n 3 Sue    Sallin… Sale… asia                30              150               10\n 4 Barb   Brown   Sr. … asia                35               79                4\n 5 Jack   Martin  Mana… europe              20               40                3\n 6 Kate   Carey   Sr. … europe              20               69                2\n 7 Emy    Parsons Sale… africa              25               87                3\n 8 Monica Oja     Sale… africa              35               98               10\n 9 Jim    Kuruzo… Sr. … usa                 35              140                3\n10 Zeke   Cohen   Mana… usa                 30              250               10\n# ℹ 14 more rows\n\n\n\n\nbase-R head() vs dplyr::slice_head()\nWhen used alone, slice_head() and head() are very similar.\nHowever, slice_head() and slice_tail(), as well as the other slice_* functions are designed to work well in conjunction with the dplyr::group_by() function (we will cover group_by() later). The head() and tail() functions are not.\nThe general rule is that if you are using several dplyr functions in a pipeline (i.e. %&gt;%&gt; or |&gt;) you should opt for the dplyr functions instead of the base-R functions that might seem similar.",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-a-tdyvrsDplyr-v005.html#other-ways-of-viewing-the-data",
    "href": "0500100-a-tdyvrsDplyr-v005.html#other-ways-of-viewing-the-data",
    "title": "25  25.  dplyr - getting and viewing the data",
    "section": "25.6 Other ways of viewing the data",
    "text": "25.6 Other ways of viewing the data\n# To see the entire tibble in a separate tab in RStudio\n# you can use the following command\n#\n#View(sales)\n\n# Show all column names and datatypes plus some actual data\nglimpse(sales)\n\n# Show just the column names (could be useful for a quick reminder when you need it)\ncolnames(sales)",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'><yrChapterNumber>25. </yrChapterNumber> dplyr - getting and viewing the data</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html",
    "href": "0500100-b-tdyvrsDplyr-v005.html",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "",
    "text": "26.1 select()\nUse the select function to get just the specified columns.\nThe first argument to the select function is named .data (notice the period before .data). It is used to specify which tibble or dataframe you are working with. The other arguments are used to specify which columns you’d like to see from the dataframe/tibble. Note that there are NO quotes around the column names (this differs from the base-r way of specifying column names in a dataframe).\nlibrary(dplyr, warn.conflicts=FALSE, quietly=TRUE)\n\n# Show just the specified columns from the sales tibble\nselect(.data=sales, first, last, salesInThousands)\n\n# A tibble: 24 × 3\n   first  last       salesInThousands\n   &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;\n 1 Joe    Smith                   100\n 2 Sam    Lee                     200\n 3 Sue    Sallinger               150\n 4 Barb   Brown                    79\n 5 Jack   Martin                   40\n 6 Kate   Carey                    69\n 7 Emy    Parsons                  87\n 8 Monica Oja                      98\n 9 Jim    Kuruzovich              140\n10 Zeke   Cohen                   250\n# ℹ 14 more rows\nThe columns appear in whatever order you specified in the select function. If you want to bring a few columns to the front followed by the rest of the columns, you can use the everything() function.\n# Show the specified columns first, followed by the other columns.\nselect(.data=sales, first, last, everything())\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith   Mana… usa                 40              100               10\n 2 Sam    Lee     Sale… usa                 40              200                6\n 3 Sue    Sallin… Sale… asia                30              150               10\n 4 Barb   Brown   Sr. … asia                35               79                4\n 5 Jack   Martin  Mana… europe              20               40                3\n 6 Kate   Carey   Sr. … europe              20               69                2\n 7 Emy    Parsons Sale… africa              25               87                3\n 8 Monica Oja     Sale… africa              35               98               10\n 9 Jim    Kuruzo… Sr. … usa                 35              140                3\n10 Zeke   Cohen   Mana… usa                 30              250               10\n# ℹ 14 more rows",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#filter",
    "href": "0500100-b-tdyvrsDplyr-v005.html#filter",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.2 filter()",
    "text": "26.2 filter()\nThe filter function is used to retrieve the rows from a tibble that match a condition.\n\nThe first argument of the filter function is also .data. Just as with the select function this argument for the filter function is expected to be a tibble or a dataframe.\nThe next argument to filter is a logical expression. You can use the names of the columns of the tibble/dataframe in the expression. filter will examine each row of the tibble/dataframe and only return those rows whose values match the stated condition.\n\nThe following code displays only those rows whose salesInThousands is less than 100.\n\nfilter(.data=sales, salesInThousands &lt; 100)\n\n# A tibble: 5 × 7\n  first  last    title  region baseInThousands salesInThousands yearsWithCompany\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Barb   Brown   Sr. S… asia                35               79                4\n2 Jack   Martin  Manag… europe              20               40                3\n3 Kate   Carey   Sr. S… europe              20               69                2\n4 Emy    Parsons Sales… africa              25               87                3\n5 Monica Oja     Sales… africa              35               98               10",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#using-the-dplyr-functions-in-a-pipeline",
    "href": "0500100-b-tdyvrsDplyr-v005.html#using-the-dplyr-functions-in-a-pipeline",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.3 Using the dplyr functions in a “pipeline”",
    "text": "26.3 Using the dplyr functions in a “pipeline”\nThe tidyverse functions are designed so that you can be apply them one after the other. The clearest way to do that is to “pipe” the output of one function into the input of another.\nThe following pipeline first applies select to generate a new tibble that only contains the specified columns. Then the filter function then chooses only those rows that meet the specified condition.\n\nsales |&gt; \n  select(first, last, salesInThousands) |&gt;\n  filter(salesInThousands &lt; 100)\n\n# A tibble: 5 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Barb   Brown                 79\n2 Jack   Martin                40\n3 Kate   Carey                 69\n4 Emy    Parsons               87\n5 Monica Oja                   98\n\n# In this example we could apply the filter before select.\n# It technically doesn't matter which order you apply the functions in \n# as long as you realize that the output of one function is fed into \n# the input of the next function.\n\nsales |&gt; \n  filter(salesInThousands &lt; 100) |&gt;\n  select(first, last, salesInThousands) \n\n# A tibble: 5 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Barb   Brown                 79\n2 Jack   Martin                40\n3 Kate   Carey                 69\n4 Emy    Parsons               87\n5 Monica Oja                   98\n\n# You can save the results in a variable as follows\nlowSales = \n  sales |&gt; \n    filter(salesInThousands &lt; 100) |&gt;\n    select(first, last, salesInThousands) \n\n# Now the lowSales variable contains the results\nlowSales\n\n# A tibble: 5 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Barb   Brown                 79\n2 Jack   Martin                40\n3 Kate   Carey                 69\n4 Emy    Parsons               87\n5 Monica Oja                   98\n\n# You can continue to use the new variable for further processing\nlowSales |&gt; filter(salesInThousands &gt; 80)\n\n# A tibble: 2 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Emy    Parsons               87\n2 Monica Oja                   98\n\n\nThe dplyr package contains many different functions. A small group of these functions are known as the dplyr “verbs” and constitue the primary functions in the package. There are many other “helper” functions in the package.\nEach “verb” function is used to manipulate tibbles (or dataframes) in a very specific way. When the different verb functions are combined together in a pipeline you can engineer very sophisticated manipulations of the data in the tibble/dataframe.\nThe main “verb functions” in dplyr are:\n\nselect() - choose existing columns\nhttps://dplyr.tidyverse.org/reference/select.html\nmutate() - add new columns, delete columns, modify columns\nhttps://dplyr.tidyverse.org/reference/mutate.html\nfilter() - choose the rows https://dplyr.tidyverse.org/reference/mutate.html\ngroup_by(), ungroup(), sumarise() (same as summarize() with a “z”) - these work together with summary functions such as mean(), max(), min(), n() and others to summarize the data in the columns.\n\nhttps://dplyr.tidyverse.org/reference/group_by.html\nhttps://dplyr.tidyverse.org/reference/summarise.html\n\narrange() - sort the rows https://dplyr.tidyverse.org/reference/arrange.html\n\nNOTE - The concept of “verb” functions was inspired greatly by the “clauses” the SQL SELECT statement. We will study SQL later.",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#arrange",
    "href": "0500100-b-tdyvrsDplyr-v005.html#arrange",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.4 arrange()",
    "text": "26.4 arrange()\nThe arrange function reorders the rows based on specified sorting rules.\n\n# Arrange all of the rows based on salesInThousands. \nsales |&gt; \n  arrange(salesInThousands)\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Jack   Martin  Mana… europe              20               40                3\n 2 Kate   Carey   Sr. … europe              20               69                2\n 3 Barb   Brown   Sr. … asia                35               79                4\n 4 Emy    Parsons Sale… africa              25               87                3\n 5 Monica Oja     Sale… africa              35               98               10\n 6 Joe    Smith   Mana… usa                 40              100               10\n 7 Jack   Aames   Sale… usa                 43              105                4\n 8 Larry  Green   Sr. … europe              20              113                4\n 9 Zeke   Smith   Sale… asia                20              120                3\n10 Jim    Kuruzo… Sr. … usa                 35              140                3\n# ℹ 14 more rows\n\n# Select the first, last names and salesInThousands\n# for only those rows for which salesInThousands is less than 100.\n# Arrange the rows so that they appear in order of the salesInThousands.\nsales |&gt; \n  select(first, last, salesInThousands) |&gt;\n  filter(salesInThousands &lt; 100) |&gt;\n  arrange(salesInThousands)\n\n# A tibble: 5 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Jack   Martin                40\n2 Kate   Carey                 69\n3 Barb   Brown                 79\n4 Emy    Parsons               87\n5 Monica Oja                   98\n\n\n\n26.4.1 arrange by more than one column\nIf you pass more than one column name to the arrange function\n\nthe rows are arranged by the first specified column\nall rows that have the same value of the first column passed to arrange are further arranged within that cluster of rows by the 2nd column name that was passed to arrange\nYou can specify as many column names as you like in the arrange function. Column names that appear later in the call to arrange only effect the final order for those rows that share the same value for the earlier column names in the arrange function.\n\nSee the examples below.\n\n# show the rows in alphabetical order based on the names of the salespeople.\nsales |&gt;\n  arrange(last,first) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Barb   Aames   Sale… usa                 21              255                7\n 2 Jack   Aames   Sale… usa                 43              105                4\n 3 Sue    Aames   Sr. … africa              35              600               10\n 4 Hugh   Black   Sr. … africa              40              261                9\n 5 Barb   Brown   Sr. … asia                35               79                4\n 6 Jim    Brown   Sale… europe              50              167                2\n 7 Kate   Carey   Sr. … europe              20               69                2\n 8 Zeke   Cohen   Mana… usa                 30              250               10\n 9 Larry  Green   Sr. … europe              20              113                4\n10 Jim    Kuruzo… Sr. … usa                 35              140                3\n11 Sam    Lee     Sale… usa                 40              200                6\n12 Sam    Lincoln Mana… europe              30              500                2\n13 Amy    Markov… Sale… asia                46              340                3\n14 Jack   Martin  Mana… europe              20               40                3\n15 Monica Oja     Sale… africa              35               98               10\n16 Emy    Parsons Sale… africa              25               87                3\n17 Sue    Sallin… Sale… asia                30              150               10\n18 Joe    Smith   Mana… usa                 40              100               10\n19 Zeke   Smith   Sale… asia                20              120                3\n20 Joe    Washin… Sr. … europe              33              370                2\n21 Laura  White   Mana… africa              20              281                8\n22 Emy    Zeitch… Mana… asia                34              166                4\n23 Kate   Zeitch… Sr. … usa                 50              187                4\n24 Monica Zeitch… Sale… asia                23              184                1\n\n# Arrange by title, then by last names and finally by first names\nsales |&gt;\n  arrange(title, last, first) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Zeke   Cohen   Mana… usa                 30              250               10\n 2 Sam    Lincoln Mana… europe              30              500                2\n 3 Jack   Martin  Mana… europe              20               40                3\n 4 Joe    Smith   Mana… usa                 40              100               10\n 5 Laura  White   Mana… africa              20              281                8\n 6 Emy    Zeitch… Mana… asia                34              166                4\n 7 Barb   Aames   Sale… usa                 21              255                7\n 8 Jack   Aames   Sale… usa                 43              105                4\n 9 Jim    Brown   Sale… europe              50              167                2\n10 Sam    Lee     Sale… usa                 40              200                6\n11 Amy    Markov… Sale… asia                46              340                3\n12 Monica Oja     Sale… africa              35               98               10\n13 Emy    Parsons Sale… africa              25               87                3\n14 Sue    Sallin… Sale… asia                30              150               10\n15 Zeke   Smith   Sale… asia                20              120                3\n16 Monica Zeitch… Sale… asia                23              184                1\n17 Sue    Aames   Sr. … africa              35              600               10\n18 Hugh   Black   Sr. … africa              40              261                9\n19 Barb   Brown   Sr. … asia                35               79                4\n20 Kate   Carey   Sr. … europe              20               69                2\n21 Larry  Green   Sr. … europe              20              113                4\n22 Jim    Kuruzo… Sr. … usa                 35              140                3\n23 Joe    Washin… Sr. … europe              33              370                2\n24 Kate   Zeitch… Sr. … usa                 50              187                4\n\n# It might help to read if you display the columns you are sorting by first.\n# To do so, simply add a select to the pipeline. \n#\n# In this example, it makes no difference\n# if the select appears before or after the arrange function.\n#\n# Notice that the names are alphabetically arranged only with a group of\n# rows that all have the same title.\n\nsales |&gt;\n  select(title, last, first, everything()) |&gt;\n  arrange(title, last, first) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   title    last  first region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Manager  Cohen Zeke  usa                 30              250               10\n 2 Manager  Linc… Sam   europe              30              500                2\n 3 Manager  Mart… Jack  europe              20               40                3\n 4 Manager  Smith Joe   usa                 40              100               10\n 5 Manager  White Laura africa              20              281                8\n 6 Manager  Zeit… Emy   asia                34              166                4\n 7 Salespe… Aames Barb  usa                 21              255                7\n 8 Salespe… Aames Jack  usa                 43              105                4\n 9 Salespe… Brown Jim   europe              50              167                2\n10 Salespe… Lee   Sam   usa                 40              200                6\n11 Salespe… Mark… Amy   asia                46              340                3\n12 Salespe… Oja   Moni… africa              35               98               10\n13 Salespe… Pars… Emy   africa              25               87                3\n14 Salespe… Sall… Sue   asia                30              150               10\n15 Salespe… Smith Zeke  asia                20              120                3\n16 Salespe… Zeit… Moni… asia                23              184                1\n17 Sr. Sal… Aames Sue   africa              35              600               10\n18 Sr. Sal… Black Hugh  africa              40              261                9\n19 Sr. Sal… Brown Barb  asia                35               79                4\n20 Sr. Sal… Carey Kate  europe              20               69                2\n21 Sr. Sal… Green Larry europe              20              113                4\n22 Sr. Sal… Kuru… Jim   usa                 35              140                3\n23 Sr. Sal… Wash… Joe   europe              33              370                2\n24 Sr. Sal… Zeit… Kate  usa                 50              187                4\n\n# We can get even more fine grained. This time we will arrange\n# by title, then by region, then by last names and finally by first names.\n#\n# Notice that the names are alphabetically arranged only with a group of\n# rows that all have the same title and region.\n\nsales |&gt;\n  select(title, region, last, first, everything()) |&gt;\n  arrange(title, region, last, first) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   title    region last  first baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Manager  africa White Laura              20              281                8\n 2 Manager  asia   Zeit… Emy                34              166                4\n 3 Manager  europe Linc… Sam                30              500                2\n 4 Manager  europe Mart… Jack               20               40                3\n 5 Manager  usa    Cohen Zeke               30              250               10\n 6 Manager  usa    Smith Joe                40              100               10\n 7 Salespe… africa Oja   Moni…              35               98               10\n 8 Salespe… africa Pars… Emy                25               87                3\n 9 Salespe… asia   Mark… Amy                46              340                3\n10 Salespe… asia   Sall… Sue                30              150               10\n11 Salespe… asia   Smith Zeke               20              120                3\n12 Salespe… asia   Zeit… Moni…              23              184                1\n13 Salespe… europe Brown Jim                50              167                2\n14 Salespe… usa    Aames Barb               21              255                7\n15 Salespe… usa    Aames Jack               43              105                4\n16 Salespe… usa    Lee   Sam                40              200                6\n17 Sr. Sal… africa Aames Sue                35              600               10\n18 Sr. Sal… africa Black Hugh               40              261                9\n19 Sr. Sal… asia   Brown Barb               35               79                4\n20 Sr. Sal… europe Carey Kate               20               69                2\n21 Sr. Sal… europe Green Larry              20              113                4\n22 Sr. Sal… europe Wash… Joe                33              370                2\n23 Sr. Sal… usa    Kuru… Jim                35              140                3\n24 Sr. Sal… usa    Zeit… Kate               50              187                4\n\n\n\n\n26.4.2 desc(SOME_COLUMN) - arrange in descending order\nUse the desc function to specify “descending” order. See the examples.\n\n# In the following, the rows with the highest values\n# of salesInThousands are at the top.\n\nsales |&gt; \n  filter(salesInThousands &lt; 100) |&gt;\n  select(first, last, salesInThousands) |&gt;\n  arrange(desc(salesInThousands))  |&gt;\n  print(n=Inf)\n\n# A tibble: 5 × 3\n  first  last    salesInThousands\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;\n1 Monica Oja                   98\n2 Emy    Parsons               87\n3 Barb   Brown                 79\n4 Kate   Carey                 69\n5 Jack   Martin                40\n\n# Arrange alphabetically by the region.\n# Within each region, show the sales in decreasing order.\nsales |&gt;\n  select(region, baseInThousands, everything()) |&gt;\n  arrange(region,  desc(baseInThousands)) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   region baseInThousands first  last    title salesInThousands yearsWithCompany\n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 africa              40 Hugh   Black   Sr. …              261                9\n 2 africa              35 Monica Oja     Sale…               98               10\n 3 africa              35 Sue    Aames   Sr. …              600               10\n 4 africa              25 Emy    Parsons Sale…               87                3\n 5 africa              20 Laura  White   Mana…              281                8\n 6 asia                46 Amy    Markov… Sale…              340                3\n 7 asia                35 Barb   Brown   Sr. …               79                4\n 8 asia                34 Emy    Zeitch… Mana…              166                4\n 9 asia                30 Sue    Sallin… Sale…              150               10\n10 asia                23 Monica Zeitch… Sale…              184                1\n11 asia                20 Zeke   Smith   Sale…              120                3\n12 europe              50 Jim    Brown   Sale…              167                2\n13 europe              33 Joe    Washin… Sr. …              370                2\n14 europe              30 Sam    Lincoln Mana…              500                2\n15 europe              20 Jack   Martin  Mana…               40                3\n16 europe              20 Kate   Carey   Sr. …               69                2\n17 europe              20 Larry  Green   Sr. …              113                4\n18 usa                 50 Kate   Zeitch… Sr. …              187                4\n19 usa                 43 Jack   Aames   Sale…              105                4\n20 usa                 40 Joe    Smith   Mana…              100               10\n21 usa                 40 Sam    Lee     Sale…              200                6\n22 usa                 35 Jim    Kuruzo… Sr. …              140                3\n23 usa                 30 Zeke   Cohen   Mana…              250               10\n24 usa                 21 Barb   Aames   Sale…              255                7\n\n\n\n# Show the names, titles and salesInThousands for\n# people who sell to the usa and for people who sell to europe.\n#\n# Arrange the results so that all titles are grouped together.\n# Within the rows of a particular title sort the results in \n# descending order based on the salesInThousands so that the \n# rows with greater salesInThousands appear earlier.\n#\n# Only show salespeople for the usa and europe\n\nsales |&gt;\n  select(first, last, title, salesInThousands, region) |&gt;\n  filter(region %in% c(\"usa\", \"europe\")) |&gt;\n  arrange(title, desc(salesInThousands))\n\n# A tibble: 13 × 5\n   first last       title           salesInThousands region\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt; \n 1 Sam   Lincoln    Manager                      500 europe\n 2 Zeke  Cohen      Manager                      250 usa   \n 3 Joe   Smith      Manager                      100 usa   \n 4 Jack  Martin     Manager                       40 europe\n 5 Barb  Aames      Salesperson                  255 usa   \n 6 Sam   Lee        Salesperson                  200 usa   \n 7 Jim   Brown      Salesperson                  167 europe\n 8 Jack  Aames      Salesperson                  105 usa   \n 9 Joe   Washington Sr. Salesperson              370 europe\n10 Kate  Zeitchik   Sr. Salesperson              187 usa   \n11 Jim   Kuruzovich Sr. Salesperson              140 usa   \n12 Larry Green      Sr. Salesperson              113 europe\n13 Kate  Carey      Sr. Salesperson               69 europe\n\n# Careful - if you do not include region in the select , it won't work\n# ERROR - object 'region' not found\n\nsales |&gt;\n  select(first, last, title, salesInThousands) |&gt;  # region is missing here\n  filter(region %in% c(\"usa\", \"europe\")) |&gt;        # but you need it here\n  arrange(title, desc(salesInThousands))\n\nError in `filter()`:\nℹ In argument: `region %in% c(\"usa\", \"europe\")`.\nCaused by error:\n! object 'region' not found\n\n# Rearrange the order of the function calls to get it to work\n\nsales |&gt;\n  arrange(title, desc(salesInThousands)) |&gt;   \n  filter(region %in% c(\"usa\", \"europe\")) |&gt;       # we use region here\n  select(first, last, title, salesInThousands)    # we can now exclude region\n\n# A tibble: 13 × 4\n   first last       title           salesInThousands\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;\n 1 Sam   Lincoln    Manager                      500\n 2 Zeke  Cohen      Manager                      250\n 3 Joe   Smith      Manager                      100\n 4 Jack  Martin     Manager                       40\n 5 Barb  Aames      Salesperson                  255\n 6 Sam   Lee        Salesperson                  200\n 7 Jim   Brown      Salesperson                  167\n 8 Jack  Aames      Salesperson                  105\n 9 Joe   Washington Sr. Salesperson              370\n10 Kate  Zeitchik   Sr. Salesperson              187\n11 Jim   Kuruzovich Sr. Salesperson              140\n12 Larry Green      Sr. Salesperson              113\n13 Kate  Carey      Sr. Salesperson               69",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#mutate---add-new-columns-modify-columns-delete-columns",
    "href": "0500100-b-tdyvrsDplyr-v005.html#mutate---add-new-columns-modify-columns-delete-columns",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.5 mutate - add new columns, modify columns, delete columns",
    "text": "26.5 mutate - add new columns, modify columns, delete columns\nUse the mutate function to add new columns. These new columns can be based on existing data. See the examples below.\n\n# All salespeople get a commission equal to 10% of their sales.\n# Create a new column with the name commission that shows the value of their commission.\n\n# The following code creates the new column as the last column in the tibble.\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, takeHome=baseInThousands + 0.10*salesInThousands ) |&gt;\n  print(width=Inf)\n\n# A tibble: 24 × 9\n   first  last       title           region baseInThousands salesInThousands\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith      Manager         usa                 40              100\n 2 Sam    Lee        Salesperson     usa                 40              200\n 3 Sue    Sallinger  Salesperson     asia                30              150\n 4 Barb   Brown      Sr. Salesperson asia                35               79\n 5 Jack   Martin     Manager         europe              20               40\n 6 Kate   Carey      Sr. Salesperson europe              20               69\n 7 Emy    Parsons    Salesperson     africa              25               87\n 8 Monica Oja        Salesperson     africa              35               98\n 9 Jim    Kuruzovich Sr. Salesperson usa                 35              140\n10 Zeke   Cohen      Manager         usa                 30              250\n   yearsWithCompany commission takeHome\n              &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1               10       10       50  \n 2                6       20       60  \n 3               10       15       45  \n 4                4        7.9     42.9\n 5                3        4       24  \n 6                2        6.9     26.9\n 7                3        8.7     33.7\n 8               10        9.8     44.8\n 9                3       14       49  \n10               10       25       55  \n# ℹ 14 more rows\n\n\n\n26.5.1 .before and .after\nUse the .before and .after argument to mutate to specify where the new columns should appear in the tibble. See the examples below.\n\n# Place the newly created column at the beginning of the tibble,\n# i.e. .before the 1st column.\n\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, .before=1)\n\n# A tibble: 24 × 8\n   commission first  last       title    region baseInThousands salesInThousands\n        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1       10   Joe    Smith      Manager  usa                 40              100\n 2       20   Sam    Lee        Salespe… usa                 40              200\n 3       15   Sue    Sallinger  Salespe… asia                30              150\n 4        7.9 Barb   Brown      Sr. Sal… asia                35               79\n 5        4   Jack   Martin     Manager  europe              20               40\n 6        6.9 Kate   Carey      Sr. Sal… europe              20               69\n 7        8.7 Emy    Parsons    Salespe… africa              25               87\n 8        9.8 Monica Oja        Salespe… africa              35               98\n 9       14   Jim    Kuruzovich Sr. Sal… usa                 35              140\n10       25   Zeke   Cohen      Manager  usa                 30              250\n# ℹ 14 more rows\n# ℹ 1 more variable: yearsWithCompany &lt;dbl&gt;\n\n# Place the newly created column as the 3rd column\n# i.e. .after the 2nd column.\n# (.before=4 would also work)\n\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, .after=2)\n\n# A tibble: 24 × 8\n   first  last       commission title    region baseInThousands salesInThousands\n   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith            10   Manager  usa                 40              100\n 2 Sam    Lee              20   Salespe… usa                 40              200\n 3 Sue    Sallinger        15   Salespe… asia                30              150\n 4 Barb   Brown             7.9 Sr. Sal… asia                35               79\n 5 Jack   Martin            4   Manager  europe              20               40\n 6 Kate   Carey             6.9 Sr. Sal… europe              20               69\n 7 Emy    Parsons           8.7 Salespe… africa              25               87\n 8 Monica Oja               9.8 Salespe… africa              35               98\n 9 Jim    Kuruzovich       14   Sr. Sal… usa                 35              140\n10 Zeke   Cohen            25   Manager  usa                 30              250\n# ℹ 14 more rows\n# ℹ 1 more variable: yearsWithCompany &lt;dbl&gt;\n\n# .before and .after can also refer to specific columns\n# For example the following also places the commission after the last name column\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, .after=last)\n\n# A tibble: 24 × 8\n   first  last       commission title    region baseInThousands salesInThousands\n   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith            10   Manager  usa                 40              100\n 2 Sam    Lee              20   Salespe… usa                 40              200\n 3 Sue    Sallinger        15   Salespe… asia                30              150\n 4 Barb   Brown             7.9 Sr. Sal… asia                35               79\n 5 Jack   Martin            4   Manager  europe              20               40\n 6 Kate   Carey             6.9 Sr. Sal… europe              20               69\n 7 Emy    Parsons           8.7 Salespe… africa              25               87\n 8 Monica Oja               9.8 Salespe… africa              35               98\n 9 Jim    Kuruzovich       14   Sr. Sal… usa                 35              140\n10 Zeke   Cohen            25   Manager  usa                 30              250\n# ℹ 14 more rows\n# ℹ 1 more variable: yearsWithCompany &lt;dbl&gt;\n\n\n\n\n26.5.2 a new column that depends on other new columns\nThe following code creates two new columns.\nThe commission column is created as in the last example.\nThe takeHome column is the total take home pay for the salesperson, i.e.  their baseInThousands plus the commission.\nIf you create both columns in a single call to mutate, you will need to repeat the calculation for the commission twice (see examples below). An alternative is to call the mutate function twice. For tibbles with many rows this can take longer to process but is less error prone when you write the code.\n\n# All salespeople get a commission equal to 10% of their sales.\n# Create a new column with the name commission that shows the value of their commission.\n# Create another column called takeHome which has their total takehome pay.\n\n# We can do it all in one call to mutate.\n# However, that require repeating the code for calculating for the commission.\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, takeHome=baseInThousands + 0.10*salesInThousands, .before=1 ) |&gt;\n  print(width=Inf)\n\n# A tibble: 24 × 9\n   commission takeHome first  last       title           region baseInThousands\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;\n 1       10       50   Joe    Smith      Manager         usa                 40\n 2       20       60   Sam    Lee        Salesperson     usa                 40\n 3       15       45   Sue    Sallinger  Salesperson     asia                30\n 4        7.9     42.9 Barb   Brown      Sr. Salesperson asia                35\n 5        4       24   Jack   Martin     Manager         europe              20\n 6        6.9     26.9 Kate   Carey      Sr. Salesperson europe              20\n 7        8.7     33.7 Emy    Parsons    Salesperson     africa              25\n 8        9.8     44.8 Monica Oja        Salesperson     africa              35\n 9       14       49   Jim    Kuruzovich Sr. Salesperson usa                 35\n10       25       55   Zeke   Cohen      Manager         usa                 30\n   salesInThousands yearsWithCompany\n              &lt;dbl&gt;            &lt;dbl&gt;\n 1              100               10\n 2              200                6\n 3              150               10\n 4               79                4\n 5               40                3\n 6               69                2\n 7               87                3\n 8               98               10\n 9              140                3\n10              250               10\n# ℹ 14 more rows\n\n# By separating the code into two calls to mutate, we can refer to the \n# newly created \"commission\" column when calculating the takeHome column.\n# Note that the 2nd call to mutate below receives a tibble that already has a\n# commission column as the first column. That is why .before=2 (and not .before=1)\nsales |&gt;\n  mutate(commission=0.10*salesInThousands, .before=1) |&gt;\n  mutate(takeHome=baseInThousands + commission, .before=2) |&gt;\n  print(width=Inf)\n\n# A tibble: 24 × 9\n   commission takeHome first  last       title           region baseInThousands\n        &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;\n 1       10       50   Joe    Smith      Manager         usa                 40\n 2       20       60   Sam    Lee        Salesperson     usa                 40\n 3       15       45   Sue    Sallinger  Salesperson     asia                30\n 4        7.9     42.9 Barb   Brown      Sr. Salesperson asia                35\n 5        4       24   Jack   Martin     Manager         europe              20\n 6        6.9     26.9 Kate   Carey      Sr. Salesperson europe              20\n 7        8.7     33.7 Emy    Parsons    Salesperson     africa              25\n 8        9.8     44.8 Monica Oja        Salesperson     africa              35\n 9       14       49   Jim    Kuruzovich Sr. Salesperson usa                 35\n10       25       55   Zeke   Cohen      Manager         usa                 30\n   salesInThousands yearsWithCompany\n              &lt;dbl&gt;            &lt;dbl&gt;\n 1              100               10\n 2              200                6\n 3              150               10\n 4               79                4\n 5               40                3\n 6               69                2\n 7               87                3\n 8               98               10\n 9              140                3\n10              250               10\n# ℹ 14 more rows\n\n\n\n\n26.5.3 modifying existing columns\nYou can modify existing columns by specifying an existing column name in mutate.\n\n\n26.5.4 deleting columns with mutate\nYou can delete existing columns by setting the value of the column to NULL.",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#distinct",
    "href": "0500100-b-tdyvrsDplyr-v005.html#distinct",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.6 distinct()",
    "text": "26.6 distinct()\nThe distinct function eliminates any duplicate rows from a tibble. Only rows for which ALL of the values are equal to ALL the values in another row will be eliminated.\nSee the examples.\n\n# Show the just the region for each row in the tibble\nsales |&gt;\n  select (region) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 1\n   region\n   &lt;chr&gt; \n 1 usa   \n 2 usa   \n 3 asia  \n 4 asia  \n 5 europe\n 6 europe\n 7 africa\n 8 africa\n 9 usa   \n10 usa   \n11 asia  \n12 asia  \n13 europe\n14 europe\n15 africa\n16 usa   \n17 usa   \n18 usa   \n19 asia  \n20 asia  \n21 europe\n22 europe\n23 africa\n24 africa\n\n# Same thing using distinct()\n# Show just the distinct (ie. different) regions\nsales |&gt;\n  select (region) |&gt;\n  distinct() |&gt;       # show just the distinct rows\n  print(n=Inf)\n\n# A tibble: 4 × 1\n  region\n  &lt;chr&gt; \n1 usa   \n2 asia  \n3 europe\n4 africa\n\n###############################################################\n# NOTE - all values in the row must be exactly the same\n# for the distinct() function to eliminate the duplicate rows.\n###############################################################\n\n# Show the region and title for rows \n# whose region is either europe or asia.\n# Arrange the output so they come up sorted.\n\nsales |&gt;\n  select(region, title) |&gt;\n  filter(region %in% c('europe', 'asia')) |&gt;\n  arrange(region, title) |&gt;\n  print(n=Inf)\n\n# A tibble: 12 × 2\n   region title          \n   &lt;chr&gt;  &lt;chr&gt;          \n 1 asia   Manager        \n 2 asia   Salesperson    \n 3 asia   Salesperson    \n 4 asia   Salesperson    \n 5 asia   Salesperson    \n 6 asia   Sr. Salesperson\n 7 europe Manager        \n 8 europe Manager        \n 9 europe Salesperson    \n10 europe Sr. Salesperson\n11 europe Sr. Salesperson\n12 europe Sr. Salesperson\n\n# Same thing using distinct()\n# Show just the distinct rows from the previous command\n\nsales |&gt;\n  select(region, title) |&gt;\n  filter(region %in% c('europe', 'asia')) |&gt;\n  distinct() |&gt;                                 # eliminate duplicate rows\n  arrange(region, title) |&gt;\n  print(n=Inf)\n\n# A tibble: 6 × 2\n  region title          \n  &lt;chr&gt;  &lt;chr&gt;          \n1 asia   Manager        \n2 asia   Salesperson    \n3 asia   Sr. Salesperson\n4 europe Manager        \n5 europe Salesperson    \n6 europe Sr. Salesperson\n\n# Notice that in the previous output, there were multiple\n# rows that contained \"europe\", multiple rows that contained \"asia\"\n# as well as multiple rows that contained each of the different titles.\n# However - there were NO rows that contained exact duplicates of both\n# the region and the title.\n#\n# The moral of the story is that distinct() will only elminate rows \n# that are EXACT duplicates in EVERY column.\n\n\nsales |&gt;\n  select(region, title, everything()) |&gt;\n  arrange(region, title)\n\n# A tibble: 24 × 7\n   region title    first last  baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 africa Manager  Laura White              20              281                8\n 2 africa Salespe… Emy   Pars…              25               87                3\n 3 africa Salespe… Moni… Oja                35               98               10\n 4 africa Sr. Sal… Sue   Aames              35              600               10\n 5 africa Sr. Sal… Hugh  Black              40              261                9\n 6 asia   Manager  Emy   Zeit…              34              166                4\n 7 asia   Salespe… Sue   Sall…              30              150               10\n 8 asia   Salespe… Zeke  Smith              20              120                3\n 9 asia   Salespe… Amy   Mark…              46              340                3\n10 asia   Salespe… Moni… Zeit…              23              184                1\n# ℹ 14 more rows\n\nsales |&gt;\n  select(region, title) |&gt;\n  arrange(region, title)\n\n# A tibble: 24 × 2\n   region title          \n   &lt;chr&gt;  &lt;chr&gt;          \n 1 africa Manager        \n 2 africa Salesperson    \n 3 africa Salesperson    \n 4 africa Sr. Salesperson\n 5 africa Sr. Salesperson\n 6 asia   Manager        \n 7 asia   Salesperson    \n 8 asia   Salesperson    \n 9 asia   Salesperson    \n10 asia   Salesperson    \n# ℹ 14 more rows\n\nsales |&gt;\n  select(region,title) |&gt;\n  arrange(region, title) |&gt;\n  distinct()\n\n# A tibble: 12 × 2\n   region title          \n   &lt;chr&gt;  &lt;chr&gt;          \n 1 africa Manager        \n 2 africa Salesperson    \n 3 africa Sr. Salesperson\n 4 asia   Manager        \n 5 asia   Salesperson    \n 6 asia   Sr. Salesperson\n 7 europe Manager        \n 8 europe Salesperson    \n 9 europe Sr. Salesperson\n10 usa    Manager        \n11 usa    Salesperson    \n12 usa    Sr. Salesperson",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#summarize-and-group_by",
    "href": "0500100-b-tdyvrsDplyr-v005.html#summarize-and-group_by",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.7 summarize and group_by",
    "text": "26.7 summarize and group_by\nThe summarize() and group_by() functions work together. We’ll explain exactly how below …\n\n26.7.1 aggregate functions\nThe summarize function is used to condense the contents of several rows of data into a single row of data. It does this by using functions such as min(), max(), mean(), etc. These functions can take several values and return a single value. Such functions are known as aggregate functions.\n\n\n26.7.2 summarize (without group_by)\nThe summarize function is often preceded by a call to the group_by() function (which we will cover in the next section). group_by() is used to contol which rows of data will be affected by a subsequent call to the summarize() function. This will be explained in more detail below. For now, we will just focus on how summarize() works when it is NOT preceded by a call to group_by().\nWhen summarize() is NOT preceded by a call to group_by() then the job of the summarize() function is to return a single row of summary by applying aggregate functions (such as min, max, mean, etc) to columns of the tibble.\nSee the examples below.\n\n# mean salesInThousands (for all rows)\n# Note that this returns a tibble with exactly one row and one column.\nsales |&gt;\n summarize(mean(salesInThousands))\n\n# A tibble: 1 × 1\n  `mean(salesInThousands)`\n                     &lt;dbl&gt;\n1                     203.\n\n# We can change the column name as shown below\nsales |&gt;\n summarize(averageSales = mean(salesInThousands))\n\n# A tibble: 1 × 1\n  averageSales\n         &lt;dbl&gt;\n1         203.\n\n# We can filter the rows before calculating the mean results (or use any other of the dplyr functions)\n#\n# Show the mean sales for just the USA.\nsales |&gt;\n  filter(region == \"usa\") |&gt;\n  summarize(usaSales = mean(salesInThousands))\n\n# A tibble: 1 × 1\n  usaSales\n     &lt;dbl&gt;\n1     177.\n\n# We can get more than one column in our summary tibble\nsales |&gt;\n  summarize(meanSales = mean(salesInThousands), maxSales=max(salesInThousands), minSales=min(salesInThousands),\n            maxBase = max(baseInThousands), minBase=min(baseInThousands))\n\n# A tibble: 1 × 5\n  meanSales maxSales minSales maxBase minBase\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1      203.      600       40      50      20\n\n\n\n\n26.7.3 The n() function\nThe n() function returns the number of rows.\nThe n_distinct() function returns the number of rows that have distinct (i.e. different) values.\n\n# Show all of the rows ordered by region and title\nsales |&gt;\n  select(region, title, everything()) |&gt;\n  print(n=Inf)\n\n# A tibble: 24 × 7\n   region title    first last  baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 usa    Manager  Joe   Smith              40              100               10\n 2 usa    Salespe… Sam   Lee                40              200                6\n 3 asia   Salespe… Sue   Sall…              30              150               10\n 4 asia   Sr. Sal… Barb  Brown              35               79                4\n 5 europe Manager  Jack  Mart…              20               40                3\n 6 europe Sr. Sal… Kate  Carey              20               69                2\n 7 africa Salespe… Emy   Pars…              25               87                3\n 8 africa Salespe… Moni… Oja                35               98               10\n 9 usa    Sr. Sal… Jim   Kuru…              35              140                3\n10 usa    Manager  Zeke  Cohen              30              250               10\n11 asia   Salespe… Zeke  Smith              20              120                3\n12 asia   Salespe… Amy   Mark…              46              340                3\n13 europe Sr. Sal… Joe   Wash…              33              370                2\n14 europe Manager  Sam   Linc…              30              500                2\n15 africa Sr. Sal… Sue   Aames              35              600               10\n16 usa    Salespe… Barb  Aames              21              255                7\n17 usa    Salespe… Jack  Aames              43              105                4\n18 usa    Sr. Sal… Kate  Zeit…              50              187                4\n19 asia   Manager  Emy   Zeit…              34              166                4\n20 asia   Salespe… Moni… Zeit…              23              184                1\n21 europe Salespe… Jim   Brown              50              167                2\n22 europe Sr. Sal… Larry Green              20              113                4\n23 africa Manager  Laura White              20              281                8\n24 africa Sr. Sal… Hugh  Black              40              261                9\n\n# n() - returns the number of rows in the tibble\n# n_distinct(COL1, COL2, ...) - returns the number of rows for which \n#   the specified columns taken all together are distinct among all other rows.\nsales |&gt;\n  select(region, title, everything()) |&gt;\n  summarize(n(), n_distinct(region), n_distinct(title), n_distinct(region, title))\n\n# A tibble: 1 × 4\n  `n()` `n_distinct(region)` `n_distinct(title)` `n_distinct(region, title)`\n  &lt;int&gt;                &lt;int&gt;               &lt;int&gt;                       &lt;int&gt;\n1    24                    4                   3                          12",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "0500100-b-tdyvrsDplyr-v005.html#group_by-and-summarize-or-summarise",
    "href": "0500100-b-tdyvrsDplyr-v005.html#group_by-and-summarize-or-summarise",
    "title": "26  26.  dplyr verbs - select, filter, etc.",
    "section": "26.8 group_by and summarize (or summarise)",
    "text": "26.8 group_by and summarize (or summarise)\nThe examples above for summarize all return a single row of data. This is because they are summarizing all of the rows for the tibble that summarize received. Note that this is true even if the rows have been filtered first. For example:\n\n# The following returns just one row.\n# The summarize function is summarizing ALL of the rows \n# for the tibble that it was given.\n\nsales %&gt;% \n  filter(region == \"usa\") |&gt;\n  summarize(numberOfRows=n(), maxUsaSales = max(salesInThousands), minUsaSales=min(salesInThousands))\n\n# A tibble: 1 × 3\n  numberOfRows maxUsaSales minUsaSales\n         &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1            7         255         100\n\n\nThe functions, group_by and summarize functions are designed to work together. The job of the group_by function is to separate the rows of data into different “groups” that will later be processed separately by the summarize function.\nIf you use a group_by function in general it is followed a call to summarize()\nTo understand the examples below, it is helpful to first see the rows sorted by region and title\n\nsales |&gt;\n  select(region, title, everything()) |&gt;\n  arrange(region, title)\n\n# A tibble: 24 × 7\n   region title    first last  baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 africa Manager  Laura White              20              281                8\n 2 africa Salespe… Emy   Pars…              25               87                3\n 3 africa Salespe… Moni… Oja                35               98               10\n 4 africa Sr. Sal… Sue   Aames              35              600               10\n 5 africa Sr. Sal… Hugh  Black              40              261                9\n 6 asia   Manager  Emy   Zeit…              34              166                4\n 7 asia   Salespe… Sue   Sall…              30              150               10\n 8 asia   Salespe… Zeke  Smith              20              120                3\n 9 asia   Salespe… Amy   Mark…              46              340                3\n10 asia   Salespe… Moni… Zeit…              23              184                1\n# ℹ 14 more rows\n\n\nNotice how the rows that have the same region can be thought of as a “group” of rows for that region.\nSee the examples below\n\n# Show summary information for each of the different regions.\n# Each row in the output corresponds only to the rows for the region shown on that row of output.\nsales |&gt; \n  group_by(region) |&gt;    # create a different group for each region\n  summarize(numberOfRows=n(), mean(baseInThousands), mean(salesInThousands))\n\n# A tibble: 4 × 4\n  region numberOfRows `mean(baseInThousands)` `mean(salesInThousands)`\n  &lt;chr&gt;         &lt;int&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;\n1 africa            5                    31                       265.\n2 asia              6                    31.3                     173.\n3 europe            6                    28.8                     210.\n4 usa               7                    37                       177.\n\n\nSimilarly we can group the rows by those with the same value in the title column.\n\n# Show summary information for each of the different titles.\n# Each row in the output corresponds only to the rows for the title shown on that row of output.\nsales |&gt; \n  group_by(title) |&gt;    # create a different group for each title\n  summarize(numberOfRows=n(), mean(baseInThousands), mean(salesInThousands))\n\n# A tibble: 3 × 4\n  title           numberOfRows `mean(baseInThousands)` `mean(salesInThousands)`\n  &lt;chr&gt;                  &lt;int&gt;                   &lt;dbl&gt;                    &lt;dbl&gt;\n1 Manager                    6                    29                       223.\n2 Salesperson               10                    33.3                     171.\n3 Sr. Salesperson            8                    33.5                     227.\n\n\n\n26.8.1 group_by without summarize has no noticeable effect\nNote that on its own, the group_by function doesn’t appear to do anything. It only has a tangible effect when a summarize function is called after the group_by function\n\n# group_by without a subsequent call to summarize doesn't appear to do anything special\nsales |&gt; \n  group_by(title)   \n\n# A tibble: 24 × 7\n# Groups:   title [3]\n   first  last    title region baseInThousands salesInThousands yearsWithCompany\n   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1 Joe    Smith   Mana… usa                 40              100               10\n 2 Sam    Lee     Sale… usa                 40              200                6\n 3 Sue    Sallin… Sale… asia                30              150               10\n 4 Barb   Brown   Sr. … asia                35               79                4\n 5 Jack   Martin  Mana… europe              20               40                3\n 6 Kate   Carey   Sr. … europe              20               69                2\n 7 Emy    Parsons Sale… africa              25               87                3\n 8 Monica Oja     Sale… africa              35               98               10\n 9 Jim    Kuruzo… Sr. … usa                 35              140                3\n10 Zeke   Cohen   Mana… usa                 30              250               10\n# ℹ 14 more rows\n\n\n\n\n26.8.2 group_by more than one column\nA call to group_by that has more than one column creates a separate group for the rows that have the same values for all of the specified columns. To understand this look at the following output:\n\nasia_europe_sales = \n  sales |&gt;\n    select(region, title, baseInThousands) |&gt;\n    filter(region %in% c('europe', 'asia')) |&gt;\n    arrange(region, title)\n\nasia_europe_sales  |&gt;\n    print(n=Inf)\n\n# A tibble: 12 × 3\n   region title           baseInThousands\n   &lt;chr&gt;  &lt;chr&gt;                     &lt;dbl&gt;\n 1 asia   Manager                      34\n 2 asia   Salesperson                  30\n 3 asia   Salesperson                  20\n 4 asia   Salesperson                  46\n 5 asia   Salesperson                  23\n 6 asia   Sr. Salesperson              35\n 7 europe Manager                      20\n 8 europe Manager                      30\n 9 europe Salesperson                  50\n10 europe Sr. Salesperson              20\n11 europe Sr. Salesperson              33\n12 europe Sr. Salesperson              20\n\n\nWe can treat the rows that have the same values for both the region and title as a single group of rows. For example we can treat all of the rows for “asia Salesperson” as a group and the rows for “eroupe Manager” as a separate group. The output of the following command returns one row for each group.\n\n# create groups based on region and sales\nasia_europe_sales |&gt; \n  group_by(region, title) |&gt;\n  summarize(n(), max(baseInThousands), mean(baseInThousands))|&gt;\n  print(n=Inf)\n\n`summarise()` has grouped output by 'region'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   region [2]\n  region title           `n()` `max(baseInThousands)` `mean(baseInThousands)`\n  &lt;chr&gt;  &lt;chr&gt;           &lt;int&gt;                  &lt;dbl&gt;                   &lt;dbl&gt;\n1 asia   Manager             1                     34                    34  \n2 asia   Salesperson         4                     46                    29.8\n3 asia   Sr. Salesperson     1                     35                    35  \n4 europe Manager             2                     30                    25  \n5 europe Salesperson         1                     50                    50  \n6 europe Sr. Salesperson     3                     33                    24.3\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might be wondering about the somewhat cryptic message from the previous command:\n\nsummarise() has grouped output by ‘region’. You can override using the .groups argument.\n\nWhen group_by is called with more than one column name the subsequent summarize call will generate a message similar to the one shown above. You can ignore this message without any issue. If you want to understand it better - look at  this.\n\n\n\n\n26.8.3 order of columns in group_by is irrelevant\nNote that it makes no difference which order you specify the column names in the call to group by. The following command reverses the order of the columns in the call to group_by but returns the exact same result as the previous command.\n\n# create groups based on region and sales\nasia_europe_sales |&gt; \n  group_by(title, region) |&gt;   # the order of the columns in group_by makes no difference\n  summarize(n(), max(baseInThousands), mean(baseInThousands))|&gt;\n  print(n=Inf)\n\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 5\n# Groups:   title [3]\n  title           region `n()` `max(baseInThousands)` `mean(baseInThousands)`\n  &lt;chr&gt;           &lt;chr&gt;  &lt;int&gt;                  &lt;dbl&gt;                   &lt;dbl&gt;\n1 Manager         asia       1                     34                    34  \n2 Manager         europe     2                     30                    25  \n3 Salesperson     asia       4                     46                    29.8\n4 Salesperson     europe     1                     50                    50  \n5 Sr. Salesperson asia       1                     35                    35  \n6 Sr. Salesperson europe     3                     33                    24.3\n\n\n\n\n26.8.4 some other aggregate functions\nAs mentioned above, an “aggregate function” is a function that can take several values as input but returns a single value. It is these types of functions that are used by summarize()\nThe following are some other commonly used aggregate functions that can be used in summarize. There are many, many others.\n\nStatistics: mean(), median(), sd(), IQR(), mad()\nRange: min(), max(),\nPosition: first(), last(), nth(),\nCounting: n(), n_distinct()\nLogical: any(), all()",
    "crumbs": [
      "Intro to dplyr",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'><yrChapterNumber>26. </yrChapterNumber> dplyr verbs - select, filter, etc.</span>"
    ]
  },
  {
    "objectID": "sql0014-theBooksDatabase-v001.html",
    "href": "sql0014-theBooksDatabase-v001.html",
    "title": "27  27.  Get the data - “books database” and other tables",
    "section": "",
    "text": "27.1 Different ways to import data for use with SQL\nSQL is a language that is used with Relational Database Management SOftware (RDBMS).",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'><yrChapterNumber>27. </yrChapterNumber> Get the data - \"books database\" and other tables</span>"
    ]
  },
  {
    "objectID": "sql0014-theBooksDatabase-v001.html#prepare-to-import-the-data",
    "href": "sql0014-theBooksDatabase-v001.html#prepare-to-import-the-data",
    "title": "27  27.  Get the data - “books database” and other tables",
    "section": "27.2 Prepare to import the data",
    "text": "27.2 Prepare to import the data\nIn order to read the data into R and to access the data using SQL, we will need to load the R packages “readr” and “sqldf”. We do so with the following commands:\n\nif(!require(readr)){install.packages(\"readr\");require(readr);}\nif(!require(sqldf)){install.packages(\"sqldf\");require(sqldf);}",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'><yrChapterNumber>27. </yrChapterNumber> Get the data - \"books database\" and other tables</span>"
    ]
  },
  {
    "objectID": "sql0014-theBooksDatabase-v001.html#import-the-data-grades",
    "href": "sql0014-theBooksDatabase-v001.html#import-the-data-grades",
    "title": "27  27.  Get the data - “books database” and other tables",
    "section": "27.3 import the data: grades",
    "text": "27.3 import the data: grades\nDownload the CSV file: grades.csv\nThis a .csv file that contains information about students and their grades. You can read the contents of the file into R with the read.csv function as shown below.\n\ngrades = read.csv(\"grades.csv\")\n\nOnce you’ve done so, you can display the data:\n\n\n\n\n\n\nClick here - grades table\n\n\n\n\n\n\nsqldf(\"select * from grades\")\n\n      id  student year honors test1 test2 test3 major\n1  s0100      joe   fr  FALSE    70    82    NA   IDS\n2  s0200      sue   fr  FALSE    80    80    79   IDS\n3  s0300     anne   fr   TRUE    90    95    92   IDS\n4  s0400    frank   so   TRUE   100    95    91   ACC\n5  s0500      bob   so  FALSE    30    NA    NA   IDS\n6  s0600 samantha   so   TRUE   100    70    97   IDS\n7  s0700    larry   ju  FALSE    32    80    NA   FIN\n8  s0800     bill   se  FALSE    84    90    92   ACC\n9  s0900 lucricia   fr   TRUE    80   100    88   IDS\n10 s1000 margaret   se  FALSE    59    47    NA   IDS\n11 s1100    dalia   se  FALSE    85    92    93   FIN\n12 s1200      zvi   ju   TRUE    90    98    92   ACC\n13 s1300     mike   ju   TRUE    90    86    79   IDS\n14 s1400    david   se   TRUE    90    87    87   IDS\n15 s1500     dana   so  FALSE   100    93    91   FIN",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'><yrChapterNumber>27. </yrChapterNumber> Get the data - \"books database\" and other tables</span>"
    ]
  },
  {
    "objectID": "sql0014-theBooksDatabase-v001.html#import-the-data-customers-orders-movies-north_american_cities-etc",
    "href": "sql0014-theBooksDatabase-v001.html#import-the-data-customers-orders-movies-north_american_cities-etc",
    "title": "27  27.  Get the data - “books database” and other tables",
    "section": "27.4 import the data: customers, orders, movies, north_american_cities, etc",
    "text": "27.4 import the data: customers, orders, movies, north_american_cities, etc\nDownload the RData file: sqlbolt_tables-v007.RData\nA .RData file contains a copy of R’s environment variables. You can save you environment variables to an RData file with the save() function. (see ?save). Once you have an RData file, you can load those variables into your R environment with the load() function (see ?load).\nThis RData file contains the following different dataframe variables. When using the sqldf() function, each dataframe variable can used as though it were a Relational Database table.\n\ncustomers\norders\nmovies\nnorth_american_cities\nas well as some others (boxOffice, buildings, employees)\n\nUse the load function as shown below to load the data into R.\n\nload(\"sqlbolt_tables-v007.RData\")\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here - grades table\n\n\n\n\n\n\nsqldf(\"select * from grades\")\n\n      id  student year honors test1 test2 test3 major\n1  s0100      joe   fr  FALSE    70    82    NA   IDS\n2  s0200      sue   fr  FALSE    80    80    79   IDS\n3  s0300     anne   fr   TRUE    90    95    92   IDS\n4  s0400    frank   so   TRUE   100    95    91   ACC\n5  s0500      bob   so  FALSE    30    NA    NA   IDS\n6  s0600 samantha   so   TRUE   100    70    97   IDS\n7  s0700    larry   ju  FALSE    32    80    NA   FIN\n8  s0800     bill   se  FALSE    84    90    92   ACC\n9  s0900 lucricia   fr   TRUE    80   100    88   IDS\n10 s1000 margaret   se  FALSE    59    47    NA   IDS\n11 s1100    dalia   se  FALSE    85    92    93   FIN\n12 s1200      zvi   ju   TRUE    90    98    92   ACC\n13 s1300     mike   ju   TRUE    90    86    79   IDS\n14 s1400    david   se   TRUE    90    87    87   IDS\n15 s1500     dana   so  FALSE   100    93    91   FIN\n\n\n\n\n\n\n\n\n\n\n\nClick here - movies table\n\n\n\n\n\n\nsqldf(\"select * from movies\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n\n\n\n\n\n\n\n\n\n\nClick here - orders table\n\n\n\n\n\n\nsqldf(\"select * from orders\")\n\n    OrderID CustomerID EmployeeID  OrderDate ShipperID\n1     10248         90          5   7/4/1996         3\n2     10249         81          6   7/5/1996         1\n3     10250         34          4   7/8/1996         2\n4     10251         84          3   7/8/1996         1\n5     10252         76          4   7/9/1996         2\n6     10253         34          3  7/10/1996         2\n7     10254         14          5  7/11/1996         2\n8     10255         68          9  7/12/1996         3\n9     10256         88          3  7/15/1996         2\n10    10257         35          4  7/16/1996         3\n11    10258         20          1  7/17/1996         1\n12    10259         13          4  7/18/1996         3\n13    10260         55          4  7/19/1996         1\n14    10261         61          4  7/19/1996         2\n15    10262         65          8  7/22/1996         3\n16    10263         20          9  7/23/1996         3\n17    10264         24          6  7/24/1996         3\n18    10265          7          2  7/25/1996         1\n19    10266         87          3  7/26/1996         3\n20    10267         25          4  7/29/1996         1\n21    10268         33          8  7/30/1996         3\n22    10269         89          5  7/31/1996         1\n23    10270         87          1   8/1/1996         1\n24    10271         75          6   8/1/1996         2\n25    10272         65          6   8/2/1996         2\n26    10273         63          3   8/5/1996         3\n27    10274         85          6   8/6/1996         1\n28    10275         49          1   8/7/1996         1\n29    10276         80          8   8/8/1996         3\n30    10277         52          2   8/9/1996         3\n31    10278          5          8  8/12/1996         2\n32    10279         44          8  8/13/1996         2\n33    10280          5          2  8/14/1996         1\n34    10281         69          4  8/14/1996         1\n35    10282         69          4  8/15/1996         1\n36    10283         46          3  8/16/1996         3\n37    10284         44          4  8/19/1996         1\n38    10285         63          1  8/20/1996         2\n39    10286         63          8  8/21/1996         3\n40    10287         67          8  8/22/1996         3\n41    10288         66          4  8/23/1996         1\n42    10289         11          7  8/26/1996         3\n43    10290         15          8  8/27/1996         1\n44    10291         61          6  8/27/1996         2\n45    10292         81          1  8/28/1996         2\n46    10293         80          1  8/29/1996         3\n47    10294         65          4  8/30/1996         2\n48    10295         85          2   9/2/1996         2\n49    10296         46          6   9/3/1996         1\n50    10297          7          5   9/4/1996         2\n51    10298         37          6   9/5/1996         2\n52    10299         67          4   9/6/1996         2\n53    10300         49          2   9/9/1996         2\n54    10301         86          8   9/9/1996         2\n55    10302         76          4  9/10/1996         2\n56    10303         30          7  9/11/1996         2\n57    10304         80          1  9/12/1996         2\n58    10305         55          8  9/13/1996         3\n59    10306         69          1  9/16/1996         3\n60    10307         48          2  9/17/1996         2\n61    10308          2          7  9/18/1996         3\n62    10309         37          3  9/19/1996         1\n63    10310         77          8  9/20/1996         2\n64    10311         18          1  9/20/1996         3\n65    10312         86          2  9/23/1996         2\n66    10313         63          2  9/24/1996         2\n67    10314         65          1  9/25/1996         2\n68    10315         38          4  9/26/1996         2\n69    10316         65          1  9/27/1996         3\n70    10317         48          6  9/30/1996         1\n71    10318         38          8  10/1/1996         2\n72    10319         80          7  10/2/1996         3\n73    10320         87          5  10/3/1996         3\n74    10321         38          3  10/3/1996         2\n75    10322         58          7  10/4/1996         3\n76    10323         39          4  10/7/1996         1\n77    10324         71          9  10/8/1996         1\n78    10325         39          1  10/9/1996         3\n79    10326          8          4 10/10/1996         2\n80    10327         24          2 10/11/1996         1\n81    10328         28          4 10/14/1996         3\n82    10329         75          4 10/15/1996         2\n83    10330         46          3 10/16/1996         1\n84    10331          9          9 10/16/1996         1\n85    10332         51          3 10/17/1996         2\n86    10333         87          5 10/18/1996         3\n87    10334         84          8 10/21/1996         2\n88    10335         37          7 10/22/1996         2\n89    10336         60          7 10/23/1996         2\n90    10337         25          4 10/24/1996         3\n91    10338         55          4 10/25/1996         3\n92    10339         51          2 10/28/1996         2\n93    10340          9          1 10/29/1996         3\n94    10341         73          7 10/29/1996         3\n95    10342         25          4 10/30/1996         2\n96    10343         44          4 10/31/1996         1\n97    10344         89          4  11/1/1996         2\n98    10345         63          2  11/4/1996         2\n99    10346         65          3  11/5/1996         3\n100   10347         21          4  11/6/1996         3\n101   10348         86          4  11/7/1996         2\n102   10349         75          7  11/8/1996         1\n103   10350         41          6 11/11/1996         2\n104   10351         20          1 11/11/1996         1\n105   10352         28          3 11/12/1996         3\n106   10353         59          7 11/13/1996         3\n107   10354         58          8 11/14/1996         3\n108   10355          4          6 11/15/1996         1\n109   10356         86          6 11/18/1996         2\n110   10357         46          1 11/19/1996         3\n111   10358         41          5 11/20/1996         1\n112   10359         72          5 11/21/1996         3\n113   10360          7          4 11/22/1996         3\n114   10361         63          1 11/22/1996         2\n115   10362          9          3 11/25/1996         1\n116   10363         17          4 11/26/1996         3\n117   10364         19          1 11/26/1996         1\n118   10365          3          3 11/27/1996         2\n119   10366         29          8 11/28/1996         2\n120   10367         83          7 11/28/1996         3\n121   10368         20          2 11/29/1996         2\n122   10369         75          8  12/2/1996         2\n123   10370         14          6  12/3/1996         2\n124   10371         41          1  12/3/1996         1\n125   10372         62          5  12/4/1996         2\n126   10373         37          4  12/5/1996         3\n127   10374         91          1  12/5/1996         3\n128   10375         36          3  12/6/1996         2\n129   10376         51          1  12/9/1996         2\n130   10377         72          1  12/9/1996         3\n131   10378         24          5 12/10/1996         3\n132   10379         61          2 12/11/1996         1\n133   10380         37          8 12/12/1996         3\n134   10381         46          3 12/12/1996         3\n135   10382         20          4 12/13/1996         1\n136   10383          4          8 12/16/1996         3\n137   10384          5          3 12/16/1996         3\n138   10385         75          1 12/17/1996         2\n139   10386         21          9 12/18/1996         3\n140   10387         70          1 12/18/1996         2\n141   10388         72          2 12/19/1996         1\n142   10389         10          4 12/20/1996         2\n143   10390         20          6 12/23/1996         1\n144   10391         17          3 12/23/1996         3\n145   10392         59          2 12/24/1996         3\n146   10393         71          1 12/25/1996         3\n147   10394         36          1 12/25/1996         3\n148   10395         35          6 12/26/1996         1\n149   10396         25          1 12/27/1996         3\n150   10397         60          5 12/27/1996         1\n151   10398         71          2 12/30/1996         3\n152   10399         83          8 12/31/1996         3\n153   10400         19          1   1/1/1997         3\n154   10401         65          1   1/1/1997         1\n155   10402         20          8   1/2/1997         2\n156   10403         20          4   1/3/1997         3\n157   10404         49          2   1/3/1997         1\n158   10405         47          1   1/6/1997         1\n159   10406         62          7   1/7/1997         1\n160   10407         56          2   1/7/1997         2\n161   10408         23          8   1/8/1997         1\n162   10409         54          3   1/9/1997         1\n163   10410         10          3  1/10/1997         3\n164   10411         10          9  1/10/1997         3\n165   10412         87          8  1/13/1997         2\n166   10413         41          3  1/14/1997         2\n167   10414         21          2  1/14/1997         3\n168   10415         36          3  1/15/1997         1\n169   10416         87          8  1/16/1997         3\n170   10417         73          4  1/16/1997         3\n171   10418         63          4  1/17/1997         1\n172   10419         68          4  1/20/1997         2\n173   10420         88          3  1/21/1997         1\n174   10421         61          8  1/21/1997         1\n175   10422         27          2  1/22/1997         1\n176   10423         31          6  1/23/1997         3\n177   10424         51          7  1/23/1997         2\n178   10425         41          6  1/24/1997         2\n179   10426         29          4  1/27/1997         1\n180   10427         59          4  1/27/1997         2\n181   10428         66          7  1/28/1997         1\n182   10429         37          3  1/29/1997         2\n183   10430         20          4  1/30/1997         1\n184   10431         10          4  1/30/1997         2\n185   10432         75          3  1/31/1997         2\n186   10433         60          3   2/3/1997         3\n187   10434         24          3   2/3/1997         2\n188   10435         16          8   2/4/1997         2\n189   10436          7          3   2/5/1997         2\n190   10437         87          8   2/5/1997         1\n191   10438         79          3   2/6/1997         2\n192   10439         51          6   2/7/1997         3\n193   10440         71          4  2/10/1997         2\n194   10441         55          3  2/10/1997         2\n195   10442         20          3  2/11/1997         2\n196   10443         66          8  2/12/1997         1\n\n\n\n\n\n\n\n\n\n\n\nClick here - customers table\n\n\n\n\n\n\nsqldf(\"select * from customers\")\n\n   CustomerID                         CustomerName              ContactName                                           Address             City PostalCode     Country\n1           1                  Alfreds Futterkiste             Maria Anders                                     Obere Str. 57           Berlin      12209     Germany\n2           2   Ana Trujillo Emparedados y helados             Ana Trujillo                  Avda. de la Constituci\\xf3n 2222   M\\xe9xico D.F.       5021      Mexico\n3           3           Antonio Moreno Taquer\\xeda           Antonio Moreno                                    Mataderos 2312   M\\xe9xico D.F.       5023      Mexico\n4           4                      Around the Horn             Thomas Hardy                                   120 Hanover Sq.           London    WA1 1DP          UK\n5           5                Berglunds snabbk\\xf6p       Christina Berglund                                 Berguvsv\\xe4gen 8         Lule\\xe5   S-958 22      Sweden\n6           6              Blauer See Delikatessen               Hanna Moos                                    Forsterstr. 57         Mannheim      68306     Germany\n7           7              Blondel p\\xe8re et fils Fr\\xe9d\\xe9rique Citeaux                               24, place Kl\\xe9ber       Strasbourg      67000      France\n8           8         B\\xf3lido Comidas preparadas         Mart\\xedn Sommer                                    C/ Araquil, 67           Madrid      28023       Spain\n9           9                             Bon app'        Laurence Lebihans                              12, rue des Bouchers        Marseille      13008      France\n10         10               Bottom-Dollar Marketse        Elizabeth Lincoln                                23 Tsawassen Blvd.        Tsawassen    T2F 8M4      Canada\n11         11                        B's Beverages        Victoria Ashworth                                 Fauntleroy Circus           London    EC2 5NT          UK\n12         12           Cactus Comidas para llevar         Patricio Simpson                                       Cerrito 333     Buenos Aires       1010   Argentina\n13         13           Centro comercial Moctezuma          Francisco Chang                           Sierras de Granada 9993   M\\xe9xico D.F.       5022      Mexico\n14         14                    Chop-suey Chinese                Yang Wang                                      Hauptstr. 29             Bern       3012 Switzerland\n15         15                  Com\\xe9rcio Mineiro             Pedro Afonso                           Av. dos Lus\\xedadas, 23     S\\xe3o Paulo  05432-043      Brazil\n16         16                Consolidated Holdings          Elizabeth Brown                       Berkeley Gardens 12 Brewery           London    WX1 6LT          UK\n17         17            Drachenblut Delikatessend             Sven Ottlieb                                      Walserweg 21           Aachen      52066     Germany\n18         18                      Du monde entier           Janine Labrune                      67, rue des Cinquante Otages           Nantes      44000      France\n19         19                   Eastern Connection                Ann Devon                                    35 King George           London    WX3 6FW          UK\n20         20                         Ernst Handel            Roland Mendel                                      Kirchgasse 6             Graz       8010     Austria\n21         21                   Familia Arquibaldo                Aria Cruz                                   Rua Or\\xf3s, 92     S\\xe3o Paulo  05442-030      Brazil\n22         22 FISSA Fabrica Inter. Salchichas S.A.               Diego Roel                                C/ Moralzarzal, 86           Madrid      28034       Spain\n23         23                    Folies gourmandes         Martine Ranc\\xe9                       184, chauss\\xe9e de Tournai            Lille      59000      France\n24         24                    Folk och f\\xe4 HB            Maria Larsson                                   \\xc5kergatan 24        Br\\xe4cke   S-844 67      Sweden\n25         25                       Frankenversand            Peter Franken                                 Berliner Platz 43       M\\xfcnchen      80805     Germany\n26         26                  France restauration           Carine Schmitt                                    54, rue Royale           Nantes      44000      France\n27         27                       Franchi S.p.A.            Paolo Accorti                               Via Monte Bianco 34           Torino      10100       Italy\n28         28       Furia Bacalhau e Frutos do Mar           Lino Rodriguez                            Jardim das rosas n. 32           Lisboa       1675    Portugal\n29         29         Galer\\xeda del gastr\\xf3nomo         Eduardo Saavedra                         Rambla de Catalu\\xf1a, 23        Barcelona       8022       Spain\n30         30               Godos Cocina T\\xedpica     Jos\\xe9 Pedro Freyre                                     C/ Romero, 33          Sevilla      41101       Spain\n31         31                  Gourmet Lanchonetes         Andr\\xe9 Fonseca                                   Av. Brasil, 442         Campinas  04876-786      Brazil\n32         32              Great Lakes Food Market            Howard Snyder                                  2732 Baker Blvd.           Eugene      97403         USA\n33         33                 GROSELLA-Restaurante           Manuel Pereira                      5\\xaa Ave. Los Palos Grandes          Caracas       1081   Venezuela\n34         34                        Hanari Carnes             Mario Pontes                                Rua do Pa\\xe7o, 67   Rio de Janeiro  05454-876      Brazil\n35         35                  HILARI\\xd3N-Abastos      Carlos Hern\\xe1ndez        Carrera 22 con Ave. Carlos Soublette #8-35 San Crist\\xf3bal       5022   Venezuela\n36         36           Hungry Coyote Import Store            Yoshi Latimer                    City Center Plaza 516 Main St.            Elgin      97827         USA\n37         37         Hungry Owl All-Night Grocers         Patricia McKenna                                  8 Johnstown Road             Cork                Ireland\n38         38                       Island Trading            Helen Bennett                         Garden House Crowther Way            Cowes   PO31 7PJ          UK\n39         39                   K\\xf6niglich Essen            Philip Cramer                                     Maubelstr. 90      Brandenburg      14776     Germany\n40         40                 La corne d'abondance            Daniel Tonini                            67, avenue de l'Europe       Versailles      78000      France\n41         41                     La maison d'Asie           Annette Roulet                             1 rue Alsace-Lorraine         Toulouse      31000      France\n42         42        Laughing Bacchus Wine Cellars          Yoshi Tannamuri                                      1900 Oak St.        Vancouver    V3F 2K1      Canada\n43         43                 Lazy K Kountry Store               John Steel                              12 Orchestra Terrace      Walla Walla      99362         USA\n44         44                  Lehmanns Marktstand           Renate Messner                                      Magazinweg 7   Frankfurt a.M.      60528     Germany\n45         45                    Let's Stop N Shop             Jaime Yorres                               87 Polk St. Suite 5    San Francisco      94117         USA\n46         46                    LILA-Supermercado       Carlos Gonz\\xe1lez Carrera 52 con Ave. Bol\\xedvar #65-98 Llano Largo     Barquisimeto       3508   Venezuela\n47         47                     LINO-Delicateses         Felipe Izquierdo                           Ave. 5 de Mayo Porlamar  I. de Margarita       4980   Venezuela\n48         48             Lonesome Pine Restaurant              Fran Wilson                                89 Chiaroscuro Rd.         Portland      97219         USA\n49         49         Magazzini Alimentari Riuniti         Giovanni Rovelli                           Via Ludovico il Moro 22          Bergamo      24100       Italy\n50         50                         Maison Dewey          Catherine Dewey                               Rue Joseph-Bens 532        Bruxelles     B-1180     Belgium\n51         51                    M\\xe8re Paillarde        Jean Fresni\\xe8re                                43 rue St. Laurent      Montr\\xe9al    H1J 1C3      Canada\n52         52               Morgenstern Gesundkost          Alexander Feuer                                       Heerstr. 22          Leipzig       4179     Germany\n53         53                          North/South           Simon Crowther                      South House 300 Queensbridge           London    SW7 1RZ          UK\n54         54         Oc\\xe9ano Atl\\xe1ntico Ltda.           Yvonne Moncada               Ing. Gustavo Moncada 8585 Piso 20-A     Buenos Aires       1010   Argentina\n55         55               Old World Delicatessen            Rene Phillips                                   2743 Bering St.        Anchorage      99508         USA\n56         56                Ottilies K\\xe4seladen      Henriette Pfalzheim                                Mehrheimerstr. 369          K\\xf6ln      50739     Germany\n57         57              Paris sp\\xe9cialit\\xe9s           Marie Bertrand                           265, boulevard Charonne            Paris      75012      France\n58         58         Pericles Comidas cl\\xe1sicas   Guillermo Fern\\xe1ndez                          Calle Dr. Jorge Cash 321   M\\xe9xico D.F.       5033      Mexico\n59         59                     Piccolo und mehr              Georg Pipps                                       Geislweg 14         Salzburg       5020     Austria\n60         60              Princesa Isabel Vinhoss         Isabel de Castro                         Estrada da sa\\xfade n. 58           Lisboa       1756    Portugal\n61         61                       Que Del\\xedcia         Bernardo Batista                           Rua da Panificadora, 12   Rio de Janeiro  02389-673      Brazil\n62         62                        Queen Cozinha        L\\xfacia Carvalho                      Alameda dos Can\\xe0rios, 891     S\\xe3o Paulo  05487-020      Brazil\n63         63                           QUICK-Stop              Horst Kloss                               Taucherstra\\xdfe 10        Cunewalde       1307     Germany\n64         64                        Rancho grande      Sergio Guti\\xe9rrez                            Av. del Libertador 900     Buenos Aires       1010   Argentina\n65         65           Rattlesnake Canyon Grocery             Paula Wilson                                   2817 Milton Dr.      Albuquerque      87110         USA\n66         66                   Reggiani Caseifici          Maurizio Moroni                            Strada Provinciale 124    Reggio Emilia      42100       Italy\n67         67                   Ricardo Adocicados           Janete Limeira                               Av. Copacabana, 267   Rio de Janeiro  02389-890      Brazil\n68         68                   Richter Supermarkt             Michael Holz                                 Grenzacherweg 237        Gen\\xe8ve       1203 Switzerland\n69         69                     Romero y tomillo         Alejandra Camino                                    Gran V\\xeda, 1           Madrid      28001       Spain\n70         70                     Sant\\xe9 Gourmet         Jonas Bergulfsen                            Erling Skakkes gate 78          Stavern       4110      Norway\n71         71                   Save-a-lot Markets           Jose Pavarotti                                   187 Suffolk Ln.            Boise      83720         USA\n72         72                   Seven Seas Imports               Hari Kumar                                   90 Wadhurst Rd.           London   OX15 4NB          UK\n73         73                        Simons bistro           Jytte Petersen                                   Vinb\\xe6ltet 34     K\\xf8benhavn       1734     Denmark\n74         74           Sp\\xe9cialit\\xe9s du monde        Dominique Perrier                                 25, rue Lauriston            Paris      75016      France\n75         75                Split Rail Beer & Ale       Art Braunschweiger                                      P.O. Box 555           Lander      82520         USA\n76         76               Supr\\xeames d\\xe9lices         Pascale Cartrain                              Boulevard Tirou, 255        Charleroi     B-6000     Belgium\n77         77                       The Big Cheese                Liz Nixon                          89 Jefferson Way Suite 2         Portland      97201         USA\n78         78                      The Cracker Box                 Liu Wong                               55 Grizzly Peak Rd.            Butte      59801         USA\n79         79                Toms Spezialit\\xe4ten            Karin Josephs                                     Luisenstr. 48       M\\xfcnster      44087     Germany\n80         80                  Tortuga Restaurante     Miguel Angel Paolino                                  Avda. Azteca 123   M\\xe9xico D.F.       5033      Mexico\n81         81         Tradi\\xe7\\xe3o Hipermercados        Anabela Domingues                        Av. In\\xeas de Castro, 414     S\\xe3o Paulo  05634-030      Brazil\n82         82    Trail's Head Gourmet Provisioners           Helvetius Nagy                                 722 DaVinci Blvd.         Kirkland      98034         USA\n83         83                         Vaffeljernet              Palle Ibsen                                  Smagsl\\xf8get 45         \\xc5rhus       8200     Denmark\n84         84                 Victuailles en stock             Mary Saveley                                2, rue du Commerce             Lyon      69004      France\n85         85            Vins et alcools Chevalier             Paul Henriot                                59 rue de l'Abbaye            Reims      51100      France\n86         86                    Die Wandernde Kuh           Rita M\\xfcller                                 Adenauerallee 900        Stuttgart      70563     Germany\n87         87                       Wartian Herkku         Pirkko Koskitalo                                       Torikatu 38             Oulu      90110     Finland\n88         88               Wellington Importadora            Paula Parente                                Rua do Mercado, 12          Resende  08737-363      Brazil\n89         89                 White Clover Markets           Karl Jablonski                       305 - 14th Ave. S. Suite 3B          Seattle      98128         USA\n90         90                          Wilman Kala          Matti Karttunen                                     Keskuskatu 45         Helsinki      21240     Finland\n91         91                               Wolski                  Zbyszek                                   ul. Filtrowa 68            Walla     01-012      Poland\n\n\n\n\n\n\n\n\n\n\n\nClick here - north_american_cities table\n\n\n\n\n\n\nsqldf(\"select * from north_american_cities\")\n\n                  city       country population latitude  longitude\n1          Guadalajara        Mexico    1500800 20.65970 -103.34961\n2              Toronto        Canada    2795060 43.65323  -79.38318\n3              Houston United States    2195914 29.76043  -95.36980\n4             New York United States    8405837 40.71278  -74.00594\n5         Philadelphia United States    1553165 39.95258  -75.16522\n6               Havana          Cuba    2106146 23.05407  -82.34519\n7          Mexico City        Mexico    8555500 19.43261  -99.13321\n8              Phoenix United States    1513367 33.44838 -112.07404\n9          Los Angeles United States    3884307 34.05223 -118.24368\n10 Ecatepec de Morelos        Mexico    1742000 19.60184  -99.05067\n11            Montreal        Canada    1717767 45.50169  -73.56726\n12             Chicago United States    2718782 41.87811  -87.62980",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'><yrChapterNumber>27. </yrChapterNumber> Get the data - \"books database\" and other tables</span>"
    ]
  },
  {
    "objectID": "sql0014-theBooksDatabase-v001.html#yrBooksDatabase",
    "href": "sql0014-theBooksDatabase-v001.html#yrBooksDatabase",
    "title": "27  27.  Get the data - “books database” and other tables",
    "section": "27.5 The “Books Database”",
    "text": "27.5 The “Books Database”\nThe “books database” is a collection of several tables that contains data relating to books (i.e. “titles”), authors, publishers, etc.\nYou can download the data for the database in the following ways:\n\nas a set of several CSV files or (see below)\nas a single sqlite3 database file (see below)\n\nSee below for details.\n\n27.5.1 SQLite file for “Books Database”\nClick here to download all the data for the books datbase as a single “SQLite” database file.\nTo use this file you can do one of the following:\n\nInstall SQLite software on your computer. There are many such programs avaiable. I recommend “DB Viewer for SQLite” which is a free open source program.\nWebsite (download from here): https://sqlitebrowser.org/\nGithub repository: https://github.com/sqlitebrowser/sqlitebrowser\nYou can search online for other similar programs for working with sqlite3 files.\nFree web based tool: https://y-rosenthal.github.io/yrSqliteViewer/yrSqliteViewer.html\nThis is a website that lets you upload a sqlite3 file and run SQL Select statements against the data. I created this website myself (with the help of some AI coding assistants).\nPROs: you don’t need to install anything\nCONs: currently only works with SQL SELECT statment but not any other types of sql statements.\nSee this chapter (in this book) https://y-rosenthal.github.io/DataManagementUsingR/sql0950-workingWithDatabasesInR-v010.html for instructions on how to access this database file directly via R commands.\n\n\n\n27.5.2 CSV files for “Books Database”\nDownload the CSV files for the “books database”. These CSV files together comprise the data for the “books database”.\n\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties.csv\n\nDownload a tutorial on using Relational databases.\n\nbooksDatabase description and Questions\n\nThis word document contains:\n\na complete description of the “books database”\na tutorial on how “relational databases” are structured\na tutorial on Entity Relationship Diagrams (ERDs)\na set of questions and answers\n\nOnce you’ve downloaded the various CSV files you can run the following commands to import the data into R.\n\n\n27.5.3 Import the data (books database)\n\nif(!require(readr)){install.packages(\"readr\");require(readr);}\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"royalties.csv\", na=\"NULL\", show_col_types=FALSE)\n\n\n\n27.5.4 Books Database - Entity Relationship Diagram (ERD)\nThe following is an “Entity Relationship Diagram (ERD)” that describes the relationships between the tables. See the Word document linked above for more info:\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#0000ff', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'mainBkg': '#ffffff', 'background': '#ffffff', 'edgeLabelBackground': '#ffffff' }}}%%\nerDiagram\n    authors ||--o{ title_authors : \"\"\n    title_authors }|--|| titles : \"\"\n    titles ||--o| royalties : \"\"\n    publishers |o--o{ titles : \"\"\n    \n    authors {\n        string au_id PK\n        string au_fname\n        string au_lname\n        string phone\n        string address\n        string city\n        string state\n        string zip\n    }\n    \n    title_authors {\n        string title_id FK1\n        string au_id FK2\n        int au_order\n        float royalty_share\n    }\n    \n    titles {\n        string title_id PK\n        string title_name\n        string type\n        string pub_id FK\n        int pages\n        float price\n        int sales\n        date pubdate\n    }\n    \n    publishers {\n        string pub_id PK\n        string pub_name\n        string city\n        string state\n        string country\n    }\n    \n    royalties {\n        string title_id PK\n        float advance\n        float royalty_rate\n    }\n\n\n\n\n\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here - titles table\n\n\n\n\n\n\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n\n\n\n\n\n\n\n\n\n\nClick here - publishers table\n\n\n\n\n\n\nsqldf(\"select * from publishers\")\n\n  pub_id             pub_name          city state country\n1    P01    Abatis Publishers      New York    NY     USA\n2    P02      Core Dump Books San Francisco    CA     USA\n3    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    P04    Tneterhooks Press      Berkeley    CA     USA\n5    P05       AAA Publishing      Berkeley    CA     USA\n\n\n\n\n\n\n\n\n\n\n\nClick here - title_authors table\n\n\n\n\n\n\nsqldf(\"select * from title_authors\")\n\n   title_id au_id au_order royalty_shares\n1       T01   A01        1            1.0\n2       T02   A01        1            1.0\n3       T03   A05        1            1.0\n4       T04   A03        1            0.6\n5       T04   A04        2            0.4\n6       T05   A04        1            1.0\n7       T06   A02        1            1.0\n8       T07   A02        1            0.5\n9       T07   A04        2            0.5\n10      T08   A06        1            1.0\n11      T09   A06        1            1.0\n12      T10   A02        1            1.0\n13      T11   A03        2            0.3\n14      T11   A04        3            0.3\n15      T11   A06        1            0.4\n16      T12   A02        1            1.0\n17      T13   A01        1            1.0\n\n\n\n\n\n\n\n\n\n\n\nClick here - authors table\n\n\n\n\n\n\nsqldf(\"select * from authors\")\n\n  au_id  au_fname    au_lname        phone              address          city state   zip\n1   A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468\n2   A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303\n3   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n4   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n5   A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014\n6   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305\n7   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236\n\n\n\n\n\n\n\n\n\n\n\nClick here - royalties table\n\n\n\n\n\n\nsqldf(\"select * from royalties\")\n\n   title_id advance royalty_rate\n1       T01   10000         0.05\n2       T02    1000         0.06\n3       T03   15000         0.07\n4       T04   20000         0.08\n5       T05  100000         0.09\n6       T06   20000         0.08\n7       T07 1000000         0.11\n8       T08       0         0.04\n9       T09       0         0.05\n10      T10      NA           NA\n11      T11  100000         0.07\n12      T12   50000         0.09\n13      T13   20000         0.06",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'><yrChapterNumber>27. </yrChapterNumber> Get the data - \"books database\" and other tables</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html",
    "href": "sql0006-introToDatabases-v023.html",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "",
    "text": "28.1 Common features of databases\nMost databases and DBMS software provide the following features:",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#common-features-of-databases",
    "href": "sql0006-introToDatabases-v023.html#common-features-of-databases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "",
    "text": "Able to be contolled by other software - Databases and DBMS software are usually used as part of a larger software system. Software that is coded in programming languages such as R, Python, Java, etc. have the ability to interact with a DBMS in order to store an retrieve data from a database.\nFor example, websites that manipulate a lot of data generally consist of a “web server software program” that users communicate with. The “web server software” program in turn communicates with a DBMS in order to store and retrieve data for each user. For example, facebook.com makes use of one or more databases to store and retrieve messages, friend information, advertisements, etc. The web server program retrieves the information it needs from a database and uses that information to create a web page that is then returned to the user and displayed in the browser.\nReliable and Accurate Data – Databases have built-in rules to keep data correct and consistent. They prevent mistakes like duplicate entries or missing information by ensuring that all data follows set guidelines.\nMulti User Access – Databases allow many people to access and update data at the same time without causing errors or losing information. They use special features to keep everything organized, even when multiple users make changes at once.\nHandles Tremendous Amounts of Data – Databases are designed to store and manage huge amounts of data efficiently. They remain fast and responsive even as the amount of data grows.\nSecurity – Databases have security settings to control who can see or change the data.\nReliability - Databases have safeguards to ensure that changes are properly saved and that information isn’t lost, even if something goes wrong (e.g. a blackout, earthquake, etc.).\n\n\nNote that Excel is not usually considered a “database”. Excel is FANTASTIC as a tool for quickly gaining insights into data. However, Excel does not have most of the features listed above that describe typical databases.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#different-categories-of-databases",
    "href": "sql0006-introToDatabases-v023.html#different-categories-of-databases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.2 Different categories of databases",
    "text": "28.2 Different categories of databases\nThere are different types of databases and Database Management System (DBMS) software. These types can be categorized by how their data is structured.\nRelational databases (sometimes referred to as “SQL” databases) arrange data in “tables” that are comprised of “rows” and “columns” (similar to an R dataframe or an Excel spreadsheet).\nNon-Relational databases (often referred to as “NoSQL” databases) structure data in other forms (e.g. JSON or graphs - see below)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#what-is-a-relational-database",
    "href": "sql0006-introToDatabases-v023.html#what-is-a-relational-database",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.3 What is a “Relational Database”",
    "text": "28.3 What is a “Relational Database”\nRelational databases store information in “tables” that have rows and columns of information - very similar to the way the data is arranged in an R dataframe. A single relational database is a collection of one or more tables. Each column of a relational database table (a) has a name (b) has a datatype (e.g. numeric, logical, character, etc)\n\nThere are MANY different relational database products in use today. Some very popular ones are:\n\nPostgres\nMySql\nMariaDb\nMicrosoft SQL Server\nOracle Relational Database Management System\nSQLite\nmany, many others",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#what-is-sql-often-pronounced-sequel",
    "href": "sql0006-introToDatabases-v023.html#what-is-sql-often-pronounced-sequel",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.4 What is SQL (often pronounced “sequel”)",
    "text": "28.4 What is SQL (often pronounced “sequel”)\n\n\nRelational database software uses Structured Query Language (SQL) to manipulate database contents. SQL commands are categorized as follows:\nRelational database software uses the Structured Query Language (SQL) (often pronounced “sequel”) to manipulate the contents of the database. Commands in the SQL language are often grouped into the following categories:\n(For this class, we will focus on the first two categories listed below,  i.e. DDL and DML)\n\n\n\nCategory\n\n\nCommands\n\n\n\n\nDDL (Data Definition Language) Commands Defines and modifies database structure.\n\n\nCREATE DATABASE – Creates a new database.  CREATE TABLE – Creates a new table in an existing database.  DROP TABLE – Removes an entire table from a database.\n\n\n\n\nDML (Data Manipulation Language) Commands Manipulates data within tables.\n\n\nSELECT – retrieves data from database tables. The SELECT statement has many different options that allow the data to be retireved in very customized formats and arrangements. E.g. SELECT can be used to summarize data, retrieve only data that has specific attributes, display data in specific order, etc. INSERT – Adds new rows to a table.  DELETE – Removes rows from a table.  UPDATE – Modifies existing data in a table.\n\n\n\n\nDCL (Data Control Language) Commands Manages user permissions.\n\n\nGRANT – Give permission to specific users to access specific data in the database. REVOKE – Remove permission from specific users to access specific info in the database.\n\n\n\n\nTCL (Transaction Control Language) Commands Transactions ensure that a group of operations either all succeed or none take effect. If a failure occurs, the system ensures consistency using transaction control commands. (* see below for more info about transactions)\n\nSTART TRANSACTION – Begins a transaction.  COMMIT – Saves changes permanently.  ROLLBACK – Reverts changes if needed.  SAVEPOINT – Creates checkpoints within a transaction.\n\n\n\n\n(* transactions explained more fully): Most Relational databases offer “transaction control”. A transaction is a set of operations that are supposed to either all happen as a group or not happen at all. For example, when processing a money transfer between two bank accounts, the following steps must happen:\nStep 1: remove money from account 1\nStep 2: deposit money in account 2\nThese two steps are viewed as a “transaction”. Either both steps should happen - in which case the transaction was successful - or neither step should happen. Imagine that after the computer did Step 1, the computer crashes (e.g. due to a blackout) before it had a chance to do Step 2. Ideally, when the computer is restarted, either, Step 2 should happen or Step 1 should be “undone”. Most Relational Database Management Software contains features that ensure that this logic happens correctly. \n\n\n\n(For this class, we will mostly focus on the DDL an DML categories of commands. Most of our time will be spent studying the numerous features of the SELECT statement)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#different-flavors-of-sql",
    "href": "sql0006-introToDatabases-v023.html#different-flavors-of-sql",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.5 Different “flavors” of SQL",
    "text": "28.5 Different “flavors” of SQL\nEach relational database software product has its own “flavor” of the SQL language. You can think of this as different “dialects” of the same language. For example the difference between English as it’s used in the USA vs as it is used in England.\nANSI (American National Standards Institute) publishes a SQL Standard that is generally recognized by all relational database software programs.\nHowever, most relational database products add additional capabilities and commands to its version of SQL to differentiate it from other relational database products in the market.\nIn this class we will be working with the SQLite version of SQL. However, we will try to focus primarily on the standard SQL features that are included in all relational database products and not on the extensions that are provided only by SQLite. When appropriate, we will try to highlight differences between the way that SQLite and other RDBMS products work.\nThe ANSI standard was first introduced in 1986 and has been reissued every few years since then. The following gives a flavor of features that were introduced or “deprecated” (keep reading) in different versions of the ANSI standard. To “deprecate” a feature means to recommend that it not be used as the feature might actually be removed in a future version.\n\n\n\n\n\n\nInfo about ANSI SQL standard\n\n\n\nOfficial ANSI document is not free:\nhttps://webstore.ansi.org/standards/iso/isoiec90752023-2502159?source=blog&_gl=1\nUnofficial info:\nhttps://modern-sql.com/standard\n\n\n\n\n\n\n\n\nClick here for summary of what was included in different releases of ANSI SQL\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Version\nYear\nKey Features Introduced\nDeprecated Features\n\n\n\n\nSQL-86 (SQL-87)\n1986/1987\nFirst ANSI SQL standard, basic SQL syntax (SELECT, INSERT, UPDATE, DELETE), table creation, constraints\nNone\n\n\nSQL-89\n1989\nGRANT and REVOKE for access control\nNone\n\n\nSQL-92\n1992\nExpanded data types (DATE, TIME, INTERVAL), subqueries, joins (INNER JOIN, LEFT JOIN, RIGHT JOIN), string pattern matching (LIKE)\nSome legacy syntax\n\n\nSQL:1999\n1999\nRecursive queries (WITH RECURSIVE), OLAP operations (ROLLUP, CUBE), triggers, procedural constructs (CASE, IF, LOOP), user-defined types (UDTs)\nCertain outdated syntax elements\n\n\nSQL:2003\n2003\nWindow functions (OVER, PARTITION BY), XML support, auto-increment (IDENTITY, GENERATED), MERGE statement\nOutdated join syntax\n\n\nSQL:2006\n2006\nFull XML data handling, querying, and updates\nNone\n\n\nSQL:2008\n2008\nTRUNCATE TABLE, FETCH FIRST (limit control), improvements to MERGE\nNone\n\n\nSQL:2011\n2011\nTemporal tables (SYSTEM VERSIONING for time-travel queries)\nNone\n\n\nSQL:2016\n2016\nJSON support (JSON_VALUE, JSON_QUERY), row pattern recognition (MATCH_RECOGNIZE)\nNone\n\n\nSQL:2019\n2019\nPolymorphic table functions, new JSON functions and enhancements\nNone",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#non-relational-databases-aka-nosql-databases",
    "href": "sql0006-introToDatabases-v023.html#non-relational-databases-aka-nosql-databases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.6 Non-Relational Databases (AKA NoSQL databases)",
    "text": "28.6 Non-Relational Databases (AKA NoSQL databases)\nNoSQL (or non-relational) databases organize information in formats other than rows and columns.\nSOME NoSql databases actually CAN be manipulated with the SQL language. However, the term NoSql has come to include any database that stores information in a form other than in strict rows and columns.\nSome examples are shown below.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#yrDocumentDatabases",
    "href": "sql0006-introToDatabases-v023.html#yrDocumentDatabases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.7 Document databases - e.g. MongoDB",
    "text": "28.7 Document databases - e.g. MongoDB\nMongoDb is an example of Database Management software that stores information in “JSON” notation. Other similar products include CouchDB, Firebase and Firestore. We will explore more about JSON later in the semester. However as an example, the above information could be represented in a JSON file as shown below:\n           [\n             {\n               \"name\" : \"annie\",\n               \"friends\" : [ \"sam\", \"jack\" ]\n             },\n             {\n               \"name\" : \"jack\",\n               \"friends\" : [ \"annie\" , \"mac\" , \"doug\" , \"harry\" ]\n             },\n             {\n               \"name\" : \"harry\",\n               \"friends\" : [ \"jack\" ]\n             },\n             {\n               \"name\" : \"sam\",\n               \"friends\" : [ \"annie\" ]\n             },\n             {\n               \"name\" : \"mac\",\n               \"friends\" : [ \"jack\" ]\n             },\n             {\n               \"name\" : \"harry\",\n               \"friends\" : [ \"jack\" ]\n             },\n             {\n               \"name\" : \"doug\",\n               \"friends\" : [ \"jack\" ]\n             }\n           ]",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#yrGraphDatabases",
    "href": "sql0006-introToDatabases-v023.html#yrGraphDatabases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.8 Graph databases - e.g. Neo4J",
    "text": "28.8 Graph databases - e.g. Neo4J\nNeo4J is an example of a “graph database”. These types of databases organize their information in a “graph” structure. A graph looks like a “web” of information. It is comprised of “nodes” that contain data which are connected via “edges” that represent relationships between the nodes.\nFor example:\n\n\n\n\n\ngraph TD\n    %% Position nodes to match example layout\n    sam([sam])\n    annie([annie])\n    mac([mac])\n    jack([jack])\n    doug([doug])\n    harry([harry])\n    \n    %% Create bidirectional connections with straight lines\n    annie &lt;--&gt;|friend| sam\n    jack &lt;--&gt;|friend| annie\n    jack &lt;--&gt;|friend| mac\n    jack &lt;--&gt;|friend| doug\n    jack &lt;--&gt;|friend| harry\n    \n    %% Black and white styling\n    classDef person fill:white,stroke:black,stroke-width:1px;\n    class sam,annie,mac,jack,doug,harry person;\n    \n    %% Force specific layout\n    sam ~~~ annie\n    mac ~~~ jack ~~~ doug\n    jack ~~~ harry\n    \n    %% Make invisible linkage lines transparent\n    linkStyle 5,6,7 stroke:none;\n\n\n\n\n\n\nThis example shows a set of people and who is friends with whom. There is a different icon for each person. “Friend arrows” indicate who is friends with whom.\nFor many types of information a diagram as shown above is much more intuitive and understandable than a table of information. This is just one reason why graph databases are popular.\nHowever, this same information could be converted to the following table and stored in a relational database.\n            name      is_friends_with\n            ----      ---------------\n            sam       annie\n            annie     sam\n            annie     jack\n            jack      annie\n            jack      mac\n            jack      doug\n            jack      harry\n            doug      jack\n            harry     jack\n            mac       jack\nThe following is version of the graph also includes “company” nodes and relationships indicating where people work. In general, graphs can be quite complex and be used to store all sorts of information.\n\n\n\n\n\ngraph LR\n    %% Define nodes with type labels\n    sam([Person: sam])\n    annie([Person: annie])\n    mac([Person: mac])\n    jack([Person: jack])\n    doug([Person: doug])\n    harry([Person: harry])\n    Acme([Company: Acme Inc.])\n    MikroTek([Company: MikroTek])\n    \n    %% Friend relationships (bidirectional)\n    annie &lt;--&gt;|friend| sam\n    jack &lt;--&gt;|friend| annie\n    jack &lt;--&gt;|friend| mac\n    jack &lt;--&gt;|friend| doug\n    jack &lt;--&gt;|friend| harry\n    \n    %% Work relationships\n    sam --&gt;|works at| Acme\n    annie --&gt;|works at| Acme\n    mac --&gt;|works at| MikroTek\n    jack --&gt;|works at| MikroTek\n    doug --&gt;|works at| MikroTek\n    \n    %% Styling for nodes - white background with black borders and text\n    classDef person fill:white,stroke:black,stroke-width:1px,color:brown;\n    classDef company fill:white,stroke:black,stroke-width:2px,stroke-dasharray:5 2,color:green;\n    \n    %% Apply styles\n    class sam,annie,mac,jack,doug,harry person;\n    class Acme,MikroTek company;",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "sql0006-introToDatabases-v023.html#other-types-of-nosql-databases",
    "href": "sql0006-introToDatabases-v023.html#other-types-of-nosql-databases",
    "title": "28  28.  “databases” and “database management systems”",
    "section": "28.9 Other types of NoSQL databases",
    "text": "28.9 Other types of NoSQL databases\n\nThe are other types of NoSQL databases. The following are also popular. We will not go into these in more detail now.\nThe main thing that NoSQL databases have in common is that the data is NOT stored in a rectangular form (i.e. rows and columns).\n\nKey-Value Stores\n\nStore data as a collection of key-value pairs.\n\nFast reads and writes, ideal for caching and session management.\n\nExamples: Redis, Amazon DynamoDB, Riak\n\nColumn-Family Stores\n\nOrganize data into columns rather than rows, optimized for analytical queries.\n\nDesigned for high scalability and distributed storage.\n\nExamples: Apache Cassandra, HBase, ScyllaDB",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'><yrChapterNumber>28. </yrChapterNumber>  \"databases\" and \"database management systems\"</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html",
    "href": "0500400-sqlSelectIntro-v003.html",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "",
    "text": "29.1 NOTE - this page is for “IDS2040 - Data Management” (not “IDS1020 - Intro to IDS”)\nNOTE - If you are in my “IDS1020 - Intro to IDS” class, this is NOT the page you should be reading. Rather, jump to the next section entitled “Relational Databases and SQL” and start from there.\nThis page is intended for my “IDS460 - Data Management” class. The info below assumes that you have some knowledge of the dplyr package in R (which was covered in the previous section). After you get a basic idea of what the SQL SELECT statement does and how it is very similar to the dplyr functions, you can move onto the next section entitled “Relational Databases and SQL” and continue with that information.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#get-the-data",
    "href": "0500400-sqlSelectIntro-v003.html#get-the-data",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.2 Get the data",
    "text": "29.2 Get the data\nThe data we are using in this section contains information about salespeople who are employees of a company. Each row of the data contains info about one salesperson. The salespeople get paid some “base pay” as well as a commission that is a percent of the total dollar amount of sales they make.\nFollow the instructions below to get the data and start using it in R.\n\n29.2.1 Load the R packages we’ll neeed\n\nif(!require(tibble)){install.packages(\"tibble\");require(tibble);}\nif(!require(dplyr)){install.packages(\"dplyr\");require(dplyr);}\nif(!require(readr)){install.packages(\"readr\");require(readr);}\nif(!require(sqldf)){install.packages(\"sqldf\");require(sqldf);}\n\n\n\n29.2.2 Download the file\nThe data is contained in a csv file. If you’d like to follow along with this tutorial on your own computer you can download the .csv file we are using by clicking here.\n\n\n29.2.3 Import the data by clicking on some buttons …\n\nThe code in the next section below uses read_csv function to read the data into R. If you are not comfortable with R, I recommend that you instead, follow the instructions starting in the next bullet to import the data into R.\nIf you are not familiar with R, you may have some trouble running the read_csv() code shown below. Instead, I recommend that you follow the following instructions to import the file into R.\nTo do so, click on “Import Dataset” button on the “Environment” tab (usually found in the upper-right-hand window pane in RStudio).\nChoose “From Text (base)” and locate your file. You should see something like this:\n\nMake sure to change the “Name” portion (see circled section in picture) to read “sales”, then press the “import” button. This will open up a new tab in RStudio that shows the contents of the file. You can safely navigate away from this tab or close the tab and the data will remain imported and can be seen by typing “sales” (without the “quotes”).\n\n\n\n29.2.4 Import the data by typing some R code …\nThe following code reads the data into R. Alternatively, you can follow the instructions above to click on some buttons to import the data.\n\n# Read in the data into a tibble\n#\n# Note that the following code uses the readr::read_csv function from the readr\n# package which is part of the tidyverse collection of packages. \n# This function is similar to the base-R read.csv function.\n# \n# read_csv returns a tibble, which is the data structure that the \n# tidyverse packages use in lieu of dataframes. A tibble is basically\n# a dataframe with extra features.\n# By contrast, the base-r read.csv function returns a dataframe.\n \nsales = read_csv(\"salespeople-v002.csv\", na=c(\"\",\"NULL\"), show_col_types=FALSE)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#databases-and-sql",
    "href": "0500400-sqlSelectIntro-v003.html#databases-and-sql",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.3 Databases and SQL",
    "text": "29.3 Databases and SQL\nA database is an organized collection of data that is designed to be accessed and manipulated by computer programs. We’ll cover a little more about exactly what a database is and how it differs from Excel, R and other programs in the next section of this book. Below is just a brief explanation to give you some background before we start getting into the details of the SQL language that is used to communicate with databases.\nA “relational database” is a type of database in which data is arranged in “tables” that are organized into “rows” and “columns”. A “relational database table” is very similar to an R dataframe or tibble. Relational databases are controlled and managed with software known as “Relational DataBase Management System” (RDBMS) software. Relational database technology dates back to the 1970s and has been around long before R has been around. Relational databases are used by just about every major company all around the world.\n“Structured Query Language” (or SQL for short - pronounced “sequel”) is the standard language that is used to communicate with a DBMS to manage and manipulate the data in a Relational Database. SQL has many different commands. One of the most important SQL commands for data analysts to know is the SQL “SELECT” command. It is this command that is used to extract data from a database and organize the data into a desired form. We will focus in this section on an intro to the SQL SELECT statement.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#using-sql-in-r",
    "href": "0500400-sqlSelectIntro-v003.html#using-sql-in-r",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.4 Using SQL in R",
    "text": "29.4 Using SQL in R\nAs we said above, R is NOT a Database Management System. However SQL is a very popular language. Many technologists come to R, already having a deep knowledge of the SQL language. Therefore it is nice to know that the SQL language can also be used to manipulate R dataframes. This is possible because R dataframes are very, very similar in structure to Relational Database “tables” (i.e. they have rows, columns, column names and specific datatypes for each column).\nThe R “sqldf” package includes a function named “sqldf” which takes a SQL command as its argument. Wherever the SQL command refers to the name of a relational database “table”, sqldf runs the SQL command using the R dataframe with that name.\nThere are other more advanced ways for R to connect to SQL databases (DB) and database management systems (DBMS). We will cover the more advanced ways later. For now, sqldf will suit our purposes for this intro.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#dplyr-is-very-similar-to-the-sql-select-command",
    "href": "0500400-sqlSelectIntro-v003.html#dplyr-is-very-similar-to-the-sql-select-command",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.5 dplyr is very similar to the SQL SELECT command",
    "text": "29.5 dplyr is very similar to the SQL SELECT command\nThe designers of dplyr got inspiration for many of their ideas from SQL. Therefore once you know the basics of dplyr it should be very easy to transfer that knowledge to learning the SQL SELECT statement.\nThe following examples show how the concepts you learned in the previous section about dplyr carry over to the SQL SELECT statement.\nThe English word “query” means a question. A SQL SELECT statement is often referred to as a database “query”. In essence a SQL SELECT statement in essence asks the database a question and gets back an answer.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#sql-select-clauses-compared-with-dplyr-functions",
    "href": "0500400-sqlSelectIntro-v003.html#sql-select-clauses-compared-with-dplyr-functions",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.6 SQL SELECT “clauses” compared with dplyr “functions”",
    "text": "29.6 SQL SELECT “clauses” compared with dplyr “functions”\nWhile dplyr uses different “functions”, the SQL SELECT statement is comprised of different “clauses”. The clauses in the SQL SELECT statement are listed below. We will elaborate on the details of these clauses in the sections below.\n\nSELECT\nThis is used to “select” the columns you want - similar to dplyr select function.\nFROM\nUsed to specify which “tables” contain the information you will be working with. In dplyr this is accomplished via the first argument, .data, of each of the dplyr “verb” functions that we learned about in the previous section.\nWHERE\nChoose the rows you want. Directly analogous to the dplyr “filter” function.\nGROUP BY\nDirectly analogous to the dplyr “group_by” function.\nHAVING\nThis is also similar to the dplyr “filter” function. We will learn later how this differs from the SQL SELECT WHERE clause.\nORDER BY\nDirectly analogous to the dplyr “arrange” function\nLIMIT\nDirectly analogous to the dplyr print(n=…) function or the dplyr slice_head(n=…) function.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#order-of-the-clauses-is-important",
    "href": "0500400-sqlSelectIntro-v003.html#order-of-the-clauses-is-important",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.7 Order of the clauses is important",
    "text": "29.7 Order of the clauses is important\nA SQL SELECT statement may contain some or all of the above clauses. Only the SELECT clause is absolutely required. If the SQL SELECT statement contains more than one clause then whichever clauses do appear must appear in the order listed above.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#is-sql-case-sensitive-yes-and-no",
    "href": "0500400-sqlSelectIntro-v003.html#is-sql-case-sensitive-yes-and-no",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.8 Is SQL case sensitive? yes and no",
    "text": "29.8 Is SQL case sensitive? yes and no\nDifferent SQL products treat case sensitivity differently. In general the names of tables and columns ARE case sensitive. The “keywords” of the language are generally NOT case sensitive. However, it is often the convention of many SQL books and references to show SQL keywords in UPPER CASE. We have done that below but it is not strictly necessary.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#whitespace-in-sql-commands-is-ignored",
    "href": "0500400-sqlSelectIntro-v003.html#whitespace-in-sql-commands-is-ignored",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.9 Whitespace in SQL commands is ignored",
    "text": "29.9 Whitespace in SQL commands is ignored\nA SQL command may be written entirely on one line. However, extra whitespace (i.e. spaces, tabs, newlines) may be added to make the code more readable. It is common to start each SQL SELECT statement clause on a new line and to add extra whitespace to make the code more readable. In general we tried to follow that practice in the code.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#intro-to-erds---i.e.-entity-relationship-diagrams",
    "href": "0500400-sqlSelectIntro-v003.html#intro-to-erds---i.e.-entity-relationship-diagrams",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.10 Intro to ERDs - i.e. “Entity Relationship Diagrams”",
    "text": "29.10 Intro to ERDs - i.e. “Entity Relationship Diagrams”\nAn “Entity Relationship Diagram” (ERD) is a diagram that highlights the structure of the tables in a database (in database terminology tables are also known as “entity sets” or “entity types” - see below). The ERD does NOT show the data in the tables, just the structure of the tables. The following is an ERD for the sales data we have been using.\nNote that so far we’ve only been using one table. An ERD gets more complicated when a database contains several tables. We will revisit ERDs again later.\n\n\n\n\n\nerDiagram\n    SALES {\n        first character \"employee first name\"\n        last character \"employee last name\"\n        title character \"employee title\"\n        region character \"region that employee sells to\"\n        baseInThousands numeric \"employee's base pay (in thousands)\"\n        salesInThousands numeric \"total sales dollars (in thousands)\"\n        yearsWithCompany numeric \"number years employee with company\"\n    }\n\n\n\n\n\n\n\nThe table name is shown at the top\nUnder the table name\n\nThe 1st column contains the names of the table’s fields (i.e. columns)\nThe 2nd column contains the datatype of the fields\nThe 3rd column contains a description of the field\n\n\nSome Entity Relationship Diagrams don’t contain as much information. Often an ERD will NOT contain a description of the fields. This is still a “valid” ERD, just not as descriptive.\n\n\n\n\n\nerDiagram\n    SALES {\n        first character\n        last character\n        title character\n        region character\n        baseInThousands numeric\n        salesInThousands numeric\n        yearsWithCompany numeric\n    }\n\n\n\n\n\n\nIn the extreme case, an ERD may only contain the names of the tables. This type of ERD is only really useful when the database contains multiple tables.\n\n\n\n\n\nerDiagram\n    SALES",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#database-terminology---fields-entities-etc",
    "href": "0500400-sqlSelectIntro-v003.html#database-terminology---fields-entities-etc",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.11 Database terminology - fields, entities, etc",
    "text": "29.11 Database terminology - fields, entities, etc\nThere are different terms used to refer to the rows and columns of a table, dataframe or tibble. Statisticians often refer to each row of a table as an observation and each column of a table as a variable.\nIn the world of databases, a column can be referred to as a “column” or a “field”. Rows are referred to as just “rows” but sometimes are referred to as “records” or “entities”. Often an entire table is referred to as an “entity” (technically a table, which is a collection of rows, is an “entity set” or an “entity type”).\nThe truth is that not many people use the word “entity” or “entity set”. However, an “Entity Relationship Diagram” is used to show the tables in a database and how the tables are “related” to each other.\nIn this section we are just focusing on a single table. In a later sections, we’ll learn more about “relationships” between multiple tables and how the relationships are displayed on an Entity Relationship Diagram.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#select-and-from-clauses",
    "href": "0500400-sqlSelectIntro-v003.html#select-and-from-clauses",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.12 SELECT and FROM clauses",
    "text": "29.12 SELECT and FROM clauses\n\n# The SELECT clause specifies which columns you want.\n# The FROM clause specifies the table (or tables) that contain the data.\nsqldf(\"SELECT title, first, last\n       FROM sales\")\n\n             title  first       last\n1          Manager    Joe      Smith\n2      Salesperson    Sam        Lee\n3      Salesperson    Sue  Sallinger\n4  Sr. Salesperson   Barb      Brown\n5          Manager   Jack     Martin\n6  Sr. Salesperson   Kate      Carey\n7      Salesperson    Emy    Parsons\n8      Salesperson Monica        Oja\n9  Sr. Salesperson    Jim Kuruzovich\n10         Manager   Zeke      Cohen\n11     Salesperson   Zeke      Smith\n12     Salesperson    Amy   Markovic\n13 Sr. Salesperson    Joe Washington\n14         Manager    Sam    Lincoln\n15 Sr. Salesperson    Sue      Aames\n16     Salesperson   Barb      Aames\n17     Salesperson   Jack      Aames\n18 Sr. Salesperson   Kate   Zeitchik\n19         Manager    Emy   Zeitchik\n20     Salesperson Monica   Zeitchik\n21     Salesperson    Jim      Brown\n22 Sr. Salesperson  Larry      Green\n23         Manager  Laura      White\n24 Sr. Salesperson   Hugh      Black",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#select-from",
    "href": "0500400-sqlSelectIntro-v003.html#select-from",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.13 SELECT * FROM …",
    "text": "29.13 SELECT * FROM …\n\n# SELECT * in SQL is similar to select(everything()) in dplyr.\n# * is an abbreviation for all of the column names.\n\nsqldf(\"SELECT *\n       FROM sales\")\n\n    first       last           title region baseInThousands salesInThousands yearsWithCompany\n1     Joe      Smith         Manager    usa              40              100               10\n2     Sam        Lee     Salesperson    usa              40              200                6\n3     Sue  Sallinger     Salesperson   asia              30              150               10\n4    Barb      Brown Sr. Salesperson   asia              35               79                4\n5    Jack     Martin         Manager europe              20               40                3\n6    Kate      Carey Sr. Salesperson europe              20               69                2\n7     Emy    Parsons     Salesperson africa              25               87                3\n8  Monica        Oja     Salesperson africa              35               98               10\n9     Jim Kuruzovich Sr. Salesperson    usa              35              140                3\n10   Zeke      Cohen         Manager    usa              30              250               10\n11   Zeke      Smith     Salesperson   asia              20              120                3\n12    Amy   Markovic     Salesperson   asia              46              340                3\n13    Joe Washington Sr. Salesperson europe              33              370                2\n14    Sam    Lincoln         Manager europe              30              500                2\n15    Sue      Aames Sr. Salesperson africa              35              600               10\n16   Barb      Aames     Salesperson    usa              21              255                7\n17   Jack      Aames     Salesperson    usa              43              105                4\n18   Kate   Zeitchik Sr. Salesperson    usa              50              187                4\n19    Emy   Zeitchik         Manager   asia              34              166                4\n20 Monica   Zeitchik     Salesperson   asia              23              184                1\n21    Jim      Brown     Salesperson europe              50              167                2\n22  Larry      Green Sr. Salesperson europe              20              113                4\n23  Laura      White         Manager africa              20              281                8\n24   Hugh      Black Sr. Salesperson africa              40              261                9\n\n# Unfortunately, the SQL SELECT * is not as smart as the everything() function in dplyr.\n# In dplyr, everything() does not include columns that you already typed.\n# In SQL, the * always includes ALL columns. Therefore the following\n# query displays the title and region columns a 2nd time due to the *\n\nsqldf(\"SELECT title, region, *\n       FROM sales\")\n\n             title region  first       last           title region baseInThousands salesInThousands yearsWithCompany\n1          Manager    usa    Joe      Smith         Manager    usa              40              100               10\n2      Salesperson    usa    Sam        Lee     Salesperson    usa              40              200                6\n3      Salesperson   asia    Sue  Sallinger     Salesperson   asia              30              150               10\n4  Sr. Salesperson   asia   Barb      Brown Sr. Salesperson   asia              35               79                4\n5          Manager europe   Jack     Martin         Manager europe              20               40                3\n6  Sr. Salesperson europe   Kate      Carey Sr. Salesperson europe              20               69                2\n7      Salesperson africa    Emy    Parsons     Salesperson africa              25               87                3\n8      Salesperson africa Monica        Oja     Salesperson africa              35               98               10\n9  Sr. Salesperson    usa    Jim Kuruzovich Sr. Salesperson    usa              35              140                3\n10         Manager    usa   Zeke      Cohen         Manager    usa              30              250               10\n11     Salesperson   asia   Zeke      Smith     Salesperson   asia              20              120                3\n12     Salesperson   asia    Amy   Markovic     Salesperson   asia              46              340                3\n13 Sr. Salesperson europe    Joe Washington Sr. Salesperson europe              33              370                2\n14         Manager europe    Sam    Lincoln         Manager europe              30              500                2\n15 Sr. Salesperson africa    Sue      Aames Sr. Salesperson africa              35              600               10\n16     Salesperson    usa   Barb      Aames     Salesperson    usa              21              255                7\n17     Salesperson    usa   Jack      Aames     Salesperson    usa              43              105                4\n18 Sr. Salesperson    usa   Kate   Zeitchik Sr. Salesperson    usa              50              187                4\n19         Manager   asia    Emy   Zeitchik         Manager   asia              34              166                4\n20     Salesperson   asia Monica   Zeitchik     Salesperson   asia              23              184                1\n21     Salesperson europe    Jim      Brown     Salesperson europe              50              167                2\n22 Sr. Salesperson europe  Larry      Green Sr. Salesperson europe              20              113                4\n23         Manager africa  Laura      White         Manager africa              20              281                8\n24 Sr. Salesperson africa   Hugh      Black Sr. Salesperson africa              40              261                9\n\n# The only thing you can do in a select statement without a FROM clause\n# is to perform calculations without data from a table.\n\nsqldf(\"SELECT 3+2, 100*5\")\n\n  3+2 100*5\n1   5   500",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#add-new-columns-in-select-clause",
    "href": "0500400-sqlSelectIntro-v003.html#add-new-columns-in-select-clause",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.14 add new columns in SELECT clause",
    "text": "29.14 add new columns in SELECT clause\n\n# In dplyr, to add new columns you use the mutate function.\n# In the SQL SELECT command this is accomplished as part of the SELECT clause.\n# To add new columns that are calculated from other columns\n# simply add the calculations to the select clause.\n\nsqldf(\"SELECT first, last, salesInThousands, 0.1 * salesInThousands\n       FROM sales\n       \")\n\n    first       last salesInThousands 0.1 * salesInThousands\n1     Joe      Smith              100                   10.0\n2     Sam        Lee              200                   20.0\n3     Sue  Sallinger              150                   15.0\n4    Barb      Brown               79                    7.9\n5    Jack     Martin               40                    4.0\n6    Kate      Carey               69                    6.9\n7     Emy    Parsons               87                    8.7\n8  Monica        Oja               98                    9.8\n9     Jim Kuruzovich              140                   14.0\n10   Zeke      Cohen              250                   25.0\n11   Zeke      Smith              120                   12.0\n12    Amy   Markovic              340                   34.0\n13    Joe Washington              370                   37.0\n14    Sam    Lincoln              500                   50.0\n15    Sue      Aames              600                   60.0\n16   Barb      Aames              255                   25.5\n17   Jack      Aames              105                   10.5\n18   Kate   Zeitchik              187                   18.7\n19    Emy   Zeitchik              166                   16.6\n20 Monica   Zeitchik              184                   18.4\n21    Jim      Brown              167                   16.7\n22  Larry      Green              113                   11.3\n23  Laura      White              281                   28.1\n24   Hugh      Black              261                   26.1\n\n# You can give the new column a unique name by following the definition\n# of the column with \"AS columnName\".\nsqldf(\"SELECT first, last, salesInThousands, 0.1 * salesInThousands as commission\n       FROM sales\n       \")\n\n    first       last salesInThousands commission\n1     Joe      Smith              100       10.0\n2     Sam        Lee              200       20.0\n3     Sue  Sallinger              150       15.0\n4    Barb      Brown               79        7.9\n5    Jack     Martin               40        4.0\n6    Kate      Carey               69        6.9\n7     Emy    Parsons               87        8.7\n8  Monica        Oja               98        9.8\n9     Jim Kuruzovich              140       14.0\n10   Zeke      Cohen              250       25.0\n11   Zeke      Smith              120       12.0\n12    Amy   Markovic              340       34.0\n13    Joe Washington              370       37.0\n14    Sam    Lincoln              500       50.0\n15    Sue      Aames              600       60.0\n16   Barb      Aames              255       25.5\n17   Jack      Aames              105       10.5\n18   Kate   Zeitchik              187       18.7\n19    Emy   Zeitchik              166       16.6\n20 Monica   Zeitchik              184       18.4\n21    Jim      Brown              167       16.7\n22  Larry      Green              113       11.3\n23  Laura      White              281       28.1\n24   Hugh      Black              261       26.1\n\n# The word \"AS\" is actually optional. The following adds two new columns\n# but does not use the word AS. All you need is the definition of the column\n# followed by a space followed by the name of the new column.\nsqldf(\"SELECT first, last, baseInThousands,  \n              salesInThousands, 0.1 * salesInThousands commission, \n              salesInThousands * 0.1 + baseInThousands takeHome\n       FROM sales\n       WHERE region='africa'\n       \")\n\n   first    last baseInThousands salesInThousands commission takeHome\n1    Emy Parsons              25               87        8.7     33.7\n2 Monica     Oja              35               98        9.8     44.8\n3    Sue   Aames              35              600       60.0     95.0\n4  Laura   White              20              281       28.1     48.1\n5   Hugh   Black              40              261       26.1     66.1",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#yrAggregateFunctions",
    "href": "0500400-sqlSelectIntro-v003.html#yrAggregateFunctions",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.15 use aggregate functions in SELECT to create summary rows",
    "text": "29.15 use aggregate functions in SELECT to create summary rows\n\n# The dplyr summarize function is used to summarize (or aggregate) info from\n# several rows into a single row. \n# \n# In the SQL SELECT statement, this is accomplished by simply using\n# aggregate functions in the select clause.\n#\n# SQL has several built in standard aggregate functions\n#\n#   count(*) - similar to n() in dplyr - we'll discuss why the * is there later\n#   avg(SOME_COLUMN)\n#   max(SOME_COLUMN)\n#   min(SOME_COLUMN)\n\nsqldf(\"select count(*), avg(baseInThousands), max(baseInThousands)\n       FROM sales\n       ORDER BY region ASC, salesInThousands DESC\")\n\n  count(*) avg(baseInThousands) max(baseInThousands)\n1       24             32.29167                   50\n\n# As shown above we can assign names to the new columns.\n# Again, as we said above, the word \"AS\" is optional. The following statement\n# would work exactly the same way if we did not have the word \"AS\"\n\nsqldf(\"SELECT count(*) as numberOfEmployees, \n              avg(baseInThousands) AS averagebaseInThousands, \n              max(baseInThousands) AS maxbaseInThousands\n       FROM sales\n       ORDER BY region ASC, salesInThousands DESC\")\n\n  numberOfEmployees averagebaseInThousands maxbaseInThousands\n1                24               32.29167                 50\n\n# This is the same query as above.\n# This version does not have the word \"AS\".\n# The results are exactly the same.\n\nsqldf(\"SELECT count(*) numberOfEmployees, \n              avg(baseInThousands) averagebaseInThousands, \n              max(baseInThousands) maxbaseInThousands\n       FROM sales\n       ORDER BY region ASC, salesInThousands DESC\")\n\n  numberOfEmployees averagebaseInThousands maxbaseInThousands\n1                24               32.29167                 50",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#where-clause",
    "href": "0500400-sqlSelectIntro-v003.html#where-clause",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.16 WHERE clause",
    "text": "29.16 WHERE clause\n\n# WHERE is directly analogous to the dplyr filter function.\n#\n# The WHERE clause identifies the rows that will be returned.\n# It takes a logical expression that uses the names of the columns.\n# For every row the SELECT statement analyzes the row and calculates the\n# result of the logical expression for that row. If the logical expression\n# for a row is TRUE you get the row back. If not you do not get the row.\n\nsqldf(\"SELECT *\n       FROM sales\n       WHERE salesInThousands &lt; 100\")\n\n   first    last           title region baseInThousands salesInThousands yearsWithCompany\n1   Barb   Brown Sr. Salesperson   asia              35               79                4\n2   Jack  Martin         Manager europe              20               40                3\n3   Kate   Carey Sr. Salesperson europe              20               69                2\n4    Emy Parsons     Salesperson africa              25               87                3\n5 Monica     Oja     Salesperson africa              35               98               10\n\n# This query uses aggregate functions in the SELECT but does NOT have a WHERE.\n# Therefore the result is a summary of ALL rows in the table.\n\nsqldf(\"SELECT count(*), min(baseInThousands), max(baseInThousands), avg(baseInThousands)\n       FROM sales\")\n\n  count(*) min(baseInThousands) max(baseInThousands) avg(baseInThousands)\n1       24                   20                   50             32.29167\n\n# This is the same query but adds WHERE region='asia'.\n# As a result the summary row only reflects info about the rows for 'asia'.\n# Notice the there are fewer rows in the count(*) column and some of the\n# other summary statistics are also different.\n\nsqldf(\"SELECT count(*), min(baseInThousands), max(baseInThousands), avg(baseInThousands)\n       FROM sales\n       WHERE region='asia'\")\n\n  count(*) min(baseInThousands) max(baseInThousands) avg(baseInThousands)\n1        6                   20                   46             31.33333",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#order-by-clause",
    "href": "0500400-sqlSelectIntro-v003.html#order-by-clause",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.17 ORDER BY clause",
    "text": "29.17 ORDER BY clause\n\n# The ORDER BY clause is directly anaogous to dplyr's arrange function\n\nsqldf(\"SELECT *\n       FROM sales\n       WHERE salesInThousands &lt; 100\n       ORDER By salesInThousands\")\n\n   first    last           title region baseInThousands salesInThousands yearsWithCompany\n1   Jack  Martin         Manager europe              20               40                3\n2   Kate   Carey Sr. Salesperson europe              20               69                2\n3   Barb   Brown Sr. Salesperson   asia              35               79                4\n4    Emy Parsons     Salesperson africa              25               87                3\n5 Monica     Oja     Salesperson africa              35               98               10\n\n# Just as with dplyr's arrange function the rows can be ordered\n# from largest to smallest by specifying desc, i.e. a descending order\n# for the values of a column.\n\nsqldf(\"SELECT *\n       FROM sales\n       WHERE salesInThousands &lt; 100\n       ORDER BY salesInThousands DESC\")\n\n   first    last           title region baseInThousands salesInThousands yearsWithCompany\n1 Monica     Oja     Salesperson africa              35               98               10\n2    Emy Parsons     Salesperson africa              25               87                3\n3   Barb   Brown Sr. Salesperson   asia              35               79                4\n4   Kate   Carey Sr. Salesperson europe              20               69                2\n5   Jack  Martin         Manager europe              20               40                3\n\n# Just as with dplyr's arrange function you can specify that the order\n# of the rows should depend on multiple columns. \n# \n# The first column specified is used to order all of the rows.\n# Subsequent columns mentioned in ORDER By are used only for rows\n# in which the values for the earlier columns are the same.\n#\n# Each column could have an ascending (asc) or descending (desc) order.\n# If neither asc nor desc is specified, then the default is an ascending order.\n\nsqldf(\"SELECT *\n       FROM sales\n       ORDER BY region ASC, salesInThousands DESC\")\n\n    first       last           title region baseInThousands salesInThousands yearsWithCompany\n1     Sue      Aames Sr. Salesperson africa              35              600               10\n2   Laura      White         Manager africa              20              281                8\n3    Hugh      Black Sr. Salesperson africa              40              261                9\n4  Monica        Oja     Salesperson africa              35               98               10\n5     Emy    Parsons     Salesperson africa              25               87                3\n6     Amy   Markovic     Salesperson   asia              46              340                3\n7  Monica   Zeitchik     Salesperson   asia              23              184                1\n8     Emy   Zeitchik         Manager   asia              34              166                4\n9     Sue  Sallinger     Salesperson   asia              30              150               10\n10   Zeke      Smith     Salesperson   asia              20              120                3\n11   Barb      Brown Sr. Salesperson   asia              35               79                4\n12    Sam    Lincoln         Manager europe              30              500                2\n13    Joe Washington Sr. Salesperson europe              33              370                2\n14    Jim      Brown     Salesperson europe              50              167                2\n15  Larry      Green Sr. Salesperson europe              20              113                4\n16   Kate      Carey Sr. Salesperson europe              20               69                2\n17   Jack     Martin         Manager europe              20               40                3\n18   Barb      Aames     Salesperson    usa              21              255                7\n19   Zeke      Cohen         Manager    usa              30              250               10\n20    Sam        Lee     Salesperson    usa              40              200                6\n21   Kate   Zeitchik Sr. Salesperson    usa              50              187                4\n22    Jim Kuruzovich Sr. Salesperson    usa              35              140                3\n23   Jack      Aames     Salesperson    usa              43              105                4\n24    Joe      Smith         Manager    usa              40              100               10",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#yrGroupBy",
    "href": "0500400-sqlSelectIntro-v003.html#yrGroupBy",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.18 GROUP BY clause",
    "text": "29.18 GROUP BY clause\n\n# The GROUP BY clause in SQL is directly analogous to the group_by function in dplyr.\n#\n# All of the rows that have the same value for the specified GROUP BY columns\n# are aggregated (i.e. summarized) in a single line of output.\n#\n# GROUP BY should only be used if the SELECT clause includes aggregate functions. \n\nsqldf(\"SELECT title, count(*), avg(baseInThousands) avgBase, max(baseInThousands) maxBase\n       FROM sales\n       GROUP BY title\n       ORDER BY title\")\n\n            title count(*) avgBase maxBase\n1         Manager        6    29.0      40\n2     Salesperson       10    33.3      50\n3 Sr. Salesperson        8    33.5      50\n\n# Grouping by a different column - region\nsqldf(\"SELECT region, count(*), avg(baseInThousands) avgBase, max(baseInThousands) maxBase\n       FROM sales\n       GROUP BY region\n       ORDER BY region\")\n\n  region count(*)  avgBase maxBase\n1 africa        5 31.00000      40\n2   asia        6 31.33333      46\n3 europe        6 28.83333      50\n4    usa        7 37.00000      50\n\n# Just as with dplyr, the groups can be defined by more than one column.\n#\n# The following query treats all of the rows that match in both the\n# region and title columns as a single group.\n#\n# For example, all of the original rows from the sales table\n# that have a title of \"Salesperson\" and a region of \"asia\" \n# are considered to be part of the same group and are summarized \n# in a single row of the output.\n\nsqldf(\"SELECT title, region, count(*), avg(baseInThousands) avgBase, max(baseInThousands) maxBase\n       FROM sales\n       GROUP BY title, region\n       ORDER BY title, region\")\n\n             title region count(*)  avgBase maxBase\n1          Manager africa        1 20.00000      20\n2          Manager   asia        1 34.00000      34\n3          Manager europe        2 25.00000      30\n4          Manager    usa        2 35.00000      40\n5      Salesperson africa        2 30.00000      35\n6      Salesperson   asia        4 29.75000      46\n7      Salesperson europe        1 50.00000      50\n8      Salesperson    usa        3 34.66667      43\n9  Sr. Salesperson africa        2 37.50000      40\n10 Sr. Salesperson   asia        1 35.00000      35\n11 Sr. Salesperson europe        3 24.33333      33\n12 Sr. Salesperson    usa        2 42.50000      50\n\n# A similar query without a GROUP BY returns just a single row that\n# summarizes the data from all rows of the table.\n\nsqldf(\"SELECT count(*), avg(baseInThousands) avgBase, max(baseInThousands) maxBase\n       FROM sales\")\n\n  count(*)  avgBase maxBase\n1       24 32.29167      50",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#limit-clause",
    "href": "0500400-sqlSelectIntro-v003.html#limit-clause",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.19 LIMIT clause",
    "text": "29.19 LIMIT clause\nThe LIMIT clause is similar to print(n=…) and slice_head(n=…) with dplyr. It limits the number of rows returned to the first few that would have been returned had the query not included the LIMIT clause.\nLIMIT is often used in combination with the ORDER BY clause.\nLIMIT must come at the very end of the SQL statement.\n\nsqldf(\"SELECT *\n       FROM sales\n       ORDER BY salesInThousands desc\n       LIMIT 10\")\n\n   first       last           title region baseInThousands salesInThousands yearsWithCompany\n1    Sue      Aames Sr. Salesperson africa              35              600               10\n2    Sam    Lincoln         Manager europe              30              500                2\n3    Joe Washington Sr. Salesperson europe              33              370                2\n4    Amy   Markovic     Salesperson   asia              46              340                3\n5  Laura      White         Manager africa              20              281                8\n6   Hugh      Black Sr. Salesperson africa              40              261                9\n7   Barb      Aames     Salesperson    usa              21              255                7\n8   Zeke      Cohen         Manager    usa              30              250               10\n9    Sam        Lee     Salesperson    usa              40              200                6\n10  Kate   Zeitchik Sr. Salesperson    usa              50              187                4\n\n\n\n29.19.1 LIMIT      vs     LIMIT  OFFSET \nIn general the limit clause has two different forms:\n      &lt;SOME SELECT STATEMENT&gt;\n      limit M\n(where M is an integer) results in only the first M rows of data from what would normally have been displayed had the LIMIT clause not been specified.\n      &lt;SOME SELECT STATEMENT&gt;\n      limit M offset N\n(where both M and N are integers)\nstarts the output from the N+1’th row of what would normally have been displayed without the limit clause and then displays the next M rows of data.\nFor example:\n\n# Display the first 3 people\nsqldf(\"SELECT *\n       FROM sales\n       ORDER BY last, first\n       LIMIT 3\")\n\n  first  last           title region baseInThousands salesInThousands yearsWithCompany\n1  Barb Aames     Salesperson    usa              21              255                7\n2  Jack Aames     Salesperson    usa              43              105                4\n3   Sue Aames Sr. Salesperson africa              35              600               10\n\n# Display the next 3 people\nsqldf(\"SELECT *\n       FROM sales\n       ORDER BY last, first\n       LIMIT 3 OFFSET 3\")\n\n  first  last           title region baseInThousands salesInThousands yearsWithCompany\n1  Hugh Black Sr. Salesperson africa              40              261                9\n2  Barb Brown Sr. Salesperson   asia              35               79                4\n3   Jim Brown     Salesperson europe              50              167                2\n\n# Display the next 3 people\nsqldf(\"SELECT *\n       FROM sales\n       ORDER BY last, first\n       LIMIT 3 OFFSET 6\")\n\n  first  last           title region baseInThousands salesInThousands yearsWithCompany\n1  Kate Carey Sr. Salesperson europe              20               69                2\n2  Zeke Cohen         Manager    usa              30              250               10\n3 Larry Green Sr. Salesperson europe              20              113                4\n\n\nNote that the LIMIT clause is not standard SQL - some SQL flavors do not contain a LIMIT clause. However, the LIMIT clause or something similar is part of most SQL flavors.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "0500400-sqlSelectIntro-v003.html#do-not-select-both-aggregate-and-non-aggregate-data",
    "href": "0500400-sqlSelectIntro-v003.html#do-not-select-both-aggregate-and-non-aggregate-data",
    "title": "29  29.  Intro to SQL Select (assumes some knowledge of dplyr)",
    "section": "29.20 DO NOT select both aggregate and non-aggregate data",
    "text": "29.20 DO NOT select both aggregate and non-aggregate data\nSUMMARY OF THIS SECTION: If a SELECT clause includes aggregate functions then the SELECT clause should NOT include non-aggregated columns, unless those columns are part of the GROUP BY clause. (The rest of this section explains this rule.) IMPORTANT - if you don’t follow this rule, then you will NOT get an error but your output will be misleading.\nBefore reading the rest of this section I highly recommend that you review the sections above on aggreate functions and the GROUP BY clause\n\n29.20.1 Example of INCORRECT select that includes both aggregate and non-aggregated columns\n\n\nExamine the query shown below. The SELECT incorrectly uses the aggregate functions avg and max mixed together with non-aggregate data, i.e. region (Note that the region column is NOT part of the GROUP BY). The results are misleading.\nEven though the underlying data in the sales table contains Managers for all of the regions (africa, asia, europe, usa), the results below seem to imply that the data in the “Manager” row is only for Managers for the “usa” region. However, that is NOT the case. The maxBase and avgYears columns show the maxBase and avgYears for ALL managers (not just for those from the usa region). The fact that the output shows “usa” and not “africa”, “asia” or “europe” is by chance. There is no guarantee that one of the other regions would be shown in similar queries.\n\n\n# NOTE: the -- in the command below introduces a comment in SQLite\n\nsqldf(\"select title,\n              region, -- DO NOT DO THIS - region is NOT in the GROUP BY\n              avg(baseInThousands) as avgBase, \n              max(baseInThousands) as maxBase, \n              avg(yearsWithCompany) as avgYears\n       from sales\n       group by title\n       order by title\")\n\n            title region avgBase maxBase avgYears\n1         Manager    usa    29.0      40 6.166667\n2     Salesperson europe    33.3      50 4.900000\n3 Sr. Salesperson    usa    33.5      50 4.750000\n\n\n\n\n\n\nThe following has the same problem but for a query that doesn’t contain a GROUP BY. The results shown below seem to imply that the avgBase, maxBase and avgYears columns only take into account rows from the “usa” region. However, that is NOT the case. the avgBase, maxBase and avgYears take into account rows from ALL regions.\n\n\n# NOTE: the -- in the command below introduces a comment in SQLite\n\nsqldf(\"select \n              region, -- DO NOT DO THIS - region is NOT in the GROUP BY\n              avg(baseInThousands) as avgBase, \n              max(baseInThousands) as maxBase, \n              avg(yearsWithCompany) as avgYears\n       from sales\n       \")\n\n  region  avgBase maxBase avgYears\n1    usa 32.29167      50 5.166667\n\n\n\n\n\n29.20.2 Explanation\nIf a SELECT clause contains an aggregate function then one of the following two things happen:\n\nIf there is no GROUP BY, then only one row is returned.\n\n\n\n\n\n\nclick for example\n\n\n\n\n\nThe following SELECT clause includes aggregate functions and there is NO group by. Therefore only one row is returned.\n\nsqldf(\"select avg(baseInThousands) as avgBase, \n              max(baseInThousands) as maxBase, \n              avg(yearsWithCompany) as avgYears\n       from sales\")\n\n   avgBase maxBase avgYears\n1 32.29167      50 5.166667\n\n\n\n\n\nIf there IS a GROUP BY then one row is returned per group of rows in the table. (see the GROUP BY clause above)\n\n\n\n\n\n\nclick for example\n\n\n\n\n\nThe following statement is very similar to the statement above. However, this statement includes a GROUP BY. Therefore a single row is returned PER GROUP. Note that the SELECT clause also includes the title column, which is also in the GROUP BY clause.\n\nsqldf(\"select title,\n              avg(baseInThousands) as avgBase, \n              max(baseInThousands) as maxBase, \n              avg(yearsWithCompany) as avgYears\n       from sales\n       group by title\")\n\n            title avgBase maxBase avgYears\n1         Manager    29.0      40 6.166667\n2     Salesperson    33.3      50 4.900000\n3 Sr. Salesperson    33.5      50 4.750000\n\n\n\n\n\n\nThis makes sense since an aggregate function (e.g. avg, sum, min, max, count) takes multiple values from different rows in the table and returns only a single value. A select statement that doesn’t have a GROUP BY passes the values from all rows in the table to the aggregate function. By contraste, a select with a GROUP BY calls the aggregate functions once for each group of rows.\nA SELECT clause that includes aggregated columns as well as\nnon-aggregated columns is problematic.0 The aggregate function(s) return a single value so only a single row should be diplayed. However, the non-aggregated values could have multiple values and hence multiple rows. This causes a conflict (a) should only one row be returned (because of the aggregate functions) or (b) should more than one row be returned (because of the non-aggregated data)? The rules of SQL say that ONLY ONE ROW is returned. However, ONE OF THE SEVERAL non-aggregated values will also be returned. This could lead to misleading output.\n\n\n29.20.3 ANSI SQL standard vs SQLite vs other databases\nAccording to the ANSI SQL standard, queries such as those listed above that include both aggregate and non-aggregate columns should cause an ERROR. However, in actuality, different RDBMS software treat this issue differently. By default, when using sqldf() within R, the SQLite version of the SQL is used. SQLite does NOT return an error when issuing such queries. However, as shown above, the results can be very misleading. SQLite’s approach is that the person writing the query is expected to be responsible enough to not break this rule. Other RDBMS software may issue ERRORs or warnings or take a similar approach to SQLite (see the documentation for the specific RDBMS software).\n\nWhy doesn’t SQLite check for this error?\nThere are situations where this rule can be too limiting. For example, the following grades table lists different grades that students got in a class. Each row lists the student’s ID and the student’s social security number (ssn), both of which could theoretically be a primary key for the table (i.e. they are “candiate primary keys”).\n\n\nsqldf(\"select * from grades\")\n\n  studentId         ssn assessment grade\n1       001 111-11-1111      test1    70\n2       001 111-11-1111      test2    80\n3       002 222-22-2222      test1    90\n4       002 222-22-2222      test2   100\n\n\n\nThe following query breaks the rule described above (ssn is in the SELECT but not in the GROUP BY). Nevertheless, the query gives accurate results. This is because the studentId and the ssn are guaranteed to each contain the same unique values for a specific student.\n\n\nsqldf(\"select studentId, ssn, avg(grade)\n       from grades\n       group by studentId\n       order by studentId\")\n\n  studentId         ssn avg(grade)\n1       001 111-11-1111         75\n2       002 222-22-2222         95\n\n\n\nSimilarly, this also breaks the rule (i.e. studentId is in the SELECT but not in the GROUP BY) but the results are accurate:\n\n\nsqldf(\"select studentId, ssn, avg(grade)\n       from grades\n       group by ssn\n       order by studentId\")\n\n  studentId         ssn avg(grade)\n1       001 111-11-1111         75\n2       002 222-22-2222         95\n\n\n\nBottom line is that SQLite requires the person who is writing the SQL to do it correctly!",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'><yrChapterNumber>29. </yrChapterNumber> Intro to SQL Select ***(assumes some knowledge of dplyr)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html",
    "href": "sql0010-introToSql-v023.html",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "",
    "text": "30.1 IMPORTANT NOTE\nThis section contains a general intro to the SQL SELECT statement. If you already know the basics of the R dplyr package, you can get a quicker intro to SQL SELECT from the section that assumes some dplyr knowledge that appears above.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#intro-to-sql",
    "href": "sql0010-introToSql-v023.html#intro-to-sql",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "30.2 Intro to SQL",
    "text": "30.2 Intro to SQL\nWe will spend the next few sections focusing on “Structured Query Language (SQL)”, the language that is used for accessing data in a relational database. Remember that a relational database stores its information as a collection of several “tables”. Each table arranges its data in rows and columns.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#web-resources-for-learning-sql",
    "href": "sql0010-introToSql-v023.html#web-resources-for-learning-sql",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "30.3 Web resources for learning SQL",
    "text": "30.3 Web resources for learning SQL\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The following websites contains a basic overview of many SQL \n# commands and concepts. We will be going through the concepts and many \n# of the interactive exercises from these websites\n#\n# https://www.w3schools.com/sql/\n#\n# https://sqlbolt.com/\n#\n# https://www.sqlitetutorial.net/\n#\n# https://mode.com/sql-tutorial/\n#\n# https://sqlzoo.com/\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# See this page \n#\n#   https://sqlbolt.com/lesson/introduction\n# \n# For an overview of :\n#\n# - What is Structured Query Language (SQL) - pronounced \"sequel\"\n# \n# - What is a \"Relational Database\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#what-software-do-i-need-to-use-sql",
    "href": "sql0010-introToSql-v023.html#what-software-do-i-need-to-use-sql",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "30.4 What software do I need to use SQL?",
    "text": "30.4 What software do I need to use SQL?\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The sqldf package in R.  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# To use SQL you must have access to software that interacts with \n# a relational database. As mentioned above many such software packages\n# exist, e.g.\n#\n#      - Microsoft SQL Server\n#      - Oracle Relational Database Management System\n#      - MySql\n#      - Postgres\n#      - MariaDb\n#      - many many others\n#\n# In addition many custom software packages exist that use\n# SQL to manipulate relational databases. \n#\n# Many of these software packages tend to be overly complex to install. \n# However, the underlying SQL language that is used by all of these \n# different software packages tends to be very, very similar. \n# \n# For this class we will be using R to learn the basic SQL commands.\n# The sqldf package in R includes the sqldf function that takes a single\n# character value. The value is expected to be a valid SQL command. \n# Instead of referring to database \"tables\" the sqldf function interprets\n# database table names found in the commands as referring to R dataframes.\n# This makes sense since R dataframes are very similar in structure to \n# relational database tables.\n#\n# The version of SQL that is used by default with the sqldf package\n# is the \"sqlite\" version. You can search online for more information\n# about sqlite if you want to understand more about the specifics \n# of this \"flavor\" of SQL. The following website is a good place to\n# start: https://www.sqlitetutorial.net/\n#\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n30.4.1 The sqldf R package",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#data-for-this-part-of-the-tutorial",
    "href": "sql0010-introToSql-v023.html#data-for-this-part-of-the-tutorial",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.1 data for this part of the tutorial",
    "text": "31.1 data for this part of the tutorial\nYou can download the data for this tutorial from the following files.\n\ngrades.csv This is a .csv file. You can read the contents of the file into R with the read.csv function as shown below.\nsqlbolt_tables-v007.RData This is a .RData file. You can read the contents of the file into R with the load function as shown below.\n\nYou can load these tables into your R session with the following commands.\n\ngrades = read.csv(\"grades.csv\", na.strings=c(\"\", \"NULL\", \"NA\"))\nload(\"sqlbolt_tables-v007.RData\")\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here to see the grades table\n\n\n\n\n\n\nsqldf(\"select * from grades\")\n\n      id  student year honors test1 test2 test3 major\n1  s0100      joe   fr  FALSE    70    82    NA   IDS\n2  s0200      sue   fr  FALSE    80    80    79   IDS\n3  s0300     anne   fr   TRUE    90    95    92   IDS\n4  s0400    frank   so   TRUE   100    95    91   ACC\n5  s0500      bob   so  FALSE    30    NA    NA   IDS\n6  s0600 samantha   so   TRUE   100    70    97   IDS\n7  s0700    larry   ju  FALSE    32    80    NA   FIN\n8  s0800     bill   se  FALSE    84    90    92   ACC\n9  s0900 lucricia   fr   TRUE    80   100    88   IDS\n10 s1000 margaret   se  FALSE    59    47    NA   IDS\n11 s1100    dalia   se  FALSE    85    92    93   FIN\n12 s1200      zvi   ju   TRUE    90    98    92   ACC\n13 s1300     mike   ju   TRUE    90    86    79   IDS\n14 s1400    david   se   TRUE    90    87    87   IDS\n15 s1500     dana   so  FALSE   100    93    91   FIN\n\n\n\n\n\n\n\n\n\n\n\nClick here to see the movies table\n\n\n\n\n\n\nsqldf(\"select * from movies\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n\n\n\n\n\n\n\n\n\n\nClick here to see the orders table\n\n\n\n\n\n\nsqldf(\"select * from orders\")\n\n    OrderID CustomerID EmployeeID  OrderDate ShipperID\n1     10248         90          5   7/4/1996         3\n2     10249         81          6   7/5/1996         1\n3     10250         34          4   7/8/1996         2\n4     10251         84          3   7/8/1996         1\n5     10252         76          4   7/9/1996         2\n6     10253         34          3  7/10/1996         2\n7     10254         14          5  7/11/1996         2\n8     10255         68          9  7/12/1996         3\n9     10256         88          3  7/15/1996         2\n10    10257         35          4  7/16/1996         3\n11    10258         20          1  7/17/1996         1\n12    10259         13          4  7/18/1996         3\n13    10260         55          4  7/19/1996         1\n14    10261         61          4  7/19/1996         2\n15    10262         65          8  7/22/1996         3\n16    10263         20          9  7/23/1996         3\n17    10264         24          6  7/24/1996         3\n18    10265          7          2  7/25/1996         1\n19    10266         87          3  7/26/1996         3\n20    10267         25          4  7/29/1996         1\n21    10268         33          8  7/30/1996         3\n22    10269         89          5  7/31/1996         1\n23    10270         87          1   8/1/1996         1\n24    10271         75          6   8/1/1996         2\n25    10272         65          6   8/2/1996         2\n26    10273         63          3   8/5/1996         3\n27    10274         85          6   8/6/1996         1\n28    10275         49          1   8/7/1996         1\n29    10276         80          8   8/8/1996         3\n30    10277         52          2   8/9/1996         3\n31    10278          5          8  8/12/1996         2\n32    10279         44          8  8/13/1996         2\n33    10280          5          2  8/14/1996         1\n34    10281         69          4  8/14/1996         1\n35    10282         69          4  8/15/1996         1\n36    10283         46          3  8/16/1996         3\n37    10284         44          4  8/19/1996         1\n38    10285         63          1  8/20/1996         2\n39    10286         63          8  8/21/1996         3\n40    10287         67          8  8/22/1996         3\n41    10288         66          4  8/23/1996         1\n42    10289         11          7  8/26/1996         3\n43    10290         15          8  8/27/1996         1\n44    10291         61          6  8/27/1996         2\n45    10292         81          1  8/28/1996         2\n46    10293         80          1  8/29/1996         3\n47    10294         65          4  8/30/1996         2\n48    10295         85          2   9/2/1996         2\n49    10296         46          6   9/3/1996         1\n50    10297          7          5   9/4/1996         2\n51    10298         37          6   9/5/1996         2\n52    10299         67          4   9/6/1996         2\n53    10300         49          2   9/9/1996         2\n54    10301         86          8   9/9/1996         2\n55    10302         76          4  9/10/1996         2\n56    10303         30          7  9/11/1996         2\n57    10304         80          1  9/12/1996         2\n58    10305         55          8  9/13/1996         3\n59    10306         69          1  9/16/1996         3\n60    10307         48          2  9/17/1996         2\n61    10308          2          7  9/18/1996         3\n62    10309         37          3  9/19/1996         1\n63    10310         77          8  9/20/1996         2\n64    10311         18          1  9/20/1996         3\n65    10312         86          2  9/23/1996         2\n66    10313         63          2  9/24/1996         2\n67    10314         65          1  9/25/1996         2\n68    10315         38          4  9/26/1996         2\n69    10316         65          1  9/27/1996         3\n70    10317         48          6  9/30/1996         1\n71    10318         38          8  10/1/1996         2\n72    10319         80          7  10/2/1996         3\n73    10320         87          5  10/3/1996         3\n74    10321         38          3  10/3/1996         2\n75    10322         58          7  10/4/1996         3\n76    10323         39          4  10/7/1996         1\n77    10324         71          9  10/8/1996         1\n78    10325         39          1  10/9/1996         3\n79    10326          8          4 10/10/1996         2\n80    10327         24          2 10/11/1996         1\n81    10328         28          4 10/14/1996         3\n82    10329         75          4 10/15/1996         2\n83    10330         46          3 10/16/1996         1\n84    10331          9          9 10/16/1996         1\n85    10332         51          3 10/17/1996         2\n86    10333         87          5 10/18/1996         3\n87    10334         84          8 10/21/1996         2\n88    10335         37          7 10/22/1996         2\n89    10336         60          7 10/23/1996         2\n90    10337         25          4 10/24/1996         3\n91    10338         55          4 10/25/1996         3\n92    10339         51          2 10/28/1996         2\n93    10340          9          1 10/29/1996         3\n94    10341         73          7 10/29/1996         3\n95    10342         25          4 10/30/1996         2\n96    10343         44          4 10/31/1996         1\n97    10344         89          4  11/1/1996         2\n98    10345         63          2  11/4/1996         2\n99    10346         65          3  11/5/1996         3\n100   10347         21          4  11/6/1996         3\n101   10348         86          4  11/7/1996         2\n102   10349         75          7  11/8/1996         1\n103   10350         41          6 11/11/1996         2\n104   10351         20          1 11/11/1996         1\n105   10352         28          3 11/12/1996         3\n106   10353         59          7 11/13/1996         3\n107   10354         58          8 11/14/1996         3\n108   10355          4          6 11/15/1996         1\n109   10356         86          6 11/18/1996         2\n110   10357         46          1 11/19/1996         3\n111   10358         41          5 11/20/1996         1\n112   10359         72          5 11/21/1996         3\n113   10360          7          4 11/22/1996         3\n114   10361         63          1 11/22/1996         2\n115   10362          9          3 11/25/1996         1\n116   10363         17          4 11/26/1996         3\n117   10364         19          1 11/26/1996         1\n118   10365          3          3 11/27/1996         2\n119   10366         29          8 11/28/1996         2\n120   10367         83          7 11/28/1996         3\n121   10368         20          2 11/29/1996         2\n122   10369         75          8  12/2/1996         2\n123   10370         14          6  12/3/1996         2\n124   10371         41          1  12/3/1996         1\n125   10372         62          5  12/4/1996         2\n126   10373         37          4  12/5/1996         3\n127   10374         91          1  12/5/1996         3\n128   10375         36          3  12/6/1996         2\n129   10376         51          1  12/9/1996         2\n130   10377         72          1  12/9/1996         3\n131   10378         24          5 12/10/1996         3\n132   10379         61          2 12/11/1996         1\n133   10380         37          8 12/12/1996         3\n134   10381         46          3 12/12/1996         3\n135   10382         20          4 12/13/1996         1\n136   10383          4          8 12/16/1996         3\n137   10384          5          3 12/16/1996         3\n138   10385         75          1 12/17/1996         2\n139   10386         21          9 12/18/1996         3\n140   10387         70          1 12/18/1996         2\n141   10388         72          2 12/19/1996         1\n142   10389         10          4 12/20/1996         2\n143   10390         20          6 12/23/1996         1\n144   10391         17          3 12/23/1996         3\n145   10392         59          2 12/24/1996         3\n146   10393         71          1 12/25/1996         3\n147   10394         36          1 12/25/1996         3\n148   10395         35          6 12/26/1996         1\n149   10396         25          1 12/27/1996         3\n150   10397         60          5 12/27/1996         1\n151   10398         71          2 12/30/1996         3\n152   10399         83          8 12/31/1996         3\n153   10400         19          1   1/1/1997         3\n154   10401         65          1   1/1/1997         1\n155   10402         20          8   1/2/1997         2\n156   10403         20          4   1/3/1997         3\n157   10404         49          2   1/3/1997         1\n158   10405         47          1   1/6/1997         1\n159   10406         62          7   1/7/1997         1\n160   10407         56          2   1/7/1997         2\n161   10408         23          8   1/8/1997         1\n162   10409         54          3   1/9/1997         1\n163   10410         10          3  1/10/1997         3\n164   10411         10          9  1/10/1997         3\n165   10412         87          8  1/13/1997         2\n166   10413         41          3  1/14/1997         2\n167   10414         21          2  1/14/1997         3\n168   10415         36          3  1/15/1997         1\n169   10416         87          8  1/16/1997         3\n170   10417         73          4  1/16/1997         3\n171   10418         63          4  1/17/1997         1\n172   10419         68          4  1/20/1997         2\n173   10420         88          3  1/21/1997         1\n174   10421         61          8  1/21/1997         1\n175   10422         27          2  1/22/1997         1\n176   10423         31          6  1/23/1997         3\n177   10424         51          7  1/23/1997         2\n178   10425         41          6  1/24/1997         2\n179   10426         29          4  1/27/1997         1\n180   10427         59          4  1/27/1997         2\n181   10428         66          7  1/28/1997         1\n182   10429         37          3  1/29/1997         2\n183   10430         20          4  1/30/1997         1\n184   10431         10          4  1/30/1997         2\n185   10432         75          3  1/31/1997         2\n186   10433         60          3   2/3/1997         3\n187   10434         24          3   2/3/1997         2\n188   10435         16          8   2/4/1997         2\n189   10436          7          3   2/5/1997         2\n190   10437         87          8   2/5/1997         1\n191   10438         79          3   2/6/1997         2\n192   10439         51          6   2/7/1997         3\n193   10440         71          4  2/10/1997         2\n194   10441         55          3  2/10/1997         2\n195   10442         20          3  2/11/1997         2\n196   10443         66          8  2/12/1997         1\n\n\n\n\n\n\n\n\n\n\n\nClick here to see the customers table\n\n\n\n\n\n\nsqldf(\"select * from customers\")\n\n   CustomerID                         CustomerName              ContactName                                           Address             City PostalCode     Country\n1           1                  Alfreds Futterkiste             Maria Anders                                     Obere Str. 57           Berlin      12209     Germany\n2           2   Ana Trujillo Emparedados y helados             Ana Trujillo                  Avda. de la Constituci\\xf3n 2222   M\\xe9xico D.F.       5021      Mexico\n3           3           Antonio Moreno Taquer\\xeda           Antonio Moreno                                    Mataderos 2312   M\\xe9xico D.F.       5023      Mexico\n4           4                      Around the Horn             Thomas Hardy                                   120 Hanover Sq.           London    WA1 1DP          UK\n5           5                Berglunds snabbk\\xf6p       Christina Berglund                                 Berguvsv\\xe4gen 8         Lule\\xe5   S-958 22      Sweden\n6           6              Blauer See Delikatessen               Hanna Moos                                    Forsterstr. 57         Mannheim      68306     Germany\n7           7              Blondel p\\xe8re et fils Fr\\xe9d\\xe9rique Citeaux                               24, place Kl\\xe9ber       Strasbourg      67000      France\n8           8         B\\xf3lido Comidas preparadas         Mart\\xedn Sommer                                    C/ Araquil, 67           Madrid      28023       Spain\n9           9                             Bon app'        Laurence Lebihans                              12, rue des Bouchers        Marseille      13008      France\n10         10               Bottom-Dollar Marketse        Elizabeth Lincoln                                23 Tsawassen Blvd.        Tsawassen    T2F 8M4      Canada\n11         11                        B's Beverages        Victoria Ashworth                                 Fauntleroy Circus           London    EC2 5NT          UK\n12         12           Cactus Comidas para llevar         Patricio Simpson                                       Cerrito 333     Buenos Aires       1010   Argentina\n13         13           Centro comercial Moctezuma          Francisco Chang                           Sierras de Granada 9993   M\\xe9xico D.F.       5022      Mexico\n14         14                    Chop-suey Chinese                Yang Wang                                      Hauptstr. 29             Bern       3012 Switzerland\n15         15                  Com\\xe9rcio Mineiro             Pedro Afonso                           Av. dos Lus\\xedadas, 23     S\\xe3o Paulo  05432-043      Brazil\n16         16                Consolidated Holdings          Elizabeth Brown                       Berkeley Gardens 12 Brewery           London    WX1 6LT          UK\n17         17            Drachenblut Delikatessend             Sven Ottlieb                                      Walserweg 21           Aachen      52066     Germany\n18         18                      Du monde entier           Janine Labrune                      67, rue des Cinquante Otages           Nantes      44000      France\n19         19                   Eastern Connection                Ann Devon                                    35 King George           London    WX3 6FW          UK\n20         20                         Ernst Handel            Roland Mendel                                      Kirchgasse 6             Graz       8010     Austria\n21         21                   Familia Arquibaldo                Aria Cruz                                   Rua Or\\xf3s, 92     S\\xe3o Paulo  05442-030      Brazil\n22         22 FISSA Fabrica Inter. Salchichas S.A.               Diego Roel                                C/ Moralzarzal, 86           Madrid      28034       Spain\n23         23                    Folies gourmandes         Martine Ranc\\xe9                       184, chauss\\xe9e de Tournai            Lille      59000      France\n24         24                    Folk och f\\xe4 HB            Maria Larsson                                   \\xc5kergatan 24        Br\\xe4cke   S-844 67      Sweden\n25         25                       Frankenversand            Peter Franken                                 Berliner Platz 43       M\\xfcnchen      80805     Germany\n26         26                  France restauration           Carine Schmitt                                    54, rue Royale           Nantes      44000      France\n27         27                       Franchi S.p.A.            Paolo Accorti                               Via Monte Bianco 34           Torino      10100       Italy\n28         28       Furia Bacalhau e Frutos do Mar           Lino Rodriguez                            Jardim das rosas n. 32           Lisboa       1675    Portugal\n29         29         Galer\\xeda del gastr\\xf3nomo         Eduardo Saavedra                         Rambla de Catalu\\xf1a, 23        Barcelona       8022       Spain\n30         30               Godos Cocina T\\xedpica     Jos\\xe9 Pedro Freyre                                     C/ Romero, 33          Sevilla      41101       Spain\n31         31                  Gourmet Lanchonetes         Andr\\xe9 Fonseca                                   Av. Brasil, 442         Campinas  04876-786      Brazil\n32         32              Great Lakes Food Market            Howard Snyder                                  2732 Baker Blvd.           Eugene      97403         USA\n33         33                 GROSELLA-Restaurante           Manuel Pereira                      5\\xaa Ave. Los Palos Grandes          Caracas       1081   Venezuela\n34         34                        Hanari Carnes             Mario Pontes                                Rua do Pa\\xe7o, 67   Rio de Janeiro  05454-876      Brazil\n35         35                  HILARI\\xd3N-Abastos      Carlos Hern\\xe1ndez        Carrera 22 con Ave. Carlos Soublette #8-35 San Crist\\xf3bal       5022   Venezuela\n36         36           Hungry Coyote Import Store            Yoshi Latimer                    City Center Plaza 516 Main St.            Elgin      97827         USA\n37         37         Hungry Owl All-Night Grocers         Patricia McKenna                                  8 Johnstown Road             Cork                Ireland\n38         38                       Island Trading            Helen Bennett                         Garden House Crowther Way            Cowes   PO31 7PJ          UK\n39         39                   K\\xf6niglich Essen            Philip Cramer                                     Maubelstr. 90      Brandenburg      14776     Germany\n40         40                 La corne d'abondance            Daniel Tonini                            67, avenue de l'Europe       Versailles      78000      France\n41         41                     La maison d'Asie           Annette Roulet                             1 rue Alsace-Lorraine         Toulouse      31000      France\n42         42        Laughing Bacchus Wine Cellars          Yoshi Tannamuri                                      1900 Oak St.        Vancouver    V3F 2K1      Canada\n43         43                 Lazy K Kountry Store               John Steel                              12 Orchestra Terrace      Walla Walla      99362         USA\n44         44                  Lehmanns Marktstand           Renate Messner                                      Magazinweg 7   Frankfurt a.M.      60528     Germany\n45         45                    Let's Stop N Shop             Jaime Yorres                               87 Polk St. Suite 5    San Francisco      94117         USA\n46         46                    LILA-Supermercado       Carlos Gonz\\xe1lez Carrera 52 con Ave. Bol\\xedvar #65-98 Llano Largo     Barquisimeto       3508   Venezuela\n47         47                     LINO-Delicateses         Felipe Izquierdo                           Ave. 5 de Mayo Porlamar  I. de Margarita       4980   Venezuela\n48         48             Lonesome Pine Restaurant              Fran Wilson                                89 Chiaroscuro Rd.         Portland      97219         USA\n49         49         Magazzini Alimentari Riuniti         Giovanni Rovelli                           Via Ludovico il Moro 22          Bergamo      24100       Italy\n50         50                         Maison Dewey          Catherine Dewey                               Rue Joseph-Bens 532        Bruxelles     B-1180     Belgium\n51         51                    M\\xe8re Paillarde        Jean Fresni\\xe8re                                43 rue St. Laurent      Montr\\xe9al    H1J 1C3      Canada\n52         52               Morgenstern Gesundkost          Alexander Feuer                                       Heerstr. 22          Leipzig       4179     Germany\n53         53                          North/South           Simon Crowther                      South House 300 Queensbridge           London    SW7 1RZ          UK\n54         54         Oc\\xe9ano Atl\\xe1ntico Ltda.           Yvonne Moncada               Ing. Gustavo Moncada 8585 Piso 20-A     Buenos Aires       1010   Argentina\n55         55               Old World Delicatessen            Rene Phillips                                   2743 Bering St.        Anchorage      99508         USA\n56         56                Ottilies K\\xe4seladen      Henriette Pfalzheim                                Mehrheimerstr. 369          K\\xf6ln      50739     Germany\n57         57              Paris sp\\xe9cialit\\xe9s           Marie Bertrand                           265, boulevard Charonne            Paris      75012      France\n58         58         Pericles Comidas cl\\xe1sicas   Guillermo Fern\\xe1ndez                          Calle Dr. Jorge Cash 321   M\\xe9xico D.F.       5033      Mexico\n59         59                     Piccolo und mehr              Georg Pipps                                       Geislweg 14         Salzburg       5020     Austria\n60         60              Princesa Isabel Vinhoss         Isabel de Castro                         Estrada da sa\\xfade n. 58           Lisboa       1756    Portugal\n61         61                       Que Del\\xedcia         Bernardo Batista                           Rua da Panificadora, 12   Rio de Janeiro  02389-673      Brazil\n62         62                        Queen Cozinha        L\\xfacia Carvalho                      Alameda dos Can\\xe0rios, 891     S\\xe3o Paulo  05487-020      Brazil\n63         63                           QUICK-Stop              Horst Kloss                               Taucherstra\\xdfe 10        Cunewalde       1307     Germany\n64         64                        Rancho grande      Sergio Guti\\xe9rrez                            Av. del Libertador 900     Buenos Aires       1010   Argentina\n65         65           Rattlesnake Canyon Grocery             Paula Wilson                                   2817 Milton Dr.      Albuquerque      87110         USA\n66         66                   Reggiani Caseifici          Maurizio Moroni                            Strada Provinciale 124    Reggio Emilia      42100       Italy\n67         67                   Ricardo Adocicados           Janete Limeira                               Av. Copacabana, 267   Rio de Janeiro  02389-890      Brazil\n68         68                   Richter Supermarkt             Michael Holz                                 Grenzacherweg 237        Gen\\xe8ve       1203 Switzerland\n69         69                     Romero y tomillo         Alejandra Camino                                    Gran V\\xeda, 1           Madrid      28001       Spain\n70         70                     Sant\\xe9 Gourmet         Jonas Bergulfsen                            Erling Skakkes gate 78          Stavern       4110      Norway\n71         71                   Save-a-lot Markets           Jose Pavarotti                                   187 Suffolk Ln.            Boise      83720         USA\n72         72                   Seven Seas Imports               Hari Kumar                                   90 Wadhurst Rd.           London   OX15 4NB          UK\n73         73                        Simons bistro           Jytte Petersen                                   Vinb\\xe6ltet 34     K\\xf8benhavn       1734     Denmark\n74         74           Sp\\xe9cialit\\xe9s du monde        Dominique Perrier                                 25, rue Lauriston            Paris      75016      France\n75         75                Split Rail Beer & Ale       Art Braunschweiger                                      P.O. Box 555           Lander      82520         USA\n76         76               Supr\\xeames d\\xe9lices         Pascale Cartrain                              Boulevard Tirou, 255        Charleroi     B-6000     Belgium\n77         77                       The Big Cheese                Liz Nixon                          89 Jefferson Way Suite 2         Portland      97201         USA\n78         78                      The Cracker Box                 Liu Wong                               55 Grizzly Peak Rd.            Butte      59801         USA\n79         79                Toms Spezialit\\xe4ten            Karin Josephs                                     Luisenstr. 48       M\\xfcnster      44087     Germany\n80         80                  Tortuga Restaurante     Miguel Angel Paolino                                  Avda. Azteca 123   M\\xe9xico D.F.       5033      Mexico\n81         81         Tradi\\xe7\\xe3o Hipermercados        Anabela Domingues                        Av. In\\xeas de Castro, 414     S\\xe3o Paulo  05634-030      Brazil\n82         82    Trail's Head Gourmet Provisioners           Helvetius Nagy                                 722 DaVinci Blvd.         Kirkland      98034         USA\n83         83                         Vaffeljernet              Palle Ibsen                                  Smagsl\\xf8get 45         \\xc5rhus       8200     Denmark\n84         84                 Victuailles en stock             Mary Saveley                                2, rue du Commerce             Lyon      69004      France\n85         85            Vins et alcools Chevalier             Paul Henriot                                59 rue de l'Abbaye            Reims      51100      France\n86         86                    Die Wandernde Kuh           Rita M\\xfcller                                 Adenauerallee 900        Stuttgart      70563     Germany\n87         87                       Wartian Herkku         Pirkko Koskitalo                                       Torikatu 38             Oulu      90110     Finland\n88         88               Wellington Importadora            Paula Parente                                Rua do Mercado, 12          Resende  08737-363      Brazil\n89         89                 White Clover Markets           Karl Jablonski                       305 - 14th Ave. S. Suite 3B          Seattle      98128         USA\n90         90                          Wilman Kala          Matti Karttunen                                     Keskuskatu 45         Helsinki      21240     Finland\n91         91                               Wolski                  Zbyszek                                   ul. Filtrowa 68            Walla     01-012      Poland\n\n\n\n\n\n\n\n\n\n\n\nClick here to see the north_american_cities table\n\n\n\n\n\n\nsqldf(\"select * from north_american_cities\")\n\n                  city       country population latitude  longitude\n1          Guadalajara        Mexico    1500800 20.65970 -103.34961\n2              Toronto        Canada    2795060 43.65323  -79.38318\n3              Houston United States    2195914 29.76043  -95.36980\n4             New York United States    8405837 40.71278  -74.00594\n5         Philadelphia United States    1553165 39.95258  -75.16522\n6               Havana          Cuba    2106146 23.05407  -82.34519\n7          Mexico City        Mexico    8555500 19.43261  -99.13321\n8              Phoenix United States    1513367 33.44838 -112.07404\n9          Los Angeles United States    3884307 34.05223 -118.24368\n10 Ecatepec de Morelos        Mexico    1742000 19.60184  -99.05067\n11            Montreal        Canada    1717767 45.50169  -73.56726\n12             Chicago United States    2718782 41.87811  -87.62980",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#sql-select-statement",
    "href": "sql0010-introToSql-v023.html#sql-select-statement",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.2 SQL SELECT statement",
    "text": "31.2 SQL SELECT statement\nThe “select” statement is used to retrieve information from a relational database table (or in our case, from an R dataframe). The select statement has several “clauses”. The following are the allowed clauses. The … after each clause name represents additional text that needs to be typed. We will learn how to use each of these different clauses below.\nThe following are the clauses of a SQL select statement:\n    select ...\n    from ...\n    where ...\n    group by ..\n    having ...\n    order by ..\n    limit ...        (NOTE: the limit clause is found in many SQL implementations but is not standard)\nNOTES:\n\nAll of the clauses are optional, except for the select clause.\nThe clauses that appear in a particular select statement must be in the order shown above.\nThe limit clause is not a standard part of SQL but is commonly used in many flavors of the SQL language.\nAccording to the ANSI SQL standard, SQL is NOT case sensitive. In other words you can type “SELECT …” or “select …”.\nOften, programmers will capitalize the clause names in select statements however, this is not required.\nSome SQL products do make SQL case sensitive. This is especially true when it comes to names of tables and names of columns in the tables.\nThe sqldf package that we will be using is case-INsensitve (i.e. case does NOT matter) for everything EXCEPT for table names (i.e. dataframe names).",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#how-to-use-the-r-sqldf-function",
    "href": "sql0010-introToSql-v023.html#how-to-use-the-r-sqldf-function",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.3 How to use the R sqldf() function",
    "text": "31.3 How to use the R sqldf() function\nThe sqldf function accepts a single argument that is expected to be a SQL SELECT command - e.g. sqldf( SOME_SQL_COMMAND ).\nThe table names in the SELECT command are expected to be R dataframes.\nYou can get help with the sqldf package by typing: help(package=\"sqldf\")\nYou can get help about the sqldf function that is part of the sqldf package by typing: ?sqldf",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#intro-to-the-sql-select-command",
    "href": "sql0010-introToSql-v023.html#intro-to-the-sql-select-command",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.4 Intro to the SQL SELECT command",
    "text": "31.4 Intro to the SQL SELECT command\n\n# select clause specifies which columns you'd like to \n# see in the results.\n\nsqldf(\"select student, test1, test2 \n      from grades\")\n\n    student test1 test2\n1       joe    70    82\n2       sue    80    80\n3      anne    90    95\n4     frank   100    95\n5       bob    30    NA\n6  samantha   100    70\n7     larry    32    80\n8      bill    84    90\n9  lucricia    80   100\n10 margaret    59    47\n11    dalia    85    92\n12      zvi    90    98\n13     mike    90    86\n14    david    90    87\n15     dana   100    93",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#select-from-..",
    "href": "sql0010-introToSql-v023.html#select-from-..",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.5 SELECT * FROM …..",
    "text": "31.5 SELECT * FROM …..\nYou can type * instead of the column names to get all of the columns.\n\nsqldf(\"select * from grades\")   # display the entire grades table\n\n      id  student year honors test1 test2 test3 major\n1  s0100      joe   fr  FALSE    70    82    NA   IDS\n2  s0200      sue   fr  FALSE    80    80    79   IDS\n3  s0300     anne   fr   TRUE    90    95    92   IDS\n4  s0400    frank   so   TRUE   100    95    91   ACC\n5  s0500      bob   so  FALSE    30    NA    NA   IDS\n6  s0600 samantha   so   TRUE   100    70    97   IDS\n7  s0700    larry   ju  FALSE    32    80    NA   FIN\n8  s0800     bill   se  FALSE    84    90    92   ACC\n9  s0900 lucricia   fr   TRUE    80   100    88   IDS\n10 s1000 margaret   se  FALSE    59    47    NA   IDS\n11 s1100    dalia   se  FALSE    85    92    93   FIN\n12 s1200      zvi   ju   TRUE    90    98    92   ACC\n13 s1300     mike   ju   TRUE    90    86    79   IDS\n14 s1400    david   se   TRUE    90    87    87   IDS\n15 s1500     dana   so  FALSE   100    93    91   FIN",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#capturing-the-output-of-sqldf-in-an-r-variable",
    "href": "sql0010-introToSql-v023.html#capturing-the-output-of-sqldf-in-an-r-variable",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.6 capturing the output of sqldf in an R variable",
    "text": "31.6 capturing the output of sqldf in an R variable\nIn R, when using the sqldf function, you can capture the output of sqldf in another variable. This simply creates a new R dataframe (which in sqldf is analogous to a database table).\n\n# Example: Capture the output of the sql command in a new dataframe\n\njust_test1_and_test2 = sqldf(\"select student, test1, test2\n                              from grades\")\n\n# Here is the original table\nsqldf(\"select * from grades\")\n\n      id  student year honors test1 test2 test3 major\n1  s0100      joe   fr  FALSE    70    82    NA   IDS\n2  s0200      sue   fr  FALSE    80    80    79   IDS\n3  s0300     anne   fr   TRUE    90    95    92   IDS\n4  s0400    frank   so   TRUE   100    95    91   ACC\n5  s0500      bob   so  FALSE    30    NA    NA   IDS\n6  s0600 samantha   so   TRUE   100    70    97   IDS\n7  s0700    larry   ju  FALSE    32    80    NA   FIN\n8  s0800     bill   se  FALSE    84    90    92   ACC\n9  s0900 lucricia   fr   TRUE    80   100    88   IDS\n10 s1000 margaret   se  FALSE    59    47    NA   IDS\n11 s1100    dalia   se  FALSE    85    92    93   FIN\n12 s1200      zvi   ju   TRUE    90    98    92   ACC\n13 s1300     mike   ju   TRUE    90    86    79   IDS\n14 s1400    david   se   TRUE    90    87    87   IDS\n15 s1500     dana   so  FALSE   100    93    91   FIN\n\n# Here is the new table\nsqldf(\"select * from just_test1_and_test2\")\n\n    student test1 test2\n1       joe    70    82\n2       sue    80    80\n3      anne    90    95\n4     frank   100    95\n5       bob    30    NA\n6  samantha   100    70\n7     larry    32    80\n8      bill    84    90\n9  lucricia    80   100\n10 margaret    59    47\n11    dalia    85    92\n12      zvi    90    98\n13     mike    90    86\n14    david    90    87\n15     dana   100    93\n\n\nRemember though that the actual sqldf() function in R is NOT technically part of the SQL language. It is simply how we can use SQL commands to manipulate R dataframes.\nIn a relational database there usually is some way to capture the output of a select statement in a different table however, the actual mechanism is not standard. MANY sql database software products use the “into” clause of the select statement to do so. That is not the case for SQLite (the version of SQL that we are using). The bottom line is that the exact technique of storing the output of a SELECT statement in a new table is different from SQL product to SQL product.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#a-note-about-case-sensitivity-in-sqldf",
    "href": "sql0010-introToSql-v023.html#a-note-about-case-sensitivity-in-sqldf",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.7 A note about case-sensitivity in sqldf()",
    "text": "31.7 A note about case-sensitivity in sqldf()\nAs mentioned above, sqldf is case-INsensitive regarding parts of the command OTHER THAN the table names (i.e. the dataframe names). To demonstrate, the following will produce the same results as the command above.\n\n# same results as above\nsqldf(\"SELECT STUDENT, TEST1, TEST2   \n      FROM grades\")\n\n    student test1 test2\n1       joe    70    82\n2       sue    80    80\n3      anne    90    95\n4     frank   100    95\n5       bob    30    NA\n6  samantha   100    70\n7     larry    32    80\n8      bill    84    90\n9  lucricia    80   100\n10 margaret    59    47\n11    dalia    85    92\n12      zvi    90    98\n13     mike    90    86\n14    david    90    87\n15     dana   100    93\n\n# However, the following command will NOT work since there is no variable\n# named GRADES (the name is grades)\n\n# ERROR - should be \"grades\", not \"GRADES\"\n\nsqldf(\"SELECT STUDENT, TEST1, TEST2    \n       FROM GRADES\")                      # Error: no such table: GRADES\n\nError: no such table: GRADES",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#order-by",
    "href": "sql0010-introToSql-v023.html#order-by",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.8 order by",
    "text": "31.8 order by\nThe ORDER BY clause allows you to specify the order that the rows should appear in the results. Numbers are ordered numerically. Character data is ordered alphabetically.\n\n# Order by test1 \nsqldf(\"select student, year, test1, test2\n      from grades\n      order by test1\")\n\n    student year test1 test2\n1       bob   so    30    NA\n2     larry   ju    32    80\n3  margaret   se    59    47\n4       joe   fr    70    82\n5       sue   fr    80    80\n6  lucricia   fr    80   100\n7      bill   se    84    90\n8     dalia   se    85    92\n9      anne   fr    90    95\n10      zvi   ju    90    98\n11     mike   ju    90    86\n12    david   se    90    87\n13    frank   so   100    95\n14 samantha   so   100    70\n15     dana   so   100    93\n\n# Order by student\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by student\")\n\n    student year test1 test2\n1      anne   fr    90    95\n2      bill   se    84    90\n3       bob   so    30    NA\n4     dalia   se    85    92\n5      dana   so   100    93\n6     david   se    90    87\n7     frank   so   100    95\n8       joe   fr    70    82\n9     larry   ju    32    80\n10 lucricia   fr    80   100\n11 margaret   se    59    47\n12     mike   ju    90    86\n13 samantha   so   100    70\n14      sue   fr    80    80\n15      zvi   ju    90    98\n\n# Order by year\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by year\")\n\n    student year test1 test2\n1       joe   fr    70    82\n2       sue   fr    80    80\n3      anne   fr    90    95\n4  lucricia   fr    80   100\n5     larry   ju    32    80\n6       zvi   ju    90    98\n7      mike   ju    90    86\n8      bill   se    84    90\n9  margaret   se    59    47\n10    dalia   se    85    92\n11    david   se    90    87\n12    frank   so   100    95\n13      bob   so    30    NA\n14 samantha   so   100    70\n15     dana   so   100    93\n\n\nYou can specify asc to indicate “ascending” order (this is the default)\nYou can specify desc to indicate “descending” order\n\n# specify asc for order (i.e. ascending) - this is the default.\n# if you dont' specify asc you will get the same results.\n\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by test1 asc\")\n\n    student year test1 test2\n1       bob   so    30    NA\n2     larry   ju    32    80\n3  margaret   se    59    47\n4       joe   fr    70    82\n5       sue   fr    80    80\n6  lucricia   fr    80   100\n7      bill   se    84    90\n8     dalia   se    85    92\n9      anne   fr    90    95\n10      zvi   ju    90    98\n11     mike   ju    90    86\n12    david   se    90    87\n13    frank   so   100    95\n14 samantha   so   100    70\n15     dana   so   100    93\n\n# reversed - order is descending, ie. desc\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by test1 desc\")\n\n    student year test1 test2\n1     frank   so   100    95\n2  samantha   so   100    70\n3      dana   so   100    93\n4      anne   fr    90    95\n5       zvi   ju    90    98\n6      mike   ju    90    86\n7     david   se    90    87\n8     dalia   se    85    92\n9      bill   se    84    90\n10      sue   fr    80    80\n11 lucricia   fr    80   100\n12      joe   fr    70    82\n13 margaret   se    59    47\n14    larry   ju    32    80\n15      bob   so    30    NA\n\n\n\n31.8.1 Ordering by 2 or more columns\nYou can specify more than one column in the order by clause The 2nd specified column will only have an effect for rows that have the same value in the first specified column.\n\nExamples - ordering by 1 column (we did this above)\n\n#.............................\n# ordering by a single column\n#.............................\n\n# order the data in increasing order of test1 value\n# Notice that the year values are NOT in order\n\nsqldf(\"select student, year, test1\n      from grades\n      order by test1 asc\")\n\n    student year test1\n1       bob   so    30\n2     larry   ju    32\n3  margaret   se    59\n4       joe   fr    70\n5       sue   fr    80\n6  lucricia   fr    80\n7      bill   se    84\n8     dalia   se    85\n9      anne   fr    90\n10      zvi   ju    90\n11     mike   ju    90\n12    david   se    90\n13    frank   so   100\n14 samantha   so   100\n15     dana   so   100\n\n# order the data in increasing order of year value (alphabetical order)\n# Notice that the test values are NOT in order\n\nsqldf(\"select student, year, test1\n      from grades\n      order by year asc\")\n\n    student year test1\n1       joe   fr    70\n2       sue   fr    80\n3      anne   fr    90\n4  lucricia   fr    80\n5     larry   ju    32\n6       zvi   ju    90\n7      mike   ju    90\n8      bill   se    84\n9  margaret   se    59\n10    dalia   se    85\n11    david   se    90\n12    frank   so   100\n13      bob   so    30\n14 samantha   so   100\n15     dana   so   100\n\n\n\n\n\n31.8.2 Examples - ordering by 2 different columns\n\n#..................................................\n# order the data by two different columns\n#..................................................\n\n# In the queries below we order the data by two different columns.\n# Notice that there is a separate **asc** or **desc** indicator\n# for each of the columns in the order by clause.\n\n# Order the data alphabetically by year.\n# For all of the rows for a particular year, sort the test1 values\n# in ascending numeric order.\n\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by year asc, test1 asc\")\n\n    student year test1 test2\n1       joe   fr    70    82\n2       sue   fr    80    80\n3  lucricia   fr    80   100\n4      anne   fr    90    95\n5     larry   ju    32    80\n6       zvi   ju    90    98\n7      mike   ju    90    86\n8  margaret   se    59    47\n9      bill   se    84    90\n10    dalia   se    85    92\n11    david   se    90    87\n12      bob   so    30    NA\n13    frank   so   100    95\n14 samantha   so   100    70\n15     dana   so   100    93\n\n# Order the data by alphabetically by year.\n# For all of the rows for a particular year, sort the test1 values\n# in descending numeric order.\n\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by year asc, test1 desc\")\n\n    student year test1 test2\n1      anne   fr    90    95\n2       sue   fr    80    80\n3  lucricia   fr    80   100\n4       joe   fr    70    82\n5       zvi   ju    90    98\n6      mike   ju    90    86\n7     larry   ju    32    80\n8     david   se    90    87\n9     dalia   se    85    92\n10     bill   se    84    90\n11 margaret   se    59    47\n12    frank   so   100    95\n13 samantha   so   100    70\n14     dana   so   100    93\n15      bob   so    30    NA\n\n# Order the data by test1.\n# For all the rows that have the same value for test1 sort the rows by test2\n\n\nsqldf(\"select student, test1, test2\n      from grades\n      order by test1 desc, test2 desc\")\n\n    student test1 test2\n1     frank   100    95\n2      dana   100    93\n3  samantha   100    70\n4       zvi    90    98\n5      anne    90    95\n6     david    90    87\n7      mike    90    86\n8     dalia    85    92\n9      bill    84    90\n10 lucricia    80   100\n11      sue    80    80\n12      joe    70    82\n13 margaret    59    47\n14    larry    32    80\n15      bob    30    NA\n\n# Show the rows in descending order of the test1 grades.\n# If several students got the same grade on test1 then show \n# those rows in alphabetical order of the student names.\n\nsqldf(\"select student, test1, test2\n      from grades\n      order by test1 desc, student asc\")\n\n    student test1 test2\n1      dana   100    93\n2     frank   100    95\n3  samantha   100    70\n4      anne    90    95\n5     david    90    87\n6      mike    90    86\n7       zvi    90    98\n8     dalia    85    92\n9      bill    84    90\n10 lucricia    80   100\n11      sue    80    80\n12      joe    70    82\n13 margaret    59    47\n14    larry    32    80\n15      bob    30    NA\n\n\n\n\n31.8.3 Examples - order by 3 different columns\n\n#..................................................\n# order the data by three different columns\n#..................................................\n\n# Order all of the rows by year \n#\n# among the rows that have the same year\n# order within those rows by test1\n#\n# among the rows that have the same year and test1 values\n# order within those rows by test2\n\nsqldf(\"select student, year, test1, test2\n      from grades\n      order by year, test1 desc, test2 desc\")\n\n    student year test1 test2\n1      anne   fr    90    95\n2  lucricia   fr    80   100\n3       sue   fr    80    80\n4       joe   fr    70    82\n5       zvi   ju    90    98\n6      mike   ju    90    86\n7     larry   ju    32    80\n8     david   se    90    87\n9     dalia   se    85    92\n10     bill   se    84    90\n11 margaret   se    59    47\n12    frank   so   100    95\n13     dana   so   100    93\n14 samantha   so   100    70\n15      bob   so    30    NA",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#limit-clause",
    "href": "sql0010-introToSql-v023.html#limit-clause",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.9 limit clause",
    "text": "31.9 limit clause\n\n###############################################################.\n# LIMIT &lt;M&gt;       and     LIMIT &lt;M&gt; OFFSET &lt;N&gt;\n###############################################################.\n#\n# The LIMIT clause must come at the very end of the SQL \n# statement. You can use it to display only some of the \n# rows that would otherwise have been displayed.\n###############################################################.\n\nFor example:\n\n# Display the first 3 students\nsqldf(\"SELECT *\n      FROM grades\n      ORDER BY student\n      LIMIT 3\")\n\n     id student year honors test1 test2 test3 major\n1 s0300    anne   fr   TRUE    90    95    92   IDS\n2 s0800    bill   se  FALSE    84    90    92   ACC\n3 s0500     bob   so  FALSE    30    NA    NA   IDS\n\n\n\n###############################################################.\n# In general the limit clause has two different forms: \n#\n#      &lt;SOME SELECT STATEMENT&gt;\n#      limit M\n#\n# (where M is an integer) results in only the first\n# M rows of data from what would normally have been displayed \n# had the LIMIT clause not been specified.\n# \n#      &lt;SOME SELECT STATEMENT&gt;\n#      limit M offset N\n#\n# (where both M and N are integers) \n# starts the output from the N+1'th row of what would normally \n# have been displayed without the limit clause and then \n# displays the next M rows of data.\n#\n# LIMIT is often used in combination with the ORDER BY clause.\n###############################################################.\n\nFor example:\n\n# Display the first 3 students\nsqldf(\"SELECT *\n      FROM grades\n      ORDER BY student\n      LIMIT 3\")\n\n     id student year honors test1 test2 test3 major\n1 s0300    anne   fr   TRUE    90    95    92   IDS\n2 s0800    bill   se  FALSE    84    90    92   ACC\n3 s0500     bob   so  FALSE    30    NA    NA   IDS\n\n# Display the next 3 students\nsqldf(\"SELECT *\n      FROM grades\n      ORDER BY student\n      LIMIT 3 OFFSET 3\")\n\n     id student year honors test1 test2 test3 major\n1 s1100   dalia   se  FALSE    85    92    93   FIN\n2 s1500    dana   so  FALSE   100    93    91   FIN\n3 s1400   david   se   TRUE    90    87    87   IDS\n\n# Display the next 3 students\nsqldf(\"SELECT *\n      FROM grades\n      ORDER BY student\n      LIMIT 3 OFFSET 6\")\n\n     id student year honors test1 test2 test3 major\n1 s0400   frank   so   TRUE   100    95    91   ACC\n2 s0100     joe   fr  FALSE    70    82    NA   IDS\n3 s0700   larry   ju  FALSE    32    80    NA   FIN",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#using-calculations-in-the-select-clause",
    "href": "sql0010-introToSql-v023.html#using-calculations-in-the-select-clause",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.10 Using calculations in the select clause",
    "text": "31.10 Using calculations in the select clause\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Using calculations in the select clause    ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# You can use math or SQL functions to modify the values\n# that are returned. We will discuss SQL functions later.\n# For now, let's demonstrate the concept using math.\n# See the questions and answers below:\n\n\n#/////////////////////////////////////////////////////////////////////////\n# QUESTION\n#\n# Display the student, test1 and test2 columns from the grades table.\n# In addition, display a column named that contains\n# the values from the test2 column with 10 points added to each number.\n#/////////////////////////////////////////////////////////////////////////\n\n# ANSWER\n\nsqldf(\"select student, test1, test2, test2 + 10 \n      from grades\")\n\n    student test1 test2 test2 + 10\n1       joe    70    82         92\n2       sue    80    80         90\n3      anne    90    95        105\n4     frank   100    95        105\n5       bob    30    NA         NA\n6  samantha   100    70         80\n7     larry    32    80         90\n8      bill    84    90        100\n9  lucricia    80   100        110\n10 margaret    59    47         57\n11    dalia    85    92        102\n12      zvi    90    98        108\n13     mike    90    86         96\n14    david    90    87         97\n15     dana   100    93        103",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#renaming-columns",
    "href": "sql0010-introToSql-v023.html#renaming-columns",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.11 Renaming columns",
    "text": "31.11 Renaming columns\n\n#/////////////////////////////////////////////////////////////////////////\n# QUESTION\n#\n# Answer the same question, but this time change the name of the new\n# column to \"test2_curved\".\n#/////////////////////////////////////////////////////////////////////////\n\n# ANSWER\n\n# Use AS to change the name of the column in the output\n\nsqldf(\"select student, test1, test2, test2 + 10 as test2_curved\n      from grades\")\n\n    student test1 test2 test2_curved\n1       joe    70    82           92\n2       sue    80    80           90\n3      anne    90    95          105\n4     frank   100    95          105\n5       bob    30    NA           NA\n6  samantha   100    70           80\n7     larry    32    80           90\n8      bill    84    90          100\n9  lucricia    80   100          110\n10 margaret    59    47           57\n11    dalia    85    92          102\n12      zvi    90    98          108\n13     mike    90    86           96\n14    david    90    87           97\n15     dana   100    93          103\n\n# You don't actually need to write \"AS\" \n\nsqldf(\"select student, test1, test2, test2 + 10 test2_curved\n      from grades\")\n\n    student test1 test2 test2_curved\n1       joe    70    82           92\n2       sue    80    80           90\n3      anne    90    95          105\n4     frank   100    95          105\n5       bob    30    NA           NA\n6  samantha   100    70           80\n7     larry    32    80           90\n8      bill    84    90          100\n9  lucricia    80   100          110\n10 margaret    59    47           57\n11    dalia    85    92          102\n12      zvi    90    98          108\n13     mike    90    86           96\n14    david    90    87           97\n15     dana   100    93          103",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#quoting-column-names-that-contain-spaces-or-other-special-characters",
    "href": "sql0010-introToSql-v023.html#quoting-column-names-that-contain-spaces-or-other-special-characters",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.12 ‘Quoting’ column names that contain spaces (or other special characters)",
    "text": "31.12 ‘Quoting’ column names that contain spaces (or other special characters)\nIn general, column names and table names should only contain letters and numbers. If the name of a column or a table needs to contain spaces or other “special characters” (e.g. !@#$%^&*()_+~~ etc.) you need to ‘quote’ the name using ‘apostrophes’ or “quotes”. See the following example which contains a space in the new column name.\n\n# You must 'quote column names that contain spaces' \n\nsqldf(\"select student, test1, test2, test2 + 10 'test2 curved'\n      from grades\")\n\n    student test1 test2 test2 curved\n1       joe    70    82           92\n2       sue    80    80           90\n3      anne    90    95          105\n4     frank   100    95          105\n5       bob    30    NA           NA\n6  samantha   100    70           80\n7     larry    32    80           90\n8      bill    84    90          100\n9  lucricia    80   100          110\n10 margaret    59    47           57\n11    dalia    85    92          102\n12      zvi    90    98          108\n13     mike    90    86           96\n14    david    90    87           97\n15     dana   100    93          103\n\n\nNote that in the SQLite dialect of SQL (which is what is being used in R with sqldf) you can use ‘apostrophes’ in place of quotes. We used ‘apostrophes’ instead of “quotes” to quote the new column name to avoid issues that would arise if we used “quotes” since we are already using “quotes” to surround the entire SQL statement. See the next section for more info.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#single-quotes-vs-double-quotes",
    "href": "sql0010-introToSql-v023.html#single-quotes-vs-double-quotes",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.13 ‘single quotes’ vs “double quotes”",
    "text": "31.13 ‘single quotes’ vs “double quotes”\nIn many areas of technology, ‘apostrophes’ are used to ‘quote’ text instead of “quotation marks”. Apostrophes are used in this way, are referred to as ‘single quotes’ whereas quotation marks are referred to as “double quotes” (since an apostophe has one short line while a single quotation mark is comprised of two short lines).\nIn SQL, there are two different situations where something needs to be quoted. One situation is as shown above to create column headings that contain spaces or special characters. Another situation is where you want to include some literal text (see the description of “concatenation” below).\nIn this presentation we are using ‘single quotes’ for most purposes inside our SQL statements. This is because SQLite (i.e. the dialect of SQL we are using here) allows for the use of either ‘single quotes’ or “double quotes” wherever you need to quote something. Using ‘single quotes’ allows us to sidestep issues that would arise with “double quotes”. Using double quotes inside the SQL statement would interfere with the “double quotes” that we are using to “quote” the entire sql statement in our call to sqldf(” … “). Other SQL envirnoments that do not rely on sqldf() do not have this issue.\nIt should be noted though that the ANSI SQL standard, calls for using double quotes for column names (as in the example above) and for using single quotes to quote textual values (as in the concatenation example below).\n(see https://www.googlecloudcommunity.com/gc/Technical-Tips-Tricks/In-SQL-do-I-use-double-quotes-or-single-quotes-for-string/ta-p/584590)\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# 'single quotes' (ie. 'apostrophes')   vs     \"double quotes\" (ie. \"quotes\")  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Explicit textual values need to be 'quoted'.\n#\n# Standard SQL uses single quotes for quoting values.\n#\n# Many sql software packages allow for \"double quotes\" however, that is not \n# officially part of the standard.\n#\n# The sqlite dialect of SQL (which is the default dialect used by sqldf)\n# DOES recognize \"double quotes\" too. However, we will try to stick to \n# 'single quotes' in order to adhere to the standard SQL notation.\n#\n# The following example combines the id and the student name into a single\n# value with a dash (i.e. - ) between them. Notice the 'single quotes' around\n# the dash.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#concatenation-abc-def-is-abcdef",
    "href": "sql0010-introToSql-v023.html#concatenation-abc-def-is-abcdef",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.14 Concatenation: ‘abc’ || ‘def’ is ‘abcdef’",
    "text": "31.14 Concatenation: ‘abc’ || ‘def’ is ‘abcdef’\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# To \"concatenate\" two values means to \"paste\" them together \n# into a single character value.\n# ANSI SQL defines the  || as the \"concatenation\" operator.\n# This is similar to the & operator in Excel.\n#\n# Sqlite (which is used by R's sqldf package) uses the ANSI standard || operator.\n#\n# See the examples below.\n#\n# NOTE: Some other database software products use non-standard \n#       operators for concatenation. For example:\n#\n#   * MySql does not have a concatenation operator. MySql uses || for logical or. \n#     Rather, it uses the function, concat(a, b), to concatenate a with b.\n#\n#   * Microsoft SQL Server uses + for concatenation.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n31.14.1 Practice\n\nQUESTION\nShow a column named id_student that contains the values from the the id and student columns concatenated together. Also show the test1, test2 and test3 columns.\n\n\n\n\n\n\nClick here for answer\n\n\n\n\n\n\n# ANSWER \n\nsqldf(\"select id || student as id_student, test1, test2, test3\n      from grades\")\n\n      id_student test1 test2 test3\n1       s0100joe    70    82    NA\n2       s0200sue    80    80    79\n3      s0300anne    90    95    92\n4     s0400frank   100    95    91\n5       s0500bob    30    NA    NA\n6  s0600samantha   100    70    97\n7     s0700larry    32    80    NA\n8      s0800bill    84    90    92\n9  s0900lucricia    80   100    88\n10 s1000margaret    59    47    NA\n11    s1100dalia    85    92    93\n12      s1200zvi    90    98    92\n13     s1300mike    90    86    79\n14    s1400david    90    87    87\n15     s1500dana   100    93    91\n\n\n\n\n\n\n\nQuestion\nModify the answer to the previous question so that the data in the “id_student” column contains a dash between the id and the name of the student.\n\n\n\n\n\n\nClick here for answer\n\n\n\n\n\n\n# ANSWER:\n# \n# You need to use 'quotes' around the '-' dash.\n# First concatenate the id with '-' (don't forget the 'single quotes').\n# Then concatenate the student's name.\n\nsqldf(\"select id || '-' || student as id_student, test1, test2, test3\n      from grades\")\n\n       id_student test1 test2 test3\n1       s0100-joe    70    82    NA\n2       s0200-sue    80    80    79\n3      s0300-anne    90    95    92\n4     s0400-frank   100    95    91\n5       s0500-bob    30    NA    NA\n6  s0600-samantha   100    70    97\n7     s0700-larry    32    80    NA\n8      s0800-bill    84    90    92\n9  s0900-lucricia    80   100    88\n10 s1000-margaret    59    47    NA\n11    s1100-dalia    85    92    93\n12      s1200-zvi    90    98    92\n13     s1300-mike    90    86    79\n14    s1400-david    90    87    87\n15     s1500-dana   100    93    91",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#rules-for-naming-tables-and-columns",
    "href": "sql0010-introToSql-v023.html#rules-for-naming-tables-and-columns",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.15 Rules for naming tables and columns",
    "text": "31.15 Rules for naming tables and columns\nNOTE: “IDS1020-Intro to IDS” students can ignore this section.\nWhat happens if you use non-standard characters in the names.\n\nANSI standard SQL: use “double quotes” for non-standard column or table names ( NOT ‘single quotes’ ) - e.g. \"table name\"\nsome other flavors of SQL may use other symbols for this purpose.\nFor example:\n\nMicrosoft Access uses [square brackets] and\nMySql uses grave accent characters (AKA backticks) (https://en.wikipedia.org/wiki/Grave_accent) - e.g. `table name`\nSQLite (i.e. the default version of SQL for sqldf) allows all of the above, i.e. “double quotes”, `back ticks` and [square brackets] to surround non-standard names. However, it is best to use “double quotes” as that is the ANSI standard.\n\n\nEvery version of SQL has its own rules for which characters are allowed to be used in table names and column names. To be safe that your code will work on any version of SQL the best bet is to stick to the following rules for both table names and for column names:\n\nonly use letters, numbers and underscores\nstart the name with a letter\n\nHowever, many SQL versions allow for non-standard characters to be included in a table name or column name. However, if you use a non-standard characters then you must ‘quote’ the name of the table or name of the column in SQL commands using the quoting rules provided by the version of SQL you are using. When using sqldf, if you use a non-standard character in a column name or table name (e.g. a period, a space, etc) then you should surround the name of the table or column with “double quotes” (backticks and [square brackets] also work but “double quotes” are preferred as that is the ANSI standard). You cannot use ‘single quotes’ for this purpose.\nEXAMPLE:\nTo demonstrate we can use the iris dataframe that is built into R.\nWARNING: In SQLite, you must use “double quotes” for non standard table or column names.\nNOTE:\n\n“IDS1020-Intro to IDS” students can ignore this section.\n“IDS2460-Data Management” you should be aware of the following issue.\n\nThis doesn’t come up too often but if you are using sqldf to work with R dataframes you should be aware of the following issue. R dataframes often use periods in the column names. This is non-standard for SQL.\n\n# Show the first 10 rows of the iris dataframe that comes built into R.\nhead(iris,10)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n\n#//////////////////////////////////////////////////////////////////////////\n# QUESTION\n#\n# Use sqldf to display just the Sepal.Length, Sepal.Width and Species columns \n# from the \"iris\" dataframe.\n#//////////////////////////////////////////////////////////////////////////\n\n\n\n\n\n\n\nclick here for answer\n\n\n\n\n\n\n# ANSWER\n#\n# This dataframe has column names that contain periods. A period\n# is NOT a standard character for table and column names. Therefore\n# we must use \"double quotes\" or `back ticks` to quote these column names.\n#\n# This does NOT work because of the periods in the column names\n\n# sqldf(\"select Sepal.Length, Sepal.Width, Species \n#         from iris\")                              # ERROR: no such column Sepal.Length\n\n# This works - i.e. use \"double quotes\". Note that we used \n# 'single quotes' to quote the entire select statement.\nsqldf('select \"Sepal.Length\", \"Sepal.Width\", Species from iris')\n\n    Sepal.Length Sepal.Width    Species\n1            5.1         3.5     setosa\n2            4.9         3.0     setosa\n3            4.7         3.2     setosa\n4            4.6         3.1     setosa\n5            5.0         3.6     setosa\n6            5.4         3.9     setosa\n7            4.6         3.4     setosa\n8            5.0         3.4     setosa\n9            4.4         2.9     setosa\n10           4.9         3.1     setosa\n11           5.4         3.7     setosa\n12           4.8         3.4     setosa\n13           4.8         3.0     setosa\n14           4.3         3.0     setosa\n15           5.8         4.0     setosa\n16           5.7         4.4     setosa\n17           5.4         3.9     setosa\n18           5.1         3.5     setosa\n19           5.7         3.8     setosa\n20           5.1         3.8     setosa\n21           5.4         3.4     setosa\n22           5.1         3.7     setosa\n23           4.6         3.6     setosa\n24           5.1         3.3     setosa\n25           4.8         3.4     setosa\n26           5.0         3.0     setosa\n27           5.0         3.4     setosa\n28           5.2         3.5     setosa\n29           5.2         3.4     setosa\n30           4.7         3.2     setosa\n31           4.8         3.1     setosa\n32           5.4         3.4     setosa\n33           5.2         4.1     setosa\n34           5.5         4.2     setosa\n35           4.9         3.1     setosa\n36           5.0         3.2     setosa\n37           5.5         3.5     setosa\n38           4.9         3.6     setosa\n39           4.4         3.0     setosa\n40           5.1         3.4     setosa\n41           5.0         3.5     setosa\n42           4.5         2.3     setosa\n43           4.4         3.2     setosa\n44           5.0         3.5     setosa\n45           5.1         3.8     setosa\n46           4.8         3.0     setosa\n47           5.1         3.8     setosa\n48           4.6         3.2     setosa\n49           5.3         3.7     setosa\n50           5.0         3.3     setosa\n51           7.0         3.2 versicolor\n52           6.4         3.2 versicolor\n53           6.9         3.1 versicolor\n54           5.5         2.3 versicolor\n55           6.5         2.8 versicolor\n56           5.7         2.8 versicolor\n57           6.3         3.3 versicolor\n58           4.9         2.4 versicolor\n59           6.6         2.9 versicolor\n60           5.2         2.7 versicolor\n61           5.0         2.0 versicolor\n62           5.9         3.0 versicolor\n63           6.0         2.2 versicolor\n64           6.1         2.9 versicolor\n65           5.6         2.9 versicolor\n66           6.7         3.1 versicolor\n67           5.6         3.0 versicolor\n68           5.8         2.7 versicolor\n69           6.2         2.2 versicolor\n70           5.6         2.5 versicolor\n71           5.9         3.2 versicolor\n72           6.1         2.8 versicolor\n73           6.3         2.5 versicolor\n74           6.1         2.8 versicolor\n75           6.4         2.9 versicolor\n76           6.6         3.0 versicolor\n77           6.8         2.8 versicolor\n78           6.7         3.0 versicolor\n79           6.0         2.9 versicolor\n80           5.7         2.6 versicolor\n81           5.5         2.4 versicolor\n82           5.5         2.4 versicolor\n83           5.8         2.7 versicolor\n84           6.0         2.7 versicolor\n85           5.4         3.0 versicolor\n86           6.0         3.4 versicolor\n87           6.7         3.1 versicolor\n88           6.3         2.3 versicolor\n89           5.6         3.0 versicolor\n90           5.5         2.5 versicolor\n91           5.5         2.6 versicolor\n92           6.1         3.0 versicolor\n93           5.8         2.6 versicolor\n94           5.0         2.3 versicolor\n95           5.6         2.7 versicolor\n96           5.7         3.0 versicolor\n97           5.7         2.9 versicolor\n98           6.2         2.9 versicolor\n99           5.1         2.5 versicolor\n100          5.7         2.8 versicolor\n101          6.3         3.3  virginica\n102          5.8         2.7  virginica\n103          7.1         3.0  virginica\n104          6.3         2.9  virginica\n105          6.5         3.0  virginica\n106          7.6         3.0  virginica\n107          4.9         2.5  virginica\n108          7.3         2.9  virginica\n109          6.7         2.5  virginica\n110          7.2         3.6  virginica\n111          6.5         3.2  virginica\n112          6.4         2.7  virginica\n113          6.8         3.0  virginica\n114          5.7         2.5  virginica\n115          5.8         2.8  virginica\n116          6.4         3.2  virginica\n117          6.5         3.0  virginica\n118          7.7         3.8  virginica\n119          7.7         2.6  virginica\n120          6.0         2.2  virginica\n121          6.9         3.2  virginica\n122          5.6         2.8  virginica\n123          7.7         2.8  virginica\n124          6.3         2.7  virginica\n125          6.7         3.3  virginica\n126          7.2         3.2  virginica\n127          6.2         2.8  virginica\n128          6.1         3.0  virginica\n129          6.4         2.8  virginica\n130          7.2         3.0  virginica\n131          7.4         2.8  virginica\n132          7.9         3.8  virginica\n133          6.4         2.8  virginica\n134          6.3         2.8  virginica\n135          6.1         2.6  virginica\n136          7.7         3.0  virginica\n137          6.3         3.4  virginica\n138          6.4         3.1  virginica\n139          6.0         3.0  virginica\n140          6.9         3.1  virginica\n141          6.7         3.1  virginica\n142          6.9         3.1  virginica\n143          5.8         2.7  virginica\n144          6.8         3.2  virginica\n145          6.7         3.3  virginica\n146          6.7         3.0  virginica\n147          6.3         2.5  virginica\n148          6.5         3.0  virginica\n149          6.2         3.4  virginica\n150          5.9         3.0  virginica\n\n# Same thing but we used \"double quotes\" to quote the entire select statement.\n# Therefore we used \\\" for every double quote that is inside the command.\nsqldf(\"select \\\"Sepal.Length\\\", \\\"Sepal.Width\\\", Species from iris\")\n\n    Sepal.Length Sepal.Width    Species\n1            5.1         3.5     setosa\n2            4.9         3.0     setosa\n3            4.7         3.2     setosa\n4            4.6         3.1     setosa\n5            5.0         3.6     setosa\n6            5.4         3.9     setosa\n7            4.6         3.4     setosa\n8            5.0         3.4     setosa\n9            4.4         2.9     setosa\n10           4.9         3.1     setosa\n11           5.4         3.7     setosa\n12           4.8         3.4     setosa\n13           4.8         3.0     setosa\n14           4.3         3.0     setosa\n15           5.8         4.0     setosa\n16           5.7         4.4     setosa\n17           5.4         3.9     setosa\n18           5.1         3.5     setosa\n19           5.7         3.8     setosa\n20           5.1         3.8     setosa\n21           5.4         3.4     setosa\n22           5.1         3.7     setosa\n23           4.6         3.6     setosa\n24           5.1         3.3     setosa\n25           4.8         3.4     setosa\n26           5.0         3.0     setosa\n27           5.0         3.4     setosa\n28           5.2         3.5     setosa\n29           5.2         3.4     setosa\n30           4.7         3.2     setosa\n31           4.8         3.1     setosa\n32           5.4         3.4     setosa\n33           5.2         4.1     setosa\n34           5.5         4.2     setosa\n35           4.9         3.1     setosa\n36           5.0         3.2     setosa\n37           5.5         3.5     setosa\n38           4.9         3.6     setosa\n39           4.4         3.0     setosa\n40           5.1         3.4     setosa\n41           5.0         3.5     setosa\n42           4.5         2.3     setosa\n43           4.4         3.2     setosa\n44           5.0         3.5     setosa\n45           5.1         3.8     setosa\n46           4.8         3.0     setosa\n47           5.1         3.8     setosa\n48           4.6         3.2     setosa\n49           5.3         3.7     setosa\n50           5.0         3.3     setosa\n51           7.0         3.2 versicolor\n52           6.4         3.2 versicolor\n53           6.9         3.1 versicolor\n54           5.5         2.3 versicolor\n55           6.5         2.8 versicolor\n56           5.7         2.8 versicolor\n57           6.3         3.3 versicolor\n58           4.9         2.4 versicolor\n59           6.6         2.9 versicolor\n60           5.2         2.7 versicolor\n61           5.0         2.0 versicolor\n62           5.9         3.0 versicolor\n63           6.0         2.2 versicolor\n64           6.1         2.9 versicolor\n65           5.6         2.9 versicolor\n66           6.7         3.1 versicolor\n67           5.6         3.0 versicolor\n68           5.8         2.7 versicolor\n69           6.2         2.2 versicolor\n70           5.6         2.5 versicolor\n71           5.9         3.2 versicolor\n72           6.1         2.8 versicolor\n73           6.3         2.5 versicolor\n74           6.1         2.8 versicolor\n75           6.4         2.9 versicolor\n76           6.6         3.0 versicolor\n77           6.8         2.8 versicolor\n78           6.7         3.0 versicolor\n79           6.0         2.9 versicolor\n80           5.7         2.6 versicolor\n81           5.5         2.4 versicolor\n82           5.5         2.4 versicolor\n83           5.8         2.7 versicolor\n84           6.0         2.7 versicolor\n85           5.4         3.0 versicolor\n86           6.0         3.4 versicolor\n87           6.7         3.1 versicolor\n88           6.3         2.3 versicolor\n89           5.6         3.0 versicolor\n90           5.5         2.5 versicolor\n91           5.5         2.6 versicolor\n92           6.1         3.0 versicolor\n93           5.8         2.6 versicolor\n94           5.0         2.3 versicolor\n95           5.6         2.7 versicolor\n96           5.7         3.0 versicolor\n97           5.7         2.9 versicolor\n98           6.2         2.9 versicolor\n99           5.1         2.5 versicolor\n100          5.7         2.8 versicolor\n101          6.3         3.3  virginica\n102          5.8         2.7  virginica\n103          7.1         3.0  virginica\n104          6.3         2.9  virginica\n105          6.5         3.0  virginica\n106          7.6         3.0  virginica\n107          4.9         2.5  virginica\n108          7.3         2.9  virginica\n109          6.7         2.5  virginica\n110          7.2         3.6  virginica\n111          6.5         3.2  virginica\n112          6.4         2.7  virginica\n113          6.8         3.0  virginica\n114          5.7         2.5  virginica\n115          5.8         2.8  virginica\n116          6.4         3.2  virginica\n117          6.5         3.0  virginica\n118          7.7         3.8  virginica\n119          7.7         2.6  virginica\n120          6.0         2.2  virginica\n121          6.9         3.2  virginica\n122          5.6         2.8  virginica\n123          7.7         2.8  virginica\n124          6.3         2.7  virginica\n125          6.7         3.3  virginica\n126          7.2         3.2  virginica\n127          6.2         2.8  virginica\n128          6.1         3.0  virginica\n129          6.4         2.8  virginica\n130          7.2         3.0  virginica\n131          7.4         2.8  virginica\n132          7.9         3.8  virginica\n133          6.4         2.8  virginica\n134          6.3         2.8  virginica\n135          6.1         2.6  virginica\n136          7.7         3.0  virginica\n137          6.3         3.4  virginica\n138          6.4         3.1  virginica\n139          6.0         3.0  virginica\n140          6.9         3.1  virginica\n141          6.7         3.1  virginica\n142          6.9         3.1  virginica\n143          5.8         2.7  virginica\n144          6.8         3.2  virginica\n145          6.7         3.3  virginica\n146          6.7         3.0  virginica\n147          6.3         2.5  virginica\n148          6.5         3.0  virginica\n149          6.2         3.4  virginica\n150          5.9         3.0  virginica\n\n# This also works - use `back ticks`\nsqldf('select `Sepal.Length`, `Sepal.Width`, Species from iris')\n\n    Sepal.Length Sepal.Width    Species\n1            5.1         3.5     setosa\n2            4.9         3.0     setosa\n3            4.7         3.2     setosa\n4            4.6         3.1     setosa\n5            5.0         3.6     setosa\n6            5.4         3.9     setosa\n7            4.6         3.4     setosa\n8            5.0         3.4     setosa\n9            4.4         2.9     setosa\n10           4.9         3.1     setosa\n11           5.4         3.7     setosa\n12           4.8         3.4     setosa\n13           4.8         3.0     setosa\n14           4.3         3.0     setosa\n15           5.8         4.0     setosa\n16           5.7         4.4     setosa\n17           5.4         3.9     setosa\n18           5.1         3.5     setosa\n19           5.7         3.8     setosa\n20           5.1         3.8     setosa\n21           5.4         3.4     setosa\n22           5.1         3.7     setosa\n23           4.6         3.6     setosa\n24           5.1         3.3     setosa\n25           4.8         3.4     setosa\n26           5.0         3.0     setosa\n27           5.0         3.4     setosa\n28           5.2         3.5     setosa\n29           5.2         3.4     setosa\n30           4.7         3.2     setosa\n31           4.8         3.1     setosa\n32           5.4         3.4     setosa\n33           5.2         4.1     setosa\n34           5.5         4.2     setosa\n35           4.9         3.1     setosa\n36           5.0         3.2     setosa\n37           5.5         3.5     setosa\n38           4.9         3.6     setosa\n39           4.4         3.0     setosa\n40           5.1         3.4     setosa\n41           5.0         3.5     setosa\n42           4.5         2.3     setosa\n43           4.4         3.2     setosa\n44           5.0         3.5     setosa\n45           5.1         3.8     setosa\n46           4.8         3.0     setosa\n47           5.1         3.8     setosa\n48           4.6         3.2     setosa\n49           5.3         3.7     setosa\n50           5.0         3.3     setosa\n51           7.0         3.2 versicolor\n52           6.4         3.2 versicolor\n53           6.9         3.1 versicolor\n54           5.5         2.3 versicolor\n55           6.5         2.8 versicolor\n56           5.7         2.8 versicolor\n57           6.3         3.3 versicolor\n58           4.9         2.4 versicolor\n59           6.6         2.9 versicolor\n60           5.2         2.7 versicolor\n61           5.0         2.0 versicolor\n62           5.9         3.0 versicolor\n63           6.0         2.2 versicolor\n64           6.1         2.9 versicolor\n65           5.6         2.9 versicolor\n66           6.7         3.1 versicolor\n67           5.6         3.0 versicolor\n68           5.8         2.7 versicolor\n69           6.2         2.2 versicolor\n70           5.6         2.5 versicolor\n71           5.9         3.2 versicolor\n72           6.1         2.8 versicolor\n73           6.3         2.5 versicolor\n74           6.1         2.8 versicolor\n75           6.4         2.9 versicolor\n76           6.6         3.0 versicolor\n77           6.8         2.8 versicolor\n78           6.7         3.0 versicolor\n79           6.0         2.9 versicolor\n80           5.7         2.6 versicolor\n81           5.5         2.4 versicolor\n82           5.5         2.4 versicolor\n83           5.8         2.7 versicolor\n84           6.0         2.7 versicolor\n85           5.4         3.0 versicolor\n86           6.0         3.4 versicolor\n87           6.7         3.1 versicolor\n88           6.3         2.3 versicolor\n89           5.6         3.0 versicolor\n90           5.5         2.5 versicolor\n91           5.5         2.6 versicolor\n92           6.1         3.0 versicolor\n93           5.8         2.6 versicolor\n94           5.0         2.3 versicolor\n95           5.6         2.7 versicolor\n96           5.7         3.0 versicolor\n97           5.7         2.9 versicolor\n98           6.2         2.9 versicolor\n99           5.1         2.5 versicolor\n100          5.7         2.8 versicolor\n101          6.3         3.3  virginica\n102          5.8         2.7  virginica\n103          7.1         3.0  virginica\n104          6.3         2.9  virginica\n105          6.5         3.0  virginica\n106          7.6         3.0  virginica\n107          4.9         2.5  virginica\n108          7.3         2.9  virginica\n109          6.7         2.5  virginica\n110          7.2         3.6  virginica\n111          6.5         3.2  virginica\n112          6.4         2.7  virginica\n113          6.8         3.0  virginica\n114          5.7         2.5  virginica\n115          5.8         2.8  virginica\n116          6.4         3.2  virginica\n117          6.5         3.0  virginica\n118          7.7         3.8  virginica\n119          7.7         2.6  virginica\n120          6.0         2.2  virginica\n121          6.9         3.2  virginica\n122          5.6         2.8  virginica\n123          7.7         2.8  virginica\n124          6.3         2.7  virginica\n125          6.7         3.3  virginica\n126          7.2         3.2  virginica\n127          6.2         2.8  virginica\n128          6.1         3.0  virginica\n129          6.4         2.8  virginica\n130          7.2         3.0  virginica\n131          7.4         2.8  virginica\n132          7.9         3.8  virginica\n133          6.4         2.8  virginica\n134          6.3         2.8  virginica\n135          6.1         2.6  virginica\n136          7.7         3.0  virginica\n137          6.3         3.4  virginica\n138          6.4         3.1  virginica\n139          6.0         3.0  virginica\n140          6.9         3.1  virginica\n141          6.7         3.1  virginica\n142          6.9         3.1  virginica\n143          5.8         2.7  virginica\n144          6.8         3.2  virginica\n145          6.7         3.3  virginica\n146          6.7         3.0  virginica\n147          6.3         2.5  virginica\n148          6.5         3.0  virginica\n149          6.2         3.4  virginica\n150          5.9         3.0  virginica\n\n# *** WARNING!!! ***\n#\n# BE CAREFUL - don't use 'single quotes' here. Single quotes will NOT\n# work to quote column names or table names.\n#\n# The following will simply display the words 'Sepal.Length' and 'Sepal.Width'\n# for every row in the output.\nsqldf(\"select 'Sepal.Length', 'Sepal.Width', Species from iris\")\n\n    'Sepal.Length' 'Sepal.Width'    Species\n1     Sepal.Length   Sepal.Width     setosa\n2     Sepal.Length   Sepal.Width     setosa\n3     Sepal.Length   Sepal.Width     setosa\n4     Sepal.Length   Sepal.Width     setosa\n5     Sepal.Length   Sepal.Width     setosa\n6     Sepal.Length   Sepal.Width     setosa\n7     Sepal.Length   Sepal.Width     setosa\n8     Sepal.Length   Sepal.Width     setosa\n9     Sepal.Length   Sepal.Width     setosa\n10    Sepal.Length   Sepal.Width     setosa\n11    Sepal.Length   Sepal.Width     setosa\n12    Sepal.Length   Sepal.Width     setosa\n13    Sepal.Length   Sepal.Width     setosa\n14    Sepal.Length   Sepal.Width     setosa\n15    Sepal.Length   Sepal.Width     setosa\n16    Sepal.Length   Sepal.Width     setosa\n17    Sepal.Length   Sepal.Width     setosa\n18    Sepal.Length   Sepal.Width     setosa\n19    Sepal.Length   Sepal.Width     setosa\n20    Sepal.Length   Sepal.Width     setosa\n21    Sepal.Length   Sepal.Width     setosa\n22    Sepal.Length   Sepal.Width     setosa\n23    Sepal.Length   Sepal.Width     setosa\n24    Sepal.Length   Sepal.Width     setosa\n25    Sepal.Length   Sepal.Width     setosa\n26    Sepal.Length   Sepal.Width     setosa\n27    Sepal.Length   Sepal.Width     setosa\n28    Sepal.Length   Sepal.Width     setosa\n29    Sepal.Length   Sepal.Width     setosa\n30    Sepal.Length   Sepal.Width     setosa\n31    Sepal.Length   Sepal.Width     setosa\n32    Sepal.Length   Sepal.Width     setosa\n33    Sepal.Length   Sepal.Width     setosa\n34    Sepal.Length   Sepal.Width     setosa\n35    Sepal.Length   Sepal.Width     setosa\n36    Sepal.Length   Sepal.Width     setosa\n37    Sepal.Length   Sepal.Width     setosa\n38    Sepal.Length   Sepal.Width     setosa\n39    Sepal.Length   Sepal.Width     setosa\n40    Sepal.Length   Sepal.Width     setosa\n41    Sepal.Length   Sepal.Width     setosa\n42    Sepal.Length   Sepal.Width     setosa\n43    Sepal.Length   Sepal.Width     setosa\n44    Sepal.Length   Sepal.Width     setosa\n45    Sepal.Length   Sepal.Width     setosa\n46    Sepal.Length   Sepal.Width     setosa\n47    Sepal.Length   Sepal.Width     setosa\n48    Sepal.Length   Sepal.Width     setosa\n49    Sepal.Length   Sepal.Width     setosa\n50    Sepal.Length   Sepal.Width     setosa\n51    Sepal.Length   Sepal.Width versicolor\n52    Sepal.Length   Sepal.Width versicolor\n53    Sepal.Length   Sepal.Width versicolor\n54    Sepal.Length   Sepal.Width versicolor\n55    Sepal.Length   Sepal.Width versicolor\n56    Sepal.Length   Sepal.Width versicolor\n57    Sepal.Length   Sepal.Width versicolor\n58    Sepal.Length   Sepal.Width versicolor\n59    Sepal.Length   Sepal.Width versicolor\n60    Sepal.Length   Sepal.Width versicolor\n61    Sepal.Length   Sepal.Width versicolor\n62    Sepal.Length   Sepal.Width versicolor\n63    Sepal.Length   Sepal.Width versicolor\n64    Sepal.Length   Sepal.Width versicolor\n65    Sepal.Length   Sepal.Width versicolor\n66    Sepal.Length   Sepal.Width versicolor\n67    Sepal.Length   Sepal.Width versicolor\n68    Sepal.Length   Sepal.Width versicolor\n69    Sepal.Length   Sepal.Width versicolor\n70    Sepal.Length   Sepal.Width versicolor\n71    Sepal.Length   Sepal.Width versicolor\n72    Sepal.Length   Sepal.Width versicolor\n73    Sepal.Length   Sepal.Width versicolor\n74    Sepal.Length   Sepal.Width versicolor\n75    Sepal.Length   Sepal.Width versicolor\n76    Sepal.Length   Sepal.Width versicolor\n77    Sepal.Length   Sepal.Width versicolor\n78    Sepal.Length   Sepal.Width versicolor\n79    Sepal.Length   Sepal.Width versicolor\n80    Sepal.Length   Sepal.Width versicolor\n81    Sepal.Length   Sepal.Width versicolor\n82    Sepal.Length   Sepal.Width versicolor\n83    Sepal.Length   Sepal.Width versicolor\n84    Sepal.Length   Sepal.Width versicolor\n85    Sepal.Length   Sepal.Width versicolor\n86    Sepal.Length   Sepal.Width versicolor\n87    Sepal.Length   Sepal.Width versicolor\n88    Sepal.Length   Sepal.Width versicolor\n89    Sepal.Length   Sepal.Width versicolor\n90    Sepal.Length   Sepal.Width versicolor\n91    Sepal.Length   Sepal.Width versicolor\n92    Sepal.Length   Sepal.Width versicolor\n93    Sepal.Length   Sepal.Width versicolor\n94    Sepal.Length   Sepal.Width versicolor\n95    Sepal.Length   Sepal.Width versicolor\n96    Sepal.Length   Sepal.Width versicolor\n97    Sepal.Length   Sepal.Width versicolor\n98    Sepal.Length   Sepal.Width versicolor\n99    Sepal.Length   Sepal.Width versicolor\n100   Sepal.Length   Sepal.Width versicolor\n101   Sepal.Length   Sepal.Width  virginica\n102   Sepal.Length   Sepal.Width  virginica\n103   Sepal.Length   Sepal.Width  virginica\n104   Sepal.Length   Sepal.Width  virginica\n105   Sepal.Length   Sepal.Width  virginica\n106   Sepal.Length   Sepal.Width  virginica\n107   Sepal.Length   Sepal.Width  virginica\n108   Sepal.Length   Sepal.Width  virginica\n109   Sepal.Length   Sepal.Width  virginica\n110   Sepal.Length   Sepal.Width  virginica\n111   Sepal.Length   Sepal.Width  virginica\n112   Sepal.Length   Sepal.Width  virginica\n113   Sepal.Length   Sepal.Width  virginica\n114   Sepal.Length   Sepal.Width  virginica\n115   Sepal.Length   Sepal.Width  virginica\n116   Sepal.Length   Sepal.Width  virginica\n117   Sepal.Length   Sepal.Width  virginica\n118   Sepal.Length   Sepal.Width  virginica\n119   Sepal.Length   Sepal.Width  virginica\n120   Sepal.Length   Sepal.Width  virginica\n121   Sepal.Length   Sepal.Width  virginica\n122   Sepal.Length   Sepal.Width  virginica\n123   Sepal.Length   Sepal.Width  virginica\n124   Sepal.Length   Sepal.Width  virginica\n125   Sepal.Length   Sepal.Width  virginica\n126   Sepal.Length   Sepal.Width  virginica\n127   Sepal.Length   Sepal.Width  virginica\n128   Sepal.Length   Sepal.Width  virginica\n129   Sepal.Length   Sepal.Width  virginica\n130   Sepal.Length   Sepal.Width  virginica\n131   Sepal.Length   Sepal.Width  virginica\n132   Sepal.Length   Sepal.Width  virginica\n133   Sepal.Length   Sepal.Width  virginica\n134   Sepal.Length   Sepal.Width  virginica\n135   Sepal.Length   Sepal.Width  virginica\n136   Sepal.Length   Sepal.Width  virginica\n137   Sepal.Length   Sepal.Width  virginica\n138   Sepal.Length   Sepal.Width  virginica\n139   Sepal.Length   Sepal.Width  virginica\n140   Sepal.Length   Sepal.Width  virginica\n141   Sepal.Length   Sepal.Width  virginica\n142   Sepal.Length   Sepal.Width  virginica\n143   Sepal.Length   Sepal.Width  virginica\n144   Sepal.Length   Sepal.Width  virginica\n145   Sepal.Length   Sepal.Width  virginica\n146   Sepal.Length   Sepal.Width  virginica\n147   Sepal.Length   Sepal.Width  virginica\n148   Sepal.Length   Sepal.Width  virginica\n149   Sepal.Length   Sepal.Width  virginica\n150   Sepal.Length   Sepal.Width  virginica\n\n\n\n\n\n\n31.15.1 Aside: using R “raw strings” (only for those learning R)\nNOTE:\n\n“IDS1020-Intro to IDS” students can ignore this section.\n\nIf you are learning SQL but are not interested in learning more about R right now you can safely skip this section.\n\n# RAW STRINGS in R  - i.e.    r\"( ... )\"\n#\n# You can also use a \"raw string\" - introduced in R 4.0\n# Any string (ie. character value) that appears between r\"( .... )\"\n# is quoted as exactly the value that appears between the r\"( and )\"\n# symbols. For detailed info about R's new raw string feature see this page:\n#\n#    https://r4ds.hadley.nz/strings.html#sec-raw-strings\n#\n# You can use an R \"raw string\" to quote anything without\n# resorting to backslahes or other techniques.\n#\n# To create a \"raw string\" in R place the text that you want to quote\n# between    r\"(      and      )\"\n# The r stands for raw, not \"R\". \n# The text being quoted can safely include any characters you like.\n# For example, the following \"strangeValue\" gets displayed just fine.\n\nstrangeValue = r\"( Backslash: \\  Quote: \"  Apostrophe: ' )\"\n\ncat(strangeValue)    # Backslash: \\  Quote: \"  Apostrophe: ' \n\n Backslash: \\  Quote: \"  Apostrophe: ' \n\n# You can use r\"(raw strings)\" to very simply quote any SQL command\n# that you want to pass to sqldf, regardless of what is in the SQL.\n\nsqldf( r\"(select \"Sepal.Length\", \"Sepal.Width\", Species from iris limit 10)\" )\n\n   Sepal.Length Sepal.Width Species\n1           5.1         3.5  setosa\n2           4.9         3.0  setosa\n3           4.7         3.2  setosa\n4           4.6         3.1  setosa\n5           5.0         3.6  setosa\n6           5.4         3.9  setosa\n7           4.6         3.4  setosa\n8           5.0         3.4  setosa\n9           4.4         2.9  setosa\n10          4.9         3.1  setosa\n\n# NOTE - R's new \"raw string\" syntax is very useful for \n# writing regular expression patterns in R\n# For example:\n#\n#   without raw strings - you NEED \\\\double-backslashes\n\npattern = \"\\\\d+\\\\.\\\\d+\" \ncat(pattern)   # \\d+\\.\\d+\n\n\\d+\\.\\d+\n\n#   with raw strings - you DON'T need the \\\\double-backslashes\npattern = r\"(\\d+\\.\\d)\"      \ncat(pattern)   # \\d+\\.\\d+\n\n\\d+\\.\\d\n\ncharNums = c(\"one\", \"1.593\", \"278.123\", \"999\")\ngrep(pattern, charNums, value=TRUE)\n\n[1] \"1.593\"   \"278.123\"",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#where-clause",
    "href": "sql0010-introToSql-v023.html#where-clause",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.16 where clause",
    "text": "31.16 where clause\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# where clause   ####\n#\n# The where clause allows you to specify the rows that you want.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# The where clause include a logical expression. The expression is evaluated\n# separately for each row in the table (i.e. dataframe in our case). \n# The data from the row is substituted into the logical expression. If the\n# logical expression results in TRUE then information from the row becomes\n# encorporated into the output. If the logical expression results in FALSE \n# for the row then the row does not appear in the output.\n\n# Only the rows that have a TRUE result are displayed\n\nsqldf(\"select student, test1, test2 \n      from grades\n      where test1 &gt; test2\")\n\n   student test1 test2\n1    frank   100    95\n2 samantha   100    70\n3 margaret    59    47\n4     mike    90    86\n5    david    90    87\n6     dana   100    93\n\n# You can get all of the columns by using the * instead of the column names\nsqldf(\"select *\n      from grades\n      where test1 &gt; test2\")\n\n     id  student year honors test1 test2 test3 major\n1 s0400    frank   so   TRUE   100    95    91   ACC\n2 s0600 samantha   so   TRUE   100    70    97   IDS\n3 s1000 margaret   se  FALSE    59    47    NA   IDS\n4 s1300     mike   ju   TRUE    90    86    79   IDS\n5 s1400    david   se   TRUE    90    87    87   IDS\n6 s1500     dana   so  FALSE   100    93    91   FIN\n\nsqldf(\"select student, test1, test2 \n      from grades\n      where test1 &gt;= test2\")\n\n   student test1 test2\n1      sue    80    80\n2    frank   100    95\n3 samantha   100    70\n4 margaret    59    47\n5     mike    90    86\n6    david    90    87\n7     dana   100    93\n\nsqldf(\"select student, honors, test1, test2 \n      from grades \n      where honors == TRUE and test1 &lt; 90 and test2 &lt; 90\")\n\n[1] student honors  test1   test2  \n&lt;0 rows&gt; (or 0-length row.names)\n\nsqldf(\"select student, honors, test1, test2 \n      from grades \n      where honors != TRUE and test1 &gt; 90 and test2 &gt; 90\")\n\n  student honors test1 test2\n1    dana  FALSE   100    93\n\n# NOTE: \n#\n# When using the relational operators &gt;  &lt;   &gt;=   &lt;=\n# with character data, values that would appear earlier in a dictionary\n# are considered \"less than\" values that would appear later in the dictionary.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#sql-operators",
    "href": "sql0010-introToSql-v023.html#sql-operators",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.17 SQL operators",
    "text": "31.17 SQL operators\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n# SQL operators\n#\n# Different flavors of SQL sometimes use different operators.\n# Look up the documentation for your particular flavor of SQL.\n# See the SQLite operators here\n# https://www.tutorialspoint.com/sqlite/sqlite_operators.htm\n# They are summarized below.\n#\n#                        R operator        sqlite operator\n#                        -----------       ---------------\n#\n#   ARITHMETIC OPERATORS\n#   addition                +                 same as R\n#   subtraction             -                 same as R\n#   multiplication          *                 same as R\n#   regular division        /                 /     depends on the type of data\n#   integer division        %/%               /     depends on the type of data\n#   modulus (remainder)     %%                %\n#\n#   LOGICAL OPERATORS\n#   equal to                ==                = or == (they both work)\n#   not equal to            !=                != or &lt;&gt; (they both work)\n#   greater than            &gt;                 same as R\n#   less than               &lt;                 same as R\n#   greater or equal        &gt;=                same as R\n#   less or equal           &lt;=                same as R\n#\n#   logical not             !                 not\n#   logical and             &&                and\n#   logical or              ||                or\n#\n#   CHARACTER OPERATORS\n#   concatenation           paste0(a,b)       a || b\n#\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n#\n#   NOTE - the following operators exist in sqlite but mean different \n#          things than what they mean in R.\n#          We will probably not cover the details of these operators\n#          (unless we have extra time) but you should be aware that\n#          they do NOT do the same thing as in R.\n#\n#                        R operator      sqlite operator\n#                        -----------     ---------------\n#   BITWISE OPERATORS - we will probably not cover these unless we have extra time.\n#\n#   bitwise \"and\"        not avaialble        &\n#   bitwise \"or\"         not avaialble        |\n#   bitwise \"not\"        not avaialble        ~\n#   left shift bits      not avaialble        &lt;&lt;\n#   right shift bits     not avaialble        &gt;&gt;\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#performing-calculations-with-sql",
    "href": "sql0010-introToSql-v023.html#performing-calculations-with-sql",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.18 Performing calculations with SQL",
    "text": "31.18 Performing calculations with SQL\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n# Using SQL to perform calculations  ####\n#\n# In the sqlite flavor of SQL you can perform calculations by simply\n# using SELECT without a FROM clause.\n#\n# Some other versions of SQL (e.g. Oracle) require a FROM clause in every select.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n\n# RESULTS of following query\n#\n#     3+2   23/5    23.0/5\n#       5      4       4.6\n\nsqldf(\"select 3+2, 23/5, 23.0 / 5\")   # 5   4   4.6 (headings show calculations)\n\n  3+2 23/5 23.0 / 5\n1   5    4      4.6\n\n# Same result but headings show Sum, IntegerDivision, RealDivision\n#\n#     Sum   IntegerDivision   RealDivision\n#       5                 4            4.6\n\nsqldf(\"select 3+2 as Sum, \n      23/5 as IntegerDivision, \n      23.0 / 5 as RealDivision\")   # 5   4   4.6   (same results with differnet headings)\n\n  Sum IntegerDivision RealDivision\n1   5               4          4.6\n\n# You don't actullay need the word \"as\" but it makes the code more readable.\n# The following will produce the same results as above.\n\nsqldf(\"select 3+2 Sum, \n      23/5 IntegerDivision, \n      23.0 / 5 RealDivision\")   # 5   4   4.6   (same results with differnet headings)\n\n  Sum IntegerDivision RealDivision\n1   5               4          4.6\n\n# The column heading must only use legal column heading characters.\n# If you want to add non-legal column-heading characters, you can put the\n# column heading in \"double quotes\".\n# The following adds spaces to some of the column headings.\n\nsqldf('select 3+2 \"Sum\", \n      23/5 \"Integer Division\", \n      23.0 / 5 \"Real Division\"')   # 5   4   4.6   (same results with differnet headings)\n\n  Sum Integer Division Real Division\n1   5                4           4.6\n\n# If you perform calculations that do not refer to the columns in a table,\n# you will get back the result of the calculation for every row of the table.\n# \n# The following will return multiple rows of the answers, one row for each\n# of the grades table.\n# \n# You generally don't want to do this, which is why we left off the FROM \n# clause in our earlier examples.\n\nsqldf('select 3+2 \"Sum\", \n      23/5 \"Integer Division\", \n      23.0 / 5 \"Real Division\"\n      from grades')                # you probably dont want the from clause here\n\n   Sum Integer Division Real Division\n1    5                4           4.6\n2    5                4           4.6\n3    5                4           4.6\n4    5                4           4.6\n5    5                4           4.6\n6    5                4           4.6\n7    5                4           4.6\n8    5                4           4.6\n9    5                4           4.6\n10   5                4           4.6\n11   5                4           4.6\n12   5                4           4.6\n13   5                4           4.6\n14   5                4           4.6\n15   5                4           4.6\n\nsqldf(\"select 3 as curve, student, test1, test1 + 3 as curved_test1 from grades\")\n\n   curve  student test1 curved_test1\n1      3      joe    70           73\n2      3      sue    80           83\n3      3     anne    90           93\n4      3    frank   100          103\n5      3      bob    30           33\n6      3 samantha   100          103\n7      3    larry    32           35\n8      3     bill    84           87\n9      3 lucricia    80           83\n10     3 margaret    59           62\n11     3    dalia    85           88\n12     3      zvi    90           93\n13     3     mike    90           93\n14     3    david    90           93\n15     3     dana   100          103",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#reminder---how-na-works-in-r",
    "href": "sql0010-introToSql-v023.html#reminder---how-na-works-in-r",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.19 Reminder - how NA works in R",
    "text": "31.19 Reminder - how NA works in R\nNOTE:\n\n“IDS1020-Intro to IDS” students can ignore this section.\n\n\n\n\n\n\n\nClick here to see the notes on NA (this is a review from last semester)\n\n\n\n\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Remember how NA works in R  ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANY expression that contains NA results in NA\n\nfranksSalary = 100\ntinasSalary = 200\nnancysSalary = NA\nnedsSalary = NA\n\n# all of the following result in NA\n\nfranksSalary &gt; nancysSalary  # NA\n\n[1] NA\n\nnancysSalary == nedsSalary   # NA\n\n[1] NA\n\nfranksSalary + nancysSalary  # NA\n\n[1] NA\n\n# The only way to test for NA in R is with the is.na function\n\nsalarys = c(100,200,NA,NA) \nsalarys == NA     # NA NA NA NA\n\n[1] NA NA NA NA\n\nis.na(salarys)    # FALSE FALSE TRUE TRUE\n\n[1] FALSE FALSE  TRUE  TRUE\n\n# In R, you can check for non-na values using not operator ( ! )\n\n!is.na(salarys)   # TRUE TRUE FALSE FALSE\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\n\n#..................................................................\n# Remember in R that when you perform any operation with NA\n# the result is NA. For example:\n#\n#    &gt; NA + 10\n#    [1] NA\n#\n#    &gt; NA == NA  \n#    [1] NA\n#\n#    &gt; NA != NA  \n#    [1] NA\n#   \n# SQL has the same approach to NULL values. Any operation with NULL\n# results in NULL.\n#..................................................................",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#null-in-sql",
    "href": "sql0010-introToSql-v023.html#null-in-sql",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.20 NULL in SQL",
    "text": "31.20 NULL in SQL\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# NULL values in SQL  ####\n#\n# In relational database tables, the value NULL is used to record \n# a value that is \"not known\". For example a salary value of NULL\n# is treated as a salary that hasn't been recorded in the database yet. \n# This is very different from a salary of 0 which would be the case if \n# someone is volunteering or donating their time.\n#\n# Note that when using sqldf to run SQL commands in R\n# the data is stored in an R dataframe. Therefore these types of values\n# appear in the dataframe as NA but the SQL statements use the term NULL.\n# In an actual relational database these values would also appear in the \n# table as NULL (not as NA)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#...............................................................\n# Check for values that are NULL in the following way:\n#     \n#     WHERE COLUMN_NAME IS NULL\n#     (see examples below)\n#\n# NOTE: DO NOT USE \"== NULL\". The == operator will NOT work with NULL\n#...............................................................\n\n# show all rows for which the test3 grade is NULL \n# \n# NOTE: Remember that R does not understand NULL, R uses NA.\n#       Remember that SQL understands NULL and does not understand NA.\n#       Since sqldf uses R dataframes, the sqldf function automatically \n#       translates NULL into NA and vice versa when using SQL code to \n#       access data in R dataframes.\n\nsqldf(\"select *\n      from grades\n      where test3 is NULL\")\n\n     id  student year honors test1 test2 test3 major\n1 s0100      joe   fr  FALSE    70    82    NA   IDS\n2 s0500      bob   so  FALSE    30    NA    NA   IDS\n3 s0700    larry   ju  FALSE    32    80    NA   FIN\n4 s1000 margaret   se  FALSE    59    47    NA   IDS\n\n# DON'T DO THE FOLLOWING !!! IT DOESN'T WORK!!!\n\nsqldf(\"select *\n      from grades\n      where test3 = NULL\")  # use IS NULL!!!\n\n[1] id      student year    honors  test1   test2   test3   major  \n&lt;0 rows&gt; (or 0-length row.names)\n\n#...............................................................\n# Check for values that aren't NULL in the following way:\n#     \n#     WHERE COLUMN_NAME IS NOT NULL\n#     (see examples below)\n#\n# NOTE: DO NOT USE \"!= NULL\". The != operator will NOT work with NULL\n#...............................................................\n\n# show all rows for which the test3 grade is NOT NULL (i.e. isn't NA)\n\nsqldf(\"select *\n      from grades\n      where test3 is NOT NULL\")\n\n      id  student year honors test1 test2 test3 major\n1  s0200      sue   fr  FALSE    80    80    79   IDS\n2  s0300     anne   fr   TRUE    90    95    92   IDS\n3  s0400    frank   so   TRUE   100    95    91   ACC\n4  s0600 samantha   so   TRUE   100    70    97   IDS\n5  s0800     bill   se  FALSE    84    90    92   ACC\n6  s0900 lucricia   fr   TRUE    80   100    88   IDS\n7  s1100    dalia   se  FALSE    85    92    93   FIN\n8  s1200      zvi   ju   TRUE    90    98    92   ACC\n9  s1300     mike   ju   TRUE    90    86    79   IDS\n10 s1400    david   se   TRUE    90    87    87   IDS\n11 s1500     dana   so  FALSE   100    93    91   FIN\n\n# DON'T DO THE FOLLOWING !!! IT DOESN'T WORK!!!\n\nsqldf(\"select *\n      from grades\n      where test3 != NULL\")   # use IS NOT NULL !!!\n\n[1] id      student year    honors  test1   test2   test3   major  \n&lt;0 rows&gt; (or 0-length row.names)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#practice-1",
    "href": "sql0010-introToSql-v023.html#practice-1",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.21 Practice",
    "text": "31.21 Practice\n\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n# QUESTION\n#\n# curve all test3 grades by 10 points (NULLs remain NULL)\n#\n# (remember that since we are using R, the NULL values\n#  appear as NA)\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nsqldf(\"select student, test3, test3 + 10 as curved_test3\n       from grades\n       order by student\")\n\n    student test3 curved_test3\n1      anne    92          102\n2      bill    92          102\n3       bob    NA           NA\n4     dalia    93          103\n5      dana    91          101\n6     david    87           97\n7     frank    91          101\n8       joe    NA           NA\n9     larry    NA           NA\n10 lucricia    88           98\n11 margaret    NA           NA\n12     mike    79           89\n13 samantha    97          107\n14      sue    79           89\n15      zvi    92          102\n\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n# QUESTION\n#\n# Show rows where both test2 and test3 are NULL\n# REMEMBER when looking for NULL values: \n#\n#    USE:        value IS NULL      # correct\n#    DON'T use:  value = NULL       # wrong\n#\n#    USE:        value IS NOT NULL  # correct\n#    DON'T use:  value != NULL      # wrong\n# . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nsqldf(\"select student, test1, test2, test3\n       from grades\n       where test2 IS NULL and test3 IS NULL  \n       order by student\")\n\n  student test1 test2 test3\n1     bob    30    NA    NA\n\n# DON'T DO THE FOLLOWING !!! IT DOESN'T WORK!!!\n\nsqldf(\"select student, test1, test2, test3\n       from grades\n       where test2 == NULL and test3 == NULL  \n       order by student\")\n\n[1] student test1   test2   test3  \n&lt;0 rows&gt; (or 0-length row.names)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#tutorials-sqlbolt.com-and-w3chools.com",
    "href": "sql0010-introToSql-v023.html#tutorials-sqlbolt.com-and-w3chools.com",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.22 Tutorials sqlbolt.com and w3chools.com",
    "text": "31.22 Tutorials sqlbolt.com and w3chools.com\nPlease see the tutorials at the following locations.\n\nhttps://sqlbolt.com/ These pages cover basic SELECT statements. They refer to the following tables\n\nmovies\nnorth_american_cities\n\nhttps://www.w3schools.com/sql/sql_join.asp This page teaches how to work with more than one table. They refer to the following tables\n\ncustomers\norders\n\n\nThe file &lt;sqlbolt_tables-v007.RData&gt; contains the dataframes with the information for each table that are used on these websites. These dataframes can be loaded into R from this file with the following command.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# sqlbolt.com    and    https://www.w3schools.com/sql/sql_join.asp    ####\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nload(\"sqlbolt_tables-v007.RData\")\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nclick to see movies table\n\n\n\n\n\n\nsqldf(\"select * from movies\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n\n\n\n\n\n\n\n\n\n\nclick to see orders table\n\n\n\n\n\n\nsqldf(\"select * from orders\")\n\n    OrderID CustomerID EmployeeID  OrderDate ShipperID\n1     10248         90          5   7/4/1996         3\n2     10249         81          6   7/5/1996         1\n3     10250         34          4   7/8/1996         2\n4     10251         84          3   7/8/1996         1\n5     10252         76          4   7/9/1996         2\n6     10253         34          3  7/10/1996         2\n7     10254         14          5  7/11/1996         2\n8     10255         68          9  7/12/1996         3\n9     10256         88          3  7/15/1996         2\n10    10257         35          4  7/16/1996         3\n11    10258         20          1  7/17/1996         1\n12    10259         13          4  7/18/1996         3\n13    10260         55          4  7/19/1996         1\n14    10261         61          4  7/19/1996         2\n15    10262         65          8  7/22/1996         3\n16    10263         20          9  7/23/1996         3\n17    10264         24          6  7/24/1996         3\n18    10265          7          2  7/25/1996         1\n19    10266         87          3  7/26/1996         3\n20    10267         25          4  7/29/1996         1\n21    10268         33          8  7/30/1996         3\n22    10269         89          5  7/31/1996         1\n23    10270         87          1   8/1/1996         1\n24    10271         75          6   8/1/1996         2\n25    10272         65          6   8/2/1996         2\n26    10273         63          3   8/5/1996         3\n27    10274         85          6   8/6/1996         1\n28    10275         49          1   8/7/1996         1\n29    10276         80          8   8/8/1996         3\n30    10277         52          2   8/9/1996         3\n31    10278          5          8  8/12/1996         2\n32    10279         44          8  8/13/1996         2\n33    10280          5          2  8/14/1996         1\n34    10281         69          4  8/14/1996         1\n35    10282         69          4  8/15/1996         1\n36    10283         46          3  8/16/1996         3\n37    10284         44          4  8/19/1996         1\n38    10285         63          1  8/20/1996         2\n39    10286         63          8  8/21/1996         3\n40    10287         67          8  8/22/1996         3\n41    10288         66          4  8/23/1996         1\n42    10289         11          7  8/26/1996         3\n43    10290         15          8  8/27/1996         1\n44    10291         61          6  8/27/1996         2\n45    10292         81          1  8/28/1996         2\n46    10293         80          1  8/29/1996         3\n47    10294         65          4  8/30/1996         2\n48    10295         85          2   9/2/1996         2\n49    10296         46          6   9/3/1996         1\n50    10297          7          5   9/4/1996         2\n51    10298         37          6   9/5/1996         2\n52    10299         67          4   9/6/1996         2\n53    10300         49          2   9/9/1996         2\n54    10301         86          8   9/9/1996         2\n55    10302         76          4  9/10/1996         2\n56    10303         30          7  9/11/1996         2\n57    10304         80          1  9/12/1996         2\n58    10305         55          8  9/13/1996         3\n59    10306         69          1  9/16/1996         3\n60    10307         48          2  9/17/1996         2\n61    10308          2          7  9/18/1996         3\n62    10309         37          3  9/19/1996         1\n63    10310         77          8  9/20/1996         2\n64    10311         18          1  9/20/1996         3\n65    10312         86          2  9/23/1996         2\n66    10313         63          2  9/24/1996         2\n67    10314         65          1  9/25/1996         2\n68    10315         38          4  9/26/1996         2\n69    10316         65          1  9/27/1996         3\n70    10317         48          6  9/30/1996         1\n71    10318         38          8  10/1/1996         2\n72    10319         80          7  10/2/1996         3\n73    10320         87          5  10/3/1996         3\n74    10321         38          3  10/3/1996         2\n75    10322         58          7  10/4/1996         3\n76    10323         39          4  10/7/1996         1\n77    10324         71          9  10/8/1996         1\n78    10325         39          1  10/9/1996         3\n79    10326          8          4 10/10/1996         2\n80    10327         24          2 10/11/1996         1\n81    10328         28          4 10/14/1996         3\n82    10329         75          4 10/15/1996         2\n83    10330         46          3 10/16/1996         1\n84    10331          9          9 10/16/1996         1\n85    10332         51          3 10/17/1996         2\n86    10333         87          5 10/18/1996         3\n87    10334         84          8 10/21/1996         2\n88    10335         37          7 10/22/1996         2\n89    10336         60          7 10/23/1996         2\n90    10337         25          4 10/24/1996         3\n91    10338         55          4 10/25/1996         3\n92    10339         51          2 10/28/1996         2\n93    10340          9          1 10/29/1996         3\n94    10341         73          7 10/29/1996         3\n95    10342         25          4 10/30/1996         2\n96    10343         44          4 10/31/1996         1\n97    10344         89          4  11/1/1996         2\n98    10345         63          2  11/4/1996         2\n99    10346         65          3  11/5/1996         3\n100   10347         21          4  11/6/1996         3\n101   10348         86          4  11/7/1996         2\n102   10349         75          7  11/8/1996         1\n103   10350         41          6 11/11/1996         2\n104   10351         20          1 11/11/1996         1\n105   10352         28          3 11/12/1996         3\n106   10353         59          7 11/13/1996         3\n107   10354         58          8 11/14/1996         3\n108   10355          4          6 11/15/1996         1\n109   10356         86          6 11/18/1996         2\n110   10357         46          1 11/19/1996         3\n111   10358         41          5 11/20/1996         1\n112   10359         72          5 11/21/1996         3\n113   10360          7          4 11/22/1996         3\n114   10361         63          1 11/22/1996         2\n115   10362          9          3 11/25/1996         1\n116   10363         17          4 11/26/1996         3\n117   10364         19          1 11/26/1996         1\n118   10365          3          3 11/27/1996         2\n119   10366         29          8 11/28/1996         2\n120   10367         83          7 11/28/1996         3\n121   10368         20          2 11/29/1996         2\n122   10369         75          8  12/2/1996         2\n123   10370         14          6  12/3/1996         2\n124   10371         41          1  12/3/1996         1\n125   10372         62          5  12/4/1996         2\n126   10373         37          4  12/5/1996         3\n127   10374         91          1  12/5/1996         3\n128   10375         36          3  12/6/1996         2\n129   10376         51          1  12/9/1996         2\n130   10377         72          1  12/9/1996         3\n131   10378         24          5 12/10/1996         3\n132   10379         61          2 12/11/1996         1\n133   10380         37          8 12/12/1996         3\n134   10381         46          3 12/12/1996         3\n135   10382         20          4 12/13/1996         1\n136   10383          4          8 12/16/1996         3\n137   10384          5          3 12/16/1996         3\n138   10385         75          1 12/17/1996         2\n139   10386         21          9 12/18/1996         3\n140   10387         70          1 12/18/1996         2\n141   10388         72          2 12/19/1996         1\n142   10389         10          4 12/20/1996         2\n143   10390         20          6 12/23/1996         1\n144   10391         17          3 12/23/1996         3\n145   10392         59          2 12/24/1996         3\n146   10393         71          1 12/25/1996         3\n147   10394         36          1 12/25/1996         3\n148   10395         35          6 12/26/1996         1\n149   10396         25          1 12/27/1996         3\n150   10397         60          5 12/27/1996         1\n151   10398         71          2 12/30/1996         3\n152   10399         83          8 12/31/1996         3\n153   10400         19          1   1/1/1997         3\n154   10401         65          1   1/1/1997         1\n155   10402         20          8   1/2/1997         2\n156   10403         20          4   1/3/1997         3\n157   10404         49          2   1/3/1997         1\n158   10405         47          1   1/6/1997         1\n159   10406         62          7   1/7/1997         1\n160   10407         56          2   1/7/1997         2\n161   10408         23          8   1/8/1997         1\n162   10409         54          3   1/9/1997         1\n163   10410         10          3  1/10/1997         3\n164   10411         10          9  1/10/1997         3\n165   10412         87          8  1/13/1997         2\n166   10413         41          3  1/14/1997         2\n167   10414         21          2  1/14/1997         3\n168   10415         36          3  1/15/1997         1\n169   10416         87          8  1/16/1997         3\n170   10417         73          4  1/16/1997         3\n171   10418         63          4  1/17/1997         1\n172   10419         68          4  1/20/1997         2\n173   10420         88          3  1/21/1997         1\n174   10421         61          8  1/21/1997         1\n175   10422         27          2  1/22/1997         1\n176   10423         31          6  1/23/1997         3\n177   10424         51          7  1/23/1997         2\n178   10425         41          6  1/24/1997         2\n179   10426         29          4  1/27/1997         1\n180   10427         59          4  1/27/1997         2\n181   10428         66          7  1/28/1997         1\n182   10429         37          3  1/29/1997         2\n183   10430         20          4  1/30/1997         1\n184   10431         10          4  1/30/1997         2\n185   10432         75          3  1/31/1997         2\n186   10433         60          3   2/3/1997         3\n187   10434         24          3   2/3/1997         2\n188   10435         16          8   2/4/1997         2\n189   10436          7          3   2/5/1997         2\n190   10437         87          8   2/5/1997         1\n191   10438         79          3   2/6/1997         2\n192   10439         51          6   2/7/1997         3\n193   10440         71          4  2/10/1997         2\n194   10441         55          3  2/10/1997         2\n195   10442         20          3  2/11/1997         2\n196   10443         66          8  2/12/1997         1\n\n\n\n\n\n\n\n\n\n\n\nclick to see customers table\n\n\n\n\n\n\nsqldf(\"select * from customers\")\n\n   CustomerID                         CustomerName              ContactName                                           Address             City PostalCode     Country\n1           1                  Alfreds Futterkiste             Maria Anders                                     Obere Str. 57           Berlin      12209     Germany\n2           2   Ana Trujillo Emparedados y helados             Ana Trujillo                  Avda. de la Constituci\\xf3n 2222   M\\xe9xico D.F.       5021      Mexico\n3           3           Antonio Moreno Taquer\\xeda           Antonio Moreno                                    Mataderos 2312   M\\xe9xico D.F.       5023      Mexico\n4           4                      Around the Horn             Thomas Hardy                                   120 Hanover Sq.           London    WA1 1DP          UK\n5           5                Berglunds snabbk\\xf6p       Christina Berglund                                 Berguvsv\\xe4gen 8         Lule\\xe5   S-958 22      Sweden\n6           6              Blauer See Delikatessen               Hanna Moos                                    Forsterstr. 57         Mannheim      68306     Germany\n7           7              Blondel p\\xe8re et fils Fr\\xe9d\\xe9rique Citeaux                               24, place Kl\\xe9ber       Strasbourg      67000      France\n8           8         B\\xf3lido Comidas preparadas         Mart\\xedn Sommer                                    C/ Araquil, 67           Madrid      28023       Spain\n9           9                             Bon app'        Laurence Lebihans                              12, rue des Bouchers        Marseille      13008      France\n10         10               Bottom-Dollar Marketse        Elizabeth Lincoln                                23 Tsawassen Blvd.        Tsawassen    T2F 8M4      Canada\n11         11                        B's Beverages        Victoria Ashworth                                 Fauntleroy Circus           London    EC2 5NT          UK\n12         12           Cactus Comidas para llevar         Patricio Simpson                                       Cerrito 333     Buenos Aires       1010   Argentina\n13         13           Centro comercial Moctezuma          Francisco Chang                           Sierras de Granada 9993   M\\xe9xico D.F.       5022      Mexico\n14         14                    Chop-suey Chinese                Yang Wang                                      Hauptstr. 29             Bern       3012 Switzerland\n15         15                  Com\\xe9rcio Mineiro             Pedro Afonso                           Av. dos Lus\\xedadas, 23     S\\xe3o Paulo  05432-043      Brazil\n16         16                Consolidated Holdings          Elizabeth Brown                       Berkeley Gardens 12 Brewery           London    WX1 6LT          UK\n17         17            Drachenblut Delikatessend             Sven Ottlieb                                      Walserweg 21           Aachen      52066     Germany\n18         18                      Du monde entier           Janine Labrune                      67, rue des Cinquante Otages           Nantes      44000      France\n19         19                   Eastern Connection                Ann Devon                                    35 King George           London    WX3 6FW          UK\n20         20                         Ernst Handel            Roland Mendel                                      Kirchgasse 6             Graz       8010     Austria\n21         21                   Familia Arquibaldo                Aria Cruz                                   Rua Or\\xf3s, 92     S\\xe3o Paulo  05442-030      Brazil\n22         22 FISSA Fabrica Inter. Salchichas S.A.               Diego Roel                                C/ Moralzarzal, 86           Madrid      28034       Spain\n23         23                    Folies gourmandes         Martine Ranc\\xe9                       184, chauss\\xe9e de Tournai            Lille      59000      France\n24         24                    Folk och f\\xe4 HB            Maria Larsson                                   \\xc5kergatan 24        Br\\xe4cke   S-844 67      Sweden\n25         25                       Frankenversand            Peter Franken                                 Berliner Platz 43       M\\xfcnchen      80805     Germany\n26         26                  France restauration           Carine Schmitt                                    54, rue Royale           Nantes      44000      France\n27         27                       Franchi S.p.A.            Paolo Accorti                               Via Monte Bianco 34           Torino      10100       Italy\n28         28       Furia Bacalhau e Frutos do Mar           Lino Rodriguez                            Jardim das rosas n. 32           Lisboa       1675    Portugal\n29         29         Galer\\xeda del gastr\\xf3nomo         Eduardo Saavedra                         Rambla de Catalu\\xf1a, 23        Barcelona       8022       Spain\n30         30               Godos Cocina T\\xedpica     Jos\\xe9 Pedro Freyre                                     C/ Romero, 33          Sevilla      41101       Spain\n31         31                  Gourmet Lanchonetes         Andr\\xe9 Fonseca                                   Av. Brasil, 442         Campinas  04876-786      Brazil\n32         32              Great Lakes Food Market            Howard Snyder                                  2732 Baker Blvd.           Eugene      97403         USA\n33         33                 GROSELLA-Restaurante           Manuel Pereira                      5\\xaa Ave. Los Palos Grandes          Caracas       1081   Venezuela\n34         34                        Hanari Carnes             Mario Pontes                                Rua do Pa\\xe7o, 67   Rio de Janeiro  05454-876      Brazil\n35         35                  HILARI\\xd3N-Abastos      Carlos Hern\\xe1ndez        Carrera 22 con Ave. Carlos Soublette #8-35 San Crist\\xf3bal       5022   Venezuela\n36         36           Hungry Coyote Import Store            Yoshi Latimer                    City Center Plaza 516 Main St.            Elgin      97827         USA\n37         37         Hungry Owl All-Night Grocers         Patricia McKenna                                  8 Johnstown Road             Cork                Ireland\n38         38                       Island Trading            Helen Bennett                         Garden House Crowther Way            Cowes   PO31 7PJ          UK\n39         39                   K\\xf6niglich Essen            Philip Cramer                                     Maubelstr. 90      Brandenburg      14776     Germany\n40         40                 La corne d'abondance            Daniel Tonini                            67, avenue de l'Europe       Versailles      78000      France\n41         41                     La maison d'Asie           Annette Roulet                             1 rue Alsace-Lorraine         Toulouse      31000      France\n42         42        Laughing Bacchus Wine Cellars          Yoshi Tannamuri                                      1900 Oak St.        Vancouver    V3F 2K1      Canada\n43         43                 Lazy K Kountry Store               John Steel                              12 Orchestra Terrace      Walla Walla      99362         USA\n44         44                  Lehmanns Marktstand           Renate Messner                                      Magazinweg 7   Frankfurt a.M.      60528     Germany\n45         45                    Let's Stop N Shop             Jaime Yorres                               87 Polk St. Suite 5    San Francisco      94117         USA\n46         46                    LILA-Supermercado       Carlos Gonz\\xe1lez Carrera 52 con Ave. Bol\\xedvar #65-98 Llano Largo     Barquisimeto       3508   Venezuela\n47         47                     LINO-Delicateses         Felipe Izquierdo                           Ave. 5 de Mayo Porlamar  I. de Margarita       4980   Venezuela\n48         48             Lonesome Pine Restaurant              Fran Wilson                                89 Chiaroscuro Rd.         Portland      97219         USA\n49         49         Magazzini Alimentari Riuniti         Giovanni Rovelli                           Via Ludovico il Moro 22          Bergamo      24100       Italy\n50         50                         Maison Dewey          Catherine Dewey                               Rue Joseph-Bens 532        Bruxelles     B-1180     Belgium\n51         51                    M\\xe8re Paillarde        Jean Fresni\\xe8re                                43 rue St. Laurent      Montr\\xe9al    H1J 1C3      Canada\n52         52               Morgenstern Gesundkost          Alexander Feuer                                       Heerstr. 22          Leipzig       4179     Germany\n53         53                          North/South           Simon Crowther                      South House 300 Queensbridge           London    SW7 1RZ          UK\n54         54         Oc\\xe9ano Atl\\xe1ntico Ltda.           Yvonne Moncada               Ing. Gustavo Moncada 8585 Piso 20-A     Buenos Aires       1010   Argentina\n55         55               Old World Delicatessen            Rene Phillips                                   2743 Bering St.        Anchorage      99508         USA\n56         56                Ottilies K\\xe4seladen      Henriette Pfalzheim                                Mehrheimerstr. 369          K\\xf6ln      50739     Germany\n57         57              Paris sp\\xe9cialit\\xe9s           Marie Bertrand                           265, boulevard Charonne            Paris      75012      France\n58         58         Pericles Comidas cl\\xe1sicas   Guillermo Fern\\xe1ndez                          Calle Dr. Jorge Cash 321   M\\xe9xico D.F.       5033      Mexico\n59         59                     Piccolo und mehr              Georg Pipps                                       Geislweg 14         Salzburg       5020     Austria\n60         60              Princesa Isabel Vinhoss         Isabel de Castro                         Estrada da sa\\xfade n. 58           Lisboa       1756    Portugal\n61         61                       Que Del\\xedcia         Bernardo Batista                           Rua da Panificadora, 12   Rio de Janeiro  02389-673      Brazil\n62         62                        Queen Cozinha        L\\xfacia Carvalho                      Alameda dos Can\\xe0rios, 891     S\\xe3o Paulo  05487-020      Brazil\n63         63                           QUICK-Stop              Horst Kloss                               Taucherstra\\xdfe 10        Cunewalde       1307     Germany\n64         64                        Rancho grande      Sergio Guti\\xe9rrez                            Av. del Libertador 900     Buenos Aires       1010   Argentina\n65         65           Rattlesnake Canyon Grocery             Paula Wilson                                   2817 Milton Dr.      Albuquerque      87110         USA\n66         66                   Reggiani Caseifici          Maurizio Moroni                            Strada Provinciale 124    Reggio Emilia      42100       Italy\n67         67                   Ricardo Adocicados           Janete Limeira                               Av. Copacabana, 267   Rio de Janeiro  02389-890      Brazil\n68         68                   Richter Supermarkt             Michael Holz                                 Grenzacherweg 237        Gen\\xe8ve       1203 Switzerland\n69         69                     Romero y tomillo         Alejandra Camino                                    Gran V\\xeda, 1           Madrid      28001       Spain\n70         70                     Sant\\xe9 Gourmet         Jonas Bergulfsen                            Erling Skakkes gate 78          Stavern       4110      Norway\n71         71                   Save-a-lot Markets           Jose Pavarotti                                   187 Suffolk Ln.            Boise      83720         USA\n72         72                   Seven Seas Imports               Hari Kumar                                   90 Wadhurst Rd.           London   OX15 4NB          UK\n73         73                        Simons bistro           Jytte Petersen                                   Vinb\\xe6ltet 34     K\\xf8benhavn       1734     Denmark\n74         74           Sp\\xe9cialit\\xe9s du monde        Dominique Perrier                                 25, rue Lauriston            Paris      75016      France\n75         75                Split Rail Beer & Ale       Art Braunschweiger                                      P.O. Box 555           Lander      82520         USA\n76         76               Supr\\xeames d\\xe9lices         Pascale Cartrain                              Boulevard Tirou, 255        Charleroi     B-6000     Belgium\n77         77                       The Big Cheese                Liz Nixon                          89 Jefferson Way Suite 2         Portland      97201         USA\n78         78                      The Cracker Box                 Liu Wong                               55 Grizzly Peak Rd.            Butte      59801         USA\n79         79                Toms Spezialit\\xe4ten            Karin Josephs                                     Luisenstr. 48       M\\xfcnster      44087     Germany\n80         80                  Tortuga Restaurante     Miguel Angel Paolino                                  Avda. Azteca 123   M\\xe9xico D.F.       5033      Mexico\n81         81         Tradi\\xe7\\xe3o Hipermercados        Anabela Domingues                        Av. In\\xeas de Castro, 414     S\\xe3o Paulo  05634-030      Brazil\n82         82    Trail's Head Gourmet Provisioners           Helvetius Nagy                                 722 DaVinci Blvd.         Kirkland      98034         USA\n83         83                         Vaffeljernet              Palle Ibsen                                  Smagsl\\xf8get 45         \\xc5rhus       8200     Denmark\n84         84                 Victuailles en stock             Mary Saveley                                2, rue du Commerce             Lyon      69004      France\n85         85            Vins et alcools Chevalier             Paul Henriot                                59 rue de l'Abbaye            Reims      51100      France\n86         86                    Die Wandernde Kuh           Rita M\\xfcller                                 Adenauerallee 900        Stuttgart      70563     Germany\n87         87                       Wartian Herkku         Pirkko Koskitalo                                       Torikatu 38             Oulu      90110     Finland\n88         88               Wellington Importadora            Paula Parente                                Rua do Mercado, 12          Resende  08737-363      Brazil\n89         89                 White Clover Markets           Karl Jablonski                       305 - 14th Ave. S. Suite 3B          Seattle      98128         USA\n90         90                          Wilman Kala          Matti Karttunen                                     Keskuskatu 45         Helsinki      21240     Finland\n91         91                               Wolski                  Zbyszek                                   ul. Filtrowa 68            Walla     01-012      Poland\n\n\n\n\n\n\n\n\n\n\n\nclick to see north_american_cities table\n\n\n\n\n\n\nsqldf(\"select * from north_american_cities\")\n\n                  city       country population latitude  longitude\n1          Guadalajara        Mexico    1500800 20.65970 -103.34961\n2              Toronto        Canada    2795060 43.65323  -79.38318\n3              Houston United States    2195914 29.76043  -95.36980\n4             New York United States    8405837 40.71278  -74.00594\n5         Philadelphia United States    1553165 39.95258  -75.16522\n6               Havana          Cuba    2106146 23.05407  -82.34519\n7          Mexico City        Mexico    8555500 19.43261  -99.13321\n8              Phoenix United States    1513367 33.44838 -112.07404\n9          Los Angeles United States    3884307 34.05223 -118.24368\n10 Ecatepec de Morelos        Mexico    1742000 19.60184  -99.05067\n11            Montreal        Canada    1717767 45.50169  -73.56726\n12             Chicago United States    2718782 41.87811  -87.62980",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#practice---select-clause",
    "href": "sql0010-introToSql-v023.html#practice---select-clause",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.23 Practice - select clause",
    "text": "31.23 Practice - select clause\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# https://sqlbolt.com/lesson/select_queries_introduction\n#\n# Concepts:\n#\n#  Select query for a specific columns\n#\n#   SELECT column, another_column, .\n#   FROM mytable;\n#\n#\n#  Select query for all columns\n#   \n#    SELECT * \n#    FROM mytable;\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#https://sqlbolt.com/lesson/select_queries_introduction\n\nmovies  \n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n###########################\n#Exercise 1 - Tasks\n###########################\n\n#Find the title of each film\n\nsql = \n  \"select title\n   from movies\"\n\nsql\n\n[1] \"select title\\n   from movies\"\n\nsqldf(sql)\n\n                 title\n1            Toy Story\n2         A Bug's Life\n3          Toy Story 2\n4       Monsters, Inc.\n5         Finding Nemo\n6      The Incredibles\n7                 Cars\n8          Ratatouille\n9               WALL-E\n10                  Up\n11         Toy Story 3\n12              Cars 2\n13               Brave\n14 Monsters University\n15              WALL-G\n\n#Find the director of each film\n\nsql = \n  \"select director\n   from movies\"\n\nsqldf(sql)\n\n         director\n1   John Lasseter\n2   John Lasseter\n3   John Lasseter\n4     Pete Docter\n5  Andrew Stanton\n6       Brad Bird\n7   John Lasseter\n8       Brad Bird\n9  Andrew Stanton\n10    Pete Docter\n11    Lee Unkrich\n12  John Lasseter\n13 Brenda Chapman\n14    Dan Scanlon\n15 Brenda Chapman\n\n#Find the title and director of each film\n\nsql = \n  \"select title, director\n   from movies\"\n\nsqldf(sql)\n\n                 title       director\n1            Toy Story  John Lasseter\n2         A Bug's Life  John Lasseter\n3          Toy Story 2  John Lasseter\n4       Monsters, Inc.    Pete Docter\n5         Finding Nemo Andrew Stanton\n6      The Incredibles      Brad Bird\n7                 Cars  John Lasseter\n8          Ratatouille      Brad Bird\n9               WALL-E Andrew Stanton\n10                  Up    Pete Docter\n11         Toy Story 3    Lee Unkrich\n12              Cars 2  John Lasseter\n13               Brave Brenda Chapman\n14 Monsters University    Dan Scanlon\n15              WALL-G Brenda Chapman\n\n#Find the title and year of each film\n\nsql = \n  \"select title, year\n   from movies\"\n\nsqldf(sql)\n\n                 title year\n1            Toy Story 1995\n2         A Bug's Life 1998\n3          Toy Story 2 1999\n4       Monsters, Inc. 2001\n5         Finding Nemo 2003\n6      The Incredibles 2004\n7                 Cars 2006\n8          Ratatouille 2007\n9               WALL-E 2008\n10                  Up 2009\n11         Toy Story 3 2010\n12              Cars 2 2011\n13               Brave 2012\n14 Monsters University 2013\n15              WALL-G 2042\n\n#Find all the information about each film\n\nsqldf(\"select *\n       from movies\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n# The following produces the exact same results as above\n\nsqldf(\"select id, title, director, year, length_minutes\n       from movies\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#practice---additional-questions-not-on-sqlbolt.com",
    "href": "sql0010-introToSql-v023.html#practice---additional-questions-not-on-sqlbolt.com",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.24 Practice - ADDITIONAL QUESTIONS (not on sqlbolt.com)",
    "text": "31.24 Practice - ADDITIONAL QUESTIONS (not on sqlbolt.com)\n\n# QUESTION\n# Show the movie name, length_minutes and a third column that you \n# calculate which is the length in hours\n\n# ANSWER\n#\n# when you divide an integer by an integer the result in sql in an integer\n# if you divide a floating point number by an integer or vice versa the \n# result is floating point (i.e. a number that has values after the decimal point)\n\n# This is a good start but the length_hours column will always show up \n# as a whole number - this is NOT what we wanted.\n\nsql = \"select title, length_minutes, length_minutes/60 as length_hours\n       from movies\"\nsqldf(sql)\n\n                 title length_minutes length_hours\n1            Toy Story             81            1\n2         A Bug's Life             95            1\n3          Toy Story 2             93            1\n4       Monsters, Inc.             92            1\n5         Finding Nemo            107            1\n6      The Incredibles            116            1\n7                 Cars            117            1\n8          Ratatouille            115            1\n9               WALL-E            104            1\n10                  Up            101            1\n11         Toy Story 3            103            1\n12              Cars 2            120            2\n13               Brave            102            1\n14 Monsters University            110            1\n15              WALL-G             92            1\n\n# Change 60 to 60.0 to produce a \"floating point\" value (i.e. a value\n# that contains numbers after the decimal point)\n\nsql = \"select title, length_minutes, length_minutes/60.0 as length_hours\n       from movies\"\nsqldf(sql)\n\n                 title length_minutes length_hours\n1            Toy Story             81     1.350000\n2         A Bug's Life             95     1.583333\n3          Toy Story 2             93     1.550000\n4       Monsters, Inc.             92     1.533333\n5         Finding Nemo            107     1.783333\n6      The Incredibles            116     1.933333\n7                 Cars            117     1.950000\n8          Ratatouille            115     1.916667\n9               WALL-E            104     1.733333\n10                  Up            101     1.683333\n11         Toy Story 3            103     1.716667\n12              Cars 2            120     2.000000\n13               Brave            102     1.700000\n14 Monsters University            110     1.833333\n15              WALL-G             92     1.533333\n\n# QUESTION\n# show the length in hours, minutes - eg. 81 minutes show as 2 hours and 21 minutes\n\n# ANSWER\nsql =\n  \"select title, length_minutes, length_minutes / 60 as hours,\n                  length_minutes - 60 * (length_minutes/60) as minutes\n   from movies\"\nsqldf(sql)\n\n                 title length_minutes hours minutes\n1            Toy Story             81     1      21\n2         A Bug's Life             95     1      35\n3          Toy Story 2             93     1      33\n4       Monsters, Inc.             92     1      32\n5         Finding Nemo            107     1      47\n6      The Incredibles            116     1      56\n7                 Cars            117     1      57\n8          Ratatouille            115     1      55\n9               WALL-E            104     1      44\n10                  Up            101     1      41\n11         Toy Story 3            103     1      43\n12              Cars 2            120     2       0\n13               Brave            102     1      42\n14 Monsters University            110     1      50\n15              WALL-G             92     1      32\n\n# ANSWER - using sqlite's % operator\nsql =\n  \"select title, length_minutes, length_minutes / 60 as hours,\n                  length_minutes % 60 as minutes\n   from movies\"\nsqldf(sql)\n\n                 title length_minutes hours minutes\n1            Toy Story             81     1      21\n2         A Bug's Life             95     1      35\n3          Toy Story 2             93     1      33\n4       Monsters, Inc.             92     1      32\n5         Finding Nemo            107     1      47\n6      The Incredibles            116     1      56\n7                 Cars            117     1      57\n8          Ratatouille            115     1      55\n9               WALL-E            104     1      44\n10                  Up            101     1      41\n11         Toy Story 3            103     1      43\n12              Cars 2            120     2       0\n13               Brave            102     1      42\n14 Monsters University            110     1      50\n15              WALL-G             92     1      32\n\n# Note that while it is tempting to refer to hours on line 2 of the following\n# query, R's version of SQL (ie. sqlite) will not allow for this.\n# Some SQL flavors will allow this.\n#\n# The following WILL NOT WORK IN R's VERSION OF SQL\nsql =\n  \"select title, length_minutes, length_minutes / 60 as hours,\n                  length_minutes - 60 * hours as minutes\n   from movies\"\n\n# sqldf(sql)       # ERROR - no such column: hours",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#andor-between-and-x-in-some-list",
    "href": "sql0010-introToSql-v023.html#andor-between-and-x-in-some-list",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.25 AND/OR     BETWEEN … AND …     x IN (some list)",
    "text": "31.25 AND/OR     BETWEEN … AND …     x IN (some list)\nWHERE conditions can include any/all of the following\n\nAND/OR (and is done before or), you can use (parentheses) to change the order of operations\nx BETWEEN y AND z\n\nif x is numeric then no qoutes - e.g. \n\nWHERE x BETWEEN 100 AND 200\n\nif x is TEXT (i.e. character) then values have ‘single quotes’ - e.g. \n\nWHERE x BETWEEN ‘A’ AND ‘D’ – includes all text that starts with A,B or C (not D)\n\n\nx IN (‘a’,‘list’,‘of’,‘values’)\n\nif x is numeric then the numbers in parentheses do NOT have quotes, e.g. \n\nWHERE x IN (100,200,300)\n\nif x is TEXT (ie. character) then the values in parentheses must have ‘single quotes’\n\nWHERE x IN (‘value1’, ‘value2’, ‘value3’)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#practice---where-clause",
    "href": "sql0010-introToSql-v023.html#practice---where-clause",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.26 Practice - where clause",
    "text": "31.26 Practice - where clause\n\n#-------------------------------------------------------------------------\n# https://sqlbolt.com/lesson/select_queries_with_constraints\n# \n# CONCEPTS\n#\n# Select query with constraints\n# SELECT column, another_column, .\n# FROM mytable\n# WHERE condition\n#       AND/OR another_condition\n#       AND/OR another_conditiion, etc\n# \n# \n# \n# # List of SQL operators (see webpage) \n# Operator  Condition   SQL Example\n# =, !=, &lt; &lt;=, &gt;, &gt;=    Standard numerical operators    col_name != 4\n# BETWEEN . AND .   Number is within range of two values (inclusive)    col_name BETWEEN 1.5 AND 10.5\n# NOT BETWEEN . AND .   Number is not within range of two values (inclusive)    col_name NOT BETWEEN 1 AND 10\n# IN (.)    Number exists in a list     col_name IN (2, 4, 6)\n# NOT IN (.)    Number does not exist in a list     col_name NOT IN (1, 3, 5)\n\n###########################\n# Exercise 2 - Tasks\n###########################\n \n# Find the movie with a row id of 6\n\nsql = \"\n  select *\n  from movies\n  where id=6\"\n\nsqldf(sql)\n\n\n\n###############################################################.\n#  BETWEEN ... AND ...\n#\n#    and \n#\n#  NOT BETWEEN ... AND ...\n###############################################################.\n\n# Find the movies released in the years between 2000 and 2010\n\nsql = \"select * \n       from movies\n       where year &gt;= 2000 and year &lt;= 2010 \"\nsqldf(sql)\n\n# you can combine the above where clause with a different select clause:\n# For example:\n\nsql = \"select title, length_minutes, length_minutes / 60 as hours,\n                  length_minutes - 60 * (length_minutes/60) as minutes,\n                  year\n       from movies\n       where year &gt;= 2000 and year &lt;= 2010 \"\nsqldf(sql)\n\n# use the BETWEEN ... AND ...\n# to do the same thing\n\nsql = \"select * \n       from movies\n       where year between 2000 and 2010\"\n\nsqldf(sql)\n\n# Find the movies not released in the years between 2000 and 2010\n\nsql = \"select * \n       from movies\n       where year &lt; 2000 or year &gt; 2010\"\nsqldf(sql)\n\n# use the NOT BETWEEN ... AND ...\n# to do the same thing\n\nsql = \"select * \n       from movies\n       where year not between 2000 and 2010\"\n\nsqldf(sql)\n\n31.26.1 limit clause\n\n###############################################################.\n# LIMIT &lt;M&gt;       and     LIMIT &lt;M&gt; OFFSET &lt;N&gt;\n###############################################################.\n#\n# The LIMIT clause must come at the very end of the SQL \n# statement. For example: \n#   \n#      SELECT *\n#      FROM grades\n#      ORDER BY student\n#      LIMIT 3\n# \n# Displayes the first 3 students.\n#\n# In general:\n#\n#      &lt;SOME SELECT STATEMENT&gt;\n#      limit M\n#\n# (where M is an integer) results in only the first\n# M rows of data from what would normally have been displayed \n# had the LIMIT clause not been specified.\n# \n#      &lt;SOME SELECT STATEMENT&gt;\n#      limit M offset N\n#\n# (where both M and N are integers) \n# starts the output from the N+1'th row of what would normally \n# have been displayed without the limit clause and then \n# displays the next M rows of data.\n###############################################################.\n\n\n\n31.26.2 PRACTICE\n\n# Find the first 5 movies and their release year\n\n# We will learn the best way to do this a little later. The\n# answer on the sqlbolt website is not ideal.\n\nsqldf(\"select * \n      from movies\n      order by year\")\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n# The limit clause allows to retrieve the first several values from an \n# ordered query\n\nsqldf(\"select * \n      from movies\n      order by year\n      limit 5\")\n\n  id          title       director year length_minutes\n1  1      Toy Story  John Lasseter 1995             81\n2  2   A Bug's Life  John Lasseter 1998             95\n3  3    Toy Story 2  John Lasseter 1999             93\n4  4 Monsters, Inc.    Pete Docter 2001             92\n5  5   Finding Nemo Andrew Stanton 2003            107\n\n# Get the next 2 movies\n\nsqldf(\"select * \n      from movies\n      order by year\n      limit 2 offset 5\")\n\n  id           title      director year length_minutes\n1  6 The Incredibles     Brad Bird 2004            116\n2  7            Cars John Lasseter 2006            117\n\n# Find all movies that were released in 2000, 2004 or 2008\n\nsql = \n  \"select * \n  from movies\n  where year = 2000 or year = 2004 or year = 2008\"\n\nsqldf(sql)\n\n  id           title       director year length_minutes\n1  6 The Incredibles      Brad Bird 2004            116\n2  9          WALL-E Andrew Stanton 2008            104\n\n# Do this again using the IN operator\n#\n# The IN operators takes a LIST of information\n# A LIST is a set of values in parentheses, separated by commans.\n\nsql = \n  \"select * \n  from movies\n  where year in (2000,2004,2008)\"\nsqldf(sql)\n\n  id           title       director year length_minutes\n1  6 The Incredibles      Brad Bird 2004            116\n2  9          WALL-E Andrew Stanton 2008            104\n\n# show the movies that were released in years other than 2000, 2004, 2008\n\nsql = \n  \"select * \n  from movies\n  where year != 2000 and year != 2004 and year != 2008\"\n\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   7                Cars  John Lasseter 2006            117\n7   8         Ratatouille      Brad Bird 2007            115\n8  10                  Up    Pete Docter 2009            101\n9  11         Toy Story 3    Lee Unkrich 2010            103\n10 12              Cars 2  John Lasseter 2011            120\n11 13               Brave Brenda Chapman 2012            102\n12 14 Monsters University    Dan Scanlon 2013            110\n13 87              WALL-G Brenda Chapman 2042             92\n\n# A student asked if we could change the and's in the previous question\n# to be or's instead. The answer is you can but in addition to \n# changing the and's to or's you must also modify the logical expression\n# in other ways. Specifically, DeMorgan's law describes how to \n# rewrite any logical expression that uses and's and or's into an\n# equivalent expression.\n#\n# DeMorgans law says: to convert a logical expression \n# step 1: negate it twice (ie. use two not's)\n# Step 2: (a) distribute one of the not's over the expression\n#         (b) change the and's to or's \n#         (c) change the or's to and's\nsql = \n  \"select * \n  from movies\n  where not ( year = 2000 or year = 2004 or year = 2008)\"\n\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   7                Cars  John Lasseter 2006            117\n7   8         Ratatouille      Brad Bird 2007            115\n8  10                  Up    Pete Docter 2009            101\n9  11         Toy Story 3    Lee Unkrich 2010            103\n10 12              Cars 2  John Lasseter 2011            120\n11 13               Brave Brenda Chapman 2012            102\n12 14 Monsters University    Dan Scanlon 2013            110\n13 87              WALL-G Brenda Chapman 2042             92\n\n# do this with the \"not in\" operator\n\nsql = \n  \"select * \n  from movies\n  where year not in (2000,2004,2008)\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   7                Cars  John Lasseter 2006            117\n7   8         Ratatouille      Brad Bird 2007            115\n8  10                  Up    Pete Docter 2009            101\n9  11         Toy Story 3    Lee Unkrich 2010            103\n10 12              Cars 2  John Lasseter 2011            120\n11 13               Brave Brenda Chapman 2012            102\n12 14 Monsters University    Dan Scanlon 2013            110\n13 87              WALL-G Brenda Chapman 2042             92\n\n# warning - the \"in\" operator is followed by a list of values.\n# The list of values is in parentheses.\n#\n# if the values are numbers there are no quotes e.g. (2000,2004,2005)\n#\n# If the values are character values then in SQL you must use\n# 'single quotes' around the different values.\n#\n# See the next question\n\n\n# Write a SQL SELECT statment to show all the movies that\n# were directed by any of the following people:\n#     Pete Docter , Brad Bird, Dan Scanlon\n#\n# (a) Write it using the IN operator\n# (b) write it without using the IN operator\n\n# ANSWER - part (a)\nsql = \"select * \n       from movies\n       where director IN ('Pete Docter', 'Brad Bird', 'Dan Scanlon')\n       order by director\"\n\nsqldf(sql)\n\n  id               title    director year length_minutes\n1  6     The Incredibles   Brad Bird 2004            116\n2  8         Ratatouille   Brad Bird 2007            115\n3 14 Monsters University Dan Scanlon 2013            110\n4  4      Monsters, Inc. Pete Docter 2001             92\n5 10                  Up Pete Docter 2009            101\n\n# ANSWER - part (b) - without the IN operator\nsql = \"select * \n       from movies\n       where director='Pete Docter' or  director='Brad Bird' or director = 'Dan Scanlon'\n       order by director\"\n\nsqldf(sql)\n\n  id               title    director year length_minutes\n1  6     The Incredibles   Brad Bird 2004            116\n2  8         Ratatouille   Brad Bird 2007            115\n3 14 Monsters University Dan Scanlon 2013            110\n4  4      Monsters, Inc. Pete Docter 2001             92\n5 10                  Up Pete Docter 2009            101\n\n\n\n\n31.26.3 The “like” operator\n\n######################################################################.\n# The \"like\" operator\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The like operator in sql uses a \"pattern\" to match data values\n# This is similar to regular expressions. However, the SQL like \n# operator uses very different meta characters. \n#\n# The two standard metacharacters that are used by the sql like \n# operator in the pattern are:\n#\n#   %    will \"match\" 0 or more characters  (equivalent to .* in regex)\n#\n#   _    i.e. an underscore, will \"match\" exactly one character (equivalent to . in regex)\n#\n# Some DBMS software allows for additional metacharacters in LIKE clauses\n# For example, MySql allows for the following. (sqlite does not)\n#\n#   [abc]  MySQL DBMS software recognizes these \"character classes\" \n#   [a-c]  similar to regular expressions. Not all DBMS software recognizes\n#   [^a-c] these. sqlite does not.\n######################################################################.\n\n\n\n31.26.4 PRACTICE\n\n# exercise 3\n\n# Exercise 3 - Tasks\n# Find all the Toy Story movies\nsql = \"select *\n       from movies\n       where title like 'Toy Story%'\"\nsqldf(sql)\n\n  id       title      director year length_minutes\n1  1   Toy Story John Lasseter 1995             81\n2  3 Toy Story 2 John Lasseter 1999             93\n3 11 Toy Story 3   Lee Unkrich 2010            103\n\n# Find all the movies directed by John Lasseter\n\nsql = \"select *\n       from movies\n       where director = 'John Lasseter'\"\nsqldf(sql)\n\n  id        title      director year length_minutes\n1  1    Toy Story John Lasseter 1995             81\n2  2 A Bug's Life John Lasseter 1998             95\n3  3  Toy Story 2 John Lasseter 1999             93\n4  7         Cars John Lasseter 2006            117\n5 12       Cars 2 John Lasseter 2011            120\n\n# Find all the movies (and director) not directed by John Lasseter\nsql = \"select * \n       from movies \n       where director != 'John Lasseter'\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   4      Monsters, Inc.    Pete Docter 2001             92\n2   5        Finding Nemo Andrew Stanton 2003            107\n3   6     The Incredibles      Brad Bird 2004            116\n4   8         Ratatouille      Brad Bird 2007            115\n5   9              WALL-E Andrew Stanton 2008            104\n6  10                  Up    Pete Docter 2009            101\n7  11         Toy Story 3    Lee Unkrich 2010            103\n8  13               Brave Brenda Chapman 2012            102\n9  14 Monsters University    Dan Scanlon 2013            110\n10 87              WALL-G Brenda Chapman 2042             92\n\n# The following will work but is NOT recommended (no pun intended)\nsql = \"select * \n       from movies \n       where NOT ( director = 'John Lasseter' ) \"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   4      Monsters, Inc.    Pete Docter 2001             92\n2   5        Finding Nemo Andrew Stanton 2003            107\n3   6     The Incredibles      Brad Bird 2004            116\n4   8         Ratatouille      Brad Bird 2007            115\n5   9              WALL-E Andrew Stanton 2008            104\n6  10                  Up    Pete Docter 2009            101\n7  11         Toy Story 3    Lee Unkrich 2010            103\n8  13               Brave Brenda Chapman 2012            102\n9  14 Monsters University    Dan Scanlon 2013            110\n10 87              WALL-G Brenda Chapman 2042             92\n\n# Find all the WALL-* movies\nsql = \"select *\n       from movies\n       where title like 'WALL-_'\"\nsqldf(sql)\n\n  id  title       director year length_minutes\n1  9 WALL-E Andrew Stanton 2008            104\n2 87 WALL-G Brenda Chapman 2042             92\n\nmovies\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n# QUESTION \n# Show all movies that start with a \"T\" and end with an \"s\"\n\nsql = \"select * \n       from movies\n       where title like 'T%s'\n       order by title\"\nsqldf(sql)\n\n  id           title  director year length_minutes\n1  6 The Incredibles Brad Bird 2004            116\n\n# QUESTION \n# Show all movies whose title is exactly 6 charcters (including spaces)\n\nsql = \"select * \n       from movies\n       where title like '______'\n       order by title\"\nsqldf(sql)\n\n  id  title       director year length_minutes\n1 12 Cars 2  John Lasseter 2011            120\n2  9 WALL-E Andrew Stanton 2008            104\n3 87 WALL-G Brenda Chapman 2042             92\n\n# QUESTION\n# Show all movies whose title starts with A,B or C\n# and whose title is 10 characters or longer\n\nsql =\"select title\n      from movies\n      where title BETWEEN 'A' AND 'D' and\n            title LIKE '__________%'\"\nsqldf(sql)\n\n         title\n1 A Bug's Life\n\n# QUESTION\n# Show all movies whose title starts with A,B or C\n# and whose title is fewer than 10 characters or long\n\nsql =\"select title\n      from movies\n      where title BETWEEN 'A' AND 'D' and\n            title NOT LIKE '__________%'\"\nsqldf(sql)\n\n   title\n1   Cars\n2 Cars 2\n3  Brave\n\nsql=\"select substr(title, 1,1) from movies order by 1\"\nsqldf(sql)\n\n   substr(title, 1,1)\n1                   A\n2                   B\n3                   C\n4                   C\n5                   F\n6                   M\n7                   M\n8                   R\n9                   T\n10                  T\n11                  T\n12                  T\n13                  U\n14                  W\n15                  W\n\nsqldf(\"select * from movies where substr(title,1,1) = 'M'\")\n\n  id               title    director year length_minutes\n1  4      Monsters, Inc. Pete Docter 2001             92\n2 14 Monsters University Dan Scanlon 2013            110\n\nsqldf(\"select * from movies where title &lt; 'D'\")\n\n  id        title       director year length_minutes\n1  2 A Bug's Life  John Lasseter 1998             95\n2  7         Cars  John Lasseter 2006            117\n3 12       Cars 2  John Lasseter 2011            120\n4 13        Brave Brenda Chapman 2012            102\n\n\n\n\n31.26.5 SELECT DISTINCT …\n\n################################################################.\n# SELECT DISTINCT ...\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Depending on which values are selected, a SELECT ... statement\n# might return exactly the same row in the output multiple times. \n#\n# To instead, show only one copy of each row in the output,\n# start the statement with  SELECT DISTINCT ... \n# (instead of just SELECT ...)\n#\n# See the examples below.\n################################################################.\n\n\n# The output of the following query includes some rows\n# that are exactly the same as other rows.\n\nsqldf(\"select year, honors, major \n      from grades\n      where honors = TRUE and major = 'IDS'\n      order by year\")\n\n  year honors major\n1   fr   TRUE   IDS\n2   fr   TRUE   IDS\n3   ju   TRUE   IDS\n4   se   TRUE   IDS\n5   so   TRUE   IDS\n\n# The following query is exactly the same as the previous query, except\n# that the following query includes the word DISTINCT at the beginning\n# of the query.\n#\n# This version of the query does not include any duplicated rows in the output.\n\nsqldf(\"select DISTINCT year, honors, major \n      from grades\n      where honors = TRUE and major = 'IDS'\n      order by year, major\")\n\n  year honors major\n1   fr   TRUE   IDS\n2   ju   TRUE   IDS\n3   se   TRUE   IDS\n4   so   TRUE   IDS\n\nsqldf(\"select DISTINCT student, year, honors, major \n      from grades\n      where honors = TRUE and major = 'IDS'\n      order by year, major\")\n\n   student year honors major\n1     anne   fr   TRUE   IDS\n2 lucricia   fr   TRUE   IDS\n3     mike   ju   TRUE   IDS\n4    david   se   TRUE   IDS\n5 samantha   so   TRUE   IDS\n\n\n\n\n31.26.6 Practice\n\n\n\n\n\n\nclick here for practice\n\n\n\n\n\n\n# Exercise 4\n\n# Show just the director column\nsql = \"select director \n       from movies\"\nsqldf(sql)\n\n         director\n1   John Lasseter\n2   John Lasseter\n3   John Lasseter\n4     Pete Docter\n5  Andrew Stanton\n6       Brad Bird\n7   John Lasseter\n8       Brad Bird\n9  Andrew Stanton\n10    Pete Docter\n11    Lee Unkrich\n12  John Lasseter\n13 Brenda Chapman\n14    Dan Scanlon\n15 Brenda Chapman\n\n# List all directors of movies, without duplicates\n\nsql = \"select DISTINCT director \n       from movies\"\nsqldf(sql)\n\n        director\n1  John Lasseter\n2    Pete Docter\n3 Andrew Stanton\n4      Brad Bird\n5    Lee Unkrich\n6 Brenda Chapman\n7    Dan Scanlon\n\n# List all directors of movies (alphabetically), without duplicates\nsql = \"select DISTINCT director \n       from movies\n       order by director\"\nsqldf(sql)\n\n        director\n1 Andrew Stanton\n2      Brad Bird\n3 Brenda Chapman\n4    Dan Scanlon\n5  John Lasseter\n6    Lee Unkrich\n7    Pete Docter\n\n# Both the where clause and the order by clause may refer to information\n# that is not actually displayed by the select clause\n# \n# For example\n\n# List the title and year for all movies directed by John Lasseter.\n# Do not John Lasseter's name in the output.\nsql = \"select title, year \n       from movies\n       where director = 'John Lasseter'\"\nsqldf(sql)\n\n         title year\n1    Toy Story 1995\n2 A Bug's Life 1998\n3  Toy Story 2 1999\n4         Cars 2006\n5       Cars 2 2011\n\n# List the title of all movies in \"reverse chronological order\". \n# In other words, the most recent movie should be listed first and the \n# oldest movie listed last.\n# Do NOT show the actual year in the output.\nsql = \"select title \n       from movies\n       order by year desc\"\nsqldf(sql)\n\n                 title\n1               WALL-G\n2  Monsters University\n3                Brave\n4               Cars 2\n5          Toy Story 3\n6                   Up\n7               WALL-E\n8          Ratatouille\n9                 Cars\n10     The Incredibles\n11        Finding Nemo\n12      Monsters, Inc.\n13         Toy Story 2\n14        A Bug's Life\n15           Toy Story\n\n# List the last four movies released (ordered from least recent to most recent)\nsql = \"select * \n       from movies\n       order by year\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n# the following does the same thing\nsql = \"select * \n       from movies\n       order by year asc\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   1           Toy Story  John Lasseter 1995             81\n2   2        A Bug's Life  John Lasseter 1998             95\n3   3         Toy Story 2  John Lasseter 1999             93\n4   4      Monsters, Inc.    Pete Docter 2001             92\n5   5        Finding Nemo Andrew Stanton 2003            107\n6   6     The Incredibles      Brad Bird 2004            116\n7   7                Cars  John Lasseter 2006            117\n8   8         Ratatouille      Brad Bird 2007            115\n9   9              WALL-E Andrew Stanton 2008            104\n10 10                  Up    Pete Docter 2009            101\n11 11         Toy Story 3    Lee Unkrich 2010            103\n12 12              Cars 2  John Lasseter 2011            120\n13 13               Brave Brenda Chapman 2012            102\n14 14 Monsters University    Dan Scanlon 2013            110\n15 87              WALL-G Brenda Chapman 2042             92\n\n# List the last four movies released (ordered from most recent to last)\n#\n# To order a column in descending order follow the name of the column with desc\nsql = \"select * \n       from movies\n       order by year desc\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1  87              WALL-G Brenda Chapman 2042             92\n2  14 Monsters University    Dan Scanlon 2013            110\n3  13               Brave Brenda Chapman 2012            102\n4  12              Cars 2  John Lasseter 2011            120\n5  11         Toy Story 3    Lee Unkrich 2010            103\n6  10                  Up    Pete Docter 2009            101\n7   9              WALL-E Andrew Stanton 2008            104\n8   8         Ratatouille      Brad Bird 2007            115\n9   7                Cars  John Lasseter 2006            117\n10  6     The Incredibles      Brad Bird 2004            116\n11  5        Finding Nemo Andrew Stanton 2003            107\n12  4      Monsters, Inc.    Pete Docter 2001             92\n13  3         Toy Story 2  John Lasseter 1999             93\n14  2        A Bug's Life  John Lasseter 1998             95\n15  1           Toy Story  John Lasseter 1995             81\n\nsql = \"select * \n       from movies\n       order by year desc\n       limit 4\"\nsqldf(sql)\n\n  id               title       director year length_minutes\n1 87              WALL-G Brenda Chapman 2042             92\n2 14 Monsters University    Dan Scanlon 2013            110\n3 13               Brave Brenda Chapman 2012            102\n4 12              Cars 2  John Lasseter 2011            120\n\n# List all movies sorted alphabetically by title\n\nsql = \"select * \n       from movies\n       order by title\"\nsqldf(sql)\n\n   id               title       director year length_minutes\n1   2        A Bug's Life  John Lasseter 1998             95\n2  13               Brave Brenda Chapman 2012            102\n3   7                Cars  John Lasseter 2006            117\n4  12              Cars 2  John Lasseter 2011            120\n5   5        Finding Nemo Andrew Stanton 2003            107\n6  14 Monsters University    Dan Scanlon 2013            110\n7   4      Monsters, Inc.    Pete Docter 2001             92\n8   8         Ratatouille      Brad Bird 2007            115\n9   6     The Incredibles      Brad Bird 2004            116\n10  1           Toy Story  John Lasseter 1995             81\n11  3         Toy Story 2  John Lasseter 1999             93\n12 11         Toy Story 3    Lee Unkrich 2010            103\n13 10                  Up    Pete Docter 2009            101\n14  9              WALL-E Andrew Stanton 2008            104\n15 87              WALL-G Brenda Chapman 2042             92\n\n# List the first five movies sorted alphabetically\nsql = \"select * \n       from movies\n       order by title\n       limit 5\"\nsqldf(sql)\n\n  id        title       director year length_minutes\n1  2 A Bug's Life  John Lasseter 1998             95\n2 13        Brave Brenda Chapman 2012            102\n3  7         Cars  John Lasseter 2006            117\n4 12       Cars 2  John Lasseter 2011            120\n5  5 Finding Nemo Andrew Stanton 2003            107\n\n# List the next five movies sorted alphabetically\n\nsql = \"select * \n       from movies\n       order by title\n       limit 5 offset 5\"\nsqldf(sql)\n\n  id               title      director year length_minutes\n1 14 Monsters University   Dan Scanlon 2013            110\n2  4      Monsters, Inc.   Pete Docter 2001             92\n3  8         Ratatouille     Brad Bird 2007            115\n4  6     The Incredibles     Brad Bird 2004            116\n5  1           Toy Story John Lasseter 1995             81\n\n# List the next five movies sorted alphabetically\n\nsql = \"select * \n       from movies\n       order by title\n       limit 5 offset 10\"\nsqldf(sql)\n\n  id       title       director year length_minutes\n1  3 Toy Story 2  John Lasseter 1999             93\n2 11 Toy Story 3    Lee Unkrich 2010            103\n3 10          Up    Pete Docter 2009            101\n4  9      WALL-E Andrew Stanton 2008            104\n5 87      WALL-G Brenda Chapman 2042             92\n\n\n\n\n\n\n\n31.26.7 More practice - north_american_cities\n\n\n\n\n\n\nclick here for practice\n\n\n\n\n\n\n# Review 1 - Tasks\n#\n# https://sqlbolt.com/lesson/select_queries_review\n\n\n# See the table (i.e. dataframe)\nsqldf(\"select * north_american_cities\")\n\nError: near \"north_american_cities\": syntax error\n\n# List all the Canadian cities and their populations\n\nsql = \"select country, city, population\n       from north_american_cities\n       where country='Canada'\"\nsqldf(sql)\n\n  country     city population\n1  Canada  Toronto    2795060\n2  Canada Montreal    1717767\n\n# Order all the cities in the United States by their latitude from north to south\nsql = \"\nselect *\nfrom north_american_cities\nwhere country = 'United States'\norder by latitude desc\"\nsqldf(sql)\n\n          city       country population latitude  longitude\n1      Chicago United States    2718782 41.87811  -87.62980\n2     New York United States    8405837 40.71278  -74.00594\n3 Philadelphia United States    1553165 39.95258  -75.16522\n4  Los Angeles United States    3884307 34.05223 -118.24368\n5      Phoenix United States    1513367 33.44838 -112.07404\n6      Houston United States    2195914 29.76043  -95.36980\n\n# List all the cities west of Chicago, ordered from west to east\n\nsql = \"select *\nfrom north_american_cities\nwhere longitude &lt; -87.62980\norder by longitude\n\"\nsqldf(sql)\n\n                 city       country population latitude  longitude\n1         Los Angeles United States    3884307 34.05223 -118.24368\n2             Phoenix United States    1513367 33.44838 -112.07404\n3         Guadalajara        Mexico    1500800 20.65970 -103.34961\n4         Mexico City        Mexico    8555500 19.43261  -99.13321\n5 Ecatepec de Morelos        Mexico    1742000 19.60184  -99.05067\n6             Houston United States    2195914 29.76043  -95.36980\n\n# List the two largest cities in Mexico (by population)\n\n# Start by listing all of the cities in Mexico in order of decreasing population\nsql = \"select *\n       from north_american_cities\n       where country = 'Mexico'\n       order by population desc\"\nsqldf(sql)\n\n                 city country population latitude  longitude\n1         Mexico City  Mexico    8555500 19.43261  -99.13321\n2 Ecatepec de Morelos  Mexico    1742000 19.60184  -99.05067\n3         Guadalajara  Mexico    1500800 20.65970 -103.34961\n\n# use the limit clause to limit results to only a specified number of values\n# Start by listing all of the cities in Mexico\nsql = \"select *\n       from north_american_cities\n       where country = 'Mexico'\n       order by population desc\n       limit 2\"\nsqldf(sql)\n\n                 city country population latitude longitude\n1         Mexico City  Mexico    8555500 19.43261 -99.13321\n2 Ecatepec de Morelos  Mexico    1742000 19.60184 -99.05067\n\n# The limit clause takes an option \"offset\"\n# Each row in the data has an imaginary number starting with 0 for the 1st row\n# and 1 for the 2nd row , etc.\n\nsql = \"select * from north_american_cities\n       order by population desc\"\nsqldf(sql)\n\n                  city       country population latitude  longitude\n1          Mexico City        Mexico    8555500 19.43261  -99.13321\n2             New York United States    8405837 40.71278  -74.00594\n3          Los Angeles United States    3884307 34.05223 -118.24368\n4              Toronto        Canada    2795060 43.65323  -79.38318\n5              Chicago United States    2718782 41.87811  -87.62980\n6              Houston United States    2195914 29.76043  -95.36980\n7               Havana          Cuba    2106146 23.05407  -82.34519\n8  Ecatepec de Morelos        Mexico    1742000 19.60184  -99.05067\n9             Montreal        Canada    1717767 45.50169  -73.56726\n10        Philadelphia United States    1553165 39.95258  -75.16522\n11             Phoenix United States    1513367 33.44838 -112.07404\n12         Guadalajara        Mexico    1500800 20.65970 -103.34961\n\n# Do the same thing but only get the 3 largest cities\nsql = \"select * from north_american_cities\n       order by population desc\n       limit 3\"\nsqldf(sql)\n\n         city       country population latitude  longitude\n1 Mexico City        Mexico    8555500 19.43261  -99.13321\n2    New York United States    8405837 40.71278  -74.00594\n3 Los Angeles United States    3884307 34.05223 -118.24368\n\n# Do the same thing but get the 4th, 5th and 6th largest cities\nsql = \"select * from north_american_cities\n       order by population desc\n       limit 3 offset 3\"\nsqldf(sql)\n\n     city       country population latitude longitude\n1 Toronto        Canada    2795060 43.65323 -79.38318\n2 Chicago United States    2718782 41.87811 -87.62980\n3 Houston United States    2195914 29.76043 -95.36980\n\n# Do the same thing but get the 7th 8th and 9th largest cities\nsql = \"select * from north_american_cities\n       order by population desc\n       limit 3 offset 6\"\nsqldf(sql)\n\n                 city country population latitude longitude\n1              Havana    Cuba    2106146 23.05407 -82.34519\n2 Ecatepec de Morelos  Mexico    1742000 19.60184 -99.05067\n3            Montreal  Canada    1717767 45.50169 -73.56726\n\n# List the third and fourth largest cities (by population) in the United States and their population\n\n# Let's start by showing all USA cities\nsql = \"select * \n       from north_american_cities\n       where country = 'United States'\n       order by population desc\n       \"\nsqldf(sql)\n\n          city       country population latitude  longitude\n1     New York United States    8405837 40.71278  -74.00594\n2  Los Angeles United States    3884307 34.05223 -118.24368\n3      Chicago United States    2718782 41.87811  -87.62980\n4      Houston United States    2195914 29.76043  -95.36980\n5 Philadelphia United States    1553165 39.95258  -75.16522\n6      Phoenix United States    1513367 33.44838 -112.07404\n\n# Use limit 2 offset 3 to just show the 3rd and 4th largest cities\nsql = \"select * \n       from north_american_cities\n       where country = 'United States'\n       order by population desc\n       limit 2 offset 2\n       \"\nsqldf(sql)\n\n     city       country population latitude longitude\n1 Chicago United States    2718782 41.87811  -87.6298\n2 Houston United States    2195914 29.76043  -95.3698",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0010-introToSql-v023.html#dynamic-sql---a-brief-introduction",
    "href": "sql0010-introToSql-v023.html#dynamic-sql---a-brief-introduction",
    "title": "30  30.  Intro to Relational Databases and SQL (no dplyr required)",
    "section": "31.27 dynamic SQL - a brief introduction",
    "text": "31.27 dynamic SQL - a brief introduction\nNOTE:\n\n“IDS1020-Intro to IDS” students can ignore this section.\n\n\n########################################################################.\n# It is very common for SQL to be \"embedded\" in another language\n# very similar to what we are doing with R. There are ways to \n# \"embed\" SQL in all popular programming languages.\n#\n# This technique of building a sql statement  from different parts of\n# info using another language (in this case R) is known\n# as \"dynamic sql\".\n########################################################################.\n\n#################################################################.\n# Brief intro to an advanced concept: \"dynamic SQL\"?\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# As we've already learned, the sqldf function takes a\n# single character value that contains\n# a SQL command, e.g. sqldf(\"SELECT * FROM SOME_TABLE\")\n# This allows us to use the SQL language inside our R programs.\n# However, you must remember that SQL and R are very different \n# languages. As we learn SQL, we will focus primarily on how to write\n# various forms of SQL commands. We wont focus much on R\n# other than to use the sqldf function to run the SQL code.\n#\n# However, sometimes it is adventageous to \n# use R code (more than just sqldf) to \"dynamically\" construct a\n# SQL statement based on some other information that is \n# available to your R code but would not have been available\n# at the time you are writing the SQL code.\n#\n# For example, the following function, mySelect\n# takes two arguments: \n#\n#    table - The name of a table \n#    cols - The names of the columns to display from the table\n#\n# It displays just the specified columns from the specified table.\n\nmySelect = function( table, cols){\n  colsList = paste(cols, collapse=\",\")\n  selectStatement = paste0( \"select \", colsList, \" from \", table)\n  sqldf(selectStatement)\n}\n\n# The function can now be called with different tables and \n# lists of columns\n\nmySelect(\"movies\", c(\"title\",\"director\"))\n\n                 title       director\n1            Toy Story  John Lasseter\n2         A Bug's Life  John Lasseter\n3          Toy Story 2  John Lasseter\n4       Monsters, Inc.    Pete Docter\n5         Finding Nemo Andrew Stanton\n6      The Incredibles      Brad Bird\n7                 Cars  John Lasseter\n8          Ratatouille      Brad Bird\n9               WALL-E Andrew Stanton\n10                  Up    Pete Docter\n11         Toy Story 3    Lee Unkrich\n12              Cars 2  John Lasseter\n13               Brave Brenda Chapman\n14 Monsters University    Dan Scanlon\n15              WALL-G Brenda Chapman\n\nmySelect(\"grades\", c(\"student\", \"year\", \"test1\", \"test2\"))\n\n    student year test1 test2\n1       joe   fr    70    82\n2       sue   fr    80    80\n3      anne   fr    90    95\n4     frank   so   100    95\n5       bob   so    30    NA\n6  samantha   so   100    70\n7     larry   ju    32    80\n8      bill   se    84    90\n9  lucricia   fr    80   100\n10 margaret   se    59    47\n11    dalia   se    85    92\n12      zvi   ju    90    98\n13     mike   ju    90    86\n14    david   se    90    87\n15     dana   so   100    93",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'><yrChapterNumber>30. </yrChapterNumber> Intro to Relational Databases and SQL ***(no dplyr required)***</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html",
    "href": "sql0035-workingWithMultipleTables-v012.html",
    "title": "31  31.  Inner Joins.",
    "section": "",
    "text": "31.1 Importing the data into R\nIn section we will be referring to the “customers” and the “orders” tables as well as various tables from the “books database”. Instructions for downloading the necessary files and importing them into R are included in section above that describes how to “Get the data”.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#intro-to-joining-two-tables",
    "href": "sql0035-workingWithMultipleTables-v012.html#intro-to-joining-two-tables",
    "title": "31  31.  Inner Joins.",
    "section": "31.2 Intro to “JOIN”ing two tables",
    "text": "31.2 Intro to “JOIN”ing two tables\n\n# read the data for the tables we'll be using\ncustomers = read_csv(\"data/customers.csv\", show_col_types=FALSE)\norders = read_csv(\"data/orders.csv\", show_col_types=FALSE)\n\n\n31.2.1 Overview of the customers and orders tables\n\n# The examples in this page use the customers and orders tables.\n# The data for those tables can be imported into R using the\n# customers.csv and the orders.csv files.\n# This code to read in the data from the csv files appears \n# further up in this file. \n#\n# see this page for more info: https://www.w3schools.com/sql/sql_join.asp\n\n\n#........................................................................\n# Lets see a few rows from the customers table and from the orders table\n#........................................................................\n\nsqldf(\"select * from customers order by CustomerId limit 10\")\n\n   CustomerID                       CustomerName        ContactName                       Address        City PostalCode Country\n1           1                Alfreds Futterkiste       Maria Anders                 Obere Str. 57      Berlin      12209 Germany\n2           2 Ana Trujillo Emparedados y helados       Ana Trujillo Avda. de la Constitución 2222 México D.F.      05021  Mexico\n3           3            Antonio Moreno Taquería     Antonio Moreno                Mataderos 2312 México D.F.      05023  Mexico\n4           4                    Around the Horn       Thomas Hardy               120 Hanover Sq.      London    WA1 1DP      UK\n5           5                 Berglunds snabbköp Christina Berglund                Berguvsvägen 8       Luleå   S-958 22  Sweden\n6           6            Blauer See Delikatessen         Hanna Moos                Forsterstr. 57    Mannheim      68306 Germany\n7           7               Blondel père et fils Frédérique Citeaux              24, place Kléber  Strasbourg      67000  France\n8           8          Bólido Comidas preparadas      Martín Sommer                C/ Araquil, 67      Madrid      28023   Spain\n9           9                           Bon app'  Laurence Lebihans          12, rue des Bouchers   Marseille      13008  France\n10         10             Bottom-Dollar Marketse  Elizabeth Lincoln            23 Tsawassen Blvd.   Tsawassen    T2F 8M4  Canada\n\nsqldf(\"select * from orders order by CustomerId limit 10\")\n\n   OrderID CustomerID EmployeeID  OrderDate ShipperID\n1    10308          2          7  9/18/1996         3\n2    10365          3          3 11/27/1996         2\n3    10355          4          6 11/15/1996         1\n4    10383          4          8 12/16/1996         3\n5    10278          5          8  8/12/1996         2\n6    10280          5          2  8/14/1996         1\n7    10384          5          3 12/16/1996         3\n8    10265          7          2  7/25/1996         1\n9    10297          7          5   9/4/1996         2\n10   10360          7          4 11/22/1996         3",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#primary-keys-and-foreign-keys",
    "href": "sql0035-workingWithMultipleTables-v012.html#primary-keys-and-foreign-keys",
    "title": "31  31.  Inner Joins.",
    "section": "31.3 “Primary Keys” and “Foreign Keys”",
    "text": "31.3 “Primary Keys” and “Foreign Keys”\n\n#------------------------------------------------\n# Primary keys and Foreign keys\n#------------------------------------------------\n\n# - Primary Key\n#\n#   Many relational database tables have a column that contains a unique value\n#   for every row in the table. For example the customers table in the example\n#   below contains a CustomerId column in the customers table. Each row in the \n#   customers table contains a different value for the CustomerId. \n#   This allows you to identify a specific customer (i.e a specific row in the table)\n#   by the customer id. For example CustomerId 100 corresponds to one and only one\n#   row in the customers table.\n#   The CustomerId column in the Customers table is known as a \"primary key\". \n#\n#   Similarly, the orders table in the example below contains an OrderId\n#   column. Each row in the orders table contains a unique value for the\n#   OrderId. For example, orderId 10248 corresponds to exactly one row \n#   in the orders table. Therefore we say that the OrderId column is the \n#   \"primary key\" for the orders table.\n#\n#\n#\n# - Foreign Key\n#\n#   Note that the orders table ALSO contains a CustomerId column.\n#   The customerId in the orders table indicates which customer\n#   placed a specific order. To find the name of the customer, the address\n#   of the customer etc. you can look in the customers table for the row\n#   that contains the specific customerId for the order.\n#\n#   Since a single customer can have many different orders,\n#   the values of the CustomerId column in the orders table contains\n#   repeats of the same CustomerId value (in different rows of the orders table). \n#   The customerId column in the orders table is NOT a primary key.\n#   The customerId column in the orders table exists so that we\n#   can look up the customer information for a particular order.\n#   The CustomerId column in the orders table is known as a \"foreign key\".\n#   In general a column in a table that contains information from\n#   the primary key of a different table is known as a \"foreign key\".\n#\n# In summary\n# - The CustomerId column in the customers table is a primary key\n# - The OrderId column in the orders table is a primary key\n# - The CustomerId column in the orders table is a foreign key\n#\n\n\n31.3.1 More about the customers and orders tables\n\n# show the orders table arranged in the order of the customerID\n\nsqldf(\"select *\n       from orders\n       order by CustomerID\")\n\n    OrderID CustomerID EmployeeID  OrderDate ShipperID\n1     10308          2          7  9/18/1996         3\n2     10365          3          3 11/27/1996         2\n3     10355          4          6 11/15/1996         1\n4     10383          4          8 12/16/1996         3\n5     10278          5          8  8/12/1996         2\n6     10280          5          2  8/14/1996         1\n7     10384          5          3 12/16/1996         3\n8     10265          7          2  7/25/1996         1\n9     10297          7          5   9/4/1996         2\n10    10360          7          4 11/22/1996         3\n11    10436          7          3   2/5/1997         2\n12    10326          8          4 10/10/1996         2\n13    10331          9          9 10/16/1996         1\n14    10340          9          1 10/29/1996         3\n15    10362          9          3 11/25/1996         1\n16    10389         10          4 12/20/1996         2\n17    10410         10          3  1/10/1997         3\n18    10411         10          9  1/10/1997         3\n19    10431         10          4  1/30/1997         2\n20    10289         11          7  8/26/1996         3\n21    10259         13          4  7/18/1996         3\n22    10254         14          5  7/11/1996         2\n23    10370         14          6  12/3/1996         2\n24    10290         15          8  8/27/1996         1\n25    10435         16          8   2/4/1997         2\n26    10363         17          4 11/26/1996         3\n27    10391         17          3 12/23/1996         3\n28    10311         18          1  9/20/1996         3\n29    10364         19          1 11/26/1996         1\n30    10400         19          1   1/1/1997         3\n31    10258         20          1  7/17/1996         1\n32    10263         20          9  7/23/1996         3\n33    10351         20          1 11/11/1996         1\n34    10368         20          2 11/29/1996         2\n35    10382         20          4 12/13/1996         1\n36    10390         20          6 12/23/1996         1\n37    10402         20          8   1/2/1997         2\n38    10403         20          4   1/3/1997         3\n39    10430         20          4  1/30/1997         1\n40    10442         20          3  2/11/1997         2\n41    10347         21          4  11/6/1996         3\n42    10386         21          9 12/18/1996         3\n43    10414         21          2  1/14/1997         3\n44    10408         23          8   1/8/1997         1\n45    10264         24          6  7/24/1996         3\n46    10327         24          2 10/11/1996         1\n47    10378         24          5 12/10/1996         3\n48    10434         24          3   2/3/1997         2\n49    10267         25          4  7/29/1996         1\n50    10337         25          4 10/24/1996         3\n51    10342         25          4 10/30/1996         2\n52    10396         25          1 12/27/1996         3\n53    10422         27          2  1/22/1997         1\n54    10328         28          4 10/14/1996         3\n55    10352         28          3 11/12/1996         3\n56    10366         29          8 11/28/1996         2\n57    10426         29          4  1/27/1997         1\n58    10303         30          7  9/11/1996         2\n59    10423         31          6  1/23/1997         3\n60    10268         33          8  7/30/1996         3\n61    10250         34          4   7/8/1996         2\n62    10253         34          3  7/10/1996         2\n63    10257         35          4  7/16/1996         3\n64    10395         35          6 12/26/1996         1\n65    10375         36          3  12/6/1996         2\n66    10394         36          1 12/25/1996         3\n67    10415         36          3  1/15/1997         1\n68    10298         37          6   9/5/1996         2\n69    10309         37          3  9/19/1996         1\n70    10335         37          7 10/22/1996         2\n71    10373         37          4  12/5/1996         3\n72    10380         37          8 12/12/1996         3\n73    10429         37          3  1/29/1997         2\n74    10315         38          4  9/26/1996         2\n75    10318         38          8  10/1/1996         2\n76    10321         38          3  10/3/1996         2\n77    10323         39          4  10/7/1996         1\n78    10325         39          1  10/9/1996         3\n79    10350         41          6 11/11/1996         2\n80    10358         41          5 11/20/1996         1\n81    10371         41          1  12/3/1996         1\n82    10413         41          3  1/14/1997         2\n83    10425         41          6  1/24/1997         2\n84    10279         44          8  8/13/1996         2\n85    10284         44          4  8/19/1996         1\n86    10343         44          4 10/31/1996         1\n87    10283         46          3  8/16/1996         3\n88    10296         46          6   9/3/1996         1\n89    10330         46          3 10/16/1996         1\n90    10357         46          1 11/19/1996         3\n91    10381         46          3 12/12/1996         3\n92    10405         47          1   1/6/1997         1\n93    10307         48          2  9/17/1996         2\n94    10317         48          6  9/30/1996         1\n95    10275         49          1   8/7/1996         1\n96    10300         49          2   9/9/1996         2\n97    10404         49          2   1/3/1997         1\n98    10332         51          3 10/17/1996         2\n99    10339         51          2 10/28/1996         2\n100   10376         51          1  12/9/1996         2\n101   10424         51          7  1/23/1997         2\n102   10439         51          6   2/7/1997         3\n103   10277         52          2   8/9/1996         3\n104   10409         54          3   1/9/1997         1\n105   10260         55          4  7/19/1996         1\n106   10305         55          8  9/13/1996         3\n107   10338         55          4 10/25/1996         3\n108   10441         55          3  2/10/1997         2\n109   10407         56          2   1/7/1997         2\n110   10322         58          7  10/4/1996         3\n111   10354         58          8 11/14/1996         3\n112   10353         59          7 11/13/1996         3\n113   10392         59          2 12/24/1996         3\n114   10427         59          4  1/27/1997         2\n115   10336         60          7 10/23/1996         2\n116   10397         60          5 12/27/1996         1\n117   10433         60          3   2/3/1997         3\n118   10261         61          4  7/19/1996         2\n119   10291         61          6  8/27/1996         2\n120   10379         61          2 12/11/1996         1\n121   10421         61          8  1/21/1997         1\n122   10372         62          5  12/4/1996         2\n123   10406         62          7   1/7/1997         1\n124   10273         63          3   8/5/1996         3\n125   10285         63          1  8/20/1996         2\n126   10286         63          8  8/21/1996         3\n127   10313         63          2  9/24/1996         2\n128   10345         63          2  11/4/1996         2\n129   10361         63          1 11/22/1996         2\n130   10418         63          4  1/17/1997         1\n131   10262         65          8  7/22/1996         3\n132   10272         65          6   8/2/1996         2\n133   10294         65          4  8/30/1996         2\n134   10314         65          1  9/25/1996         2\n135   10316         65          1  9/27/1996         3\n136   10346         65          3  11/5/1996         3\n137   10401         65          1   1/1/1997         1\n138   10288         66          4  8/23/1996         1\n139   10428         66          7  1/28/1997         1\n140   10443         66          8  2/12/1997         1\n141   10287         67          8  8/22/1996         3\n142   10299         67          4   9/6/1996         2\n143   10255         68          9  7/12/1996         3\n144   10419         68          4  1/20/1997         2\n145   10281         69          4  8/14/1996         1\n146   10282         69          4  8/15/1996         1\n147   10306         69          1  9/16/1996         3\n148   10387         70          1 12/18/1996         2\n149   10324         71          9  10/8/1996         1\n150   10393         71          1 12/25/1996         3\n151   10398         71          2 12/30/1996         3\n152   10440         71          4  2/10/1997         2\n153   10359         72          5 11/21/1996         3\n154   10377         72          1  12/9/1996         3\n155   10388         72          2 12/19/1996         1\n156   10341         73          7 10/29/1996         3\n157   10417         73          4  1/16/1997         3\n158   10271         75          6   8/1/1996         2\n159   10329         75          4 10/15/1996         2\n160   10349         75          7  11/8/1996         1\n161   10369         75          8  12/2/1996         2\n162   10385         75          1 12/17/1996         2\n163   10432         75          3  1/31/1997         2\n164   10252         76          4   7/9/1996         2\n165   10302         76          4  9/10/1996         2\n166   10310         77          8  9/20/1996         2\n167   10438         79          3   2/6/1997         2\n168   10276         80          8   8/8/1996         3\n169   10293         80          1  8/29/1996         3\n170   10304         80          1  9/12/1996         2\n171   10319         80          7  10/2/1996         3\n172   10249         81          6   7/5/1996         1\n173   10292         81          1  8/28/1996         2\n174   10367         83          7 11/28/1996         3\n175   10399         83          8 12/31/1996         3\n176   10251         84          3   7/8/1996         1\n177   10334         84          8 10/21/1996         2\n178   10274         85          6   8/6/1996         1\n179   10295         85          2   9/2/1996         2\n180   10301         86          8   9/9/1996         2\n181   10312         86          2  9/23/1996         2\n182   10348         86          4  11/7/1996         2\n183   10356         86          6 11/18/1996         2\n184   10266         87          3  7/26/1996         3\n185   10270         87          1   8/1/1996         1\n186   10320         87          5  10/3/1996         3\n187   10333         87          5 10/18/1996         3\n188   10412         87          8  1/13/1997         2\n189   10416         87          8  1/16/1997         3\n190   10437         87          8   2/5/1997         1\n191   10256         88          3  7/15/1996         2\n192   10420         88          3  1/21/1997         1\n193   10269         89          5  7/31/1996         1\n194   10344         89          4  11/1/1996         2\n195   10248         90          5   7/4/1996         3\n196   10374         91          1  12/5/1996         3\n\n# Show customer info for customer id 10\n\nsqldf(\"select * from customers where CustomerId = 10\")\n\n  CustomerID           CustomerName       ContactName            Address      City PostalCode Country\n1         10 Bottom-Dollar Marketse Elizabeth Lincoln 23 Tsawassen Blvd. Tsawassen    T2F 8M4  Canada\n\n# Show all orders for customer id 10\n\nsqldf(\"select * from orders where CustomerId = 10\")\n\n  OrderID CustomerID EmployeeID  OrderDate ShipperID\n1   10389         10          4 12/20/1996         2\n2   10410         10          3  1/10/1997         3\n3   10411         10          9  1/10/1997         3\n4   10431         10          4  1/30/1997         2\n\n\n\n\n31.3.2 JOINing customers and orders into a single output\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# using the JOIN to combine information from two tables into a single output\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# Combine information from the customers and the orders table.\n#\n# Specifically, show the following data\n# - customer name and country from the customers table\n# - orderId, OrderDate from the orders table\n#\n# Only show data for all orders placed by customer 10\n\nsqldf(\"select OrderID, customers.CustomerId, CustomerName, Country, OrderDate\n       from orders join customers on orders.customerId = customers.CustomerId\n       where customers.CustomerId = 10\")   # ame result if you say orders.customerId\n\n  OrderID CustomerID           CustomerName Country  OrderDate\n1   10389         10 Bottom-Dollar Marketse  Canada 12/20/1996\n2   10410         10 Bottom-Dollar Marketse  Canada  1/10/1997\n3   10411         10 Bottom-Dollar Marketse  Canada  1/10/1997\n4   10431         10 Bottom-Dollar Marketse  Canada  1/30/1997\n\n# do the same for customerids 1 through 5\n\nsql=\"select OrderID, customers.CustomerId, CustomerName, OrderDate\n     from orders join customers on orders.customerId = customers.CustomerId\n     where customers.CustomerId &gt;= 1 and customers.CustomerId &lt;= 5\" \nsqldf(sql)\n\n  OrderID CustomerID                       CustomerName  OrderDate\n1   10308          2 Ana Trujillo Emparedados y helados  9/18/1996\n2   10365          3            Antonio Moreno Taquería 11/27/1996\n3   10355          4                    Around the Horn 11/15/1996\n4   10383          4                    Around the Horn 12/16/1996\n5   10278          5                 Berglunds snabbköp  8/12/1996\n6   10280          5                 Berglunds snabbköp  8/14/1996\n7   10384          5                 Berglunds snabbköp 12/16/1996\n\n# another way, with the IN operator\nsql=\"select OrderID, customers.CustomerId, CustomerName, OrderDate\n     from orders join customers on orders.customerId = customers.CustomerId\n     where customers.CustomerId IN (1,2,3,4,5)\"   # you would get the same result if you said orders.customerId\nsqldf(sql)\n\n  OrderID CustomerID                       CustomerName  OrderDate\n1   10308          2 Ana Trujillo Emparedados y helados  9/18/1996\n2   10365          3            Antonio Moreno Taquería 11/27/1996\n3   10355          4                    Around the Horn 11/15/1996\n4   10383          4                    Around the Horn 12/16/1996\n5   10278          5                 Berglunds snabbköp  8/12/1996\n6   10280          5                 Berglunds snabbköp  8/14/1996\n7   10384          5                 Berglunds snabbköp 12/16/1996\n\n# show the orders that were shipped by shipper #3 for customers who\n# are from Mexico. Sort the results by the orderId\nsql=\"select orderId, customerName, customers.customerId, shipperId, country\n     from orders join customers on orders.customerId = customers.customerId\n     where shipperId = 3 and country = 'Mexico'\n     order by orderId\"\nsqldf(sql)\n\n  OrderID                       CustomerName CustomerID ShipperID Country\n1   10259         Centro comercial Moctezuma         13         3  Mexico\n2   10276                Tortuga Restaurante         80         3  Mexico\n3   10293                Tortuga Restaurante         80         3  Mexico\n4   10308 Ana Trujillo Emparedados y helados          2         3  Mexico\n5   10319                Tortuga Restaurante         80         3  Mexico\n6   10322          Pericles Comidas clásicas         58         3  Mexico\n7   10354          Pericles Comidas clásicas         58         3  Mexico",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#one-to-many-relationships",
    "href": "sql0035-workingWithMultipleTables-v012.html#one-to-many-relationships",
    "title": "31  31.  Inner Joins.",
    "section": "31.4 One to Many relationships",
    "text": "31.4 One to Many relationships\n\n#####################################################################\n#\n# One to Many relationships\n#\n# EXAMPLE:\n#           a single SPECIFIC customer may have many orders\n#           a single SPECIFIC order is related to exactly one customer \n#\n# IN GENERAL:\n#\n# A single SPECIFIC row in table A may be related to many (or possibly zero)\n# rows from table B. \n#\n# However a single SPECIFIC row from table B is related to \n# EXACTLY ONE row from table A\n#\n# We'll call table A the \"one table\" and \n#            table B the \"many table\"\n#\n# - The \"one\" table has a primary key.\n#\n# - The \"many\" table has a foreign key that contains values of the\n#   other table's primary key values.\n#####################################################################",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#many-to-many-relationships",
    "href": "sql0035-workingWithMultipleTables-v012.html#many-to-many-relationships",
    "title": "31  31.  Inner Joins.",
    "section": "31.5 Many to Many relationships",
    "text": "31.5 Many to Many relationships\n\n#####################################################################\n#\n# Many to Many relationships\n#\n# EXAMPLE:\n#           a single SPECIFIC author may have written many titles\n#           a single SPECIFIC book   may have many authors\n#\n#\n# Many to many relationships cannot be directly expressed in relational\n# databases. \n# \n# To establish a many to many relationship, a \"middle\" table is created\n# that contains the to foreign keys. Each of the foreign keys relate \n# back to the primary key of one of the two tables. \n# In the books database, the middle table is called title_authors.\n#####################################################################",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#practice---one-to-many-or-many-to-many",
    "href": "sql0035-workingWithMultipleTables-v012.html#practice---one-to-many-or-many-to-many",
    "title": "31  31.  Inner Joins.",
    "section": "31.6 Practice - “one to many” or “many to many”?",
    "text": "31.6 Practice - “one to many” or “many to many”?\n\n###################################\n# Practice identifying relationships\n#\n# - one to many\n#\n# - many to many\n#\n###################################\n\n# QUESTION \n#\n# A university uses to tables to track their real estate.\n#\n# The BUILDINGS table contains one row for each building they own.\n# \n# The ROOMS table contains one row for each room in each building.\n#\n# Are the BUILDINGS and ROOMS tables in a \"1 to many\" or a \n# \"many to many\" relationship?\n#\n# Given the following definitions of a BUILDINGS table and \n# a ROOMS table draw a \"crows foot\" Entity Relationship Diagram.\n# Make sure to show the symbols for the maximum and minimum \n# cardinalities. Explain in English what each symbol on your\n# diagram means. \n#------------------------------------------------------------------ \n#    BUILDINGS TABLE\n#\n#    bldgCode    (PK)\n#    streetAddress\n#    city\n#    state\n#    zip\n#    numOfFloors\n#\n#\n#    ROOMS TABLE\n# \n#    bldgCode (PK)\n#    roomNum  (PK)\n#    capacity\n#    type    (classroom, dorm room, office)\n#    hasVideoProjection (TRUE/FALSE)\n#\n#########################################################################\n\n\n\n#########################################################################\n# \n# The same university keeps track of their professors, the courses \n# that are in the course catalog and the specific offerings every\n# semester. Think about how these concepts are related to each other.\n#\n# How are PROFESSORS related to COURSES? \"one to many\", \"many to one\", \"many to many\"?\n#\n# How are PROFESSORS related to OFFERINGS? \"one to many\", \"many to one\", \"many to many\"?\n#\n# How are COURSES related to OFFERINGS? \"one to many\", \"many to one\", \"many to many\"?\n#\n# The following tables could be used to track this info.\n# Draw the crows foot Entity Relationship Diagram.\n# Make sure to show the symbols for the maximum and minimum \n# cardinalities. Explain in English what each symbol on your\n# diagram means. \n#------------------------------------------------------------------------\n#\n# PROFESSORS TABLE\n#\n# Description: this table contains exactly one row for each professor that is\n# employed by the university. \n# \n# facultyId (PK)\n# firstName\n# lastName\n# department\n# salary\n# yearHired\n#\n#\n#\n# OFFERINGS TABLE\n#\n# Description: this table contains info about the sections that are \n# being offered in particular semesters.\n#\n# crn (PK)\n# courseNumber (FK1)\n# semester\n# year\n# room\n# buildingId\n# facutyId (FK2)\n#\n#\n#\n# COURSES TABLE \n# Description: This table contains one row for each course in the course catalog.\n# Note that not every course is offered every semester. Some courses have\n# multiple offerings in a single semester.\n#\n# courseNumber (PK)\n# courseName\n# credits\n# description\n#############################################################################",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#using-join-with-the-books-database",
    "href": "sql0035-workingWithMultipleTables-v012.html#using-join-with-the-books-database",
    "title": "31  31.  Inner Joins.",
    "section": "31.7 Using JOIN with the books database",
    "text": "31.7 Using JOIN with the books database\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The books database contains information about various books, authors,\n# publishers, etc.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"data/booksDatabase/titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"data/booksDatabase/authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"data/booksDatabase/publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"data/booksDatabase/title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"data/booksDatabase/royalties.csv\", na=\"NULL\", show_col_types=FALSE)\n\n\n# QUESTION: Write a SQL select statement (i.e. a \"query\") to display the following data.\n# For each title, show the \n# - title name\n# - num pages\n# - publisher name\n# - publisher country\n\n# ANSWER\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\"\nsqldf(sql)\n\n                            title_name pages             pub_name country\n1                                1977!   107    Abatis Publishers     USA\n2            But I Did It Unconciously   510    Abatis Publishers     USA\n3               Exchange of Platitudes   201    Abatis Publishers     USA\n4                     How About Never?   473    Abatis Publishers     USA\n5         Just Wait Until After School    86    Abatis Publishers     USA\n6                      Kiss My Boo Boo    22    Abatis Publishers     USA\n7         Ask Yor System Administrator  1226      Core Dump Books     USA\n8            200 Years of German Humor    14 Schandenfreude Press Germany\n9                    I Blame My Mother   333 Schandenfreude Press Germany\n10 What Are The Civilian Applications?   802 Schandenfreude Press Germany\n11         Not Without My Fabrerge Egg    NA       AAA Publishing     USA\n12    Perhaps It's a Glandular Problem   826       AAA Publishing     USA\n13           Spontaneous, Not Annoying   507       AAA Publishing     USA\n\n# To be more clear where each column comes from you can\n# write the columns in the format tableName.columnName.\n# (In this example it's not necessary to do so - but can make your code easier\n# to understand. Later we'll see examples where you must specify the \n# tablename.colname format).\n\nsql = \"select titles.title_name, titles.pages, publishers.pub_name, publishers.country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\"\nsqldf(sql)\n\n                            title_name pages             pub_name country\n1                                1977!   107    Abatis Publishers     USA\n2            But I Did It Unconciously   510    Abatis Publishers     USA\n3               Exchange of Platitudes   201    Abatis Publishers     USA\n4                     How About Never?   473    Abatis Publishers     USA\n5         Just Wait Until After School    86    Abatis Publishers     USA\n6                      Kiss My Boo Boo    22    Abatis Publishers     USA\n7         Ask Yor System Administrator  1226      Core Dump Books     USA\n8            200 Years of German Humor    14 Schandenfreude Press Germany\n9                    I Blame My Mother   333 Schandenfreude Press Germany\n10 What Are The Civilian Applications?   802 Schandenfreude Press Germany\n11         Not Without My Fabrerge Egg    NA       AAA Publishing     USA\n12    Perhaps It's a Glandular Problem   826       AAA Publishing     USA\n13           Spontaneous, Not Annoying   507       AAA Publishing     USA\n\n\n\n\n\n\n\n\nSee next section about “CROSS JOINS”\n\n\n\nBefore going further, please take a few minutes and read the information on “CROSS JOIN” that is found in the next section of this website. The discussion of CROSS JOINS will be moved to this location soon…\n\n\n\n# DEEPER EXPLANATION\n#\n# We learned earlier about a \"CROSS JOIN\" that takes the rows of two different\n# tables and combines them in every way. This is useful when you want to see\n# every possible combination of the rows of two different tables. For example,\n# if you have a table of shirts and a table of pants and you want to see every\n# possible combination of a shirt with a pair of pants, a cross join might\n# be appropriate. However, most of the time that is not what you want to do.\n# Most of the time, the appropropriate type of join is known a an \"INNER JOIN\"\n# (or just simply \"JOIN\", the word inner is optional).\n# An inner join matches up those rows from two\n# different tables that share a primary key and a foreign key with the same\n# values.\n#\n# However, a cross join is actually at the heart of how the database \n# performs an inner join internally. Specifically whenever the SQL code\n# specifies an inner join (i.e. \"join\") the database first performs a\n# cross join and then removes the rows from the cross join \n# results that do not match the ON clause of the INNER JOIN.\n#\n# Let's look at that in a little more detail ...\n#\n# First thing that happens when the above query is processed internally\n# is the following cross join\n\n\nsql = \"select titles.*, publishers.*\n       from titles cross join publishers\"\nsqldf(sql)\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate pub_id             pub_name          city state country\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n2       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n3       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n5       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n6       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P01    Abatis Publishers      New York    NY     USA\n7       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P02      Core Dump Books San Francisco    CA     USA\n8       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n9       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n10      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P05       AAA Publishing      Berkeley    CA     USA\n11      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n12      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n13      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n14      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n15      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n16      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n17      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P02      Core Dump Books San Francisco    CA     USA\n18      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n19      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n20      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P05       AAA Publishing      Berkeley    CA     USA\n21      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n22      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P02      Core Dump Books San Francisco    CA     USA\n23      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n24      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n25      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P05       AAA Publishing      Berkeley    CA     USA\n26      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n27      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n28      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n29      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n30      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n31      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n32      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P02      Core Dump Books San Francisco    CA     USA\n33      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n34      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n35      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P05       AAA Publishing      Berkeley    CA     USA\n36      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n37      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P02      Core Dump Books San Francisco    CA     USA\n38      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n39      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n40      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P05       AAA Publishing      Berkeley    CA     USA\n41      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P01    Abatis Publishers      New York    NY     USA\n42      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P02      Core Dump Books San Francisco    CA     USA\n43      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n44      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n45      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P05       AAA Publishing      Berkeley    CA     USA\n46      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P01    Abatis Publishers      New York    NY     USA\n47      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P02      Core Dump Books San Francisco    CA     USA\n48      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n49      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P04    Tneterhooks Press      Berkeley    CA     USA\n50      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P05       AAA Publishing      Berkeley    CA     USA\n51      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n52      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n53      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n54      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n55      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n56      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n57      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n58      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n59      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n60      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n61      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n62      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P02      Core Dump Books San Francisco    CA     USA\n63      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n64      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P04    Tneterhooks Press      Berkeley    CA     USA\n65      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P05       AAA Publishing      Berkeley    CA     USA\n\n\n\n# after that , the rows that don't match the \"on\" clause in the inner join\n# are removed from the results. This can be seen by adding a where clause \n# to the cross join shown above.\n\nsql = \"select titles.*, publishers.*\n       from titles cross join publishers\n       where titles.pub_id = publishers.pub_id\"\nsqldf(sql)\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate pub_id             pub_name          city state country\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P01    Abatis Publishers      New York    NY     USA\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P05       AAA Publishing      Berkeley    CA     USA\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n\n# The last thing that happens is that just the columns that appear in \n# the select clause are displayed. Therefore, the following two \n# queries are equivalent\n\n# This is the inner join version (we copied this from above)\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\"\nsqldf(sql)\n\n                            title_name pages             pub_name country\n1                                1977!   107    Abatis Publishers     USA\n2            But I Did It Unconciously   510    Abatis Publishers     USA\n3               Exchange of Platitudes   201    Abatis Publishers     USA\n4                     How About Never?   473    Abatis Publishers     USA\n5         Just Wait Until After School    86    Abatis Publishers     USA\n6                      Kiss My Boo Boo    22    Abatis Publishers     USA\n7         Ask Yor System Administrator  1226      Core Dump Books     USA\n8            200 Years of German Humor    14 Schandenfreude Press Germany\n9                    I Blame My Mother   333 Schandenfreude Press Germany\n10 What Are The Civilian Applications?   802 Schandenfreude Press Germany\n11         Not Without My Fabrerge Egg    NA       AAA Publishing     USA\n12    Perhaps It's a Glandular Problem   826       AAA Publishing     USA\n13           Spontaneous, Not Annoying   507       AAA Publishing     USA\n\n# Version with a cross join - this is what happens internally when\n# you write an inner join (i.e. \"join\")\nsql = \"select title_name, pages, pub_name, country\n       from titles cross join publishers\n       where titles.pub_id = publishers.pub_id\"\nsqldf(sql)\n\n                            title_name pages             pub_name country\n1                                1977!   107    Abatis Publishers     USA\n2            200 Years of German Humor    14 Schandenfreude Press Germany\n3         Ask Yor System Administrator  1226      Core Dump Books     USA\n4            But I Did It Unconciously   510    Abatis Publishers     USA\n5               Exchange of Platitudes   201    Abatis Publishers     USA\n6                     How About Never?   473    Abatis Publishers     USA\n7                    I Blame My Mother   333 Schandenfreude Press Germany\n8         Just Wait Until After School    86    Abatis Publishers     USA\n9                      Kiss My Boo Boo    22    Abatis Publishers     USA\n10         Not Without My Fabrerge Egg    NA       AAA Publishing     USA\n11    Perhaps It's a Glandular Problem   826       AAA Publishing     USA\n12           Spontaneous, Not Annoying   507       AAA Publishing     USA\n13 What Are The Civilian Applications?   802 Schandenfreude Press Germany\n\n# QUESTION\n# Only show results for books that have at least 100 pages\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where pages &gt;= 100\"\nsqldf(sql)\n\n                           title_name pages             pub_name country\n1                               1977!   107    Abatis Publishers     USA\n2        Ask Yor System Administrator  1226      Core Dump Books     USA\n3           But I Did It Unconciously   510    Abatis Publishers     USA\n4              Exchange of Platitudes   201    Abatis Publishers     USA\n5                    How About Never?   473    Abatis Publishers     USA\n6                   I Blame My Mother   333 Schandenfreude Press Germany\n7    Perhaps It's a Glandular Problem   826       AAA Publishing     USA\n8           Spontaneous, Not Annoying   507       AAA Publishing     USA\n9 What Are The Civilian Applications?   802 Schandenfreude Press Germany\n\n# QUESTION\n# Only show books that are at least 100 pages and published in the USA\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where pages &gt;= 100 and country = 'USA'\"\nsqldf(sql)\n\n                        title_name pages          pub_name country\n1                            1977!   107 Abatis Publishers     USA\n2        But I Did It Unconciously   510 Abatis Publishers     USA\n3           Exchange of Platitudes   201 Abatis Publishers     USA\n4                 How About Never?   473 Abatis Publishers     USA\n5     Ask Yor System Administrator  1226   Core Dump Books     USA\n6 Perhaps It's a Glandular Problem   826    AAA Publishing     USA\n7        Spontaneous, Not Annoying   507    AAA Publishing     USA\n\n# QUESTION\n# Show books that are shorter than 100 pages  and books that are at least 1000 pages\n# Do not show any other books.\n# Only show books from the USA.\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \"\nsqldf(sql)\n\n                    title_name pages          pub_name country\n1 Just Wait Until After School    86 Abatis Publishers     USA\n2              Kiss My Boo Boo    22 Abatis Publishers     USA\n3 Ask Yor System Administrator  1226   Core Dump Books     USA\n\n# QUESTION\n# show the results in the order of the length of the book (i.e. # of pages)\n# show the shortest books at the top\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \n       order by pages asc\"     # asc is optional - you don't have to write it - it is the default\nsqldf(sql)\n\n                    title_name pages          pub_name country\n1              Kiss My Boo Boo    22 Abatis Publishers     USA\n2 Just Wait Until After School    86 Abatis Publishers     USA\n3 Ask Yor System Administrator  1226   Core Dump Books     USA\n\n# QUESTION\n# do the same thing but put the largest books at the top\n\nsql = \"select title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \n       order by pages desc\"     # you must write desc (i.e. descending)\nsqldf(sql)\n\n                    title_name pages          pub_name country\n1 Ask Yor System Administrator  1226   Core Dump Books     USA\n2 Just Wait Until After School    86 Abatis Publishers     USA\n3              Kiss My Boo Boo    22 Abatis Publishers     USA\n\n# QUESTION - add the pub_id to the output of the previous query.\n\n\n# ANSWER\n#\n# Because the pub_id exists in both the publishers and the titles\n# tables, every time that pub_id is used in the query, you must also include\n# the name of the table.\n#\n# For more info, see the comment below entitled\n#     #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#     # select table.column ...        vs       select column ... \n#     #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsql = \"select publishers.pub_id, title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \n       order by pages desc\"     \nsqldf(sql)\n\n  pub_id                   title_name pages          pub_name country\n1    P02 Ask Yor System Administrator  1226   Core Dump Books     USA\n2    P01 Just Wait Until After School    86 Abatis Publishers     USA\n3    P01              Kiss My Boo Boo    22 Abatis Publishers     USA\n\n# it will also work if you write titles.pub_id because once the table have been\n# joined together, the row from the publishers table and the corresponding\n# row from the titles table both have the same value for the pub_id\n#\n# For more info, see the comment below entitled\n#\n#     #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#     # select table.column ...        vs       select column ... \n#     #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n\nsql = \"select titles.pub_id, title_name, pages, pub_name, country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \n       order by pages desc\"     \n sqldf(sql)\n\n  pub_id                   title_name pages          pub_name country\n1    P02 Ask Yor System Administrator  1226   Core Dump Books     USA\n2    P01 Just Wait Until After School    86 Abatis Publishers     USA\n3    P01              Kiss My Boo Boo    22 Abatis Publishers     USA\n\n# You could include the table names even for columns that don't rquire it.\nsql = \"select titles.pub_id, titles.title_name, titles.pages, publishers.pub_name, publishers.country\n       from publishers  join   titles  on  publishers.pub_id = titles.pub_id\n       where (pages &lt; 100 or pages &gt;= 1000) and country = 'USA' \n       order by pages desc\"     \nsqldf(sql)\n\n  pub_id                   title_name pages          pub_name country\n1    P02 Ask Yor System Administrator  1226   Core Dump Books     USA\n2    P01 Just Wait Until After School    86 Abatis Publishers     USA\n3    P01              Kiss My Boo Boo    22 Abatis Publishers     USA\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# select table.column ...        vs       select column ... \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You must include table names in the SELECT clause for columns that \n# appear in more than one table. Column names that that only appear \n# in a single table in the query do NOT need to be preceded by the table name. \n#\n# However, it is PERMISSIBLE to include the table name for ANY column (even\n# if the column only appears in a single table).\n# Some people prefer to include the table name even when it \n# is not necessary (as done above) to make it clearer to someone reading the \n# code which table contains which columns.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# QUESTION\n#\n# Show the title_id, the title, and the author ID\n# for each author that had a part in writing a book. \n#\n# If there is more than one author for a title then show a different row\n# in the output for each author.\n#\n# Show the rows in alphabetical order by title.\n\n# ANSWER\n\nsql = \"select titles.title_id, title_name, au_id\n       from titles join title_authors   on    titles.title_id = title_authors.title_id\n       order by title_name\"\nsqldf(sql)\n\n   title_id                          title_name au_id\n1       T01                               1977!   A01\n2       T02           200 Years of German Humor   A01\n3       T03        Ask Yor System Administrator   A05\n4       T04           But I Did It Unconciously   A03\n5       T04           But I Did It Unconciously   A04\n6       T05              Exchange of Platitudes   A04\n7       T06                    How About Never?   A02\n8       T07                   I Blame My Mother   A02\n9       T07                   I Blame My Mother   A04\n10      T08        Just Wait Until After School   A06\n11      T09                     Kiss My Boo Boo   A06\n12      T10         Not Without My Fabrerge Egg   A02\n13      T11    Perhaps It's a Glandular Problem   A03\n14      T11    Perhaps It's a Glandular Problem   A04\n15      T11    Perhaps It's a Glandular Problem   A06\n16      T12           Spontaneous, Not Annoying   A02\n17      T13 What Are The Civilian Applications?   A01",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0035-workingWithMultipleTables-v012.html#joining-three-or-more-tables",
    "href": "sql0035-workingWithMultipleTables-v012.html#joining-three-or-more-tables",
    "title": "31  31.  Inner Joins.",
    "section": "31.8 Joining three or more tables",
    "text": "31.8 Joining three or more tables\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Joining three or more tables\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# For many queries you will need to join more than two tables together.\n# For example, suppose we wanted to list the titles and the author names\n# for each title.\n#\n# The title_name column is in the titles table. \n# The au_fname and au_lname columns are in the authors tables.\n#\n# Therefore, you will obviously need to include the titles and authors\n# tables in the select statement. However, the only way to figure\n# out which authors wrote which titles is to \n# also look at the title_authors table. Therefore the title_authors\n# table must also be part of the query.\n#\n# The best way to understand which tables must be included in a query\n# is to look at the ERD (Entity Relationship Diagram) for the database.\n# If you need data from two different tables that are not directly \n# connected to each other on the diagram, you will also need to join all of the\n# tables that lie BETWEEN those tables also.\n#\n# To join multiple tables together, look at the diagram and find \"path\" on the\n# diagram starting with one table that you need data from and ending with \n# another table that you need data from. \n#\n# For example, if you want to show\n# the author names and the title names, you can start with the authors\n# table, then join onto that the title_authors table, then finally join \n# the authors table.\n# \n# Alternatively, you could start with the titles table, then join onto\n# that the title_authors table and then finally join the authors table.\n# It doesn't make a difference which table you start with and which table you\n# end with, as long as you join all of the tables in the path of tables.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# QUESTION\n# Show the the same info as the previous query but instead of the au_id,\n# show the authors first and last names.\n\nsql = \"select titles.title_id, title_name, authors.au_id, au_fname, au_lname\n       from titles join title_authors   on    titles.title_id = title_authors.title_id\n                   join authors         on    authors.au_id   = title_authors.au_id\n       order by titles.title_id\"\nsqldf(sql)\n\n   title_id                          title_name au_id  au_fname  au_lname\n1       T01                               1977!   A01     Sarah   Buchman\n2       T02           200 Years of German Humor   A01     Sarah   Buchman\n3       T03        Ask Yor System Administrator   A05 Christian     Kells\n4       T04           But I Did It Unconciously   A03    Hallie      Hull\n5       T04           But I Did It Unconciously   A04      Klee      Hull\n6       T05              Exchange of Platitudes   A04      Klee      Hull\n7       T06                    How About Never?   A02     Wendy Heydemark\n8       T07                   I Blame My Mother   A02     Wendy Heydemark\n9       T07                   I Blame My Mother   A04      Klee      Hull\n10      T08        Just Wait Until After School   A06    Harvey   Kellsey\n11      T09                     Kiss My Boo Boo   A06    Harvey   Kellsey\n12      T10         Not Without My Fabrerge Egg   A02     Wendy Heydemark\n13      T11    Perhaps It's a Glandular Problem   A03    Hallie      Hull\n14      T11    Perhaps It's a Glandular Problem   A04      Klee      Hull\n15      T11    Perhaps It's a Glandular Problem   A06    Harvey   Kellsey\n16      T12           Spontaneous, Not Annoying   A02     Wendy Heydemark\n17      T13 What Are The Civilian Applications?   A01     Sarah   Buchman\n\n# QUESTION \n# Same as previous question but show the au_id in addition to the authors' names.\n\n# ANSWER\n# The only extra issue in this question is that since the au_id column \n# appears in more than one table in the query (i.e. it's in titles\n# and in the title_authors tables), You cannot simply type au_id in \n# the select clause - that would be an ambiguous column name. \n# Rather you must write either authors.au_id or title_authors.au_id.\n# Either one authors.au_id or title_authors.au_id will yield the exact\n# same results because after the \"on\" clauses got finished processing\n# the only rows left from the cross join are rows in which the\n# authors.au_id and title_authors.au_id columns contain the same values.\n# or title_authors.au_id. You \n\n# WRONG ... the au_id column is ambiguous - you need to specify the table name\nsql = \"select titles.title_id, title_name, au_id, au_fname, au_lname\n       from titles join title_authors   on    titles.title_id = title_authors.title_id\n                   join authors         on    authors.au_id   = title_authors.au_id\n       order by titles.title_id\"\nsqldf(sql)\n\nError: ambiguous column name: au_id\n\n# This works (authors.au_id)\nsql = \"select titles.title_id, title_name, authors.au_id, au_fname, au_lname\n       from titles join title_authors   on    titles.title_id = title_authors.title_id\n                   join authors         on    authors.au_id   = title_authors.au_id\n       order by titles.title_id\"\nsqldf(sql)\n\n   title_id                          title_name au_id  au_fname  au_lname\n1       T01                               1977!   A01     Sarah   Buchman\n2       T02           200 Years of German Humor   A01     Sarah   Buchman\n3       T03        Ask Yor System Administrator   A05 Christian     Kells\n4       T04           But I Did It Unconciously   A03    Hallie      Hull\n5       T04           But I Did It Unconciously   A04      Klee      Hull\n6       T05              Exchange of Platitudes   A04      Klee      Hull\n7       T06                    How About Never?   A02     Wendy Heydemark\n8       T07                   I Blame My Mother   A02     Wendy Heydemark\n9       T07                   I Blame My Mother   A04      Klee      Hull\n10      T08        Just Wait Until After School   A06    Harvey   Kellsey\n11      T09                     Kiss My Boo Boo   A06    Harvey   Kellsey\n12      T10         Not Without My Fabrerge Egg   A02     Wendy Heydemark\n13      T11    Perhaps It's a Glandular Problem   A03    Hallie      Hull\n14      T11    Perhaps It's a Glandular Problem   A04      Klee      Hull\n15      T11    Perhaps It's a Glandular Problem   A06    Harvey   Kellsey\n16      T12           Spontaneous, Not Annoying   A02     Wendy Heydemark\n17      T13 What Are The Civilian Applications?   A01     Sarah   Buchman\n\n# This also works (title_authors.au_id)\nsql = \"select titles.title_id, title_name, title_authors.au_id, au_fname, au_lname\n       from titles join title_authors   on    titles.title_id = title_authors.title_id\n                   join authors         on    authors.au_id   = title_authors.au_id\n       order by titles.title_id\"\nsqldf(sql)\n\n   title_id                          title_name au_id  au_fname  au_lname\n1       T01                               1977!   A01     Sarah   Buchman\n2       T02           200 Years of German Humor   A01     Sarah   Buchman\n3       T03        Ask Yor System Administrator   A05 Christian     Kells\n4       T04           But I Did It Unconciously   A03    Hallie      Hull\n5       T04           But I Did It Unconciously   A04      Klee      Hull\n6       T05              Exchange of Platitudes   A04      Klee      Hull\n7       T06                    How About Never?   A02     Wendy Heydemark\n8       T07                   I Blame My Mother   A02     Wendy Heydemark\n9       T07                   I Blame My Mother   A04      Klee      Hull\n10      T08        Just Wait Until After School   A06    Harvey   Kellsey\n11      T09                     Kiss My Boo Boo   A06    Harvey   Kellsey\n12      T10         Not Without My Fabrerge Egg   A02     Wendy Heydemark\n13      T11    Perhaps It's a Glandular Problem   A03    Hallie      Hull\n14      T11    Perhaps It's a Glandular Problem   A04      Klee      Hull\n15      T11    Perhaps It's a Glandular Problem   A06    Harvey   Kellsey\n16      T12           Spontaneous, Not Annoying   A02     Wendy Heydemark\n17      T13 What Are The Civilian Applications?   A01     Sarah   Buchman\n\n# QUESTION\n# 19.   List the publisher name, the title_name, the authors first and last names,\n#     the author's state and the publishers state for all books.\n#     Make sure that in the output the authors state and publishers state\n#     columns are clearly labeled as such.\n#     Sort the rows alphabetically by the title name. If a title has more than one \n#     author then there will be a different row in the output for each author.\n#     Sort those rows alphabetically by the publisher name, then by the \n#     title and finally by the author's names.\n\n# ANSWER\n# This query requires joining 4 different tables.\n# See the code for details.\n# Notice that in the order by the au_lname comes before au_fname (this is in\n# general how people's names are sorted alphabetically)\n\nsql = \"select pub_name, title_name, au_fname, au_lname,\n              authors.state as 'author state', \n              publishers.state as 'publisher state'\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\n  order by pub_name, title_name, au_lname, au_fname\"\nsqldf(sql)\n\n               pub_name                          title_name  au_fname  au_lname author state publisher state\n1        AAA Publishing         Not Without My Fabrerge Egg     Wendy Heydemark           CO              CA\n2        AAA Publishing    Perhaps It's a Glandular Problem    Hallie      Hull           CA              CA\n3        AAA Publishing    Perhaps It's a Glandular Problem      Klee      Hull           CA              CA\n4        AAA Publishing    Perhaps It's a Glandular Problem    Harvey   Kellsey           CA              CA\n5        AAA Publishing           Spontaneous, Not Annoying     Wendy Heydemark           CO              CA\n6     Abatis Publishers                               1977!     Sarah   Buchman           NY              NY\n7     Abatis Publishers           But I Did It Unconciously    Hallie      Hull           CA              NY\n8     Abatis Publishers           But I Did It Unconciously      Klee      Hull           CA              NY\n9     Abatis Publishers              Exchange of Platitudes      Klee      Hull           CA              NY\n10    Abatis Publishers                    How About Never?     Wendy Heydemark           CO              NY\n11    Abatis Publishers        Just Wait Until After School    Harvey   Kellsey           CA              NY\n12    Abatis Publishers                     Kiss My Boo Boo    Harvey   Kellsey           CA              NY\n13      Core Dump Books        Ask Yor System Administrator Christian     Kells           NY              CA\n14 Schandenfreude Press           200 Years of German Humor     Sarah   Buchman           NY            &lt;NA&gt;\n15 Schandenfreude Press                   I Blame My Mother     Wendy Heydemark           CO            &lt;NA&gt;\n16 Schandenfreude Press                   I Blame My Mother      Klee      Hull           CA            &lt;NA&gt;\n17 Schandenfreude Press What Are The Civilian Applications?     Sarah   Buchman           NY            &lt;NA&gt;\n\n# The following shows a breakdown of what is actually happening when you \n# join multiple tables. All of the following \"steps\" happen at once when \n# you run the single query shown above in the answer. This breakdown\n# of the steps helps you to understand what exactly is happening.\n\n# step 1 - the first two tables are joined \nsql = \"select authors.*, title_authors.*\n    from authors join title_authors on authors.au_id = title_authors.au_id\"\nsqldf(sql)\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4\n\n# step 2 - the results from the join of the\n#          1st two tables (authors and title_authors) is joined with\n#          the 3rd table (titles)\n\nsql = \"select authors.*, title_authors.*, titles.*\n    from authors join title_authors on authors.au_id = title_authors.au_id\n                          join titles on title_authors.title_id = titles.title_id\" \n\nsqldf(sql)\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                          title_name       type pub_id pages price   sales         pubdate\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n\n# step 3 - the results from the join of the first 3 tables (authors,\n#          title_authors, titles) is then joined with the 4th table (publishers)\n\nsql = \"select authors.*, title_authors.*, titles.*, publishers.*\n    from authors join title_authors on authors.au_id = title_authors.au_id\n                          join titles on title_authors.title_id = titles.title_id\n                        join publishers on publishers.pub_id = titles.pub_id\" \n\nsqldf(sql)\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                          title_name       type pub_id pages price   sales         pubdate pub_id             pub_name          city state country\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P05       AAA Publishing      Berkeley    CA     USA\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P01    Abatis Publishers      New York    NY     USA\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n\n# step 4 - the order by clause puts the rows in the correct order\nsql = \"select authors.*, title_authors.*, titles.*, publishers.*\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\"\nsqldf(sql)\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                          title_name       type pub_id pages price   sales         pubdate pub_id             pub_name          city state country\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P05       AAA Publishing      Berkeley    CA     USA\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P01    Abatis Publishers      New York    NY     USA\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n\n# step 5 - the select clause specifies which columns are actually displayed\nsql = \"select pub_name, title_name, au_fname, au_lname,\n              authors.state as 'author state', \n              publishers.state as 'publisher state'\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\n  order by pub_name, title_name, au_lname, au_fname\"\nsqldf(sql)\n\n               pub_name                          title_name  au_fname  au_lname author state publisher state\n1        AAA Publishing         Not Without My Fabrerge Egg     Wendy Heydemark           CO              CA\n2        AAA Publishing    Perhaps It's a Glandular Problem    Hallie      Hull           CA              CA\n3        AAA Publishing    Perhaps It's a Glandular Problem      Klee      Hull           CA              CA\n4        AAA Publishing    Perhaps It's a Glandular Problem    Harvey   Kellsey           CA              CA\n5        AAA Publishing           Spontaneous, Not Annoying     Wendy Heydemark           CO              CA\n6     Abatis Publishers                               1977!     Sarah   Buchman           NY              NY\n7     Abatis Publishers           But I Did It Unconciously    Hallie      Hull           CA              NY\n8     Abatis Publishers           But I Did It Unconciously      Klee      Hull           CA              NY\n9     Abatis Publishers              Exchange of Platitudes      Klee      Hull           CA              NY\n10    Abatis Publishers                    How About Never?     Wendy Heydemark           CO              NY\n11    Abatis Publishers        Just Wait Until After School    Harvey   Kellsey           CA              NY\n12    Abatis Publishers                     Kiss My Boo Boo    Harvey   Kellsey           CA              NY\n13      Core Dump Books        Ask Yor System Administrator Christian     Kells           NY              CA\n14 Schandenfreude Press           200 Years of German Humor     Sarah   Buchman           NY            &lt;NA&gt;\n15 Schandenfreude Press                   I Blame My Mother     Wendy Heydemark           CO            &lt;NA&gt;\n16 Schandenfreude Press                   I Blame My Mother      Klee      Hull           CA            &lt;NA&gt;\n17 Schandenfreude Press What Are The Civilian Applications?     Sarah   Buchman           NY            &lt;NA&gt;",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'><yrChapterNumber>31. </yrChapterNumber> Inner Joins.</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "",
    "text": "32.1 The “books database”\nIn this and other sections we will be referring to the “books database”.\nClick below to download a document that describes the books database and database technology in general. This document also contains many questions/answers to test your knowledge of SQL. You can also download the CSV files that contain the data for the books database.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#click-to-download-the-specified-files",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#click-to-download-the-specified-files",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.2 click to download the specified files",
    "text": "32.2 click to download the specified files\n\nbooksDatabase description and Questions\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties.csv\n\n\n# load the package we'll need\nif (!require(sqldf)) { install.packages(\"sqldf\"); require(sqldf) }\n\nLoading required package: sqldf\n\n\nWarning: package 'sqldf' was built under R version 4.4.3\n\n\nLoading required package: gsubfn\n\n\nWarning: package 'gsubfn' was built under R version 4.4.3\n\n\nLoading required package: proto\n\n\nWarning: package 'proto' was built under R version 4.4.3\n\n\nLoading required package: RSQLite\n\n\nWarning: package 'RSQLite' was built under R version 4.4.3\n\nif (!require(readr)) { install.packages(\"readr\"); require(readr) }\n\nLoading required package: readr\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"data/booksDatabase/titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"data/booksDatabase/authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"data/booksDatabase/publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"data/booksDatabase/title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"data/booksDatabase/royalties.csv\", na=\"NULL\", show_col_types=FALSE)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#non-aggreate-functions-vs-aggregate-functions",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#non-aggreate-functions-vs-aggregate-functions",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.3 “non-aggreate functions” vs “aggregate functions”",
    "text": "32.3 “non-aggreate functions” vs “aggregate functions”\n\n############################################################################\n# Intro to SQL functions:\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#   SQL contains two different types of functions\n#\n#   - non-aggregate functions - return a different value for every row\n#\n#   - aggregate functions - return a single value for multiple rows in the table\n#     These functions combine several data values in a single value. \n#     For example the sum function will combine several values into a single sum.\n#     see below for more details\n#\n############################################################################\n\n\n32.3.1 The titles table\n\n# Let's remember what is in the titles table.\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n\n\n\n32.3.2 Example of a non-aggregate function - upper\n\n#................................................................\n# Example of non-aggregate function\n#\n# The upper function simply transforms its argument to UPPERCASE.\n#................................................................\n\n# we can get just the titles and the prices:\nsqldf(\"select title_name, price from titles\")\n\n                            title_name price\n1                                1977! 21.99\n2            200 Years of German Humor 19.95\n3         Ask Yor System Administrator 39.95\n4            But I Did It Unconciously 12.99\n5               Exchange of Platitudes  6.95\n6                     How About Never? 19.95\n7                    I Blame My Mother 23.95\n8         Just Wait Until After School 10.00\n9                      Kiss My Boo Boo 13.95\n10         Not Without My Fabrerge Egg    NA\n11    Perhaps It's a Glandular Problem  7.99\n12           Spontaneous, Not Annoying 12.99\n13 What Are The Civilian Applications? 29.99\n\n# The following uses the upper function to make each title UPPER CASE\nsqldf(\"select upper(title_name), price from titles\")\n\n                     upper(title_name) price\n1                                1977! 21.99\n2            200 YEARS OF GERMAN HUMOR 19.95\n3         ASK YOR SYSTEM ADMINISTRATOR 39.95\n4            BUT I DID IT UNCONCIOUSLY 12.99\n5               EXCHANGE OF PLATITUDES  6.95\n6                     HOW ABOUT NEVER? 19.95\n7                    I BLAME MY MOTHER 23.95\n8         JUST WAIT UNTIL AFTER SCHOOL 10.00\n9                      KISS MY BOO BOO 13.95\n10         NOT WITHOUT MY FABRERGE EGG    NA\n11    PERHAPS IT'S A GLANDULAR PROBLEM  7.99\n12           SPONTANEOUS, NOT ANNOYING 12.99\n13 WHAT ARE THE CIVILIAN APPLICATIONS? 29.99\n\n\n\n\n32.3.3 Example of an aggregate function - avg\n\n#................................................................\n# Example of an aggregate function\n#\n# The avg function takes the average of a set of a set of values.\n#................................................................\n\n# show all the prices\nsqldf(\"select price from titles\")\n\n   price\n1  21.99\n2  19.95\n3  39.95\n4  12.99\n5   6.95\n6  19.95\n7  23.95\n8  10.00\n9  13.95\n10    NA\n11  7.99\n12 12.99\n13 29.99\n\n# show the average of the prices\nsqldf(\"select avg(price) from titles\")\n\n  avg(price)\n1    18.3875",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#demonstrating-non-aggreate-functions-with-simple-select-statements",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#demonstrating-non-aggreate-functions-with-simple-select-statements",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.4 Demonstrating non-aggreate functions with simple select statements",
    "text": "32.4 Demonstrating non-aggreate functions with simple select statements\n\n##############################################################################\n# queries with JUST a select clause\n##############################################################################\n\n# It's not the most common thing to do but you can write a select statement\n# that does not actually retrieve any information from any table. \n# \n# The purpose is to use the SQL language to calculate values, similar to\n# how you can use R or a calculator to calculate sums, products, etc.\n#\n# This feature will help us demonstrate some of the sql functions below.\n# \n# Example:\n\n# perform some calculations and display the results\nsqldf(\"select 3+2,   10*2,   23/5,   23.0/5\")    # show the results of various calculations\n\n  3+2 10*2 23/5 23.0/5\n1   5   20    4    4.6\n\n# We can include our own names for the columns.\n# When you change the name of a column, the new column name is known as a 'column alias'\nsqldf(\"select 3+2 as A,   10*2 as B,   23/5 as C,   23.0/5 as D\")    \n\n  A  B C   D\n1 5 20 4 4.6\n\n# You don't need the \"as\" when creating column aliases\n# ... but some people do so anyway since it reads nicer with the \"as\"\n#\n# same results as above\nsqldf(\"select 3+2 A,   10*2 B,   23/5 C,   23.0/5 D\")    \n\n  A  B C   D\n1 5 20 4 4.6\n\n# The following is not really doing anything useful but it\n# helps to demonstrate what the upper function does.\n# This technique is used on the sqlitetutorial.net website\n# to demonstrate how many of the SQLite functions work. \n#\n# For example, click on the specific function names you find on the following\n# page for more info about the functions and examples of using the functions.\n#   https://www.sqlitetutorial.net/sqlite-string-functions/\n\nsqldf(\"select upper('abcde')\")\n\n  upper('abcde')\n1          ABCDE",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#non-aggreate-functions-in-sqlite",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#non-aggreate-functions-in-sqlite",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.5 Non-aggreate functions in SQLite",
    "text": "32.5 Non-aggreate functions in SQLite\n\n#----------------------------------------------------\n#\n# Non-aggregate functions in SQLite\n#\n#----------------------------------------------------\n\n\n# The non-aggregate functions are NOT defined as part of the ANSI SQL standard.\n# Every database management system software offers their own versions of \n# non-aggregate functions.  For example, MySQL, Microsoft SQL Server, Postgres\n# and sqlite all have different non-aggregate functions that they make available.\n# To undertand how to use these function, you need to see the documentation\n# for your particular version of SQL. See the following webpage for\n# some functions in SQLite. We will first look at the \n# \"string functions\" i.e. functions that work with \"string\" values\n# (i.e. character values):\n#\n#     https://www.sqlitetutorial.net/sqlite-string-functions/\n#\n# The string functions available in sqlite are the following.\n# See the website for more details:\n#\n#    substr, trim, rtrim, ltrim, length, replace, upper, lower, instr\n\n# first let's remember what is in the titles table\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n\n\n32.5.1 upper() and lower() functions\n\n#........................................................\n#\n# upper ( character_value ) - returns the UPPERCASE version of the character value\n#\n# lower ( character_value ) - returns the lowercase version of the character value\n# \n#........................................................\n\n# show first 5 titles \nsqldf(\"select title_name\n       from titles\n       limit 5\")\n\n                    title_name\n1                        1977!\n2    200 Years of German Humor\n3 Ask Yor System Administrator\n4    But I Did It Unconciously\n5       Exchange of Platitudes\n\n# show the first 5 titles in UPPERCASE\nsqldf(\"select upper(title_name)\n      from titles\n      limit 5\")\n\n             upper(title_name)\n1                        1977!\n2    200 YEARS OF GERMAN HUMOR\n3 ASK YOR SYSTEM ADMINISTRATOR\n4    BUT I DID IT UNCONCIOUSLY\n5       EXCHANGE OF PLATITUDES\n\n# Use a more friendly column name (i.e. a 'column alias').\n# i.e. display \"title\" at the top of the column instead of \"upper(title_name)\"\nsqldf(\"select upper(title_name) as title\n      from titles\n      limit 5\")\n\n                         title\n1                        1977!\n2    200 YEARS OF GERMAN HUMOR\n3 ASK YOR SYSTEM ADMINISTRATOR\n4    BUT I DID IT UNCONCIOUSLY\n5       EXCHANGE OF PLATITUDES\n\n# show both the original title and the uppercase version in the same query\nsqldf(\"select title_name, upper(title_name) as TITLE_NAME\n      from titles\n      limit 5\")\n\n                    title_name                   TITLE_NAME\n1                        1977!                        1977!\n2    200 Years of German Humor    200 YEARS OF GERMAN HUMOR\n3 Ask Yor System Administrator ASK YOR SYSTEM ADMINISTRATOR\n4    But I Did It Unconciously    BUT I DID IT UNCONCIOUSLY\n5       Exchange of Platitudes       EXCHANGE OF PLATITUDES\n\n# show the title in addition to other information\nsqldf(\"select upper(title_name) as TITLE_NAME, price\n      from titles\n      limit 5\")\n\n                    TITLE_NAME price\n1                        1977! 21.99\n2    200 YEARS OF GERMAN HUMOR 19.95\n3 ASK YOR SYSTEM ADMINISTRATOR 39.95\n4    BUT I DID IT UNCONCIOUSLY 12.99\n5       EXCHANGE OF PLATITUDES  6.95\n\n# In SQLite, you can use the * along with other column names\n# but note that this is not standard.\n# Not every SQL will accept the * unless it is the only value in the select clause.\n#\n# Notice that the * stands in for ALL column names, including the title_name\n# even though you already displayed the title_name.\n\nsqldf(\"select upper(title_name) as TITLE_NAME, *\n       from titles\n       limit 5\")\n\n                    TITLE_NAME title_id                   title_name       type pub_id pages price  sales        pubdate\n1                        1977!      T01                        1977!    history    P01   107 21.99    566  8/1/2000 0:00\n2    200 YEARS OF GERMAN HUMOR      T02    200 Years of German Humor    history    P03    14 19.95   9566  4/1/1998 0:00\n3 ASK YOR SYSTEM ADMINISTRATOR      T03 Ask Yor System Administrator   computer    P02  1226 39.95  25667  9/1/2000 0:00\n4    BUT I DID IT UNCONCIOUSLY      T04    But I Did It Unconciously psychology    P01   510 12.99  13001 5/31/1999 0:00\n5       EXCHANGE OF PLATITUDES      T05       Exchange of Platitudes psychology    P01   201  6.95 201440  1/1/2001 0:00\n\n\n\n\n32.5.2 ltrim() , rtrim() , trim()\n\n#.........................................................................\n#\n# ltrim ( string, [character] )\n# rtrim ( string, [character] )   \n# trim  ( string, [character] )\n#\n# ARGUMENTS\n#   - string - the value to be modified\n#\n#\n#   - [character] \n#\n#     The [brackets] in the documenation indicate that this value is optional.\n#     If specified, this is expected to be a string (ie. a character value \n#     enclosed in 'single quotes'). The characters in the string will be removed\n#     from the left (or right) hand side of the string:\n#        the l in ltrim stands for left\n#        the r in rtrim stands for right\n#.........................................................................\n\n# let's see the original data\nsqldf(\"select title_name, pubdate\n      from titles\")\n\n                            title_name         pubdate\n1                                1977!   8/1/2000 0:00\n2            200 Years of German Humor   4/1/1998 0:00\n3         Ask Yor System Administrator   9/1/2000 0:00\n4            But I Did It Unconciously  5/31/1999 0:00\n5               Exchange of Platitudes   1/1/2001 0:00\n6                     How About Never?  7/31/2000 0:00\n7                    I Blame My Mother  10/1/1999 0:00\n8         Just Wait Until After School   6/1/2001 0:00\n9                      Kiss My Boo Boo  5/31/2002 0:00\n10         Not Without My Fabrerge Egg            &lt;NA&gt;\n11    Perhaps It's a Glandular Problem 11/30/2000 0:00\n12           Spontaneous, Not Annoying  8/31/2000 0:00\n13 What Are The Civilian Applications?  5/31/1999 0:00\n\n# get rid of the time indicator from the end of the pubdate\nsqldf(\"select title_name, rtrim(pubdate, '0:') as publication_date\n      from titles\")\n\n                            title_name publication_date\n1                                1977!        8/1/2000 \n2            200 Years of German Humor        4/1/1998 \n3         Ask Yor System Administrator        9/1/2000 \n4            But I Did It Unconciously       5/31/1999 \n5               Exchange of Platitudes        1/1/2001 \n6                     How About Never?       7/31/2000 \n7                    I Blame My Mother       10/1/1999 \n8         Just Wait Until After School        6/1/2001 \n9                      Kiss My Boo Boo       5/31/2002 \n10         Not Without My Fabrerge Egg             &lt;NA&gt;\n11    Perhaps It's a Glandular Problem      11/30/2000 \n12           Spontaneous, Not Annoying       8/31/2000 \n13 What Are The Civilian Applications?       5/31/1999 \n\n\n\n\n32.5.3 substr( string, start, length )\n\n#........................................................\n# substr ( string, start, length)\n# \n# returns a \"substring\", i.e. a portion of the string (ie. charcter value).\n# ARGUMENTS:\n#    start - the position to start taking info from\n#    length - the number of characters to include\n#........................................................\n\n# get just the number from the title_id\nsqldf(\"select title_id, substr(title_id, 2, 2) as title_number from titles\")\n\n   title_id title_number\n1       T01           01\n2       T02           02\n3       T03           03\n4       T04           04\n5       T05           05\n6       T06           06\n7       T07           07\n8       T08           08\n9       T09           09\n10      T10           10\n11      T11           11\n12      T12           12\n13      T13           13\n\n# If length is too long the rest of the string is taken.\nsqldf(\"select title_id, substr(title_id, 2, 200) as title_number from titles\")\n\n   title_id title_number\n1       T01           01\n2       T02           02\n3       T03           03\n4       T04           04\n5       T05           05\n6       T06           06\n7       T07           07\n8       T08           08\n9       T09           09\n10      T10           10\n11      T11           11\n12      T12           12\n13      T13           13\n\n# abbreviate the title\nsqldf(\"select title_name, substr(title_name, 1, 20) as abbreviated_title from titles\")\n\n                            title_name    abbreviated_title\n1                                1977!                1977!\n2            200 Years of German Humor 200 Years of German \n3         Ask Yor System Administrator Ask Yor System Admin\n4            But I Did It Unconciously But I Did It Unconci\n5               Exchange of Platitudes Exchange of Platitud\n6                     How About Never?     How About Never?\n7                    I Blame My Mother    I Blame My Mother\n8         Just Wait Until After School Just Wait Until Afte\n9                      Kiss My Boo Boo      Kiss My Boo Boo\n10         Not Without My Fabrerge Egg Not Without My Fabre\n11    Perhaps It's a Glandular Problem Perhaps It's a Gland\n12           Spontaneous, Not Annoying Spontaneous, Not Ann\n13 What Are The Civilian Applications? What Are The Civilia\n\n# abbreviate the title (add on '...' at the end of every value - not just the ones that were shortened)\nsqldf(\"select title_name, substr(title_name, 1, 20) || ' ...' as abbreviated_title from titles\")\n\n                            title_name        abbreviated_title\n1                                1977!                1977! ...\n2            200 Years of German Humor 200 Years of German  ...\n3         Ask Yor System Administrator Ask Yor System Admin ...\n4            But I Did It Unconciously But I Did It Unconci ...\n5               Exchange of Platitudes Exchange of Platitud ...\n6                     How About Never?     How About Never? ...\n7                    I Blame My Mother    I Blame My Mother ...\n8         Just Wait Until After School Just Wait Until Afte ...\n9                      Kiss My Boo Boo      Kiss My Boo Boo ...\n10         Not Without My Fabrerge Egg Not Without My Fabre ...\n11    Perhaps It's a Glandular Problem Perhaps It's a Gland ...\n12           Spontaneous, Not Annoying Spontaneous, Not Ann ...\n13 What Are The Civilian Applications? What Are The Civilia ...",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.6 Practice",
    "text": "32.6 Practice\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n# Write a select statement that retrieves all books that were published \n# during a month with a single digit.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n\nsqldf(\"select *\n       from titles\n       where substr(pubdate, 2, 1) = '/'\n       order by title_name\")\n\n   title_id                          title_name       type pub_id pages price  sales        pubdate\n1       T01                               1977!    history    P01   107 21.99    566  8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95   9566  4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95  25667  9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99  13001 5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95 201440  1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95  11320 7/31/2000 0:00\n7       T08        Just Wait Until After School   children    P01    86 10.00   4095  6/1/2001 0:00\n8       T09                     Kiss My Boo Boo   children    P01    22 13.95   5000 5/31/2002 0:00\n9       T12           Spontaneous, Not Annoying  biography    P05   507 12.99 100001 8/31/2000 0:00\n10      T13 What Are The Civilian Applications?    history    P03   802 29.99  10467 5/31/1999 0:00\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Question\n# Write a select statement that lists the title_name, type and publication date\n# of those books that were published in October, November or December \n# (i.e. months whose numeric value is 2 digits)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n# We could simply use != \n\nsqldf(\"select title_name, type, pubdate\n       from titles\n       where substr(pubdate, 2, 1) != '/'\n       order by pubdate\")\n\n                        title_name       type         pubdate\n1                I Blame My Mother  biography  10/1/1999 0:00\n2 Perhaps It's a Glandular Problem psychology 11/30/2000 0:00\n\n# ANSWER\n\n# One way: Identify those rows that have a 2 digit month as those rows which\n# have a '/' in the 3rd position of the pubdate.\n#\n# Notice that the books with a 1 digit months in the pubdate\n# do not appear in the results.\n\nsqldf(\"select title_name, type, pubdate \n       from titles\n       where substr(pubdate, 3, 1) = '/'\")\n\n                        title_name       type         pubdate\n1                I Blame My Mother  biography  10/1/1999 0:00\n2 Perhaps It's a Glandular Problem psychology 11/30/2000 0:00\n\n# Another way: Identify those rows for which the first 2 characters of the \n# pubdate are '10', '11' or '12'\n\nsqldf(\"select title_name, type, pubdate \n       from titles\n       where substr(pubdate, 1, 2) in ('10','11','12')\n      \")\n\n                        title_name       type         pubdate\n1                I Blame My Mother  biography  10/1/1999 0:00\n2 Perhaps It's a Glandular Problem psychology 11/30/2000 0:00\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Question\n# Write a select statement that lists the title_name and day of the month \n# that each title was published.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# ANSWER\n\n# To find the day of the month, let's first trim away the numbers from the \n# left and the right. Then we'll trim away the /'s from the left and the right\n# to leave just the day of the month.\n\n\n# We'll start by just trimming away the numbers spaces and colons.\nsqldf(\"select title_name, trim(pubdate, '0123456789: ') \n       from titles\")\n\n                            title_name trim(pubdate, '0123456789: ')\n1                                1977!                           /1/\n2            200 Years of German Humor                           /1/\n3         Ask Yor System Administrator                           /1/\n4            But I Did It Unconciously                          /31/\n5               Exchange of Platitudes                           /1/\n6                     How About Never?                          /31/\n7                    I Blame My Mother                           /1/\n8         Just Wait Until After School                           /1/\n9                      Kiss My Boo Boo                          /31/\n10         Not Without My Fabrerge Egg                          &lt;NA&gt;\n11    Perhaps It's a Glandular Problem                          /30/\n12           Spontaneous, Not Annoying                          /31/\n13 What Are The Civilian Applications?                          /31/\n\n# Now for the final answer we can also trim away the slashes\nsqldf(\"select title_name, trim(trim(pubdate, '0123456789: '),'/') as dayOfMonth\n       from titles\")\n\n                            title_name dayOfMonth\n1                                1977!          1\n2            200 Years of German Humor          1\n3         Ask Yor System Administrator          1\n4            But I Did It Unconciously         31\n5               Exchange of Platitudes          1\n6                     How About Never?         31\n7                    I Blame My Mother          1\n8         Just Wait Until After School          1\n9                      Kiss My Boo Boo         31\n10         Not Without My Fabrerge Egg       &lt;NA&gt;\n11    Perhaps It's a Glandular Problem         30\n12           Spontaneous, Not Annoying         31\n13 What Are The Civilian Applications?         31\n\n# NOTE - the following DOES NOT WORK - because it trims off EVERY\nsqldf(\"select title_name, trim(pubdate, '0123456789: /') as dayOfMonth\n       from titles\")\n\n                            title_name dayOfMonth\n1                                1977!           \n2            200 Years of German Humor           \n3         Ask Yor System Administrator           \n4            But I Did It Unconciously           \n5               Exchange of Platitudes           \n6                     How About Never?           \n7                    I Blame My Mother           \n8         Just Wait Until After School           \n9                      Kiss My Boo Boo           \n10         Not Without My Fabrerge Egg       &lt;NA&gt;\n11    Perhaps It's a Glandular Problem           \n12           Spontaneous, Not Annoying           \n13 What Are The Civilian Applications?           \n\n\n\n32.6.1 Other functions in SQLite\n\n#------------------------------------------------------------------\n#\n# see these webpages for examples of the other functions\n#    https://www.sqlitetutorial.net/sqlite-functions/sqlite-instr/\n#    https://www.techonthenet.com/sqlite/functions/index.php\n#------------------------------------------------------------------",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#aggregate-functions",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#aggregate-functions",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.7 aggregate functions",
    "text": "32.7 aggregate functions\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Aggregate functions\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Aggregate functions take a bunch of values as input and return a single value.\n# For example the avg function averages together several numbers and returns\n# a single number.\n#\n# The following are standard aggregate functions that are included with \n# every version of SQL. These are part of the ANSI standard for SQL.\n# These are the aggregagte functions that we will focus on. \n#\n#   sum, avg, count, min, max\n#\n# Different database management systems might include some other aggregate\n# functions in addition to these. See the documentation for your particular\n# database management system for more info about other aggregate functions.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Show the contents of the titles table\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n# show the average price, minimum (i.e. lowest) price and maximum (ie. highest price) \n# of all the titles\nsqldf(\"select avg(price),  min(price) , max(price)\n      from titles\")\n\n  avg(price) min(price) max(price)\n1    18.3875       6.95      39.95",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#dont-mix-aggregate-functions-with-non-aggregated-data",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#dont-mix-aggregate-functions-with-non-aggregated-data",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.8 Don’t mix aggregate functions with non-aggregated data",
    "text": "32.8 Don’t mix aggregate functions with non-aggregated data\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# DO NO MIX AGGREGATE FUNCTIONS WITH NON-AGGREGATED DATA.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# IN MANY SQL DBMS's YOU WILL GET AN ERROR IF YOU TRY TO DO THIS.\n# sqlite does NOT report an error if you do this but the results will not \n# make much sense. \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# The following shows that in sqlite when you mix aggregate functions \n# with non-aggregated data the results are confusing. Don't do this!\n# Some DBMS's will return an ERROR for this query. sqlite does not return\n# an error but the results are very misleading.\n#\n# For example the following is the result of the code below:\n#\n#    &gt; sqldf(\"select avg(price), sum(pages),  title_name, type\n#             from titles\")\n# \n#       avg(price) sum(pages) title_name    type\n#          18.3875       5107      1977! history\n# \n# The avg function and the sum function return aggregagated data for all of\n# the rows in the titles table. Specically the average price of all the books\n# and the total number of pages from all the books. \n# \n# However, the title_name and type columns simply display\n# the name of a specific book. The title_name and type that is displayed\n# has nothing to do with the avg(price) and sum(pages). The exact book \n# that is displayed is really not predictable.\n#\n# Bottom line - do NOT mix aggregrate functions, e.g. avg(price), sum(paes)\n# with non-aggregated data, e.g. title_name, type\n\n# DON'T DO THIS!!! - see the comment above.\nsqldf(\"select avg(price), sum(pages),  title_name, type\n      from titles\")\n\n  avg(price) sum(pages) title_name    type\n1    18.3875       5107      1977! history",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#count-function---three-different-versions",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#count-function---three-different-versions",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.9 count function - three different versions",
    "text": "32.9 count function - three different versions\n\n#--------------------------------------------------------------------\n# count function - three different versions\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The count function counts the number of rows returned.\n# There are three different versions of the count function\n#\n#   count(SOME_COLUMN) \n#\n#     This returns the number of rows for the column that are NOT NULL. \n#     (Remember that in SQL NULL is basically the same as NA in R. \n#      Since we are using sqldf to analyze R dataframes instead of actual \n#      database tables, NA, is treated as NULL\n#      ie. when using sqldf, count(SOME_COLUMN) will return the number of \n#          rows in that column that don't contain NA.\n#\n#   count(*) - returns the number of rows. This version counts all rows, including\n#              NULL values and non-distinct values\n#              Note that count is the ONLY aggregate function that can be used \n#              in this way.\n#\n#   count(distinct SOME_COLUMN) - Returns the number of DISTINCT values\n#                                 (i.e. different unique values) in the column.\n#                                 Note that other aggregate functions MAY ALSO\n#                                 be used in this way, eg. avg(distinct price) \n#                                 will average the distinct prices.\n#                                 \n#--------------------------------------------------------------------\n\n# show all of the data in the titles table\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n# show various aggregate function values for the titles column\nsqldf(\"select sum(price) , avg(price) , avg(DISTINCT price), count(*), count(price) , count(DISTINCT price), min(price) , max(price), count(type), count(DISTINCT type)\n      from titles\")\n\n  sum(price) avg(price) avg(DISTINCT price) count(*) count(price) count(DISTINCT price) min(price) max(price) count(type) count(DISTINCT type)\n1     220.65    18.3875              18.771       13           12                    10       6.95      39.95          13                    5",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#min-and-max-also-work-with-character-columns.",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#min-and-max-also-work-with-character-columns.",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.10 min and max also work with character columns.",
    "text": "32.10 min and max also work with character columns.\n\n# min and max also work with character columns.\n#\n# min value of a character column is the value that would appear FIRST in alphabetical order.\n# max value of a character column is the value that would appear LAST  in alphabetical order.\nsqldf(\"select min(type), max(type), count(*), count(type), count(DISTINCT type)\n      from titles\")\n\n  min(type)  max(type) count(*) count(type) count(DISTINCT type)\n1 biography psychology       13          13                    5\n\n# QUESTION\n# WRite a query to display the different types of books. \n# Sort the results alphabetically.\n# Only include each type once.\nsqldf(\"select distinct type from titles order by type asc\")\n\n        type\n1  biography\n2   children\n3   computer\n4    history\n5 psychology",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#two-diffferent-meanings-of-distinct",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#two-diffferent-meanings-of-distinct",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.11 two diffferent meanings of “distinct”",
    "text": "32.11 two diffferent meanings of “distinct”\n\n#--------------------------------------------------------------------\n# two diffferent meanings of \"distinct\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The word \"distinct\" means two toally different things based on \n# where it appears in the select command.\n#\n# 1. select ... count (distinct SOME_COLUMN) ...\n#   \n#    see above\n#\n#\n# 2. select distinct COLUMN1, COLUMN2, ...\n#\n#    When \"distinct\" appears right after the word select, it \n#    means that the output should not include rows that are \n#    exact duplicates of each other - in that case only one of the \n#    duplicates is returned.\n# \n#--------------------------------------------------------------------\n\n# show all the different types of titles (do not show duplicates)\nsqldf(\"select DISTINCT type\n      from titles\")\n\n        type\n1    history\n2   computer\n3 psychology\n4  biography\n5   children\n\n# show just the entire contents of the type column\nsqldf(\"select type\n      from titles\")\n\n         type\n1     history\n2     history\n3    computer\n4  psychology\n5  psychology\n6   biography\n7   biography\n8    children\n9    children\n10  biography\n11 psychology\n12  biography\n13    history\n\n#---------------------------------------------------------------------\n# \"select distinct ...\"  works on ENTIRE ROWS, not individual columns.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Compare the following two queries. Notice that distinct works on entire\n# rows of output to eliminate those rows that are duplicates. \n#\n# Queries that contain more than one column  could still result with duplicate\n# values in each column even when DISTINCT is specified. DISTINCT only serves\n# to eliminate entire rows of data that are exact duplicates.\n#\n# compare the following 2 queries ...\n#---------------------------------------------------------------------\n\n# show the entire contents of the type and pub_id columns\n# sort the results by type and then by pub_id within each type\n#\n# notice in the output that some rows are exact duplicates of each other\nsqldf(\"select type, pub_id\n      from titles\n      order by type, pub_id\")\n\n         type pub_id\n1   biography    P01\n2   biography    P03\n3   biography    P05\n4   biography    P05\n5    children    P01\n6    children    P01\n7    computer    P02\n8     history    P01\n9     history    P03\n10    history    P03\n11 psychology    P01\n12 psychology    P01\n13 psychology    P05\n\n# Do the same thing but use \"distinct\" to eliminate the duplicate rows.\n# \n# Notice in the output that even though \"distinct\" is specified, each column\n# in the result DOES have duplicate values. Hoewver, there are no rows in the\n# output that are exact duplicates of each other.\n\nsqldf(\"select distinct type, pub_id\n      from titles\n      order by type, pub_id\")\n\n        type pub_id\n1  biography    P01\n2  biography    P03\n3  biography    P05\n4   children    P01\n5   computer    P02\n6    history    P01\n7    history    P03\n8 psychology    P01\n9 psychology    P05",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#using-were-in-queries-that-contain-aggregate-functions",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#using-were-in-queries-that-contain-aggregate-functions",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.12 Using WERE in queries that contain aggregate functions",
    "text": "32.12 Using WERE in queries that contain aggregate functions\n\n#-----------------------------------------------------------------------------\n# Using WERE in queries that contain aggregate functions\n#-----------------------------------------------------------------------------\n#\n# Aggregate functions work only on rows that are included by the WHERE clause.\n# \n#-----------------------------------------------------------------------------\n\n# show the complete titles table\nsqldf (\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n# show the number of titles in the titles table\n# The highest price and the average price.\nsqldf( \"select count(*), max(price), avg(price) \n        from titles\")\n\n  count(*) max(price) avg(price)\n1       13      39.95    18.3875\n\n# show the number of biographies,\n# the highest price for a biography and the average price for a biography\n# HINT: use the where clause\nsqldf( \"select count(*), max(price), avg(price)\n        from titles\n        where type = 'biography'\")\n\n  count(*) max(price) avg(price)\n1        4      23.95   18.96333",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-1",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-1",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.13 Practice",
    "text": "32.13 Practice\n\n#-----------------------------------------------------------------------------\n# \n# Practice Questions\n#\n#-----------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------\n# Question:\n#-----------------------------------------------------------------------------\n# Write a query that shows the names of the authors who have written at least one book\n# for which they got 1.00 of the royalty share.\n# Display the author's names in alphabetical order.\n# Do not repeat the same author's name twice\n#\n# HINTS\n# 1. Which tables do you need to include in your query?\n#    Make sure to join all of those tables and any intervening tables\n# \n# 2. Use DISTINCT at the front of your query to eliminate duplicate rows.\n#\n# 3. Use \"order by\" to display the author's names in alphabetical order. Remember\n#    that last name is more significant than first name when arranging names\n#    in alphabetical order.\n#-----------------------------------------------------------------------------\n\n# Answer:\n\nsqldf(\"select distinct au_fname, au_lname\n       from authors join title_authors on authors.au_id = title_authors.au_id\n       where royalty_shares = 1.0\n       order by au_lname asc, au_fname asc\")\n\n   au_fname  au_lname\n1     Sarah   Buchman\n2     Wendy Heydemark\n3      Klee      Hull\n4 Christian     Kells\n5    Harvey   Kellsey\n\n# notice what will happen if you leave out the distinct\nsqldf(\"select au_fname, au_lname\n       from authors join title_authors on authors.au_id = title_authors.au_id\n       where royalty_shares = 1.0\n       order by au_lname asc, au_fname asc\")\n\n    au_fname  au_lname\n1      Sarah   Buchman\n2      Sarah   Buchman\n3      Sarah   Buchman\n4      Wendy Heydemark\n5      Wendy Heydemark\n6      Wendy Heydemark\n7       Klee      Hull\n8  Christian     Kells\n9     Harvey   Kellsey\n10    Harvey   Kellsey\n\n#-----------------------------------------------------------------------------\n# Question\n#-----------------------------------------------------------------------------\n# Write a query that shows the names of the authors who have written at least one book\n# for which the book has a royalty_rate of at least .06\n# Display the author's names in alphabetical order.\n# Do not repeat the same author's name twice.\n\n# HINTS\n# 1. which tables do you need to include in your query?\n#    make sure to join all of those tables and any intervening tables\n# \n# 2. Use DISTINCT at the front of your query to eliminate duplicate rows.\n#\n# 3. Use \"order by\" to display the author's names in alphabetical order. Remember\n#    that last name is more significant than first name when arranging names\n#    in alphabetical order.\n#-----------------------------------------------------------------------------\n\n\n# First attempt ...\n# Not the answer yet ...\n# Let's start out by including the royalty_rate in the select clause. This lets\n# us see that we indeed are only getting authors who contributed to a book \n# whose royalty rate was at least .06. \n#\n# The problem with the following is that because we are including the royalty_rate\n# in the output we will get some authors more than once since the royalty_rate\n# may be different for different books that they worked on.\n\nsqldf(\" select distinct au_fname, au_lname, royalty_rate\n        from authors join title_authors on authors.au_id = title_authors.au_id\n                     join titles        on title_authors.title_id = titles.title_id\n                     join royalties     on titles.title_id = royalties.title_id\n        where royalty_rate &gt; .05\n        order by au_lname, au_fname\n      \")\n\n    au_fname  au_lname royalty_rate\n1      Sarah   Buchman         0.06\n2      Wendy Heydemark         0.08\n3      Wendy Heydemark         0.11\n4      Wendy Heydemark         0.09\n5     Hallie      Hull         0.08\n6     Hallie      Hull         0.07\n7       Klee      Hull         0.08\n8       Klee      Hull         0.09\n9       Klee      Hull         0.11\n10      Klee      Hull         0.07\n11 Christian     Kells         0.07\n12    Harvey   Kellsey         0.07\n\n# Final answer\n# Let's now take out the royalty_rate from the select clause. This will serve\n# to have every author only be displayed once since the distinct will eliminate\n# rows that are duplicates of each other.\nsqldf(\" select distinct au_fname, au_lname\n        from authors join title_authors on authors.au_id = title_authors.au_id\n                     join titles        on title_authors.title_id = titles.title_id\n                     join royalties     on titles.title_id = royalties.title_id\n        where royalty_rate &gt; .06\n        order by au_lname, au_fname\n      \")\n\n   au_fname  au_lname\n1     Wendy Heydemark\n2    Hallie      Hull\n3      Klee      Hull\n4 Christian     Kells\n5    Harvey   Kellsey",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#reminder-about-how-multiple-joins-work",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#reminder-about-how-multiple-joins-work",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.14 Reminder about how multiple joins work",
    "text": "32.14 Reminder about how multiple joins work\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Reminder about how multiple joins work\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Remember that every INNER JOIN (which contains an ON clause)\n# is the same as doing a cross join followed by removing the rows from the\n# cross join that don't match the on clause of the inner join.\n# The condition in the ON clause would work the same way if instead it \n# were part of the WHERE clause.\n#\n# Also remember that when you join multiple tables, the first two tables \n# are joined (as described in the previous paragraph). Then the rows and\n# columns that resulted from the first join are joined to the next table in the\n# same way (i.e a cross join of rows from the result with the rows from the\n# 2nd table followed by removing the rows that don't match the corresponding\n# on clause). This keeps happening for all the tables. The entire process\n# happens automatically when you specify the inner joins correctly.\n#\n# For example, if you run the following query, you will get the output shown\n# below.\n#\n#   QUERY:\n#\n#       sqldf(\" select distinct au_fname, au_lname\n#               from authors join title_authors on authors.au_id = title_authors.au_id\n#                            join titles        on title_authors.title_id = titles.title_id\n#                            join royalties     on titles.title_id = royalties.title_id\n#               where royalty_rate &gt; .06\n#               order by au_lname, au_fname\n#             \")\n#\n#   OUTPUT:\n#\n#           au_fname     au_lname\n#              Wendy    Heydemark\n#             Hallie         Hull\n#               Klee         Hull\n#          Christian        Kells\n#             Harvey      Kellsey      \n#\n#\n# The output looks very short and simple. It has five rows and two columns.\n# However, in order to arrive at this output, the database must go through\n# several steps to process the SQL query. At each step of the internal\n# processing there is an \"intermediate result\" that you normally would not see. \n#\n# Below, we demonstrate step by step what actually happens internally when\n# the database performs the query. I recommend that you look at\n# the output of each step to better understand the process:\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# 1st step - \n#\n# The following part of the \"from clause\" in the above query is done first:\n#\n#    select ...                             (SELECT CLAUSE IS DONE LATER)\n#    from authors join title_authors ...    (ON CLAUSE IS DONE LATER)\n#                 ...                       (OTHER JOINS ARE DONE LATER)\n#    where ...                              (WHERE CLAUSE IS DONE LATER)\n#    order by ...                           (ORDER BY CLAUSE IS DONE LATER)\n#\n# Notice that this query does an \"inner join\" (or in other words, a plain\n# \"join\"). However, what actually happens behind the scenes is that these\n# two tables are really CROSS JOINED and then the extra rows from the \n# \"cross join\" that don't satisfy the on clause of the inner join are\n# removed..\n#\n# At this point, the \"select clause\" has not executed yet so all columns from \n# both tables are still part of this intermediate result. In the output \n# of the following command every row contains 12 columns. The first 8 columns,\n# come from authors table and the last 4 come from the title_authors table.\n# \n#   The columns from the authors table are: \n#\n#     au_id, au_fname, au_lname, phone, address, city, state, zip\n# \n#   and the columns from the title_authors table are:\n#\n#     title_id, au_id, au_order, royalty_share\n#\n# Notice that in a single row of this intermediate output, the value of the\n# au_id from the authors table and the value of the au_id from the title_authors\n# table do NOT necessarily match\n\nsqldf(\"select authors.*, title_authors.* \n      from authors CROSS JOIN title_authors\")\n\n    au_id  au_fname    au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares\n1     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0\n2     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0\n3     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T03   A05        1            1.0\n4     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T04   A03        1            0.6\n5     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T04   A04        2            0.4\n6     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T05   A04        1            1.0\n7     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T06   A02        1            1.0\n8     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T07   A02        1            0.5\n9     A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T07   A04        2            0.5\n10    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T08   A06        1            1.0\n11    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T09   A06        1            1.0\n12    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T10   A02        1            1.0\n13    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T11   A03        2            0.3\n14    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T11   A04        3            0.3\n15    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T11   A06        1            0.4\n16    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T12   A02        1            1.0\n17    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0\n18    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T01   A01        1            1.0\n19    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T02   A01        1            1.0\n20    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T03   A05        1            1.0\n21    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T04   A03        1            0.6\n22    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T04   A04        2            0.4\n23    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T05   A04        1            1.0\n24    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0\n25    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5\n26    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A04        2            0.5\n27    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T08   A06        1            1.0\n28    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T09   A06        1            1.0\n29    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0\n30    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T11   A03        2            0.3\n31    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T11   A04        3            0.3\n32    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T11   A06        1            0.4\n33    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0\n34    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T13   A01        1            1.0\n35    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T01   A01        1            1.0\n36    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T02   A01        1            1.0\n37    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T03   A05        1            1.0\n38    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6\n39    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4\n40    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0\n41    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T06   A02        1            1.0\n42    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A02        1            0.5\n43    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5\n44    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T08   A06        1            1.0\n45    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T09   A06        1            1.0\n46    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T10   A02        1            1.0\n47    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3\n48    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3\n49    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A06        1            0.4\n50    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T12   A02        1            1.0\n51    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T13   A01        1            1.0\n52    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T01   A01        1            1.0\n53    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T02   A01        1            1.0\n54    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T03   A05        1            1.0\n55    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6\n56    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4\n57    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0\n58    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T06   A02        1            1.0\n59    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A02        1            0.5\n60    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5\n61    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T08   A06        1            1.0\n62    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T09   A06        1            1.0\n63    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T10   A02        1            1.0\n64    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3\n65    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3\n66    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A06        1            0.4\n67    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T12   A02        1            1.0\n68    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T13   A01        1            1.0\n69    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T01   A01        1            1.0\n70    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T02   A01        1            1.0\n71    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0\n72    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T04   A03        1            0.6\n73    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T04   A04        2            0.4\n74    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T05   A04        1            1.0\n75    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T06   A02        1            1.0\n76    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T07   A02        1            0.5\n77    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T07   A04        2            0.5\n78    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T08   A06        1            1.0\n79    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T09   A06        1            1.0\n80    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T10   A02        1            1.0\n81    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T11   A03        2            0.3\n82    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T11   A04        3            0.3\n83    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T11   A06        1            0.4\n84    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T12   A02        1            1.0\n85    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014      T13   A01        1            1.0\n86    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T01   A01        1            1.0\n87    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T02   A01        1            1.0\n88    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T03   A05        1            1.0\n89    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T04   A03        1            0.6\n90    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T04   A04        2            0.4\n91    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T05   A04        1            1.0\n92    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T06   A02        1            1.0\n93    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T07   A02        1            0.5\n94    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T07   A04        2            0.5\n95    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0\n96    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0\n97    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T10   A02        1            1.0\n98    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A03        2            0.3\n99    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A04        3            0.3\n100   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4\n101   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T12   A02        1            1.0\n102   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T13   A01        1            1.0\n103   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T01   A01        1            1.0\n104   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T02   A01        1            1.0\n105   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T03   A05        1            1.0\n106   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T04   A03        1            0.6\n107   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T04   A04        2            0.4\n108   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T05   A04        1            1.0\n109   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T06   A02        1            1.0\n110   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T07   A02        1            0.5\n111   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T07   A04        2            0.5\n112   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T08   A06        1            1.0\n113   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T09   A06        1            1.0\n114   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T10   A02        1            1.0\n115   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T11   A03        2            0.3\n116   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T11   A04        3            0.3\n117   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T11   A06        1            0.4\n118   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T12   A02        1            1.0\n119   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236      T13   A01        1            1.0\n\n# 2nd step - \n#\n# The on clause is applied to the results from the previous step 1 to eliminate\n# those rows from the cross join that don't match the on clause specified\n# in the query. Notice that in the results of the following command the \n# first au_id and the second au_id \n\nsqldf(\"select authors.*, title_authors.*\n      from authors join title_authors on authors.au_id = title_authors.au_id\")\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4\n\n# 3rd step - \n\nsqldf(\"select authors.*, title_authors.*, titles.*\n      from authors join title_authors on authors.au_id = title_authors.au_id\n                   join titles        on title_authors.title_id = titles.title_id\")\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                          title_name       type pub_id pages price   sales         pubdate\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n\n# 4th step - join royalties on titles.title_id = royalties.title_id\n\nsqldf(\"select authors.*, title_authors.*, titles.*, royalties.*\n      from authors join title_authors on authors.au_id = title_authors.au_id\n                   join titles        on title_authors.title_id = titles.title_id\n                   join royalties     on titles.title_id = royalties.title_id\")\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                          title_name       type pub_id pages price   sales         pubdate title_id advance royalty_rate\n1    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T01   A01        1            1.0      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00      T01   10000         0.05\n2    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T02   A01        1            1.0      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00      T02    1000         0.06\n3    A01     Sarah   Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468      T13   A01        1            1.0      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00      T13   20000         0.06\n4    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00      T06   20000         0.08\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T10   A02        1            1.0      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;      T10      NA           NA\n7    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00      T12   50000         0.09\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n9    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n10   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n11   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00      T05  100000         0.09\n12   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n13   A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n14   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00      T03   15000         0.07\n15   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T08   A06        1            1.0      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00      T08       0         0.04\n16   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T09   A06        1            1.0      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00      T09       0         0.05\n17   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n\n# 5th step - where\n\nsqldf(\"select authors.*, title_authors.*, titles.*, royalties.*\n               from authors join title_authors on authors.au_id = title_authors.au_id\n                            join titles        on title_authors.title_id = titles.title_id\n                            join royalties     on titles.title_id = royalties.title_id\n               where royalty_rate &gt; .06\n             \")\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                       title_name       type pub_id pages price   sales         pubdate title_id advance royalty_rate\n1    A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03     Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00      T03   15000         0.07\n2    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04        But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n3    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04        But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n4    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05           Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00      T05  100000         0.09\n5    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                 How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00      T06   20000         0.08\n6    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n7    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n8    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n9    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n10   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n11   A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12        Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00      T12   50000         0.09\n\n# 6th step - order by \n       \nsqldf(\"select authors.*, title_authors.*, titles.*, royalties.*\n               from authors join title_authors on authors.au_id = title_authors.au_id\n                            join titles        on title_authors.title_id = titles.title_id\n                            join royalties     on titles.title_id = royalties.title_id\n               where royalty_rate &gt; .06\n               order by au_lname, au_fname\n             \")\n\n   au_id  au_fname  au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id                       title_name       type pub_id pages price   sales         pubdate title_id advance royalty_rate\n1    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T06   A02        1            1.0      T06                 How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00      T06   20000         0.08\n2    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T07   A02        1            0.5      T07                I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n3    A02     Wendy Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303      T12   A02        1            1.0      T12        Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00      T12   50000         0.09\n4    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6      T04        But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n5    A03    Hallie      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n6    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4      T04        But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00      T04   20000         0.08\n7    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0      T05           Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00      T05  100000         0.09\n8    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T07   A04        2            0.5      T07                I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00      T07 1000000         0.11\n9    A04      Klee      Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n10   A05 Christian     Kells 212-771-4680       114 Horatio St      New York    NY 10014      T03   A05        1            1.0      T03     Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00      T03   15000         0.07\n11   A06    Harvey   Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00      T11  100000         0.07\n\n# 7th step - select\n\nsqldf(\"select au_fname, au_lname\n               from authors join title_authors on authors.au_id = title_authors.au_id\n                            join titles        on title_authors.title_id = titles.title_id\n                            join royalties     on titles.title_id = royalties.title_id\n               where royalty_rate &gt; .06\n               order by au_lname, au_fname\n             \")\n\n    au_fname  au_lname\n1      Wendy Heydemark\n2      Wendy Heydemark\n3      Wendy Heydemark\n4     Hallie      Hull\n5     Hallie      Hull\n6       Klee      Hull\n7       Klee      Hull\n8       Klee      Hull\n9       Klee      Hull\n10 Christian     Kells\n11    Harvey   Kellsey\n\n# 6th step - select distinct\n\nsqldf(\"select DISTINCT au_fname, au_lname\n               from authors join title_authors on authors.au_id = title_authors.au_id\n                            join titles        on title_authors.title_id = titles.title_id\n                            join royalties     on titles.title_id = royalties.title_id\n               where royalty_rate &gt; .06\n               order by au_lname, au_fname\n             \")\n\n   au_fname  au_lname\n1     Wendy Heydemark\n2    Hallie      Hull\n3      Klee      Hull\n4 Christian     Kells\n5    Harvey   Kellsey",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#warning---if-where-clause-removes-all-rows-then-avgsummaxmin-return-null",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#warning---if-where-clause-removes-all-rows-then-avgsummaxmin-return-null",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.15 WARNING - If where clause removes all rows then avg,sum,max,min return NULL",
    "text": "32.15 WARNING - If where clause removes all rows then avg,sum,max,min return NULL\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# WARNING : If no rows are returned by the where clause, then the \n#           aggregate functions (except for count) all return NULL\n#  \n#           Note that since we are using R dataframes instead\n#           of tables, NULL will be displayed as NA\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# No rows match the where - result of all aggregate functions, except count \n# is NULL (i.e. NA in R)\n\nsqldf(\"select count(*), sum(pages), avg(pages), min(pages), max(pages)\n      from titles\n      where type = 'this type does not exist' \")\n\n  count(*) sum(pages) avg(pages) min(pages) max(pages)\n1        0         NA         NA         NA         NA",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-2",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-2",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.16 Practice",
    "text": "32.16 Practice\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# show the total number of cookbooks (i.e. type = 'cookbook')\n#          total number of pages in all the cookbooks\n#          and the average number of pages in cookbooks\n#\n# NOTE that since there are currently no cookbooks in the table\n# there will be no rows returned by the where clause. Therefore\n# the sum and the avg will be NULL (i.e. NA in R)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select count(*), sum(pages), avg(pages)\n      from titles\n      where type = 'cookbook'\")\n\n  count(*) sum(pages) avg(pages)\n1        0         NA         NA",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#dont-use-aggregate-functions-non-aggregated-values-in-same-select-clause",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#dont-use-aggregate-functions-non-aggregated-values-in-same-select-clause",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.17 DON’T use aggregate functions & non-aggregated values in same select clause",
    "text": "32.17 DON’T use aggregate functions & non-aggregated values in same select clause\n\n##################################################################\n#\n# WARNING : \n#\n# DO NOT include both aggregate function and non-aggregated values\n# in the same select clause\n#\n#    (NOTE - it may help to look at the example below before reading further)\n#\n# BECAUSE ...    \n#\n# The aggregate function will return fewer rows than the \n# non-aggregated data. This presents a problem for SQL because\n# there is a contradiction as to how many rows should be shown!\n#\n# In many many versions of SQL, such queries would generate an ERROR!\n#\n# In SQLite, this query doesn't generate an error. Rather, the query \n# will only return one row. The values of the non-aggregate column\n# will be the first value that would have appeared for that non-aggregate\n# column if the aggregate function was not present.\n#\n# See this page for an in-depth analysis of this issue: \n#   https://www.dataquest.io/blog/sql-tutorial-selecting-ungrouped-columns-without-aggregate-functions/\n##################################################################\n\n\n# The output of the following produces ONE row\nsqldf(\"select avg(price) from titles\")\n\n  avg(price)\n1    18.3875\n\n# The output of the following produces MORE THAN ONE row\nsqldf(\"select title_name from titles\")\n\n                            title_name\n1                                1977!\n2            200 Years of German Humor\n3         Ask Yor System Administrator\n4            But I Did It Unconciously\n5               Exchange of Platitudes\n6                     How About Never?\n7                    I Blame My Mother\n8         Just Wait Until After School\n9                      Kiss My Boo Boo\n10         Not Without My Fabrerge Egg\n11    Perhaps It's a Glandular Problem\n12           Spontaneous, Not Annoying\n13 What Are The Civilian Applications?\n\n# The following will generate an ERROR in MANY MANY versions of SQL.\n# since avg(price) should return ONE number, but there are many different\n# values for title_name.\n#\n# SQLite allows this type of query but the results are confusing.\n# Since the avg generates only a single number, the following results in\n# only a single row with only one particular title even though there are\n# many different title_names.\n\n# DON'T DO THIS!!! (see the comment above)\nsqldf(\"select avg(price) , title_name from titles\")\n\n  avg(price) title_name\n1    18.3875      1977!",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#order-of-clauses-select-from-where-group-by-having-order-by-limit",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#order-of-clauses-select-from-where-group-by-having-order-by-limit",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.18 ORDER OF CLAUSES: select, from, where, group by, having, order by, limit",
    "text": "32.18 ORDER OF CLAUSES: select, from, where, group by, having, order by, limit\n\n# REMEMBER\n#    While select statements don't always need to use every clause, \n#    the clauses that are in a statement must appear in the following order:\n#\n# select ...\n# from ...\n# where ...\n# group by ...\n# having ...\n# order by ...\n# limit ...",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#group-by-affects-how-aggregate-functions-work",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#group-by-affects-how-aggregate-functions-work",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.19 GROUP BY affects how aggregate functions work",
    "text": "32.19 GROUP BY affects how aggregate functions work\n\n#################################################################\n# group by COLUMN1, COLUMN2, ...\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The \"group by\" clause has a direct effect on how aggregate functions work.\n#\n# As we've seen above, when a query contains an\n# aggregate function \n#   (and does not contain a group by clause -\n#    remember we didn't start discussing \"group by\" until just now),\n# the query will return a single row. The single row\n# of output would contain the result of applying the aggregate function(s)\n# to all of the rows from the table that satisfied the where clause.\n#\n# By contrast, a query that contains aggregate functions, and also contains\n# a group by clause, could return more than one row of output, i.e. one row of\n# output for each \"group\" of rows from the original table(s) [... keep reading ...] \n#\n# The group by clause specifies one or more columns. The rows from the table(s)\n# that contain the same values for these columns constitute a logical \"group\" of\n# rows. For example the following shows the format of a query that gets info\n# from the titles table. A lot of the query was left out because for now \n# I want to focus just on \"from titles\" and \"group by type\"\n#\n#     select ... (specify what you want to select)\n#     from titles\n#     ... (more stuff from the qeury)\n#     group by type\n#\n# This query segments the rows from the titles table into\n# different \"groups\". \n#   There will be one     \"group\" that contains all rows with type='biography'.\n#   There will be another \"group\" that contains all rows with type='childrens'.\n#   There will be another \"group\" that contains all rows with type='computer'.\n#   etc. \n#\n# If the query were to include aggregate functions, the output would indlude\n# one row for each \"group\" of rows from the original table. This is \n# best explained with an example:\n#################################################################",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-3",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-3",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.20 Practice",
    "text": "32.20 Practice\n\n# First let's look at all of the types and prices in order\nsqldf(\"select type, price \n      from titles \n      order by type, price\")\n\n         type price\n1   biography    NA\n2   biography 12.99\n3   biography 19.95\n4   biography 23.95\n5    children 10.00\n6    children 13.95\n7    computer 39.95\n8     history 19.95\n9     history 21.99\n10    history 29.99\n11 psychology  6.95\n12 psychology  7.99\n13 psychology 12.99\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a query that returns the following for each \n# type of book (i.e. \"childrens\", \"computer\", etc)\n#\n#     the number of copies of each type \n#     the average price of all books of that type\n#     the max price of all books of that type\n#     the min price of all books of that type\n#\n# HINT: use group by to create different groups of rows from the original table\n#       All the rows with the same value for type will be part of the same group.\n#       Then use the avg(price) function to get the average price for each type.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select type, count(*), avg(price), max(price), min(price)\n      from titles\n      group by type\n      order by type\")\n\n        type count(*) avg(price) max(price) min(price)\n1  biography        4   18.96333      23.95      12.99\n2   children        2   11.97500      13.95      10.00\n3   computer        1   39.95000      39.95      39.95\n4    history        3   23.97667      29.99      19.95\n5 psychology        3    9.31000      12.99       6.95\n\n# To make the output easier to read, change the code to display \"#copies\"\n# instead of count(*) at the top of that column in the output.\n\nsqldf(\"select type, count(*) as '#copies', avg(price), max(price), min(price)\n      from titles\n      group by type\n      order by type\")\n\n        type #copies avg(price) max(price) min(price)\n1  biography       4   18.96333      23.95      12.99\n2   children       2   11.97500      13.95      10.00\n3   computer       1   39.95000      39.95      39.95\n4    history       3   23.97667      29.99      19.95\n5 psychology       3    9.31000      12.99       6.95\n\nsqldf(\"select count(*) as '#copies', avg(price), max(price), min(price)\n      from titles\")\n\n  #copies avg(price) max(price) min(price)\n1      13    18.3875      39.95       6.95",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#group-by-more-than-one-column",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#group-by-more-than-one-column",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.21 GROUP BY more than one column",
    "text": "32.21 GROUP BY more than one column\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# GROUP BY more than one column\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The GROUP BY may contain more than one column. \n# This in effect creates a group from all rows that have the same value\n# for all of the specified columns.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# First let's look at the types, pub_id's, titles and prices of all books in order\nsqldf(\"select type, pub_id, price, title_name\n       from titles\n       order by type, pub_id, price\")\n\n         type pub_id price                          title_name\n1   biography    P01 19.95                    How About Never?\n2   biography    P03 23.95                   I Blame My Mother\n3   biography    P05    NA         Not Without My Fabrerge Egg\n4   biography    P05 12.99           Spontaneous, Not Annoying\n5    children    P01 10.00        Just Wait Until After School\n6    children    P01 13.95                     Kiss My Boo Boo\n7    computer    P02 39.95        Ask Yor System Administrator\n8     history    P01 21.99                               1977!\n9     history    P03 19.95           200 Years of German Humor\n10    history    P03 29.99 What Are The Civilian Applications?\n11 psychology    P01  6.95              Exchange of Platitudes\n12 psychology    P01 12.99           But I Did It Unconciously\n13 psychology    P05  7.99    Perhaps It's a Glandular Problem\n\n# Now let's create groups and show how many titles in each group.\nsqldf(\"select type, pub_id, count(*)\n       from titles\n       group by type, pub_id\n       order by type, pub_id\")\n\n        type pub_id count(*)\n1  biography    P01        1\n2  biography    P03        1\n3  biography    P05        2\n4   children    P01        2\n5   computer    P02        1\n6    history    P01        1\n7    history    P03        2\n8 psychology    P01        2\n9 psychology    P05        1",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-4",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#practice-4",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.22 Practice",
    "text": "32.22 Practice\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Write a query that returns the average price, max price and min price of each\n# type of book (i.e. \"childrens\", \"computer\", etc) that is published by each \n# publisher.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# IMPORTANT - REMEMBER ... \n# Clauses in a SELECT statement must appear in the following order:\n# All of the clauses are optional except for the \"select\" clause.\n#\n#   select ...\n#   from ...\n#   where ...\n#   group by  ...\n#   having ...\n#   order by ...\n#   limit ...       (limit is not part of the SQL standard but is part of many versions of SQL)\n\n\n# ANSWER\n#\n# In the output of the following command, note that only one row is\n# shown for each type/pub_id combination\n\nsqldf(\"select type, pub_id, count(*) as '#copies', avg(price), min(price), max(price)\n      from titles\n      group by type, pub_id\n      order by type\")\n\n        type pub_id #copies avg(price) min(price) max(price)\n1  biography    P01       1     19.950      19.95      19.95\n2  biography    P03       1     23.950      23.95      23.95\n3  biography    P05       2     12.990      12.99      12.99\n4   children    P01       2     11.975      10.00      13.95\n5   computer    P02       1     39.950      39.95      39.95\n6    history    P01       1     21.990      21.99      21.99\n7    history    P03       2     24.970      19.95      29.99\n8 psychology    P01       2      9.970       6.95      12.99\n9 psychology    P05       1      7.990       7.99       7.99\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# rewrite the order by of the previous query so that all books by the same publisher\n# appear together in consecutive rows (i.e. rows that are \"one after the other\" for \n# the same publisher)\n#\n# Note it makes no difference which column is listed first in the group by \n#      it DOES make a difference which column is listed first in the order by\n\n# ANSWER\n\nsqldf(\"select type, pub_id, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type, pub_id   \n      order by pub_id, type\")\n\n        type pub_id avg(price) count(*) min(price) max(price) sum(price)\n1  biography    P01     19.950        1      19.95      19.95      19.95\n2   children    P01     11.975        2      10.00      13.95      23.95\n3    history    P01     21.990        1      21.99      21.99      21.99\n4 psychology    P01      9.970        2       6.95      12.99      19.94\n5   computer    P02     39.950        1      39.95      39.95      39.95\n6  biography    P03     23.950        1      23.95      23.95      23.95\n7    history    P03     24.970        2      19.95      29.99      49.94\n8  biography    P05     12.990        2      12.99      12.99      12.99\n9 psychology    P05      7.990        1       7.99       7.99       7.99\n\n# Change the order of the pub_id and the type in the select so that the output is easier to read.\n\nsqldf(\"select pub_id, type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type, pub_id\n      order by pub_id, type\")\n\n  pub_id       type avg(price) count(*) min(price) max(price) sum(price)\n1    P01  biography     19.950        1      19.95      19.95      19.95\n2    P01   children     11.975        2      10.00      13.95      23.95\n3    P01    history     21.990        1      21.99      21.99      21.99\n4    P01 psychology      9.970        2       6.95      12.99      19.94\n5    P02   computer     39.950        1      39.95      39.95      39.95\n6    P03  biography     23.950        1      23.95      23.95      23.95\n7    P03    history     24.970        2      19.95      29.99      49.94\n8    P05  biography     12.990        2      12.99      12.99      12.99\n9    P05 psychology      7.990        1       7.99       7.99       7.99\n\n# WARNING - THE FOLLOWING QUERY IS WRONG SINCE title_name IS NOT AN AGGREGATE\n# FUNCTION AND IS ALSO NOT IN THE GROUP BY CLAUSE.\n# \n# SOME Database Management Systems (DBMS) WILL RETURN AN ERROR FOR THE \n# FOLLOWING QUERY, WHEREAS SQLITE, the database that R sqldf uses by default,\n# DOESN'T REUTRN AN ERROR BUT RATHER RETURNS CONFUSING RESULTS. -- DON'T DO THIS!\n\nsqldf(\"select pub_id, type, title_name, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type, pub_id\n      order by pub_id, type\")\n\n  pub_id       type                          title_name avg(price) count(*) min(price) max(price) sum(price)\n1    P01  biography                    How About Never?     19.950        1      19.95      19.95      19.95\n2    P01   children                     Kiss My Boo Boo     11.975        2      10.00      13.95      23.95\n3    P01    history                               1977!     21.990        1      21.99      21.99      21.99\n4    P01 psychology           But I Did It Unconciously      9.970        2       6.95      12.99      19.94\n5    P02   computer        Ask Yor System Administrator     39.950        1      39.95      39.95      39.95\n6    P03  biography                   I Blame My Mother     23.950        1      23.95      23.95      23.95\n7    P03    history What Are The Civilian Applications?     24.970        2      19.95      29.99      49.94\n8    P05  biography           Spontaneous, Not Annoying     12.990        2      12.99      12.99      12.99\n9    P05 psychology    Perhaps It's a Glandular Problem      7.990        1       7.99       7.99       7.99\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# QUESTION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Rewrite the previous query to show the publisher's name instead of the publisher's id\n# \n# HINTS\n# - you must join the appropriate tables\n#\n# - since the query has a group by, the select clause should only refer to \n#   aggregate functions and to columns that appear in the group by.\n#\n#   The select clause should NOT refer to columns that do not appear in the group by.\n#   Since we want to display the publisher's name we should also group by \n#   the publisher's name instead of the pub_id.\n#   \n#   NOTE - that this is reasonable to do since for all rows with the same\n#          pub_id, the publisher's name is guaranteed to be the same \n#          (think about it)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# show one row for each title.\n#\n# Show the info from the title table and the associated info\n# from the publisher's table in the single row of output for each title.\n\nsqldf(\"select titles.*, publishers.*\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      order by pub_name, type\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate pub_id             pub_name          city state country\n1       T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;    P05       AAA Publishing      Berkeley    CA     USA\n2       T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n3       T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00    P05       AAA Publishing      Berkeley    CA     USA\n4       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n5       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n6       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00    P01    Abatis Publishers      New York    NY     USA\n7       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00    P01    Abatis Publishers      New York    NY     USA\n8       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00    P01    Abatis Publishers      New York    NY     USA\n9       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00    P01    Abatis Publishers      New York    NY     USA\n10      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00    P02      Core Dump Books San Francisco    CA     USA\n11      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n12      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n\n# Show the pub_name in the output (not just the pub_id)\n# along with aggregate info for that publisher and type of book.\n#\n# In ANSI standard SQL and in most DBMS software (e.g. oracle, MySql, Postgres)\n# We need to include pub_name in the group by if we will include it in the \n# output. \n\nsqldf(\"select pub_name, type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      group by type, pub_name\n      order by pub_name, type\")\n\n              pub_name       type avg(price) count(*) min(price) max(price) sum(price)\n1       AAA Publishing  biography     12.990        2      12.99      12.99      12.99\n2       AAA Publishing psychology      7.990        1       7.99       7.99       7.99\n3    Abatis Publishers  biography     19.950        1      19.95      19.95      19.95\n4    Abatis Publishers   children     11.975        2      10.00      13.95      23.95\n5    Abatis Publishers    history     21.990        1      21.99      21.99      21.99\n6    Abatis Publishers psychology      9.970        2       6.95      12.99      19.94\n7      Core Dump Books   computer     39.950        1      39.95      39.95      39.95\n8 Schandenfreude Press  biography     23.950        1      23.95      23.95      23.95\n9 Schandenfreude Press    history     24.970        2      19.95      29.99      49.94\n\n# SQLite will allow the pub_name in the select clause even if it isn't\n# in the group by. However, you should avoid this - it isn't standard ANSI SQL.\n\nsqldf(\"select pub_name, type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      group by type, publishers.pub_id\n      order by pub_name, type\")\n\n              pub_name       type avg(price) count(*) min(price) max(price) sum(price)\n1       AAA Publishing  biography     12.990        2      12.99      12.99      12.99\n2       AAA Publishing psychology      7.990        1       7.99       7.99       7.99\n3    Abatis Publishers  biography     19.950        1      19.95      19.95      19.95\n4    Abatis Publishers   children     11.975        2      10.00      13.95      23.95\n5    Abatis Publishers    history     21.990        1      21.99      21.99      21.99\n6    Abatis Publishers psychology      9.970        2       6.95      12.99      19.94\n7      Core Dump Books   computer     39.950        1      39.95      39.95      39.95\n8 Schandenfreude Press  biography     23.950        1      23.95      23.95      23.95\n9 Schandenfreude Press    history     24.970        2      19.95      29.99      49.94\n\n# Rewrite the previous query to only group by the pub_name and not by the type.\n#\n# HINTS - because the type is not part of the group by it should also be eliminated\n#         from the select clause and the order by clause\n\nsqldf(\"select pub_name, avg(price), count(*), min(price), max(price), sum(price)\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      group by pub_name\n      order by pub_name\")\n\n              pub_name avg(price) count(*) min(price) max(price) sum(price)\n1       AAA Publishing     10.490        3       7.99      12.99      20.98\n2    Abatis Publishers     14.305        6       6.95      21.99      85.83\n3      Core Dump Books     39.950        1      39.95      39.95      39.95\n4 Schandenfreude Press     24.630        3      19.95      29.99      73.89\n\n#``````````````````````````````````````````````````````````````````````````````\n# SQLite doesn't follow the standard exactly ..\n#``````````````````````````````````````````````````````````````````````````````\n# Note that the SQL standard does not allow for column names in the select and order by \n# clauses that are not also part of the group by.\n# However, SQLite, does not report an error in these cases and the output could be\n# confusing. The following output shows a type, but the avg and other aggregate functions\n# include ALL types, not just the one shown.\n#\n\n# See these page for a description of the issue and suggestions for workarounds:\n#\n#   https://www.dataquest.io/blog/sql-tutorial-selecting-ungrouped-columns-without-aggregate-functions/\n#\n#   https://learnsql.com/blog/not-a-group-by-expression-error/\n#\n#\n# See this page for the official SQLite documentation \n# Below is an excerpt of the relevant info from this page.\n#   \n#   https://www.sqlite.org/lang_select.html#bareagg\n\n        # 2.5. Bare columns in an aggregate query The usual case is that all column\n        # names in an aggregate query are either arguments to aggregate functions or\n        # else appear in the GROUP BY clause. A result column which contains a column\n        # name that is not within an aggregate function and that does not appear in the\n        # GROUP BY clause (if one exists) is called a \"bare\" column. Example:\n        #\n        # SELECT a, b, sum(c) FROM tab1 GROUP BY a; In the query above, the \"a\" column\n        # is part of the GROUP BY clause and so each row of the output contains one of\n        # the distinct values for \"a\". The \"c\" column is contained within the sum()\n        # aggregate function and so that output column is the sum of all \"c\" values in\n        # rows that have the same value for \"a\". But what is the result of the bare\n        # column \"b\"? The answer is that the \"b\" result will be the value for \"b\" in one\n        # of the input rows that form the aggregate. The problem is that you usually do\n        # not know which input row is used to compute \"b\", and so in many cases the\n        # value for \"b\" is undefined.\n        #\n        # Special processing occurs when the aggregate function is either min() or\n        # max(). Example:\n        #\n        # SELECT a, b, max(c) FROM tab1 GROUP BY a; If there is exactly one min() or\n        # max() aggregate in the query, then all bare columns in the result set take\n        # values from an input row which also contains the minimum or maximum. So in the\n        # query above, the value of the \"b\" column in the output will be the value of\n        # the \"b\" column in the input row that has the largest \"c\" value. There are\n        # limitations on this special behavior of min() and max():\n        #\n        # If the same minimum or maximum value occurs on two or more rows, then bare\n        # values might be selected from any of those rows. The choice is arbitrary.\n        # There is no way to predict from which row the bare values will be choosen. The\n        # choice might be different for different bare columns within the same query.\n        #\n        # If there are two or more min() or max() aggregates in the query, then bare\n        # column values will be taken from one of the rows on which one of the\n        # aggregates has their minimum or maximum value. The choice of which min() or\n        # max() aggregate determines the selection of bare column values is arbitrary.\n        # The choice might be different for different bare columns within the same\n        # query.\n        #\n        # This special processing for min() or max() aggregates only works for the\n        # built-in implementation of those aggregates. If an application overrides the\n        # built-in min() or max() aggregates with application-defined alternatives, then\n        # the values selected for bare columns will be taken from an arbitrary row.\n        #\n        # Most other SQL database engines disallow bare columns. If you include a bare\n        # column in a query, other database engines will usually raise an error. The\n        # ability to include bare columns in a query is an SQLite-specific extension.\n#\n#\n#``````````````````````````````````````````````````````````````````````````````\n\nsqldf(\"select pub_name, type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      group by pub_name\n      order by pub_name\")\n\n              pub_name      type avg(price) count(*) min(price) max(price) sum(price)\n1       AAA Publishing biography     10.490        3       7.99      12.99      20.98\n2    Abatis Publishers   history     14.305        6       6.95      21.99      85.83\n3      Core Dump Books  computer     39.950        1      39.95      39.95      39.95\n4 Schandenfreude Press   history     24.630        3      19.95      29.99      73.89",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#having-clause",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#having-clause",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.23 HAVING clause",
    "text": "32.23 HAVING clause\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# HAVING \n#\n#    group by COLUMN1, COLUMN2, ...\n#    having SOME_LOGICAL_EXPRESSION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Select statements that contain a \"group by\" MAY ALSO contain an optional \"having clause\".\n# Select statements that do not contain a \"group by\" may NOT contain \"having\"\n#\n# Both \"where\" and \"having\" clauses contain logical expression. \n#\n# Where an having clauses differ in the following ways:\n# \n# 1. \"where\"  eliminates individual rows from the original table that do not meet the conditions.\n#    \"having\" eliminates entire groups of rows that do not meet its conditions.\n#\n# 2. \"where\" may refer to any columns in the original tables, but may NOT refer to aggregate functions.\n#    \"having\" may ONLY refer to aggregate functions and column names that appear in the group by clause.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# Example\n\n# show all the data from the titles table\nsqldf(\"select * \n       from titles\n       order by type\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n2       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n3       T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n4       T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n5       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n6       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n7       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n8       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n9       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n10      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n11      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n12      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n13      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n\n# Without having\nsqldf(\"select type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type\n      order by type\")\n\n        type avg(price) count(*) min(price) max(price) sum(price)\n1  biography   18.96333        4      12.99      23.95      56.89\n2   children   11.97500        2      10.00      13.95      23.95\n3   computer   39.95000        1      39.95      39.95      39.95\n4    history   23.97667        3      19.95      29.99      71.93\n5 psychology    9.31000        3       6.95      12.99      27.93\n\n# same query WITH THE ADDITION of \"having\" (note that having must come before \"order by\")\n#\n# Notice that the rows for groups that don't satisfy the having conditions \n# are eliminated from the output.\nsqldf(\"select type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type\n      having count(*) &gt;=3 \n      order by type\")\n\n        type avg(price) count(*) min(price) max(price) sum(price)\n1  biography   18.96333        4      12.99      23.95      56.89\n2    history   23.97667        3      19.95      29.99      71.93\n3 psychology    9.31000        3       6.95      12.99      27.93\n\n# We can see the effect of the having in the above query by running\n# the same query WITHOUT the having\nsqldf(\"select type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type\n      order by type\")\n\n        type avg(price) count(*) min(price) max(price) sum(price)\n1  biography   18.96333        4      12.99      23.95      56.89\n2   children   11.97500        2      10.00      13.95      23.95\n3   computer   39.95000        1      39.95      39.95      39.95\n4    history   23.97667        3      19.95      29.99      71.93\n5 psychology    9.31000        3       6.95      12.99      27.93\n\n# The having clause's logical expression can be as complex as you want.\n#\n# Modify the previous select to show only types of books for which\n# - there are at least 3 books of that type\n#        and also \n# - the avg price is at least $20 or is less than or equal to  $10\n\nsqldf(\"select type, avg(price), count(*), min(price), max(price), sum(price)\n      from titles\n      group by type\n      having count(*) &gt;=3 and (avg(price) &gt;= 20 or avg(price) &lt;= 10)\n      order by type\")\n\n        type avg(price) count(*) min(price) max(price) sum(price)\n1    history   23.97667        3      19.95      29.99      71.93\n2 psychology    9.31000        3       6.95      12.99      27.93",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#using-where-and-having-in-the-same-query---a-detailed-analysis",
    "href": "sql0030-sqlFunctions_aggregateFunctions_groupBy_having-v019.html#using-where-and-having-in-the-same-query---a-detailed-analysis",
    "title": "32  32.  aggregate functions, group by, having",
    "section": "32.24 using where and having in the same query - a detailed analysis",
    "text": "32.24 using where and having in the same query - a detailed analysis\n\n#############################################################################\n# using where and having in the same query\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# When a query runs the processing of the raw data in to the\n# final output happens in the following logical order\n#\n#   - FROM: all of the data from the tables is retrieved and joined into \n#           a single \"mega-table\" (mega-table is NOT a standard word). \n#\n#   - WHERE: the where clause eliminates rows from this \"mega-table\" that\n#            do not satisfy the where's logical expression\n#\n#   - GROUP BY: the remaining rows a \"grouped\" by the group by clause\n#\n#   - HAINVG:   the having clause works to eliminate groups that don't \n#               satisfy the having clause's logical expression\n#    \n#   - SELECT:   the select clause is used to generate the output of the \n#               aggregate functions \n#\n#   - ORDER BY: the resulting rows are ordered according to the \"order by\" clause\n#\n#   - LIMIT:    only the rows specified by the limit clause are shown\n#\n# To help you remember this, the clauses in a select statement must be specified\n# in the above order - EXCEPT that the select clause must be listed first. The designers\n# of SQL thought that since the select clause specifies which column appear in the \n# output it is important to see it first ...\n#\n# Notice that the where clause eliminates rows from the original tables\n# BEFORE the having clause eliminates groups. This can have an effect on the output\n# of queries that use both the where and having clauses.\n#\n##################################################################################\n\n\n#----------------------------------------------------\n# compare the following queries WITHOUT having\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The numbers are different in the 2nd query since \n# the where clause eliminated specific rows.\n# \n# After the where clause runs there are no childrens books\n# left and there are fewer biograph, history and psychology books.\n# The min and max page counts also are different to match the \n# books that are left after the where ran.\n# The only group that wasn't affected by the where was the\n# computer books.\n#  \n#----------------------------------------------------\n\n# group by ..\n\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       GROUP BY TYPE\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        4            3        333        507\n2   children        2            2         22         86\n3   computer        1            1       1226       1226\n4    history        3            3         14        802\n5 psychology        3            3        201        826\n\n# where ...\n# group by ..\n\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       WHERE PAGES &gt; 350\n       GROUP BY TYPE\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        2            2        473        507\n2   computer        1            1       1226       1226\n3    history        1            1        802        802\n4 psychology        2            2        510        826\n\n#------------------------------------------------------------------\n# Let's see the actual rows that were affected by the where clause.\n# We'll do so by running a similar query WIHTOUT the GROUP BY.\n# We will display the actual data for each row.\n#-----------------------------------------------------------------\n\n# See the rows that were processed by the 1st query above.\n\nsqldf(\"select type, pages, title_name\n       from titles\n       order by type, pages, title_name\n      \")\n\n         type pages                          title_name\n1   biography    NA         Not Without My Fabrerge Egg\n2   biography   333                   I Blame My Mother\n3   biography   473                    How About Never?\n4   biography   507           Spontaneous, Not Annoying\n5    children    22                     Kiss My Boo Boo\n6    children    86        Just Wait Until After School\n7    computer  1226        Ask Yor System Administrator\n8     history    14           200 Years of German Humor\n9     history   107                               1977!\n10    history   802 What Are The Civilian Applications?\n11 psychology   201              Exchange of Platitudes\n12 psychology   510           But I Did It Unconciously\n13 psychology   826    Perhaps It's a Glandular Problem\n\n# Now let's add in the where clause to see the rows that were\n# processed by the 2nd query above.\n\nsqldf(\"select type, pages, title_name\n       from titles\n       WHERE PAGES &gt; 350\n       order by type, pages, title_name\n      \")\n\n        type pages                          title_name\n1  biography   473                    How About Never?\n2  biography   507           Spontaneous, Not Annoying\n3   computer  1226        Ask Yor System Administrator\n4    history   802 What Are The Civilian Applications?\n5 psychology   510           But I Did It Unconciously\n6 psychology   826    Perhaps It's a Glandular Problem\n\n# We can now see why we got the results we did when we \n# ran the versions of the queries with the GROUP BY and \n# the aggregate functions.\n\n\n#--------------------------------------------------------\n# If you use\n#     a where clause \n#     and a group by \n#     and a having\n#\n# The where clause runs then \n# the group by clause runs then\n# the having clause runs\n#--------------------------------------------------------\n\n# group by ...   (same as the 1st query above)\n\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       GROUP BY TYPE\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        4            3        333        507\n2   children        2            2         22         86\n3   computer        1            1       1226       1226\n4    history        3            3         14        802\n5 psychology        3            3        201        826\n\n# group by ...\n# having ...\n\n# having gets rid of groups that don't meet it's criteria\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       group by type\n       having count(*) &gt; 1\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        4            3        333        507\n2   children        2            2         22         86\n3    history        3            3         14        802\n4 psychology        3            3        201        826\n\n# ADDING IN A WHERE CHANGES THINGS\n# \n# In the following we simply added the line: where pages &gt; 350\n# \n# This does two things\n#   (a) childrens books don't show up at all - because none are longer than 350 pages\n#   (b) only 1 history book shows up - the other 2 history books were shorter then 350 pages\n\n\n# where ...\n# group by ...\n\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       where pages &gt; 350\n       group by type\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        2            2        473        507\n2   computer        1            1       1226       1226\n3    history        1            1        802        802\n4 psychology        2            2        510        826\n\n# NOW - since the where clause was added, the having clause will now\n# also eliminate the history group in addition to the computer group\n# since now the history group also only has 1 title.\n#\n# As a result we are only left with biography and psychology groups.\n\n\n# where ...\n# group by ...\n# having ...\n\nsqldf(\"select type, count(*), count(pages), min(pages), max(pages)\n       from titles\n       where pages &gt; 350\n       group by type\n       having count(*) &gt; 1\n       order by type\")\n\n        type count(*) count(pages) min(pages) max(pages)\n1  biography        2            2        473        507\n2 psychology        2            2        510        826",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'><yrChapterNumber>32. </yrChapterNumber> aggregate functions, group by, having</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "",
    "text": "33.1 The “books database”\nIn this and other sections we will be referring to the “books database”.\nClick below to download a document that describes the books database and database technology in general. This document also contains many questions/answers to test your knowledge of SQL. You can also download the CSV files that contain the data for the books database.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#click-to-download-the-specified-files",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#click-to-download-the-specified-files",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.2 click to download the specified files",
    "text": "33.2 click to download the specified files\n\nbooksDatabase description and Questions\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#read-in-the-data",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#read-in-the-data",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.3 read in the data",
    "text": "33.3 read in the data\n\n# load the packages we'll need\nif (!require(sqldf, quietly=TRUE, warn.conflicts=FALSE)) { install.packages(\"sqldf\"); require(sqldf) }\nif (!require(readr, quietly=TRUE, warn.conflicts=FALSE)) { install.packages(\"readr\"); require(readr) }\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"data/booksDatabase/titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"data/booksDatabase/authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"data/booksDatabase/publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"data/booksDatabase/title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"data/booksDatabase/royalties.csv\", na=\"NULL\", show_col_types=FALSE)",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#cross-join",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#cross-join",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.4 CROSS JOIN",
    "text": "33.4 CROSS JOIN\n\n##############################################################################\n# A \"cross join\" is very different from the joins that we've already learned about.\n# A \"cross join\" joins EVERY row from 1st table with EVERY row from 2nd table.\n# For this reason, a cross join does not specify an on condition.\n##############################################################################\n\n# Example - suppose a kindergarten teacher is using a relational database \n# to manage her classroom :)    Suppose she has several toys and several children.\n# One table, students, describes the children,\n# Another table, toys, describes the toys.\n\nstudents &lt;- data.frame(\n  first_name = c(\"abie\", \"bobbie\", \"clara\"),\n  last_name = c(\"aames\", \"berger\", \"cohen\"),\n  gender = c(\"m\", \"m\", \"f\")\n)\nstudents\n\n  first_name last_name gender\n1       abie     aames      m\n2     bobbie    berger      m\n3      clara     cohen      f\n\ntoys = data.frame(\n  toy_name = c(\"doggie\", \"dumbo\"),\n  description = c(\"suffed dog\", \"elephant\")\n)\ntoys\n\n  toy_name description\n1   doggie  suffed dog\n2    dumbo    elephant\n\nsqldf(\"select * from students order by last_name, first_name\")\n\n  first_name last_name gender\n1       abie     aames      m\n2     bobbie    berger      m\n3      clara     cohen      f\n\nsqldf(\"select * from toys order by toy_name\")\n\n  toy_name description\n1   doggie  suffed dog\n2    dumbo    elephant\n\n# The teacher wants to make sure that each child has a chance to play\n# with each of the toys. \n#\n# Therefore she might run the following \"cross join\" to see a list\n# of each child and each toy. She can use this result as a checklist\n# to make sure that each child actually did play with each toy.\n\nsqldf(\"select students.*, toys.*\n       from students CROSS JOIN toys\n       order by last_name, first_name, toy_name\")\n\n  first_name last_name gender toy_name description\n1       abie     aames      m   doggie  suffed dog\n2       abie     aames      m    dumbo    elephant\n3     bobbie    berger      m   doggie  suffed dog\n4     bobbie    berger      m    dumbo    elephant\n5      clara     cohen      f   doggie  suffed dog\n6      clara     cohen      f    dumbo    elephant\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# About the asterisks ( * ) in the SELECT clause ...\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The asterisks in the query above (i.e. select students.*, toys.* ...)\n# means that the results should include all of the columns from the\n# students table (i.e. students.*) and all of the columns from the\n# toys table (i.e. toys.*). \n#\n# This can be done more concisely with just \"select * ...\" the query below\n# does the exact same thing as the query above since \"select * ...\" means\n# to include all of the columns from all of the tables in the output.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select *\n       from students CROSS JOIN toys\n       order by last_name, first_name, toy_name\")\n\n  first_name last_name gender toy_name description\n1       abie     aames      m   doggie  suffed dog\n2       abie     aames      m    dumbo    elephant\n3     bobbie    berger      m   doggie  suffed dog\n4     bobbie    berger      m    dumbo    elephant\n5      clara     cohen      f   doggie  suffed dog\n6      clara     cohen      f    dumbo    elephant\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The reason for using tablename.* is apparent when you want to \n# include all of the columns from some of the tables but not from the other\n# tables. For example the following query displays only some of the columns\n# from the students table (students.gender is NOT displayed). It does display\n# ALL of the columns from the toys table (ie. toys.*)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select students.first_name, students.last_name, toys.*\n       from students CROSS JOIN toys\n       order by last_name, first_name, toy_name\")\n\n  first_name last_name toy_name description\n1       abie     aames   doggie  suffed dog\n2       abie     aames    dumbo    elephant\n3     bobbie    berger   doggie  suffed dog\n4     bobbie    berger    dumbo    elephant\n5      clara     cohen   doggie  suffed dog\n6      clara     cohen    dumbo    elephant\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# select table.column ...        vs       select column ... \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You must include table names in the SELECT clause for columns that \n# appear in more than one table. Column names that that only appear \n# in a single table in the query do NOT need to be preceded by the table name. \n#\n# Therefore the following \n# query is the same as the above query. This is because first_name and\n# last_name only appear as columns in the students table and do not appear \n# in the toys table.\n#\n# However, it is PERMISSIBLE to include the table name for ANY column (even\n# if the column only appears in a single table).\n# Some people prefer to include the table name even when it \n# is not necessary (as done above) to make it clearer to someone reading the \n# code which table contains which columns.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# Same results as above (read comment above)\nsqldf(\"select first_name, last_name, toys.*\n       from students CROSS JOIN toys\n       order by last_name, first_name, toy_name\")\n\n  first_name last_name toy_name description\n1       abie     aames   doggie  suffed dog\n2       abie     aames    dumbo    elephant\n3     bobbie    berger   doggie  suffed dog\n4     bobbie    berger    dumbo    elephant\n5      clara     cohen   doggie  suffed dog\n6      clara     cohen    dumbo    elephant\n\n#-----------------------------------------------------------\n#    # rows   =   #rows from table1   X   #rows from table2\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The total number of rows in the output of a \"cross join\" is\n# the product of the number of rows in the first table times \n# the number of rows in the 2nd table. In the example above, since\n# students contains 3 rows and toys contains 2 rows, the cross \n# join contains 6 rows.\n#-----------------------------------------------------------\n\n#-----------------------------------------------------------\n# You can use any other clauses in a cross join\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You an add any of the other clauses (where, group by, having, limit) \n# to a \"cross join\". For example, if the teacher only needs the\n# results of the \"cross join\" for the boys she can add a where\n# clause as follows:\n#-----------------------------------------------------------\n\nsqldf(\"select students.*, toys.*\n       from students CROSS JOIN toys\n       where gender = 'm'\n       order by last_name, first_name, toy_name\n      \")\n\n  first_name last_name gender toy_name description\n1       abie     aames      m   doggie  suffed dog\n2       abie     aames      m    dumbo    elephant\n3     bobbie    berger      m   doggie  suffed dog\n4     bobbie    berger      m    dumbo    elephant\n\n#--------------------------------------------------------------------------\n# \"Cartesian product\" is another name for a \"cross join\"\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The concept of a \"cross join\" also exists outside the scope of databases.\n# Specifically in mathematical \"set theory\" the concept of a \"cross join\" \n# is known as a \"cartesian product\". Sometimes, people will refer to \n# the term \"cartesian product\" in discussions about databases. However in \n# a select statement the words you can type are only \"cross join\" (NOT \"cartesian product\"). \n#--------------------------------------------------------------------------\n\n#--------------------------------------------------------------------------\n# All other types of joins depend on cross joins\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# On some occasions the need for a cross join arises, but \n# cross joins are not used nearly as often as other types of joins. \n# Nevertheless, it is important to understand the concept of a cross join since\n# every other type of join (\"innner join\", \"left join\", \"right join\", \"full join\")\n# depends on a \"cross join\".\n#\n# For example when calculating the result of an\n# \"inner join\" the database management system (DBMS) starts by first performing a cross join.\n# The DBMS then removes rows from the result of the cross join based on the \n# conditions specified in the on clause and the where clause of the \"inner join\".\n#---------------------------------------------------------------------------\n\n#---------------------------------------------------------------------------\n# A more typical example\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Many people find it hard to understand why you would want to use a cross join.\n# This confusion stems from the fact that rows from the two tables are combined\n# in the result even if there is seemingly no logical reason to combine them.\n#\n# It is easiest to see the results of a cross join when you have very few rows.\n# Therefore we will create two very small tables, favTitles and favPublishers\n#---------------------------------------------------------------------------\n\n# My favorite titles\nfavTitles = sqldf(\"select title_name, type, pub_id, pages\n                   from titles\n                   where title_id in ('T04', 'T05', 'T10')\n                   order by title_name\")\nfavTitles\n\n                   title_name       type pub_id pages\n1   But I Did It Unconciously psychology    P01   510\n2      Exchange of Platitudes psychology    P01   201\n3 Not Without My Fabrerge Egg  biography    P05    NA\n\n# Publishers of those favorite titles\nfavPublishers = sqldf(\"select pub_id, pub_name, city\n                       from publishers\n                       where pub_id in ('P01', 'P05')\n                       order by pub_id\")\nfavPublishers\n\n  pub_id          pub_name     city\n1    P01 Abatis Publishers New York\n2    P05    AAA Publishing Berkeley\n\n# The following is a cross join. It returns every record from the \n# first table (favTitles) paired up with every record from the 2nd table\n# (favPublishers). \n#\n# Notice that the pub_id's on each row of the cross join results do NOT\n# necessarily match. Therefore, it may be confusing as to why you should\n# use a cross join ... (keep reading)\n\nsqldf(\"select favTitles.*, favPublishers.*\n       from favTitles CROSS JOIN favPublishers\n      \")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2   But I Did It Unconciously psychology    P01   510    P05    AAA Publishing Berkeley\n3      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n4      Exchange of Platitudes psychology    P01   201    P05    AAA Publishing Berkeley\n5 Not Without My Fabrerge Egg  biography    P05    NA    P01 Abatis Publishers New York\n6 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#every-inner-join-starts-with-a-cross-join",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#every-inner-join-starts-with-a-cross-join",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.5 Every INNER JOIN starts with a CROSS JOIN",
    "text": "33.5 Every INNER JOIN starts with a CROSS JOIN\n\n# The following adds a where clause to the code. The where clause\n# removes those rows from\n# the cross join for which the favTitles.pub_id and the favPublishers.pub_id\n# don't match. This is really what you want in this case. In order\n# to get the correct results, first a CROSS JOIN was done that created\n# all possible combinations of a row from favTitles with a row from favPublishers.\n# Then the where clause eliminated the rows in the output that didn't make sense.\n# This leaves us with rows that show the details for a title and the details\n# of the publisher who published that title. \n\nsqldf(\"select favTitles.*, favPublishers.*\n       from favTitles CROSS JOIN favPublishers\n       where favTitles.pub_id = favPublishers.pub_id\n      \")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n3 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley\n\n# Another way of writing the same query is with an \"inner join\" (which \n# can also be written as just \"join\"). This version of the query \n# uses the word \"join\" instead of \"cross join\" and moves the condition\n# that ties the tables together from the where clause into the \"on\" clause.\n# This does EXACTLY THE SAME THING as the above query (with CROSS JOIN). \n# However, the \"join\" syntax with its \"on\" clause makes the details of \n# how the two tables are related to each other more obvious. \n\nsqldf(\"select favTitles.*, favPublishers.*\n       from favTitles JOIN favPublishers ON favTitles.pub_id = favPublishers.pub_id\n      \")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n3 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#the-word-inner-in-inner-join-is-optional",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#the-word-inner-in-inner-join-is-optional",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.6 The word INNER in “INNER JOIN” is optional",
    "text": "33.6 The word INNER in “INNER JOIN” is optional\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# JOIN vs INNER JOIN\n#\n# The technical term for this type of join is an \"INNER JOIN\".\n# However, you can write either \"INNER JOIN\" or just \"JOIN\".\n# The following does the same thing as the previous query.\n# The only difference in the code is that we added the\n# word \"INNER\".\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select favTitles.*, favPublishers.*\n       from favTitles INNER JOIN favPublishers ON favTitles.pub_id = favPublishers.pub_id\n      \")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n3 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#example",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#example",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.7 Example",
    "text": "33.7 Example\n\n# QUESTION \n#\n#   For each publisher, list the name of the publisher and total\n# number of pages that the publisher has published in all their books (ie. add\n# up all the pages in all books for each publisher). Show the publishers who\n# have published the most pages at the top of the result list.\n\n# ANSWER\n#\n# Step 1 - since publishers contains the pub_name  and  titles contains the \n# number of pages (num_pages) for each book, we need to join those tables.\nsqldf(\"select *\n       from publishers join titles on publishers.pub_id = titles.pub_id\n      \")\n\n   pub_id             pub_name          city state country title_id                          title_name       type pub_id pages price   sales         pubdate\n1     P01    Abatis Publishers      New York    NY     USA      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2     P01    Abatis Publishers      New York    NY     USA      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n3     P01    Abatis Publishers      New York    NY     USA      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n4     P01    Abatis Publishers      New York    NY     USA      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n5     P01    Abatis Publishers      New York    NY     USA      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n6     P01    Abatis Publishers      New York    NY     USA      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n7     P02      Core Dump Books San Francisco    CA     USA      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n8     P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n9     P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n10    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n11    P05       AAA Publishing      Berkeley    CA     USA      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n12    P05       AAA Publishing      Berkeley    CA     USA      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n13    P05       AAA Publishing      Berkeley    CA     USA      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n\n# Step 2 - use group by and sum to add up the pages for each publisher\nsqldf(\"select pub_name, sum(pages)\n       from publishers join titles on publishers.pub_id = titles.pub_id\n       group by publishers.pub_id, pub_name\n      \")\n\n              pub_name sum(pages)\n1    Abatis Publishers       1399\n2      Core Dump Books       1226\n3 Schandenfreude Press       1149\n4       AAA Publishing       1333\n\n# Step 3 - use order by \nsqldf(\"select pub_name, sum(pages)\n       from publishers join titles on publishers.pub_id = titles.pub_id\n       group by publishers.pub_id, pub_name\n       order by sum(pages) desc\n      \")\n\n              pub_name sum(pages)\n1    Abatis Publishers       1399\n2       AAA Publishing       1333\n3      Core Dump Books       1226\n4 Schandenfreude Press       1149\n\n# REMEMBER - every inner join starts out under the covers as a cross join\n# The following two commands are equivalent\n\nsqldf(\"select au_fname, au_lname   \n      from authors INNER join title_authors on authors.au_id = title_authors.au_id    \n      where title_id = 'T11'\n      order by au_lname, au_fname\")\n\n  au_fname au_lname\n1   Hallie     Hull\n2     Klee     Hull\n3   Harvey  Kellsey\n\nsqldf(\"select au_fname, au_lname   \n      from authors CROSS join title_authors    \n      where authors.au_id = title_authors.au_id\n                  and \n            title_id = 'T11'\n      order by au_lname, au_fname\")\n\n  au_fname au_lname\n1   Hallie     Hull\n2     Klee     Hull\n3   Harvey  Kellsey",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#even-more-types-of-joins",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#even-more-types-of-joins",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.8 Even more types of JOINs",
    "text": "33.8 Even more types of JOINs\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# SEVERAL DIFFERENT TYPES OF \"JOINS\"\n#\n# NOTE: There are actually several different types of \"joins\". They all do\n# slightly different things. For example:\n#\n#    CROSS JOIN                             - see the top of this file\n#    INNER JOIN (or just JOIN)              - we just described this above\n#    LEFT OUTER JOIN  (or just LEFT JOIN)   - we'll cover this later\n#    RIGHT OUTER JOIN  (or just RIGHT JOIN) - we'll cover this later\n#    FULL OUTER JOIN  (or just FULL JOIN)   - we'll cover this later\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# INNER JOIN   ( or just \"JOIN\" )\n#\n# The following is an \"inner join\" (or just \"join\").\n# (We already covered this above. It's repeated here for completeness).\n\nsqldf(\"select title_name, pub_name\n      from favTitles JOIN favPublishers ON favTitles.pub_id = favPublishers.pub_id\")\n\n                   title_name          pub_name\n1   But I Did It Unconciously Abatis Publishers\n2      Exchange of Platitudes Abatis Publishers\n3 Not Without My Fabrerge Egg    AAA Publishing",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#old-ansi-sql-way-of-doing-inner-joins",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#old-ansi-sql-way-of-doing-inner-joins",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.9 “old” ANSI SQL way of doing inner joins",
    "text": "33.9 “old” ANSI SQL way of doing inner joins\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ANSI SQL  - American National Standards Institute \n# \n# There are different versions of the ANSI SQL standard. The versions are\n# named by the year that the version came out. See this for a full history of SQL:   \n#   https://learnsql.com/blog/history-of-sql-standards/\n#\n# Joins were done differently before SQL 92 \n# They CAN still be done the \"old way\" (and often are). Some people prefer\n# the \"old way\" since there is less to type.\n#\n# The \"old way \" for a cross join is to not specify cross join \n# at all. Just list the tables with commas between them.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"\n  select favTitles.*, favPublishers.*\n  from favTitles, favPublishers\n\")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2   But I Did It Unconciously psychology    P01   510    P05    AAA Publishing Berkeley\n3      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n4      Exchange of Platitudes psychology    P01   201    P05    AAA Publishing Berkeley\n5 Not Without My Fabrerge Egg  biography    P05    NA    P01 Abatis Publishers New York\n6 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The \"old way\" of doing an \"inner join\" (i.e. a \"regular - join\")\n# is to do a \"cross join\" the old way and then add a where clause\n# that does the job of the \"on\".\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"\n  select favTitles.*, favPublishers.*\n  from favTitles, favPublishers\n  where favTitles.pub_id = favPublishers.pub_id\n\")\n\n                   title_name       type pub_id pages pub_id          pub_name     city\n1   But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York\n2      Exchange of Platitudes psychology    P01   201    P01 Abatis Publishers New York\n3 Not Without My Fabrerge Egg  biography    P05    NA    P05    AAA Publishing Berkeley\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# in the \"old way\" of doing inner joins, you could add additional \n# conditions to the where clause just like in the \"new way\" of\n# writing queries.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"\n  select favTitles.*, favPublishers.*\n  from favTitles, favPublishers\n  where favTitles.pub_id = favPublishers.pub_id and\n        pages &gt;= 300\n\")\n\n                 title_name       type pub_id pages pub_id          pub_name     city\n1 But I Did It Unconciously psychology    P01   510    P01 Abatis Publishers New York",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#left-join",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#left-join",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.10 LEFT JOIN",
    "text": "33.10 LEFT JOIN\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# LEFT OUTER JOIN   (or just \"LEFT JOIN\")\n#\n# Before learning about \"LEFT JOIN\", you should first understand one \n# issue that comes up when using \"INNER JOIN\"s. \n#\n# This issue is best shown by way of an example. \n# Publisher, P04 - Tenterhooks Press, does not have any corresponding rows in\n# the titles table (perhaps it's a new publishing company that is just getting\n# started).  For this reason when you use an \"inner join\" to join the titles\n# table and the publishers table you will NOT see any rows for Tenterhooks press.\n# The following \"inner join\" (or just \"join\") shows the output below.\n# Tenterhooks Press does not appear in the output at all.\n# \n#    &gt; sqldf(\"select pub_name, title_name\n#           from publishers JOIN titles on publishers.pub_id = titles.pub_id\n#           order by pub_name, title_name\")\n#    \n#    pub_name                          title_name\n#    1        AAA Publishing         Not Without My Fabrerge Egg\n#    2        AAA Publishing    Perhaps It's a Glandular Problem\n#    3        AAA Publishing           Spontaneous, Not Annoying\n#    4     Abatis Publishers                               1977!\n#    5     Abatis Publishers           But I Did It Unconciously\n#    6     Abatis Publishers              Exchange of Platitudes\n#    7     Abatis Publishers                    How About Never?\n#    8     Abatis Publishers        Just Wait Until After School\n#    9     Abatis Publishers                     Kiss My Boo Boo\n#    10      Core Dump Books        Ask Yor System Administrator\n#    11 Schandenfreude Press           200 Years of German Humor\n#    12 Schandenfreude Press                   I Blame My Mother\n#    13 Schandenfreude Press What Are The Civilian Applications?\n#\n#\n# However, suppose you want to see the same output, but you'd also like\n# to see Tenterhooks Press, even though they haven't actually published\n# any titles yet? .... that is where a LEFT JOIN is useful.\n#\n\n# The output of a \"left join\" contains all of the rows of an \"inner join\"\n# plus possibly a few more rows that would not appear in the output of \n# an inner join. \n# By simply changing the \"inner join\" (i.e. \"join\") in the previous\n# query to \"left join\" (or \"left outer join\") we get the following results.\n# Notice that Tenterhooks Press now DOES appear in a single row. \n# All columns that would have come from the corresponding rows in the titles\n# table are NULL (i.e. NA in R dataframes) for the Tenterhooks Press row.\n\n\n#    &gt; sqldf(\"select pub_name, title_name\n#           from publishers LEFT JOIN titles on publishers.pub_id = titles.pub_id\n#           order by pub_name, title_name\")\n#    \n#    pub_name                          title_name\n#    1        AAA Publishing         Not Without My Fabrerge Egg\n#    2        AAA Publishing    Perhaps It's a Glandular Problem\n#    3        AAA Publishing           Spontaneous, Not Annoying\n#    4     Abatis Publishers                               1977!\n#    5     Abatis Publishers           But I Did It Unconciously\n#    6     Abatis Publishers              Exchange of Platitudes\n#    7     Abatis Publishers                    How About Never?\n#    8     Abatis Publishers        Just Wait Until After School\n#    9     Abatis Publishers                     Kiss My Boo Boo\n#    10      Core Dump Books        Ask Yor System Administrator\n#    11 Schandenfreude Press           200 Years of German Humor\n#    12 Schandenfreude Press                   I Blame My Mother\n#    13 Schandenfreude Press What Are The Civilian Applications?    \n#    14    Tneterhooks Press                                &lt;NA&gt;\n#\n#\n# WHAT DOES \"LEFT\" MEAN ???\n#\n# The word \"left\" and \"right\" in \"left join\" and \"right join\" refer to the\n# location of the table names in the SQL code. Specifically, the text of \n# the code of every JOIN has a table name to the LEFT of the word \"JOIN\" and \n# a table name to the right of the word \"JOIN\". When you specify\n#\n#       \"... tableA LEFT JOIN tableB ON ...\" \n#\n# tableA is the \"left\" table and \n# tableB is the \"right\" table\n#\n# The LEFT JOIN syntax says that if there are rows in the left table (tableA\n# in the example above) that don't have corresponding rows in the \"right\" table\n# (tableB in the example above) then those rows from tableA should still \n# appear in the output. Any columns that would have come from tableB will \n# have NULL values (i.e. NA in R dataframes) in that row.\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The following is a \"regular join\" (i.e. an \"inner join\")\n#\n# Tenterhooks press does not show up since they have no titles\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select titles.title_name, pub_name, publishers.pub_id\n      from titles join publishers on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n                            title_name             pub_name pub_id\n1          Not Without My Fabrerge Egg       AAA Publishing    P05\n2     Perhaps It's a Glandular Problem       AAA Publishing    P05\n3            Spontaneous, Not Annoying       AAA Publishing    P05\n4                                1977!    Abatis Publishers    P01\n5            But I Did It Unconciously    Abatis Publishers    P01\n6               Exchange of Platitudes    Abatis Publishers    P01\n7                     How About Never?    Abatis Publishers    P01\n8         Just Wait Until After School    Abatis Publishers    P01\n9                      Kiss My Boo Boo    Abatis Publishers    P01\n10        Ask Yor System Administrator      Core Dump Books    P02\n11           200 Years of German Humor Schandenfreude Press    P03\n12                   I Blame My Mother Schandenfreude Press    P03\n13 What Are The Civilian Applications? Schandenfreude Press    P03\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# The following is a \"left join\"\n#\n# Tenterhooks press DOES show up since a left join will add to \n# the results of an inner join, any rows that appear in the \"left\"\n# hand table that have no corresponding rows in the right hand table.\n# In the result set, the columns from the right hand table in these\n# rows are NULL (or \"NA\" when using sqldf in R)\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select titles.title_name, pub_name, publishers.pub_id\n      from publishers left join titles on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n                            title_name             pub_name pub_id\n1          Not Without My Fabrerge Egg       AAA Publishing    P05\n2     Perhaps It's a Glandular Problem       AAA Publishing    P05\n3            Spontaneous, Not Annoying       AAA Publishing    P05\n4                                1977!    Abatis Publishers    P01\n5            But I Did It Unconciously    Abatis Publishers    P01\n6               Exchange of Platitudes    Abatis Publishers    P01\n7                     How About Never?    Abatis Publishers    P01\n8         Just Wait Until After School    Abatis Publishers    P01\n9                      Kiss My Boo Boo    Abatis Publishers    P01\n10        Ask Yor System Administrator      Core Dump Books    P02\n11           200 Years of German Humor Schandenfreude Press    P03\n12                   I Blame My Mother Schandenfreude Press    P03\n13 What Are The Civilian Applications? Schandenfreude Press    P03\n14                                &lt;NA&gt;    Tneterhooks Press    P04\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ALL of the columns from the right hand table contain NULLs (i.e. NA)\n# for the extra rows that are added by the left join\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n# To prove that point, the following shows all of the columns from both\n# the left hand table (publishers) and the right hand table (titles) \n# Notice that ALL of the columns for the right hand table will have \n# NULLs (i.e. NA in sqldf in R) for those rows of the output that \n# contain data from the left hand table for which there is no corresponding\n# row in the right hand table\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsqldf(\"select publishers.*, titles.*\n      from publishers left join titles on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n   pub_id             pub_name          city state country title_id                          title_name       type pub_id pages price   sales         pubdate\n1     P05       AAA Publishing      Berkeley    CA     USA      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n2     P05       AAA Publishing      Berkeley    CA     USA      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n3     P05       AAA Publishing      Berkeley    CA     USA      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n4     P01    Abatis Publishers      New York    NY     USA      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n5     P01    Abatis Publishers      New York    NY     USA      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n6     P01    Abatis Publishers      New York    NY     USA      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n7     P01    Abatis Publishers      New York    NY     USA      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n8     P01    Abatis Publishers      New York    NY     USA      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9     P01    Abatis Publishers      New York    NY     USA      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10    P02      Core Dump Books San Francisco    CA     USA      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n11    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n12    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n13    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n14    P04    Tneterhooks Press      Berkeley    CA     USA     &lt;NA&gt;                                &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA      NA            &lt;NA&gt;\n\n# BE CAREFUL ...\n#\n# When doing an \"inner join\" (i.e. a \"join\") it doesn't make a difference\n# which table you type to the left of the word \"join\" and which to the right.\n#\n# HOWEVER - for \"LEFT JOIN\"s it DOES MATTER!!!\n#\n# If we modify the previous query and put the publishers table on the \n# right side of the \"LEFT JOIN\" keyword, we will again NOT see Teneterhooks Press\n# since a LEFT JOIN only adds extra rows to the output for the rows in the \n# LEFT hand table that don't have corresponding rows in the right hand table.\n\n\n# Tenterhooks Press does NOT appear since the publishers table is on the\n# right hand side of the \"LEFT JOIN\" keywords\n\nsqldf(\"select titles.title_name, pub_name, publishers.pub_id\n      from titles left join publishers on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n                            title_name             pub_name pub_id\n1          Not Without My Fabrerge Egg       AAA Publishing    P05\n2     Perhaps It's a Glandular Problem       AAA Publishing    P05\n3            Spontaneous, Not Annoying       AAA Publishing    P05\n4                                1977!    Abatis Publishers    P01\n5            But I Did It Unconciously    Abatis Publishers    P01\n6               Exchange of Platitudes    Abatis Publishers    P01\n7                     How About Never?    Abatis Publishers    P01\n8         Just Wait Until After School    Abatis Publishers    P01\n9                      Kiss My Boo Boo    Abatis Publishers    P01\n10        Ask Yor System Administrator      Core Dump Books    P02\n11           200 Years of German Humor Schandenfreude Press    P03\n12                   I Blame My Mother Schandenfreude Press    P03\n13 What Are The Civilian Applications? Schandenfreude Press    P03\n\n# Lets add a row to the titles table that doesn't have a correponding row\n# in the publishers table.\n\nnewBook = data.frame(title_id='014', title_name='Data Everywhere', type='computer', pub_id='P99', pages='200', price='19.99', sales=0, pubdate='1/01/2024')\ntitles2 = rbind(titles, newBook)\ntitles2\n\n# A tibble: 14 × 8\n   title_id title_name                          type       pub_id pages price   sales pubdate        \n   &lt;chr&gt;    &lt;chr&gt;                               &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1 T01      1977!                               history    P01    107   21.99     566 8/1/2000 0:00  \n 2 T02      200 Years of German Humor           history    P03    14    19.95    9566 4/1/1998 0:00  \n 3 T03      Ask Yor System Administrator        computer   P02    1226  39.95   25667 9/1/2000 0:00  \n 4 T04      But I Did It Unconciously           psychology P01    510   12.99   13001 5/31/1999 0:00 \n 5 T05      Exchange of Platitudes              psychology P01    201   6.95   201440 1/1/2001 0:00  \n 6 T06      How About Never?                    biography  P01    473   19.95   11320 7/31/2000 0:00 \n 7 T07      I Blame My Mother                   biography  P03    333   23.95 1500200 10/1/1999 0:00 \n 8 T08      Just Wait Until After School        children   P01    86    10       4095 6/1/2001 0:00  \n 9 T09      Kiss My Boo Boo                     children   P01    22    13.95    5000 5/31/2002 0:00 \n10 T10      Not Without My Fabrerge Egg         biography  P05    &lt;NA&gt;  &lt;NA&gt;       NA &lt;NA&gt;           \n11 T11      Perhaps It's a Glandular Problem    psychology P05    826   7.99    94123 11/30/2000 0:00\n12 T12      Spontaneous, Not Annoying           biography  P05    507   12.99  100001 8/31/2000 0:00 \n13 T13      What Are The Civilian Applications? history    P03    802   29.99   10467 5/31/1999 0:00 \n14 014      Data Everywhere                     computer   P99    200   19.99       0 1/01/2024      \n\nsqldf(\"select titles2.title_name, pub_name, publishers.pub_id\n      from publishers left join titles2 on titles2.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n                            title_name             pub_name pub_id\n1          Not Without My Fabrerge Egg       AAA Publishing    P05\n2     Perhaps It's a Glandular Problem       AAA Publishing    P05\n3            Spontaneous, Not Annoying       AAA Publishing    P05\n4                                1977!    Abatis Publishers    P01\n5            But I Did It Unconciously    Abatis Publishers    P01\n6               Exchange of Platitudes    Abatis Publishers    P01\n7                     How About Never?    Abatis Publishers    P01\n8         Just Wait Until After School    Abatis Publishers    P01\n9                      Kiss My Boo Boo    Abatis Publishers    P01\n10        Ask Yor System Administrator      Core Dump Books    P02\n11           200 Years of German Humor Schandenfreude Press    P03\n12                   I Blame My Mother Schandenfreude Press    P03\n13 What Are The Civilian Applications? Schandenfreude Press    P03\n14                                &lt;NA&gt;    Tneterhooks Press    P04\n\nsqldf(\"select titles2.title_name, pub_name, publishers.pub_id\n      from titles2 left join publishers on titles2.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n                            title_name             pub_name pub_id\n1                      Data Everywhere                 &lt;NA&gt;   &lt;NA&gt;\n2          Not Without My Fabrerge Egg       AAA Publishing    P05\n3     Perhaps It's a Glandular Problem       AAA Publishing    P05\n4            Spontaneous, Not Annoying       AAA Publishing    P05\n5                                1977!    Abatis Publishers    P01\n6            But I Did It Unconciously    Abatis Publishers    P01\n7               Exchange of Platitudes    Abatis Publishers    P01\n8                     How About Never?    Abatis Publishers    P01\n9         Just Wait Until After School    Abatis Publishers    P01\n10                     Kiss My Boo Boo    Abatis Publishers    P01\n11        Ask Yor System Administrator      Core Dump Books    P02\n12           200 Years of German Humor Schandenfreude Press    P03\n13                   I Blame My Mother Schandenfreude Press    P03\n14 What Are The Civilian Applications? Schandenfreude Press    P03",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#practice",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#practice",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.11 Practice",
    "text": "33.11 Practice\n\n##########################################################################\n# PRACTICE QUESTION\n#\n# Write a query that shows the publisher name (i.e. pub_name) for those publishers\n# (and only those publishers)) who have no titles in the titles table.\n##############################################################################\n\n# Remember this query\nsqldf(\"select publishers.*, titles.*\n      from publishers left join titles on publishers.pub_id = titles.pub_id\n      \")\n\n   pub_id             pub_name          city state country title_id                          title_name       type pub_id pages price   sales         pubdate\n1     P01    Abatis Publishers      New York    NY     USA      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2     P01    Abatis Publishers      New York    NY     USA      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n3     P01    Abatis Publishers      New York    NY     USA      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n4     P01    Abatis Publishers      New York    NY     USA      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n5     P01    Abatis Publishers      New York    NY     USA      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n6     P01    Abatis Publishers      New York    NY     USA      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n7     P02      Core Dump Books San Francisco    CA     USA      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n8     P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n9     P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n10    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n11    P04    Tneterhooks Press      Berkeley    CA     USA     &lt;NA&gt;                                &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA      NA            &lt;NA&gt;\n12    P05       AAA Publishing      Berkeley    CA     USA      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n13    P05       AAA Publishing      Berkeley    CA     USA      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n14    P05       AAA Publishing      Berkeley    CA     USA      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n\n# add a where clause as shown below to get ONLY those rows from the left table that have no matching rows in the right table\nsqldf(\"select publishers.*, titles.*\n      from publishers left join titles on publishers.pub_id = titles.pub_id\n      where titles.type is null\n      \")\n\n  pub_id          pub_name     city state country title_id title_name type pub_id pages price sales pubdate\n1    P04 Tneterhooks Press Berkeley    CA     USA     &lt;NA&gt;       &lt;NA&gt; &lt;NA&gt;   &lt;NA&gt;    NA    NA    NA    &lt;NA&gt;\n\n# The following works, but it isn't ideal .... because titles.type \n# is not the best column to choose to test for null (keep reading for an explanation ...)\n\nsqldf(\"select pub_name\n      from publishers left join titles on publishers.pub_id = titles.pub_id\n      where titles.type is null\n      \")\n\n           pub_name\n1 Tneterhooks Press\n\n# To explain why titles.type was not the best column to choose above compare\n# the query below where we used titles.pages to test for NULL. \n# Since there is a row in titles table that contains NULL for pages that row\n# causes the publisher for that row (i..e AAA Publishing) to be displayed\n# in the output.\n\nsqldf(\"select pub_name\n      from publishers left join titles on publishers.pub_id = titles.pub_id\n      where titles.pages is null\n      \")\n\n           pub_name\n1 Tneterhooks Press\n2    AAA Publishing\n\n#----------------------------------------------------------------------------\n# The best way to identify only those rows that were added due to the\n# \"left\" join is to look for NULL in a column that would otherwise never\n# contain a NULL.\n#\n# Best choices are\n# - a primary key field (i.e. column)\n# - or another column that is defined in the database to never allow NULLs\n#----------------------------------------------------------------------------\n\n# The best way to do the same query is by checking the primary key of the right \n# hand table for NULL in the where clause.\n\nsqldf(\"select pub_name\n      from publishers left join titles on publishers.pub_id = titles.pub_id\n      where titles.title_id is null\n      \")\n\n           pub_name\n1 Tneterhooks Press",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#right-join-and-full-join",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#right-join-and-full-join",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.12 RIGHT JOIN and FULL JOIN",
    "text": "33.12 RIGHT JOIN and FULL JOIN\n\n###############################################################.\n# RIGHT JOIN    and    FULL JOIN\n#\n# LEFT JOIN   is also known as a   LEFT OUTER JOIN\n# RIGHT JOIN   is also known as a  RIGHT OUTER JOIN\n# FULL JOIN   is also known as a   FULL OUTER JOIN\n#\n# These are all known as \"OUTER\" joins\n###############################################################.\n\n# ANSI SQL also has \"right join\" and \"full join\"\n#\n# A right join does the same thing as a left join except that\n# \"right join\" shows rows in the output that correspond to rows from the \n# right hand table that don't have corresponding rows in the left hand table.\n\n# The following LEFT JOIN ...\nsqldf(\"select publishers.*, titles.*\n      from publishers left join titles on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n   pub_id             pub_name          city state country title_id                          title_name       type pub_id pages price   sales         pubdate\n1     P05       AAA Publishing      Berkeley    CA     USA      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n2     P05       AAA Publishing      Berkeley    CA     USA      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n3     P05       AAA Publishing      Berkeley    CA     USA      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n4     P01    Abatis Publishers      New York    NY     USA      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n5     P01    Abatis Publishers      New York    NY     USA      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n6     P01    Abatis Publishers      New York    NY     USA      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n7     P01    Abatis Publishers      New York    NY     USA      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n8     P01    Abatis Publishers      New York    NY     USA      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9     P01    Abatis Publishers      New York    NY     USA      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10    P02      Core Dump Books San Francisco    CA     USA      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n11    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n12    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n13    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n14    P04    Tneterhooks Press      Berkeley    CA     USA     &lt;NA&gt;                                &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA      NA            &lt;NA&gt;\n\n# ... will generate the EXACT SAME OUTPUT as the following RIGHT JOIN\nsqldf(\"select publishers.*, titles.*\n      from titles right join publishers on titles.pub_id = publishers.pub_id\n      order by pub_name, title_id\")\n\n   pub_id             pub_name          city state country title_id                          title_name       type pub_id pages price   sales         pubdate\n1     P05       AAA Publishing      Berkeley    CA     USA      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n2     P05       AAA Publishing      Berkeley    CA     USA      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n3     P05       AAA Publishing      Berkeley    CA     USA      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n4     P01    Abatis Publishers      New York    NY     USA      T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n5     P01    Abatis Publishers      New York    NY     USA      T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n6     P01    Abatis Publishers      New York    NY     USA      T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n7     P01    Abatis Publishers      New York    NY     USA      T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n8     P01    Abatis Publishers      New York    NY     USA      T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9     P01    Abatis Publishers      New York    NY     USA      T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10    P02      Core Dump Books San Francisco    CA     USA      T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n11    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n12    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n13    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n14    P04    Tneterhooks Press      Berkeley    CA     USA     &lt;NA&gt;                                &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA      NA            &lt;NA&gt;\n\n# HOWEVER - SQLite does not implement RIGHT JOIN even though it is part of the ANSI Standard for SQL\n\n# Similarly a \"full join\" shows rows in the output for BOTH\n# - rows in the left hand table that don't have corresponding rows in the right hand table and\n# - rows in the right hand table that don't have corresponding rows in the left hand table and\n#\n# SQLite - the database that are using does not implement RIGHT JOINs or FULL JOINs\n# However, there are simple workarounds for both.\n# The workaround for a right join is simply to use a left join and type \n# the table names in the opposite order.\n#\n# The workaround for FULL JOIN requires you to understand \"UNION\" ... see below\n\n\n##########################################################################.\n### UPDATE !!!\n###\n###     As of now (in 2024) sqlite has implemented both RIGHT JOIN\n###     and FULL JOIN. I will update these notes with that info soon.\n##########################################################################.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#sql-set-operations-i.e.-union-except-intersect",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#sql-set-operations-i.e.-union-except-intersect",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.13 SQL set operations (i.e. UNION, EXCEPT, INTERSECT)",
    "text": "33.13 SQL set operations (i.e. UNION, EXCEPT, INTERSECT)\n\n##################################################\n# Combining the output of 2 queries\n#\n#     UNION\n#     UNION ALL\n#     EXCEPT\n#     INTERSECT\n##################################################\n\n# The following is a query that you should already understand.\n# There is nothing new here. We will see below how to rewrite this \n# query in a different way by using \"UNION\"\n#\n# The following displays titles that have fewer than 10 pages\n# and also titles that have more than 20 pages.\n\nsqldf(\"select title_name, price\n      from titles\n      where price &lt; 10 or price &gt; 20\n      order by price\")\n\n                           title_name price\n1              Exchange of Platitudes  6.95\n2    Perhaps It's a Glandular Problem  7.99\n3                               1977! 21.99\n4                   I Blame My Mother 23.95\n5 What Are The Civilian Applications? 29.99\n6        Ask Yor System Administrator 39.95\n\n\n\n33.13.1 UNION\n\n#----------------------------------------------------------------------\n# UNION\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# union is used to combine two different queries into one output set.\n# The 2 queries must have the same number of columns and the same type\n# of columns.\n#\n# The order by is done after both queries and combines the rows from \n# the two select statements in the specified order.\n#----------------------------------------------------------------------\n\n# First ... let's see each of the queries on their own ...\n\n# get the titles that are &lt; 10 dollars\nsqldf(\"select title_name, price\n      from titles\n      where price &lt; 10\n\")\n\n                        title_name price\n1           Exchange of Platitudes  6.95\n2 Perhaps It's a Glandular Problem  7.99\n\n# get the titles that are &gt; 20 dollars\nsqldf(\"select title_name, price\n      from titles\n      where price &gt; 20\n\")\n\n                           title_name price\n1                               1977! 21.99\n2        Ask Yor System Administrator 39.95\n3                   I Blame My Mother 23.95\n4 What Are The Civilian Applications? 29.99\n\n# Now, let's see how to use\n# UNION to combine the two previous queries into a single output.\n\nsqldf(\"select title_name, price\n      from titles\n      where price &lt; 10\n      \n      union\n      \n      select title_name, price\n      from titles\n      where price &gt; 20\n      \n      order by price\")\n\n                           title_name price\n1              Exchange of Platitudes  6.95\n2    Perhaps It's a Glandular Problem  7.99\n3                               1977! 21.99\n4                   I Blame My Mother 23.95\n5 What Are The Civilian Applications? 29.99\n6        Ask Yor System Administrator 39.95\n\n# Do the same but order by title_name\nsqldf(\"select title_name, price\n      from titles\n      where price &lt; 10\n      \n      union\n      \n      select title_name, price\n      from titles\n      where price &gt; 20\n      \n      order by title_name\")\n\n                           title_name price\n1                               1977! 21.99\n2        Ask Yor System Administrator 39.95\n3              Exchange of Platitudes  6.95\n4                   I Blame My Mother 23.95\n5    Perhaps It's a Glandular Problem  7.99\n6 What Are The Civilian Applications? 29.99\n\n# Another example\n# Show all book titles for which the price is &gt; 20 or the pages is &gt; 500\n\nsqldf(\"select title_name, price, pages\n      from titles\n      where price &gt; 20 or pages &gt; 500\n      order by price\")\n\n                           title_name price pages\n1    Perhaps It's a Glandular Problem  7.99   826\n2           But I Did It Unconciously 12.99   510\n3           Spontaneous, Not Annoying 12.99   507\n4                               1977! 21.99   107\n5                   I Blame My Mother 23.95   333\n6 What Are The Civilian Applications? 29.99   802\n7        Ask Yor System Administrator 39.95  1226\n\n\n\n\n33.13.2 UNION ALL\n\n#----------------------------------------------------------\n# UNION removes duplicate rows from the output\n#\n# UNION ALL does NOT remove duplicate rows\n#----------------------------------------------------------\n\n\n# Union removes duplicate rows from the output\n#\n# Notice that some books match both conditions but only come up in the \n# results once. \n\nsqldf(\"select title_name, price, pages\n      from titles\n      where price &gt; 20\n      \n      union\n      \n      select title_name, price, pages\n      from titles\n      where pages &gt; 500\n      \n      order by price\")\n\n                           title_name price pages\n1    Perhaps It's a Glandular Problem  7.99   826\n2           But I Did It Unconciously 12.99   510\n3           Spontaneous, Not Annoying 12.99   507\n4                               1977! 21.99   107\n5                   I Blame My Mother 23.95   333\n6 What Are The Civilian Applications? 29.99   802\n7        Ask Yor System Administrator 39.95  1226\n\n# UNION ALL\n#\n# Running the same query as above but typing \"UNION ALL\"\n# Will show the rows that meet both conditions, twice.\n\nsqldf(\"select title_name, price, pages\n      from titles\n      where price &gt; 20\n      \n      union all\n      \n      select title_name, price, pages\n      from titles\n      where pages &gt; 500\n      \n      order by price\")\n\n                           title_name price pages\n1    Perhaps It's a Glandular Problem  7.99   826\n2           But I Did It Unconciously 12.99   510\n3           Spontaneous, Not Annoying 12.99   507\n4                               1977! 21.99   107\n5                   I Blame My Mother 23.95   333\n6 What Are The Civilian Applications? 29.99   802\n7 What Are The Civilian Applications? 29.99   802\n8        Ask Yor System Administrator 39.95  1226\n9        Ask Yor System Administrator 39.95  1226\n\n\n\n\n33.13.3 Practice\n\n# PRACTICE QUESTION\n# \n# Show the names of all authors and publishers and the city that they are from.\n# Indicate if the row in the output is for a 'publisher' or for an 'author'.\n# For authors dispaly the name of the author as FIRST LAST in a single column.\n# This column should be called 'name' - this is the same column as \n# the publisher's name would appear. \n# Sort the results by the name.\n#\n# NAME       CITY       Pub_or_author\n# Joe Smith  New York   Author\n# AAA pubs   Seattle    Publisher\n# Sue Cohen  LA         Author\n# etc.\n\n# We can create two queries, one for authors and one for publishers\nsqldf(\"select au_fname || ' ' || au_lname as name, city, 'Author' as 'pub_or_author'\n      from authors\")\n\n               name          city pub_or_author\n1     Sarah Buchman         Bronx        Author\n2   Wendy Heydemark       Boulder        Author\n3       Hallie Hull San Francisco        Author\n4         Klee Hull San Francisco        Author\n5   Christian Kells      New York        Author\n6    Harvey Kellsey     Palo Alto        Author\n7 Paddy O'Furniture      Sarasota        Author\n\nsqldf(\"select pub_name, city, 'Publisher' as 'pub_or_author'\n      from publishers\")\n\n              pub_name          city pub_or_author\n1    Abatis Publishers      New York     Publisher\n2      Core Dump Books San Francisco     Publisher\n3 Schandenfreude Press       Hamburg     Publisher\n4    Tneterhooks Press      Berkeley     Publisher\n5       AAA Publishing      Berkeley     Publisher\n\n# Then combine the two queries with union\n\nsqldf(\"select au_fname || ' ' || au_lname as name, city, 'Author' as 'pub_or_author'\n      from authors\n      \n      union\n      \n      select pub_name, city, 'Publisher' as 'pub_or_author'\n      from publishers\n      \n      order by 1\")\n\n                   name          city pub_or_author\n1        AAA Publishing      Berkeley     Publisher\n2     Abatis Publishers      New York     Publisher\n3       Christian Kells      New York        Author\n4       Core Dump Books San Francisco     Publisher\n5           Hallie Hull San Francisco        Author\n6        Harvey Kellsey     Palo Alto        Author\n7             Klee Hull San Francisco        Author\n8     Paddy O'Furniture      Sarasota        Author\n9         Sarah Buchman         Bronx        Author\n10 Schandenfreude Press       Hamburg     Publisher\n11    Tneterhooks Press      Berkeley     Publisher\n12      Wendy Heydemark       Boulder        Author\n\n\n\n\n33.13.4 EXCEPT\n\n###################################\n# EXCEPT\n###################################\n\n# QUESTION\n# Show the publishers who publish biography books but not history books\n\n# ANSWER\n\n# Let's start by first writing a query that gets those publishers who have\n# published biographies (spoiler alert ... we are later going to \n# remove from this list those publishers who have published history books)\n\nsqldf(\"select distinct pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'biography'\")\n\n              pub_name\n1    Abatis Publishers\n2 Schandenfreude Press\n3       AAA Publishing\n\n# If you take out the distinct you will get duplicate copies of publisher names\nsqldf(\"select pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'biography'\")\n\n              pub_name\n1    Abatis Publishers\n2 Schandenfreude Press\n3       AAA Publishing\n4       AAA Publishing\n\n# You get the duplicates for publishers who published more than one biography\n#\n# You can see that by looking at all of the data, not just the publisher name.\n\nsqldf(\"select publishers.*, titles.*\n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'biography'\")\n\n  pub_id             pub_name     city state country title_id                  title_name      type pub_id pages price   sales        pubdate\n1    P01    Abatis Publishers New York    NY     USA      T06            How About Never? biography    P01   473 19.95   11320 7/31/2000 0:00\n2    P03 Schandenfreude Press  Hamburg  &lt;NA&gt; Germany      T07           I Blame My Mother biography    P03   333 23.95 1500200 10/1/1999 0:00\n3    P05       AAA Publishing Berkeley    CA     USA      T10 Not Without My Fabrerge Egg biography    P05    NA    NA      NA           &lt;NA&gt;\n4    P05       AAA Publishing Berkeley    CA     USA      T12   Spontaneous, Not Annoying biography    P05   507 12.99  100001 8/31/2000 0:00\n\n# Once again, this is the query that shows publishers who published biographies\nsqldf(\"select distinct pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'biography'\")\n\n              pub_name\n1    Abatis Publishers\n2 Schandenfreude Press\n3       AAA Publishing\n\n# This is the query for publishers who published history books\nsqldf(\"select distinct pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'history'\")\n\n              pub_name\n1    Abatis Publishers\n2 Schandenfreude Press\n\n# Now show publishers who published biograhpies BUT NOT history books\n\nsqldf(\"select distinct pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'biography'\n      \n      EXCEPT\n      \n      select distinct pub_name \n      from publishers join titles on publishers.pub_id = titles.pub_id\n      where titles.type = 'history'\")\n\n        pub_name\n1 AAA Publishing\n\n\n\n\n33.13.5 INTERSECT\n\n###################################\n# INTERSECT\n###################################\n\n#---------------------------------------------------------------------\n# Show the cities that have both publishers and authors residing there.\n#---------------------------------------------------------------------\n\n# show the cities that have publishers\nsqldf(\"select distinct city, state\n      from publishers\n      order by city\n      \")\n\n           city state\n1      Berkeley    CA\n2       Hamburg  &lt;NA&gt;\n3      New York    NY\n4 San Francisco    CA\n\n# Show the cities that have authors\nsqldf(\"select distinct city, state\n      from authors\n      order by city\")\n\n           city state\n1       Boulder    CO\n2         Bronx    NY\n3      New York    NY\n4     Palo Alto    CA\n5 San Francisco    CA\n6      Sarasota    FL\n\n# Combine the two queries with INTERSECT.\n# This will only show rows that are output from both of the\n# select statements.\n\nsqldf(\"   select distinct city, state\n          from publishers\n\n      INTERSECT\n\n         select distinct city, state\n         from authors\n         \n      order by city\")\n\n           city state\n1      New York    NY\n2 San Francisco    CA",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#subqueries",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#subqueries",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.14 Subqueries",
    "text": "33.14 Subqueries\nA “subquery” is a select statement that appears inside of another select statement. The larger select statement is often referred to as the “outer query” and the subquery is often referred to as the “inner query”. This will become much more understandable after seeing some of the examples shown below.\nNote that a single outer query may contain several different subqueries. Also note that a subquery may itself contain yet another subquery.\nSubqueries are most often found in the SELECT, FROM, WHERE and HAVING clauses of a SQL select statement. The easiest types of subqueries to understand are probably those that appear in the WHERE or HAVING clauses of a larger select statement. Below are some examples. We will cover other types of subqueries after these.\n\n33.14.1 Subqueries in WHERE and HAVING clauses\nSubqueries that appear in the WHERE or HAVING clause can be categorized by the “shape” (ie. number of rows and columns that are returned by the subquery).\n\nsubqueries that return just one value (i.e. one row of one column)\nThese types of subqueries can be placed anywhere in the outer query that you normally would put a single value (e.g. before or after an operator such as a “&gt;” sign or “-” sign).\nsubqueries that return just on column but possibly many rows.\nThese types of subqueries are used to generate “lists” of value that can appear after the SQL IN operator (see below for an example).\nsubqueries that return any number of rows and any number of columns.\nThese types of subqueries may appear after the SQL keyword “EXISTS” which we discuss below.\n\n\nSubqueries that return a single value\nIn the following example, the subqueries appear after a “&gt;” and after a “&lt;”. Therefore the subquery MUST return just a single value.\nQUESTION (a) Show titles whose price is greater than the average price of all titles.\n\nShow titles whose price is less than the average price of all titles.\n\nANSWER (a)\n\n# Titles whose price is greater than the average price of all titles\nsqldf(\"select title_name, price\n      from titles\n      where price &gt; \n              (\n                 select avg(price)\n                 from titles\n              )\n      order by title_name\n      \")\n\n                           title_name price\n1                               1977! 21.99\n2           200 Years of German Humor 19.95\n3        Ask Yor System Administrator 39.95\n4                    How About Never? 19.95\n5                   I Blame My Mother 23.95\n6 What Are The Civilian Applications? 29.99\n\n\nANSWER (b)\n\n# Titles whose price is less than the average price of all titles\n# (same as above answer except we use \"&lt;\" instead of \"&gt;\")\nsqldf(\"select title_name, price\n      from titles\n      where price &lt;\n              (\n                 select avg(price)\n                 from titles\n              )\n      order by title_name\n      \")\n\n                        title_name price\n1        But I Did It Unconciously 12.99\n2           Exchange of Platitudes  6.95\n3     Just Wait Until After School 10.00\n4                  Kiss My Boo Boo 13.95\n5 Perhaps It's a Glandular Problem  7.99\n6        Spontaneous, Not Annoying 12.99\n\n\nLet’s show that the above queries actually did their jobs by determining the avg price of all titles.\n\n# average price of all titles\n\nsqldf(\" select avg(price)\n        from titles\n\")\n\n  avg(price)\n1    18.3875\n\n\nYOU CANNOT WRITE THE ABOVE QUERIES WITHOUT USING A SUBQUERY!!!! THE FOLLOWING WILL PRODUCE AN ERROR SINCE YOU MAY NOT USE AN AGGREGATE FUNCTION SUCH AS AVG INSIDE A WHERE CLAUSE\n\nsqldf(\"select title_name, price\n      from titles\n      where price &gt; avg(price)\n      order by title_name\n      \")\n\nError: misuse of aggregate function avg()\n\n\n\n\n\n33.14.2 Subquery in HAVING clause\nThese are very similar to subqueries in the WHERE clause:\nQuestion:\nShow the types of books with the 3 highest avg prices. Sort the results in alphabetical order of the type name.\nANSWER:\nThe following will answer the question but it doesn’t put the rows in alphabetical order of the name of the type.\n\nsqldf(\"select type, avg(price)\n       from titles\n       group by type\n       order by avg(price) desc\n       limit 3\")\n\n       type avg(price)\n1  computer   39.95000\n2   history   23.97667\n3 biography   18.96333\n\n\nTo get just the 3 largest average prices, we had to use “order by avg(price) desc” combined with “limit 3” to get only the first 3 rows. However, that leaves the rows in the order of descending avg(price) and not in alphabetical order of the type as we had been asked to do.\nWe can get the rows in the right order by using a subquery as follows:\n\nsqldf(\"select type, avg(price)\n       from titles\n       group by type\n       HAVING avg(price) &gt;= \n                 (select avg(price) \n                  from titles\n                  group by type\n                  order by avg(price) desc\n                  limit 1 offset 2)\n       order by type\")\n\n       type avg(price)\n1 biography   18.96333\n2  computer   39.95000\n3   history   23.97667\n\n\nThe following is the output of the subquery if it would have been run on its own.\n\n                sqldf(\"\n                  select avg(price) \n                  from titles\n                  group by type\n                  order by avg(price) desc\n                  limit 1 offset 2\n                \")\n\n  avg(price)\n1   18.96333\n\n\nThe subquery returns just the 3rd most expensive average price. When this is placed in the having clause of the outer query we get the rows we want without the need to use limit in the outer query. We can now order the outer query by the type column instead of by the price.\n\n\n33.14.3 Subquery in SELECT, FROM, WHERE, HAVING\nNow that you’ve seen the basic idea of a subquery, let’s just reiterate that a subquery can appear in any of the following clauses in a select statement:\n SELECT - the subquery must return a single item (i.e. single row single column)\n\n FROM - subqueries in the FROM clause may return many rows and many columns.\n        The subquery is treated as though it were an actual table.\n        You need to follow the subquery with a 'name' that will be used\n        as the name of the pseudo-table that is generated by the subquery.\n        This is similar to the way you can name columns with aliases.\n\n WHERE or the HAVING - these subqueries may return the following:\n\n         (a)  a single row, single column , i.e. a single value\n              e.g. a subquery that appears before or after a + sign or a &gt; sign\n               or anywhere that you'd expect a single value to appear.\n\n         (b) possibly many rows of a single column -\n             after the IN operator\n\n         (c) possibly many rows, many columns -\n             after the EXISTS operator \n\n\n33.14.4 subquery in SELECT\nSubqueries in the SELECT clause must return a single value (i.e. a single row with a single column).\nQUESTION: Write a query that shows the title_name, price and the amount the price is above average for those titles whose price is above average.\nANSWER\n\nsqldf(\"select title_name, price, price - (select avg(price) from titles) as 'amt_above_avg'\n      from titles\n      where price &gt; (select avg(price) from titles)\n      order by title_name\n      \")\n\n                           title_name price amt_above_avg\n1                               1977! 21.99        3.6025\n2           200 Years of German Humor 19.95        1.5625\n3        Ask Yor System Administrator 39.95       21.5625\n4                    How About Never? 19.95        1.5625\n5                   I Blame My Mother 23.95        5.5625\n6 What Are The Civilian Applications? 29.99       11.6025\n\n\nNote that the subquery was required since you shouldn’t mix aggregate functions, e.g. avg(price), in the select statement with non-aggregated columns (e.g. title_name). If you do, then the avg function implies that you will get the average of sevaral prices (i.e. the prices from several rows) being displayed in a single row of output. Howver, the non-aggregated columns such as title_name cannot be summarized into a single row, so the implication is that there should be several rows of output.\nIn ANSI SQL this is an error. However, the SQLite flavor of SQL that we are using allows this but only returns ONE row of data. This is VERY misleading. For example, with a subquery, using sqlite we get the following:\n\nsqldf(\"select title_name, price, price - avg(price) as 'amt_above_avg'\n      from titles\n      where price &gt; (select avg(price) from titles)\n      order by title_name\n      \")\n\n  title_name price amt_above_avg\n1      1977! 21.99     -3.973333\n\n\nWhich seems to imply that there is only one row for a book whose price is above average. However, as we saw in the correct answer shown above, there are actually six such book in our titles table.\nBottom line is that you need to use the version above that has the subquery to answer this question correctly.\n\n\n33.14.5 You may have more than one subquery\nQUESTION: Same as above but also show the average price of all books as a column in the results.\nANSWER\n\nsqldf(\"select title_name, \n              price,\n              (select avg(price) from titles) as avgPriceOfAllBooks,\n              price - (select avg(price) from titles) as 'amt_above_avg'\n      from titles\n      where price &gt; (select avg(price) from titles)\n      order by title_name\n      \")\n\n                           title_name price avgPriceOfAllBooks amt_above_avg\n1                               1977! 21.99            18.3875        3.6025\n2           200 Years of German Humor 19.95            18.3875        1.5625\n3        Ask Yor System Administrator 39.95            18.3875       21.5625\n4                    How About Never? 19.95            18.3875        1.5625\n5                   I Blame My Mother 23.95            18.3875        5.5625\n6 What Are The Civilian Applications? 29.99            18.3875       11.6025\n\n\nWhen the query above is processed, the subqueries get run and produce the result 18.3875. That value is then plugged into the outer query and the outer query is run. The following is what the outer query would look like after the result of the subqueries are plugged into the outer query. You never have to type this. It is done automatically.\n\nsqldf(\"select title_name, \n              price,\n              18.3875 as avgPriceOfAllBooks,\n              price - 18.3875 as 'amt_above_avg'\n      from titles\n      where price &gt; 18.3875\n      order by title_name\n      \")\n\n                           title_name price avgPriceOfAllBooks amt_above_avg\n1                               1977! 21.99            18.3875        3.6025\n2           200 Years of German Humor 19.95            18.3875        1.5625\n3        Ask Yor System Administrator 39.95            18.3875       21.5625\n4                    How About Never? 19.95            18.3875        1.5625\n5                   I Blame My Mother 23.95            18.3875        5.5625\n6 What Are The Civilian Applications? 29.99            18.3875       11.6025\n\n\n\nAnother example - subquery in SELECT\n\n# GOAL: \n#\n# Show the avg price of each type of book and how much that differs\n# from the overall avg price of all books\n\n# THINKING IT THROUGH: \n# start with getting the avg price of each type of book\nsqldf(\"select type, avg(price)\n       from titles\n       group by type\n       order by type\n       \")\n\n        type avg(price)\n1  biography   18.96333\n2   children   11.97500\n3   computer   39.95000\n4    history   23.97667\n5 psychology    9.31000\n\n# THINKING IT THROUGH: \n# get overall avg price for all books\nsqldf(\"select avg(price) from titles\")\n\n  avg(price)\n1    18.3875\n\n# FINAL ANSWER:\n#\n# Put is all together into a single answer with a subquery in the SELECT clause\n\nsqldf(\"select type, avg(price), avg(price) - (select avg(price) from titles) as 'diff from overall avg'\n       from titles\n       group by type\n       order by type\n       \")\n\n        type avg(price) diff from overall avg\n1  biography   18.96333             0.5758333\n2   children   11.97500            -6.4125000\n3   computer   39.95000            21.5625000\n4    history   23.97667             5.5891667\n5 psychology    9.31000            -9.0775000\n\n\n\n\n\n33.14.6 subquery after IN can return a single column\n\n#------------------------------------------------------------------------\n# Subqueries that return a single column of values\n# can appear after the IN operator.\n#------------------------------------------------------------------------\n\n# Show authors that live in the same city as at least one  publisher\n\n # First let's get all the cities where publishers are located\n\nsqldf(\"select distinct city\n      from publishers\")\n\n           city\n1      New York\n2 San Francisco\n3       Hamburg\n4      Berkeley\n\n# You can use the \"in\" operator in sql to compare a value to a \"list\" of\n# values.\n# \n# The following will work ... but it forces us to type in the names \n# of the publisher's cities. If we change the data in the publishers table,\n# this \"hard-coded\" version of the query may not work correctly anymore.\n\nsqldf(\"select au_fname, au_lname, city\n       from authors\n       where city in ('New York','San Francisco','Hamburg','Berkeley')\n       order by au_lname, au_fname\")\n\n   au_fname au_lname          city\n1    Hallie     Hull San Francisco\n2      Klee     Hull San Francisco\n3 Christian    Kells      New York\n\n# We can replace the list of cities with a subquery that automatically\n# figures out that list. This subquery returns a single column\n# that can be used in place of the hard-coded data from the previous query.\n\nsqldf(\"select au_fname, au_lname, city\n       from authors\n       where city in \n           (\n              select distinct city \n              from publishers\n           )\n       order by au_lname, au_fname\")\n\n   au_fname au_lname          city\n1    Hallie     Hull San Francisco\n2      Klee     Hull San Francisco\n3 Christian    Kells      New York\n\n\n\n\n33.14.7 Subqueries in the FROM clause\nSubqueries in the FROM clause may return many rows and many columns. The subquery is treated as though it were an actual table. You need to follow the subquery with a ‘name’ that will be used as the name of the pseudo-table that is generated by the subquery. This name is known as an ‘alias’ for the table. This is similar to the way you can name columns with aliases.\nQUESTION\nShow the title, price and type of each book. Also show the max and min and average prices of books of that category as additional columns in the output.\nThe results should appear as follows.\n\n\n                            title_name price       type typeMaxPrice typeMinPrice typeAvgPrice\n1                                1977! 21.99    history        29.99        19.95        23.98\n2            200 Years of German Humor 19.95    history        29.99        19.95        23.98\n3         Ask Yor System Administrator 39.95   computer        39.95        39.95        39.95\n4            But I Did It Unconciously 12.99 psychology        12.99         6.95         9.31\n5               Exchange of Platitudes  6.95 psychology        12.99         6.95         9.31\n6                     How About Never? 19.95  biography        23.95        12.99        18.96\n7                    I Blame My Mother 23.95  biography        23.95        12.99        18.96\n8         Just Wait Until After School 10.00   children        13.95        10.00        11.97\n9                      Kiss My Boo Boo 13.95   children        13.95        10.00        11.97\n10         Not Without My Fabrerge Egg    NA  biography        23.95        12.99        18.96\n11    Perhaps It's a Glandular Problem  7.99 psychology        12.99         6.95         9.31\n12           Spontaneous, Not Annoying 12.99  biography        23.95        12.99        18.96\n13 What Are The Civilian Applications? 29.99    history        29.99        19.95        23.98\n\n\nANSWER\n\nsqldf(\"select title_name, price, titles.type, typeMaxPrice, typeMinPrice, typeAvgPrice\n      from titles JOIN (select type,\n                               max(price) as typeMaxPrice, \n                               min(price) as typeMinPrice, \n                               round(avg(price),2) as typeAvgPrice\n                        from titles\n                        group by type) as summaryData\n        \n        ON titles.type = summaryData.type\n        \n      order by title_name\")\n\nExplanation:\nThe subquery produces the result shown below.\n\n           sqldf(\"select type,\n                         max(price) as typeMaxPrice, \n                         min(price) as typeMinPrice, \n                         round(avg(price),2) as typeAvgPrice\n                  from titles\n                  group by type\")\n\n        type typeMaxPrice typeMinPrice typeAvgPrice\n1  biography        23.95        12.99        18.96\n2   children        13.95        10.00        11.97\n3   computer        39.95        39.95        39.95\n4    history        29.99        19.95        23.98\n5 psychology        12.99         6.95         9.31\n\n\nThis result is called “summaryData” and is treated as a new table. This is not really a table in the database but can be used in the query as though there were such a table in the database.\nThis new “summaryData table” contains just one row for each type. We can therefore consider the “type” to be the primary key of this new “summaryData table. We then to JOIN the titles table with this new”summaryData table” by thinking of the summaryData.type column as the “primary key” of the summaryData table and the title.type column as a foreign key in the titles table that references the type column in the summaryData table.\nThe result is that we can mix non-aggregated data (e.g. title_name column) with aggregate data (e.g. max, min and avg of prices for each type) in the output of the query.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#correlated-subqueries",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#correlated-subqueries",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.15 Correlated Subqueries",
    "text": "33.15 Correlated Subqueries\nIn a “correlated” subquery, the subquery is executed once for each row in the outer query. The subquery can refer to the data in the row for the outer query. Notice that in the following query the sub-query refers to the authors.au_id table even thought the from clause in the subquery does NOT include the authors table. This is actually a reference to the row from the authors table that is being processed while the subqery is running.\nThis subquery will be run again and again for every row that is processed in the outer query.\nNotice that the subquery is in the where clause and the order by clause at the end is once again from the outer query.\nExample: Show authors who published more than 2 titles\n\nsqldf(\"select au_fname, au_lname\n      from authors\n      where \n         (select count(*)\n          from title_authors\n          where authors.au_id = title_authors.au_id) &gt; 2\n      order by au_lname, au_fname\")\n\n  au_fname  au_lname\n1    Sarah   Buchman\n2    Wendy Heydemark\n3     Klee      Hull\n4   Harvey   Kellsey\n\n\nIf you want to show the actual count in the output you would have to repeat the subquery in the SELECT clause of the outer query.\n\nsqldf(\"select au_fname, au_lname,\n          (select count(*)\n          from title_authors\n          where authors.au_id = title_authors.au_id) as numTitles\n      from authors\n      where \n         (select count(*)\n          from title_authors\n          where authors.au_id = title_authors.au_id) &gt; 2\n      order by au_lname, au_fname\")\n\n  au_fname  au_lname numTitles\n1    Sarah   Buchman         3\n2    Wendy Heydemark         4\n3     Klee      Hull         4\n4   Harvey   Kellsey         3\n\n\nYou cannot run the subquery from a correlated-subquery on its own. It must be part of a larger query. For example the following TRIES to run the subquery from the previous example but fails with an error\n\nsqldf(\"select count(*)\n          from title_authors\n          where authors.au_id = title_authors.au_id\")  # ERROR - no such column authors.au_id\n\nError: no such column: authors.au_id\n\n\n\n33.15.1 A separate inner query is run for every row that is processed in the outer query\n\n# This is the subquery that is run for the row in the authors for Sarah Buchman\nsqldf(\"select count(*)\n       from title_authors\n       where 'A01' = title_authors.au_id\")\n\n  count(*)\n1        3\n\n# This is the subquery that is run for the row in the authors for Wendy Heydemark\nsqldf(\"select count(*)\n       from title_authors\n       where 'A02' = title_authors.au_id\")\n\n  count(*)\n1        4\n\n# This is the subquery that is run for the row in the authors for Hallie Hull\nsqldf(\"select count(*)\n       from title_authors\n       where 'A03' = title_authors.au_id\")\n\n  count(*)\n1        2\n\n# etc ...\n\n\nThere are often ways to avoid subqueries\nThe following is another way to get the same results without using any subqueries. Instead of a subquery, the following query uses a join and makes use of the group by and having clauses.\n\nsqldf(\"select au_fname, au_lname\n      from authors join title_authors on authors.au_id = title_authors.au_id\n      group by authors.au_id, au_fname, au_lname\n      having count(*) &gt; 2\n      order by au_lname, au_fname\")\n\n  au_fname  au_lname\n1    Sarah   Buchman\n2    Wendy Heydemark\n3     Klee      Hull\n4   Harvey   Kellsey\n\n\nSome people will think that the 1st version (with the subquery) is easier to understand.\nSome people will think that the 2nd version (with the join, group by, having) is easier to understand.\nGenerally it is a matter of style - they are both valid.\n(NOTE - As a general rule - you should avoid correlated subqueries if possible as they tend to run slower than other types of queries since the correlated subquery is run many times. This is a rule of thumb but depending on the exact queries, it is possible that a correlated subquery will run as fast or faster than alternate queries that return the same results.)\nHOWEVER\nIf you want to actually see how many books each author wrote, only the 2nd version (without the subquery) will work. See the following. This is exactly the same as the 2nd query above with the addition of\n            count(*) as num_titles\non the first line of the query.\n\nsqldf(\"select au_fname, au_lname, count(*) as num_titles \n      from authors join title_authors on authors.au_id = title_authors.au_id\n      group by authors.au_id\n      having count(*) &gt; 2\n      order by au_lname, au_fname\")\n\n  au_fname  au_lname num_titles\n1    Sarah   Buchman          3\n2    Wendy Heydemark          4\n3     Klee      Hull          4\n4   Harvey   Kellsey          3\n\n\nYou CANNOT add the number of titles to the version with the subquery as the outer query does not have access to ANY information about the titles since the “from” in the outer query only includes the “authors” table. Therefore the “select” clause in the outer query may only refer to information from the authors table.\nTAKEAWAY: When using a subquery in the where or having clauses The only information that can be displayed by the outer query is info that is available in the tables listed in the “from” of the outer query.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#table-aliases-with-subqueries",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#table-aliases-with-subqueries",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.16 Table aliases with subqueries",
    "text": "33.16 Table aliases with subqueries\n\n##################################################################\n# USING TABLE ALIASES FOR CORRELATED SUBQUERIES THAT USE THE SAME TABLES\n# IN THE INNER AND OUTER QUERIES\n#\n# In correlated subqueries if the inner query refers to the same \n# table(s) as the outer query - you must use a table alias.\n##################################################################\n\n# Show the books whose length in pages is longer than the average\n# length of books OF THE SAME TYPE\n# \n# You need to use a correlated subquery here. \n#\n# You need to use a subquery to compare the length of a particular\n# book (which is a non-aggregated value) to the avg(length) \n# (which is an aggregated value). \n#\n# To be more specific, you must use a \"correlated\" subquery\n# since each title will have a potentially differnt type. Therefore\n# the average length will be potentially different for books\n# of different types and sql will need to run the subquery once\n# for each row in the titles table.\n#\n# To be even more specific ... the inner query and the \n# outer query refer to the SAME table, i.e. the titles table.\n# AND the inner query must refer to information that is taken from \n# the outer query for that table. Therefore you must somehow make \n# it clear in the inner query which information is from the outer-query-title-table\n# and which information is from the version of the title table that is\n# in the inner query's from clause. \n# This is done by using \"alias\" names for the outer query's copy of the titles table\n# and a different \"alias\" for the inner query's copy of the titles table as shown below.\n\nsqldf(\"\n      select title_name, type, pages\n      from titles as outerTitles \n      where pages &gt; \n          (\n            select avg(pages) \n            from titles as innerTitles\n            where innerTitles.type = outerTitles.type\n          )\n      order by type, pages desc\n      \")\n\n                           title_name       type pages\n1           Spontaneous, Not Annoying  biography   507\n2                    How About Never?  biography   473\n3        Just Wait Until After School   children    86\n4 What Are The Civilian Applications?    history   802\n5    Perhaps It's a Glandular Problem psychology   826\n\n# We can prove to ourselves that this worked by showing the average\n# number of pages for each type of title\n\nsqldf(\"select type, avg(pages)\n      from titles\n      group by type\n      order by type\")\n\n        type avg(pages)\n1  biography   437.6667\n2   children    54.0000\n3   computer  1226.0000\n4    history   307.6667\n5 psychology   512.3333\n\n# We can see those books that are below average in length for their\n# type of book by simply changing the &gt; to a &lt; in \n# the query\n\nsqldf(\"\n      select title_name, type, pages\n      from titles as outerTitles \n      where pages &lt;\n          (\n            select avg(pages) \n            from titles as innerTitles\n            where innerTitles.type = outerTitles.type\n          )\n      order by type, pages desc\n      \")\n\n                 title_name       type pages\n1         I Blame My Mother  biography   333\n2           Kiss My Boo Boo   children    22\n3                     1977!    history   107\n4 200 Years of German Humor    history    14\n5 But I Did It Unconciously psychology   510\n6    Exchange of Platitudes psychology   201",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#exists-some-subquery",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#exists-some-subquery",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.17 EXISTS (some subquery)",
    "text": "33.17 EXISTS (some subquery)\n\n#-------------------------------------------------------------------------------\n# Using \"EXISTS\" with correlated subqueries that may or may not return any rows.\n#\n# The EXISTS keyword can be used in WHERE and in HAVING clauses.\n#\n# e.g.    WHERE EXISTS (SOME_SUBQUERY)\n#\n#         HAVING EXISTS (SOME_SUBQUERY)\n#\n# The EXISTS keyword is always followed by a subquery.\n#\n# If the subquery returns any rows then EXISTS (subquery) evaluates to TRUE. \n# If the subquery returns zero rows then EXISTS (subquery) evaluates to FALSE. \n#-------------------------------------------------------------------------------\n\n# \"EXISTS ( some subquery )\"  is used inside a where or having clause. \n# The row from the outer query is included in the results if the subquery\n# contains ANY rows. The actual data resturned by the subqeury is not significant.\n# The only significance is if the subquery returns SOMETHING or returns NOTHING.\n\n# EXAMPLE:\n# Show authors that live in the same city as at least one publisher.\n#\n# [NOTE: above we saw a different way to do this by using \"IN (subquery)\" ]\n\nsqldf(\"select au_fname, au_lname, authors.city\n      from authors\n      where EXISTS\n              (select *\n               from publishers\n               where publishers.city = authors.city)\n      order by au_lname, au_fname\n      \")\n\n   au_fname au_lname          city\n1    Hallie     Hull San Francisco\n2      Klee     Hull San Francisco\n3 Christian    Kells      New York\n\n# You know that the subquery is a CORRELATED subquery because it \n# cannot be run by itself (as shown below). This is because\n# the query refers to the the authors table but does not include\n# the authors table in the from clause.\nsqldf( \"select *\n          from publishers\n        where publishers.city = authors.city\")\n\nError: no such column: authors.city",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#multiple-ways-of-accomplishing-the-same-thing",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#multiple-ways-of-accomplishing-the-same-thing",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.18 MULTIPLE WAYS OF ACCOMPLISHING THE SAME THING",
    "text": "33.18 MULTIPLE WAYS OF ACCOMPLISHING THE SAME THING\n\n#############################################################.\n# MULTIPLE WAYS OF ACCOMPLISHING THE SAME THING\n#\n# There are often multiple different ways you can write queries\n# that result in the same output. Different people will approach\n# the same problem differently. \n#\n# When faced with different approaches, you should try if possible to\n# avoid correlated subqueries. This is because\n# as a \"rule of thumb\" correlated subqueries will run slower\n# than other approaches since the subquery is re-run many times, \n# i.e. once for  each row in the outer query.\n#\n# The following is an example of a query that can be accomplished\n# in many several different ways.\n#############################################################.\n\n#-------------------------------------------------------.\n# QUESTION:\n#\n#   Write a query to show the names of the authors\n#   who did NOT write any psychology books\n#-------------------------------------------------------.\n\n\n#...........................................................\n# One answer - using a correlated subquery with NOT EXISTS\n#...........................................................\n\n# Authors who did not write any psychology books:\n\nsqldf(\"select au_fname, au_lname\n       from authors\n       where NOT EXISTS\n             (select * \n              from title_authors join titles \n                                   on title_authors.title_id = titles.title_id\n              where title_authors.au_id = authors.au_id and\n                    type = 'psychology')\n      \n       order by au_lname, au_fname\n      \")\n\n   au_fname    au_lname\n1     Sarah     Buchman\n2     Wendy   Heydemark\n3 Christian       Kells\n4     Paddy O'Furniture\n\n# FYI - in case you're curious - this is a query for the authors who DID\n# write psychology books\n\n# authors who did write psychology books\nsqldf(\"select distinct au_fname, au_lname\n       from authors join title_authors on authors.au_id = title_authors.au_id\n                    join titles on title_authors.title_id = titles.title_id\n       where type = 'psychology'\")\n\n  au_fname au_lname\n1   Hallie     Hull\n2     Klee     Hull\n3   Harvey  Kellsey\n\n#...........................................................\n# Another answer - using EXCEPT\n#\n# The first query before the except retrieves all the authors names.\n# The 2nd query after the EXCEPT retrieves authors who wrote psychology books.\n# The EXCEPT removes the rows returned by the 2nd query from the results of\n# the first query.\n#...........................................................\n\n# Authors who did not write any psychology books:\n\nsqldf(\"-- get all the authors' names\n       select au_fname, au_lname\n       from authors\n      \n   EXCEPT   -- except removes any row that appears in the 2nd query\n   \n      -- get the authors who wrote psychology books\n      select distinct au_fname, au_lname\n      from authors join title_authors on authors.au_id = title_authors.au_id\n                   join titles on titles.title_id = title_authors.title_id\n      where type = 'psychology'\n   \n   order by au_lname, au_fname\")\n\n   au_fname    au_lname\n1     Sarah     Buchman\n2     Wendy   Heydemark\n3 Christian       Kells\n4     Paddy O'Furniture\n\n#...........................................................\n# \n# Another answer - using a LEFT JOIN combined with a subquery in a FROM clause.\n#\n# You can use a subquery in a FROM clause to create a temporary table\n# that will be used for the query. The temporary table needs to be\n# given an alias. \n#\n# The example below uses a left join and a subquery in the FROM clause\n# to retrieve the authors who did not write psychology books.\n# This is yet one more way to accomplish the same results as the \n# examples above.\n#---------------------------------------------------------------------.\n\n# Yet another way of doing the same thing - by using a LEFT join.\n# This also shows an example of using a subquery in a from clause\n# to create a temporary table.\n\nsqldf(\"\n  select au_fname, au_lname\n  from authors LEFT JOIN \n\n           -- the following subquery creates a temporary table\n           -- that contains the authors/titles of psychology books\n  \n           (select *                 \n            from title_authors join titles \n                                 on title_authors.title_id = titles.title_id\n            where type = 'psychology') AS psychologyTitles\n  \n       on authors.au_id = psychologyTitles.au_id\n  \n  where psychologyTitles.au_id is NULL\n  order by au_lname, au_fname\")\n\n   au_fname    au_lname\n1     Sarah     Buchman\n2     Wendy   Heydemark\n3 Christian       Kells\n4     Paddy O'Furniture\n\n# You can see the contents of the temporary table by running the\n# subquery by itself as shown here:\nsqldf(\"select *\n       from title_authors join titles \n                            on title_authors.title_id = titles.title_id\n       where type = 'psychology'\")\n\n  title_id au_id au_order royalty_shares title_id                       title_name       type pub_id pages price  sales         pubdate\n1      T04   A03        1            0.6      T04        But I Did It Unconciously psychology    P01   510 12.99  13001  5/31/1999 0:00\n2      T04   A04        2            0.4      T04        But I Did It Unconciously psychology    P01   510 12.99  13001  5/31/1999 0:00\n3      T05   A04        1            1.0      T05           Exchange of Platitudes psychology    P01   201  6.95 201440   1/1/2001 0:00\n4      T11   A03        2            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n5      T11   A04        3            0.3      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n6      T11   A06        1            0.4      T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n\n# You can see all the columns from the left join here\nsqldf(\"\n  select *\n  from authors LEFT JOIN \n\n           -- the following subquery creates a temporary table\n           -- that contains the authors/titles of psychology books\n  \n           (select *                 \n            from title_authors join titles \n                                 on title_authors.title_id = titles.title_id\n            where type = 'psychology') AS psychologyTitles\n  \n       on authors.au_id = psychologyTitles.au_id\n  \n  -- where psychologyTitles.au_id is NULL\n  order by au_lname, au_fname\")\n\n   au_id  au_fname    au_lname        phone              address          city state   zip title_id au_id au_order royalty_shares title_id:1                       title_name       type pub_id pages price  sales         pubdate\n1    A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468     &lt;NA&gt;  &lt;NA&gt;       NA             NA       &lt;NA&gt;                             &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA     NA            &lt;NA&gt;\n2    A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303     &lt;NA&gt;  &lt;NA&gt;       NA             NA       &lt;NA&gt;                             &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA     NA            &lt;NA&gt;\n3    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A03        1            0.6        T04        But I Did It Unconciously psychology    P01   510 12.99  13001  5/31/1999 0:00\n4    A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A03        2            0.3        T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n5    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T04   A04        2            0.4        T04        But I Did It Unconciously psychology    P01   510 12.99  13001  5/31/1999 0:00\n6    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T05   A04        1            1.0        T05           Exchange of Platitudes psychology    P01   201  6.95 201440   1/1/2001 0:00\n7    A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123      T11   A04        3            0.3        T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n8    A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014     &lt;NA&gt;  &lt;NA&gt;       NA             NA       &lt;NA&gt;                             &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA     NA            &lt;NA&gt;\n9    A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305      T11   A06        1            0.4        T11 Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 11/30/2000 0:00\n10   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236     &lt;NA&gt;  &lt;NA&gt;       NA             NA       &lt;NA&gt;                             &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;    NA    NA     NA            &lt;NA&gt;",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#case---like-an-if-for-sql",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#case---like-an-if-for-sql",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.19 case - like an if for SQL",
    "text": "33.19 case - like an if for SQL\n\n######################################################################.\n# The case operator can appear in a SELECT clause to create a custom column.\n# It works similar to IF/ELSE/ELSE IF constructs that appear in other \n# languages.\n#\n# The example below should be easy to understand.\n#\n# If there is no ELSE part and no conditions are true, it returns NULL.\n#\n# SELECT ... SOME COLUMNS ... , \n#\n#       case when LOGICAL EXPRESSION then RESULT1\n#            when LOGICAL EXPRESSION then RESULT2\n#            when LOGICAL EXPRESSION then RESULT3\n#              ... etc\n#            else RESULT_IF_NO_LOGICAL_EXPRESSION_WAS_TRUE\n#       end   as   COLUMN_NAME,\n#\n#       ... SOME OTHER COLUMNS ...\n#\n# FROM ... etc\n######################################################################.\n\n# EXAMPLE - this shows the lengths and prices of books in categories \n#           rather than actual numbers.\n\nsqldf(\"select title_name,\n              case\n                 when pages is null then 'unknown'\n                 when pages &lt; 100 then 'short'\n                 when pages &lt; 500 then 'medium'\n                 when pages &lt; 1000 then 'long'\n                 else  'crazy long'\n              end as length,\n              pages,\n              case\n                 when price &lt; 25 then 'cheap'\n                 when price is null then 'unknown'\n                 when price &lt; 30 then 'midrange'\n                 else 'expensive'\n              end as afforability,\n              price\n       from titles\n       order by pages, title_name\")\n\n                            title_name     length pages afforability price\n1          Not Without My Fabrerge Egg    unknown    NA      unknown    NA\n2            200 Years of German Humor      short    14        cheap 19.95\n3                      Kiss My Boo Boo      short    22        cheap 13.95\n4         Just Wait Until After School      short    86        cheap 10.00\n5                                1977!     medium   107        cheap 21.99\n6               Exchange of Platitudes     medium   201        cheap  6.95\n7                    I Blame My Mother     medium   333        cheap 23.95\n8                     How About Never?     medium   473        cheap 19.95\n9            Spontaneous, Not Annoying       long   507        cheap 12.99\n10           But I Did It Unconciously       long   510        cheap 12.99\n11 What Are The Civilian Applications?       long   802     midrange 29.99\n12    Perhaps It's a Glandular Problem       long   826        cheap  7.99\n13        Ask Yor System Administrator crazy long  1226    expensive 39.95\n\n# EXAMPLE:\n#\n# NOTE: Even if you don't understand this description the results below\n# should be pretty easy to understand.\n#\n# DESCRIPTION: \n# Show a list of all the authors.\n#\n# Authors who wrote at least one psychology\n# book should appear on a row with the word psycology in the column \n# named psycology_or_other.\n#\n# Authors who wrote non-psychology books should appear in a row with the\n# word \"other\" in the psychology_or_other column.\n#\n# Authors who wrote both psychology and non-psychology books should\n# appear in two different rows. One for pyschology and one for other.\n\nsqldf(\"select distinct\n              case when type = 'psychology' then 'psychology'\n                   else 'other'\n              end as pyschology_or_other,\n              au_lname, au_fname\n      from authors join title_authors on authors.au_id = title_authors.au_id\n                   join titles on titles.title_id = title_authors.title_id\n      order by pyschology_or_other, au_lname, au_fname\")\n\n  pyschology_or_other  au_lname  au_fname\n1               other   Buchman     Sarah\n2               other Heydemark     Wendy\n3               other      Hull      Klee\n4               other     Kells Christian\n5               other   Kellsey    Harvey\n6          psychology      Hull    Hallie\n7          psychology      Hull      Klee\n8          psychology   Kellsey    Harvey",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#self-joins",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#self-joins",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.20 self joins",
    "text": "33.20 self joins\n\n######################################################\n# Self joins\n######################################################\n\n# A \"self join\" is used when you want to compare two different rows \n# FROM THE SAME TABLE. To do so, the from clause uses two different\n# copies of the same table in a join. \n# The best way to understand this is with an example:\n\n#--------------------------------------------------------------------------\n# find the names of authors who share the same address as another author\n#--------------------------------------------------------------------------\n\n# We want to compare two different rows from the authors table to see if\n# they are from the same address. \n\n\n# Look at the where clause ... why do you think it uses a \"&gt;\" sign?\nsqldf(\"\n      select authors1.au_fname, authors1.au_lname, authors2.au_fname, authors2.au_lname, authors1.address\n      from authors authors1 join authors authors2 on authors1.address = authors2.address\n      where authors1.au_id &gt; authors2.au_id \")\n\n  au_fname au_lname au_fname au_lname              address\n1     Klee     Hull   Hallie     Hull 3800 Waldo Ave, #14F\n\n# let's analyze what's really going on by showing the CROSS JOIN that happens\n# internally as the above select statement is being processed\nsqldf(\"select authors1.au_id, authors1.au_fname, authors1.au_lname, authors1.address,\n              authors2.au_id, authors2.au_fname, authors2.au_lname, authors2.address\n       from authors authors1 CROSS JOIN authors authors2\n       order by authors1.au_id, authors2.au_id\")\n\n   au_id  au_fname    au_lname              address au_id  au_fname    au_lname              address\n1    A01     Sarah     Buchman       75 West 205 St   A01     Sarah     Buchman       75 West 205 St\n2    A01     Sarah     Buchman       75 West 205 St   A02     Wendy   Heydemark     2922 Baseline Rd\n3    A01     Sarah     Buchman       75 West 205 St   A03    Hallie        Hull 3800 Waldo Ave, #14F\n4    A01     Sarah     Buchman       75 West 205 St   A04      Klee        Hull 3800 Waldo Ave, #14F\n5    A01     Sarah     Buchman       75 West 205 St   A05 Christian       Kells       114 Horatio St\n6    A01     Sarah     Buchman       75 West 205 St   A06    Harvey     Kellsey       390 Serra Mall\n7    A01     Sarah     Buchman       75 West 205 St   A07     Paddy O'Furniture         1442 Main St\n8    A02     Wendy   Heydemark     2922 Baseline Rd   A01     Sarah     Buchman       75 West 205 St\n9    A02     Wendy   Heydemark     2922 Baseline Rd   A02     Wendy   Heydemark     2922 Baseline Rd\n10   A02     Wendy   Heydemark     2922 Baseline Rd   A03    Hallie        Hull 3800 Waldo Ave, #14F\n11   A02     Wendy   Heydemark     2922 Baseline Rd   A04      Klee        Hull 3800 Waldo Ave, #14F\n12   A02     Wendy   Heydemark     2922 Baseline Rd   A05 Christian       Kells       114 Horatio St\n13   A02     Wendy   Heydemark     2922 Baseline Rd   A06    Harvey     Kellsey       390 Serra Mall\n14   A02     Wendy   Heydemark     2922 Baseline Rd   A07     Paddy O'Furniture         1442 Main St\n15   A03    Hallie        Hull 3800 Waldo Ave, #14F   A01     Sarah     Buchman       75 West 205 St\n16   A03    Hallie        Hull 3800 Waldo Ave, #14F   A02     Wendy   Heydemark     2922 Baseline Rd\n17   A03    Hallie        Hull 3800 Waldo Ave, #14F   A03    Hallie        Hull 3800 Waldo Ave, #14F\n18   A03    Hallie        Hull 3800 Waldo Ave, #14F   A04      Klee        Hull 3800 Waldo Ave, #14F\n19   A03    Hallie        Hull 3800 Waldo Ave, #14F   A05 Christian       Kells       114 Horatio St\n20   A03    Hallie        Hull 3800 Waldo Ave, #14F   A06    Harvey     Kellsey       390 Serra Mall\n21   A03    Hallie        Hull 3800 Waldo Ave, #14F   A07     Paddy O'Furniture         1442 Main St\n22   A04      Klee        Hull 3800 Waldo Ave, #14F   A01     Sarah     Buchman       75 West 205 St\n23   A04      Klee        Hull 3800 Waldo Ave, #14F   A02     Wendy   Heydemark     2922 Baseline Rd\n24   A04      Klee        Hull 3800 Waldo Ave, #14F   A03    Hallie        Hull 3800 Waldo Ave, #14F\n25   A04      Klee        Hull 3800 Waldo Ave, #14F   A04      Klee        Hull 3800 Waldo Ave, #14F\n26   A04      Klee        Hull 3800 Waldo Ave, #14F   A05 Christian       Kells       114 Horatio St\n27   A04      Klee        Hull 3800 Waldo Ave, #14F   A06    Harvey     Kellsey       390 Serra Mall\n28   A04      Klee        Hull 3800 Waldo Ave, #14F   A07     Paddy O'Furniture         1442 Main St\n29   A05 Christian       Kells       114 Horatio St   A01     Sarah     Buchman       75 West 205 St\n30   A05 Christian       Kells       114 Horatio St   A02     Wendy   Heydemark     2922 Baseline Rd\n31   A05 Christian       Kells       114 Horatio St   A03    Hallie        Hull 3800 Waldo Ave, #14F\n32   A05 Christian       Kells       114 Horatio St   A04      Klee        Hull 3800 Waldo Ave, #14F\n33   A05 Christian       Kells       114 Horatio St   A05 Christian       Kells       114 Horatio St\n34   A05 Christian       Kells       114 Horatio St   A06    Harvey     Kellsey       390 Serra Mall\n35   A05 Christian       Kells       114 Horatio St   A07     Paddy O'Furniture         1442 Main St\n36   A06    Harvey     Kellsey       390 Serra Mall   A01     Sarah     Buchman       75 West 205 St\n37   A06    Harvey     Kellsey       390 Serra Mall   A02     Wendy   Heydemark     2922 Baseline Rd\n38   A06    Harvey     Kellsey       390 Serra Mall   A03    Hallie        Hull 3800 Waldo Ave, #14F\n39   A06    Harvey     Kellsey       390 Serra Mall   A04      Klee        Hull 3800 Waldo Ave, #14F\n40   A06    Harvey     Kellsey       390 Serra Mall   A05 Christian       Kells       114 Horatio St\n41   A06    Harvey     Kellsey       390 Serra Mall   A06    Harvey     Kellsey       390 Serra Mall\n42   A06    Harvey     Kellsey       390 Serra Mall   A07     Paddy O'Furniture         1442 Main St\n43   A07     Paddy O'Furniture         1442 Main St   A01     Sarah     Buchman       75 West 205 St\n44   A07     Paddy O'Furniture         1442 Main St   A02     Wendy   Heydemark     2922 Baseline Rd\n45   A07     Paddy O'Furniture         1442 Main St   A03    Hallie        Hull 3800 Waldo Ave, #14F\n46   A07     Paddy O'Furniture         1442 Main St   A04      Klee        Hull 3800 Waldo Ave, #14F\n47   A07     Paddy O'Furniture         1442 Main St   A05 Christian       Kells       114 Horatio St\n48   A07     Paddy O'Furniture         1442 Main St   A06    Harvey     Kellsey       390 Serra Mall\n49   A07     Paddy O'Furniture         1442 Main St   A07     Paddy O'Furniture         1442 Main St\n\n# If you take out the where clause, you will get extra rows\nsqldf(\"\n      select authors1.au_fname, authors1.au_lname, authors2.au_fname, authors2.au_lname, authors1.address\n      from authors authors1 join authors authors2 on authors1.address = authors2.address\n      \")\n\n   au_fname    au_lname  au_fname    au_lname              address\n1     Sarah     Buchman     Sarah     Buchman       75 West 205 St\n2     Wendy   Heydemark     Wendy   Heydemark     2922 Baseline Rd\n3    Hallie        Hull    Hallie        Hull 3800 Waldo Ave, #14F\n4    Hallie        Hull      Klee        Hull 3800 Waldo Ave, #14F\n5      Klee        Hull    Hallie        Hull 3800 Waldo Ave, #14F\n6      Klee        Hull      Klee        Hull 3800 Waldo Ave, #14F\n7 Christian       Kells Christian       Kells       114 Horatio St\n8    Harvey     Kellsey    Harvey     Kellsey       390 Serra Mall\n9     Paddy O'Furniture     Paddy O'Furniture         1442 Main St\n\n# To understand why you get extra rows without the where take a look at the output\n# of the cross join of the authors table with itself. It is this cross join\n# that is then filtered by the on clause and by the where clause.\nsqldf(\"select authors1.au_fname, authors1.au_lname , authors1.address, authors2.au_fname, authors2.au_lname, authors2.address\n      from authors as authors1   cross join    authors as authors2\n      order by authors1.au_lname, authors1.au_fname, authors2.au_lname, authors2.au_fname\")\n\n    au_fname    au_lname              address  au_fname    au_lname              address\n1      Sarah     Buchman       75 West 205 St     Sarah     Buchman       75 West 205 St\n2      Sarah     Buchman       75 West 205 St     Wendy   Heydemark     2922 Baseline Rd\n3      Sarah     Buchman       75 West 205 St    Hallie        Hull 3800 Waldo Ave, #14F\n4      Sarah     Buchman       75 West 205 St      Klee        Hull 3800 Waldo Ave, #14F\n5      Sarah     Buchman       75 West 205 St Christian       Kells       114 Horatio St\n6      Sarah     Buchman       75 West 205 St    Harvey     Kellsey       390 Serra Mall\n7      Sarah     Buchman       75 West 205 St     Paddy O'Furniture         1442 Main St\n8      Wendy   Heydemark     2922 Baseline Rd     Sarah     Buchman       75 West 205 St\n9      Wendy   Heydemark     2922 Baseline Rd     Wendy   Heydemark     2922 Baseline Rd\n10     Wendy   Heydemark     2922 Baseline Rd    Hallie        Hull 3800 Waldo Ave, #14F\n11     Wendy   Heydemark     2922 Baseline Rd      Klee        Hull 3800 Waldo Ave, #14F\n12     Wendy   Heydemark     2922 Baseline Rd Christian       Kells       114 Horatio St\n13     Wendy   Heydemark     2922 Baseline Rd    Harvey     Kellsey       390 Serra Mall\n14     Wendy   Heydemark     2922 Baseline Rd     Paddy O'Furniture         1442 Main St\n15    Hallie        Hull 3800 Waldo Ave, #14F     Sarah     Buchman       75 West 205 St\n16    Hallie        Hull 3800 Waldo Ave, #14F     Wendy   Heydemark     2922 Baseline Rd\n17    Hallie        Hull 3800 Waldo Ave, #14F    Hallie        Hull 3800 Waldo Ave, #14F\n18    Hallie        Hull 3800 Waldo Ave, #14F      Klee        Hull 3800 Waldo Ave, #14F\n19    Hallie        Hull 3800 Waldo Ave, #14F Christian       Kells       114 Horatio St\n20    Hallie        Hull 3800 Waldo Ave, #14F    Harvey     Kellsey       390 Serra Mall\n21    Hallie        Hull 3800 Waldo Ave, #14F     Paddy O'Furniture         1442 Main St\n22      Klee        Hull 3800 Waldo Ave, #14F     Sarah     Buchman       75 West 205 St\n23      Klee        Hull 3800 Waldo Ave, #14F     Wendy   Heydemark     2922 Baseline Rd\n24      Klee        Hull 3800 Waldo Ave, #14F    Hallie        Hull 3800 Waldo Ave, #14F\n25      Klee        Hull 3800 Waldo Ave, #14F      Klee        Hull 3800 Waldo Ave, #14F\n26      Klee        Hull 3800 Waldo Ave, #14F Christian       Kells       114 Horatio St\n27      Klee        Hull 3800 Waldo Ave, #14F    Harvey     Kellsey       390 Serra Mall\n28      Klee        Hull 3800 Waldo Ave, #14F     Paddy O'Furniture         1442 Main St\n29 Christian       Kells       114 Horatio St     Sarah     Buchman       75 West 205 St\n30 Christian       Kells       114 Horatio St     Wendy   Heydemark     2922 Baseline Rd\n31 Christian       Kells       114 Horatio St    Hallie        Hull 3800 Waldo Ave, #14F\n32 Christian       Kells       114 Horatio St      Klee        Hull 3800 Waldo Ave, #14F\n33 Christian       Kells       114 Horatio St Christian       Kells       114 Horatio St\n34 Christian       Kells       114 Horatio St    Harvey     Kellsey       390 Serra Mall\n35 Christian       Kells       114 Horatio St     Paddy O'Furniture         1442 Main St\n36    Harvey     Kellsey       390 Serra Mall     Sarah     Buchman       75 West 205 St\n37    Harvey     Kellsey       390 Serra Mall     Wendy   Heydemark     2922 Baseline Rd\n38    Harvey     Kellsey       390 Serra Mall    Hallie        Hull 3800 Waldo Ave, #14F\n39    Harvey     Kellsey       390 Serra Mall      Klee        Hull 3800 Waldo Ave, #14F\n40    Harvey     Kellsey       390 Serra Mall Christian       Kells       114 Horatio St\n41    Harvey     Kellsey       390 Serra Mall    Harvey     Kellsey       390 Serra Mall\n42    Harvey     Kellsey       390 Serra Mall     Paddy O'Furniture         1442 Main St\n43     Paddy O'Furniture         1442 Main St     Sarah     Buchman       75 West 205 St\n44     Paddy O'Furniture         1442 Main St     Wendy   Heydemark     2922 Baseline Rd\n45     Paddy O'Furniture         1442 Main St    Hallie        Hull 3800 Waldo Ave, #14F\n46     Paddy O'Furniture         1442 Main St      Klee        Hull 3800 Waldo Ave, #14F\n47     Paddy O'Furniture         1442 Main St Christian       Kells       114 Horatio St\n48     Paddy O'Furniture         1442 Main St    Harvey     Kellsey       390 Serra Mall\n49     Paddy O'Furniture         1442 Main St     Paddy O'Furniture         1442 Main St\n\n# You might think that we'd solve the problem if we used != instead of &gt;\n# However, this is still a problem. Can you figure out why?\nsqldf(\"\n      select authors1.au_fname, authors1.au_lname, authors2.au_fname, authors2.au_lname, authors1.address\n      from authors authors1 join authors authors2 on authors1.address = authors2.address\n      where authors1.au_id != authors2.au_id \")\n\n  au_fname au_lname au_fname au_lname              address\n1   Hallie     Hull     Klee     Hull 3800 Waldo Ave, #14F\n2     Klee     Hull   Hallie     Hull 3800 Waldo Ave, #14F\n\n# if we want to check that multiple columns are equal we can do that too \nsqldf(\"\n      select authors1.au_fname, authors1.au_lname, authors2.au_fname, authors2.au_lname, authors1.address\n      from authors authors1 join authors authors2 \n                  on \n           authors1.address = authors2.address and\n           authors1.city = authors2.city and\n           authors1.state = authors2.state and\n           authors1.zip = authors2.zip\n      where authors1.au_id &gt; authors2.au_id \")\n\n  au_fname au_lname au_fname au_lname              address\n1     Klee     Hull   Hallie     Hull 3800 Waldo Ave, #14F\n\n# If we used a subquery for this type of question\n# we could not see both authors being displayed on the same row",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#self-joins---another-example",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#self-joins---another-example",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.21 self joins - another example",
    "text": "33.21 self joins - another example\n\n#------------------------------------------------------------------------\n# ANOTHER SELF JOIN EXAMPLE\n#------------------------------------------------------------------------\n# Organization management hierarchy \n# - see the file \"orgChart_large.png\"\n#------------------------------------------------------------------------\n\nemployees = data.frame(\n  empid = c( 1,         2,        3,          4,         5,   6,          7,           8,    9,         10,     11),\n  name = c(\"ann\",      \"floyd\", \"randall\",   \"albert\",\"shawn\",\"tyrone\", \"calvin\", \"harold\", \"tanya\", \"jorge\", \"claire\" ),\n  lastname = c(\"sanders\",\"flake\",\"rodgers\",   \"aames\",\"stevens\",\"tex\", \"coolidge\", \"hughs\", \"tamashevsky\",\"jones\", \"coolidge\" ),\n  salary = c(200,       50,     75,            90,     150,     250,     75,        190,      150,    175,      110 ),\n  mgr = c(NA,            1,        1,           1,       2,      3,        3,       2,         7,      2,        7)\n)\n\nemployees\n\n   empid    name    lastname salary mgr\n1      1     ann     sanders    200  NA\n2      2   floyd       flake     50   1\n3      3 randall     rodgers     75   1\n4      4  albert       aames     90   1\n5      5   shawn     stevens    150   2\n6      6  tyrone         tex    250   3\n7      7  calvin    coolidge     75   3\n8      8  harold       hughs    190   2\n9      9   tanya tamashevsky    150   7\n10    10   jorge       jones    175   2\n11    11  claire    coolidge    110   7\n\n# write a query that shows SO_AND_SO is managed by SO_AND_SO for each person\n\nsqldf(\"select emp.name || ' is managed by ' || manager.name\n       from employees AS emp join employees AS manager on emp.mgr = manager.empid\n       order by emp.name\n      \n      \")\n\n   emp.name || ' is managed by ' || manager.name\n1                       albert is managed by ann\n2                   calvin is managed by randall\n3                    claire is managed by calvin\n4                        floyd is managed by ann\n5                     harold is managed by floyd\n6                      jorge is managed by floyd\n7                      randall is managed by ann\n8                      shawn is managed by floyd\n9                     tanya is managed by calvin\n10                  tyrone is managed by randall\n\n# To make it a little easier to read we could use the \n# aliasses emps and mgrs instead of emp1 and emp2\n\nsqldf(\"select emps.name || ' is managed by ' || mgrs.name\n       from employees emps join employees mgrs on emps.mgr = mgrs.empid\n       order by emps.name\n      \n      \")\n\n   emps.name || ' is managed by ' || mgrs.name\n1                     albert is managed by ann\n2                 calvin is managed by randall\n3                  claire is managed by calvin\n4                      floyd is managed by ann\n5                   harold is managed by floyd\n6                    jorge is managed by floyd\n7                    randall is managed by ann\n8                    shawn is managed by floyd\n9                   tanya is managed by calvin\n10                tyrone is managed by randall\n\nsqldf(\"select emps.name || ' is managed by ' || mgrs.name\n       from employees as emps join employees as mgrs on emps.mgr = mgrs.empid\n       order by emps.name\n      \n      \")\n\n   emps.name || ' is managed by ' || mgrs.name\n1                     albert is managed by ann\n2                 calvin is managed by randall\n3                  claire is managed by calvin\n4                      floyd is managed by ann\n5                   harold is managed by floyd\n6                    jorge is managed by floyd\n7                    randall is managed by ann\n8                    shawn is managed by floyd\n9                   tanya is managed by calvin\n10                tyrone is managed by randall\n\nsqldf(\"select publishers.pub_name, titles.title_name\n       from publishers join titles on publishers.pub_id = titles.pub_id\n       where publishers.pub_name like 'A%'\")\n\n           pub_name                       title_name\n1 Abatis Publishers                            1977!\n2 Abatis Publishers        But I Did It Unconciously\n3 Abatis Publishers           Exchange of Platitudes\n4 Abatis Publishers                 How About Never?\n5 Abatis Publishers     Just Wait Until After School\n6 Abatis Publishers                  Kiss My Boo Boo\n7    AAA Publishing      Not Without My Fabrerge Egg\n8    AAA Publishing Perhaps It's a Glandular Problem\n9    AAA Publishing        Spontaneous, Not Annoying\n\nsqldf(\"select publishers.pub_name, titles.title_name\n       from publishers join titles on publishers.pub_id = titles.pub_id\n                                      and publishers.pub_name like 'A%'\")\n\n           pub_name                       title_name\n1 Abatis Publishers                            1977!\n2 Abatis Publishers        But I Did It Unconciously\n3 Abatis Publishers           Exchange of Platitudes\n4 Abatis Publishers                 How About Never?\n5 Abatis Publishers     Just Wait Until After School\n6 Abatis Publishers                  Kiss My Boo Boo\n7    AAA Publishing      Not Without My Fabrerge Egg\n8    AAA Publishing Perhaps It's a Glandular Problem\n9    AAA Publishing        Spontaneous, Not Annoying\n\nsqldf(\"select publishers.pub_name, titles.title_name\n       from publishers, titles \n       where publishers.pub_id = titles.pub_id\n             and publishers.pub_name like 'A%'\")\n\n           pub_name                       title_name\n1 Abatis Publishers                            1977!\n2 Abatis Publishers        But I Did It Unconciously\n3 Abatis Publishers           Exchange of Platitudes\n4 Abatis Publishers                 How About Never?\n5 Abatis Publishers     Just Wait Until After School\n6 Abatis Publishers                  Kiss My Boo Boo\n7    AAA Publishing      Not Without My Fabrerge Egg\n8    AAA Publishing Perhaps It's a Glandular Problem\n9    AAA Publishing        Spontaneous, Not Annoying\n\n# It might help to understand what's going on by looking\n# at the cross join of the two tables.\n\nsqldf(\"select emps.*, mgrs.*\n       from employees as emps CROSS JOIN employees as mgrs\n       order by emps.empid, mgrs.empid\")\n\n    empid    name    lastname salary mgr empid    name    lastname salary mgr\n1       1     ann     sanders    200  NA     1     ann     sanders    200  NA\n2       1     ann     sanders    200  NA     2   floyd       flake     50   1\n3       1     ann     sanders    200  NA     3 randall     rodgers     75   1\n4       1     ann     sanders    200  NA     4  albert       aames     90   1\n5       1     ann     sanders    200  NA     5   shawn     stevens    150   2\n6       1     ann     sanders    200  NA     6  tyrone         tex    250   3\n7       1     ann     sanders    200  NA     7  calvin    coolidge     75   3\n8       1     ann     sanders    200  NA     8  harold       hughs    190   2\n9       1     ann     sanders    200  NA     9   tanya tamashevsky    150   7\n10      1     ann     sanders    200  NA    10   jorge       jones    175   2\n11      1     ann     sanders    200  NA    11  claire    coolidge    110   7\n12      2   floyd       flake     50   1     1     ann     sanders    200  NA\n13      2   floyd       flake     50   1     2   floyd       flake     50   1\n14      2   floyd       flake     50   1     3 randall     rodgers     75   1\n15      2   floyd       flake     50   1     4  albert       aames     90   1\n16      2   floyd       flake     50   1     5   shawn     stevens    150   2\n17      2   floyd       flake     50   1     6  tyrone         tex    250   3\n18      2   floyd       flake     50   1     7  calvin    coolidge     75   3\n19      2   floyd       flake     50   1     8  harold       hughs    190   2\n20      2   floyd       flake     50   1     9   tanya tamashevsky    150   7\n21      2   floyd       flake     50   1    10   jorge       jones    175   2\n22      2   floyd       flake     50   1    11  claire    coolidge    110   7\n23      3 randall     rodgers     75   1     1     ann     sanders    200  NA\n24      3 randall     rodgers     75   1     2   floyd       flake     50   1\n25      3 randall     rodgers     75   1     3 randall     rodgers     75   1\n26      3 randall     rodgers     75   1     4  albert       aames     90   1\n27      3 randall     rodgers     75   1     5   shawn     stevens    150   2\n28      3 randall     rodgers     75   1     6  tyrone         tex    250   3\n29      3 randall     rodgers     75   1     7  calvin    coolidge     75   3\n30      3 randall     rodgers     75   1     8  harold       hughs    190   2\n31      3 randall     rodgers     75   1     9   tanya tamashevsky    150   7\n32      3 randall     rodgers     75   1    10   jorge       jones    175   2\n33      3 randall     rodgers     75   1    11  claire    coolidge    110   7\n34      4  albert       aames     90   1     1     ann     sanders    200  NA\n35      4  albert       aames     90   1     2   floyd       flake     50   1\n36      4  albert       aames     90   1     3 randall     rodgers     75   1\n37      4  albert       aames     90   1     4  albert       aames     90   1\n38      4  albert       aames     90   1     5   shawn     stevens    150   2\n39      4  albert       aames     90   1     6  tyrone         tex    250   3\n40      4  albert       aames     90   1     7  calvin    coolidge     75   3\n41      4  albert       aames     90   1     8  harold       hughs    190   2\n42      4  albert       aames     90   1     9   tanya tamashevsky    150   7\n43      4  albert       aames     90   1    10   jorge       jones    175   2\n44      4  albert       aames     90   1    11  claire    coolidge    110   7\n45      5   shawn     stevens    150   2     1     ann     sanders    200  NA\n46      5   shawn     stevens    150   2     2   floyd       flake     50   1\n47      5   shawn     stevens    150   2     3 randall     rodgers     75   1\n48      5   shawn     stevens    150   2     4  albert       aames     90   1\n49      5   shawn     stevens    150   2     5   shawn     stevens    150   2\n50      5   shawn     stevens    150   2     6  tyrone         tex    250   3\n51      5   shawn     stevens    150   2     7  calvin    coolidge     75   3\n52      5   shawn     stevens    150   2     8  harold       hughs    190   2\n53      5   shawn     stevens    150   2     9   tanya tamashevsky    150   7\n54      5   shawn     stevens    150   2    10   jorge       jones    175   2\n55      5   shawn     stevens    150   2    11  claire    coolidge    110   7\n56      6  tyrone         tex    250   3     1     ann     sanders    200  NA\n57      6  tyrone         tex    250   3     2   floyd       flake     50   1\n58      6  tyrone         tex    250   3     3 randall     rodgers     75   1\n59      6  tyrone         tex    250   3     4  albert       aames     90   1\n60      6  tyrone         tex    250   3     5   shawn     stevens    150   2\n61      6  tyrone         tex    250   3     6  tyrone         tex    250   3\n62      6  tyrone         tex    250   3     7  calvin    coolidge     75   3\n63      6  tyrone         tex    250   3     8  harold       hughs    190   2\n64      6  tyrone         tex    250   3     9   tanya tamashevsky    150   7\n65      6  tyrone         tex    250   3    10   jorge       jones    175   2\n66      6  tyrone         tex    250   3    11  claire    coolidge    110   7\n67      7  calvin    coolidge     75   3     1     ann     sanders    200  NA\n68      7  calvin    coolidge     75   3     2   floyd       flake     50   1\n69      7  calvin    coolidge     75   3     3 randall     rodgers     75   1\n70      7  calvin    coolidge     75   3     4  albert       aames     90   1\n71      7  calvin    coolidge     75   3     5   shawn     stevens    150   2\n72      7  calvin    coolidge     75   3     6  tyrone         tex    250   3\n73      7  calvin    coolidge     75   3     7  calvin    coolidge     75   3\n74      7  calvin    coolidge     75   3     8  harold       hughs    190   2\n75      7  calvin    coolidge     75   3     9   tanya tamashevsky    150   7\n76      7  calvin    coolidge     75   3    10   jorge       jones    175   2\n77      7  calvin    coolidge     75   3    11  claire    coolidge    110   7\n78      8  harold       hughs    190   2     1     ann     sanders    200  NA\n79      8  harold       hughs    190   2     2   floyd       flake     50   1\n80      8  harold       hughs    190   2     3 randall     rodgers     75   1\n81      8  harold       hughs    190   2     4  albert       aames     90   1\n82      8  harold       hughs    190   2     5   shawn     stevens    150   2\n83      8  harold       hughs    190   2     6  tyrone         tex    250   3\n84      8  harold       hughs    190   2     7  calvin    coolidge     75   3\n85      8  harold       hughs    190   2     8  harold       hughs    190   2\n86      8  harold       hughs    190   2     9   tanya tamashevsky    150   7\n87      8  harold       hughs    190   2    10   jorge       jones    175   2\n88      8  harold       hughs    190   2    11  claire    coolidge    110   7\n89      9   tanya tamashevsky    150   7     1     ann     sanders    200  NA\n90      9   tanya tamashevsky    150   7     2   floyd       flake     50   1\n91      9   tanya tamashevsky    150   7     3 randall     rodgers     75   1\n92      9   tanya tamashevsky    150   7     4  albert       aames     90   1\n93      9   tanya tamashevsky    150   7     5   shawn     stevens    150   2\n94      9   tanya tamashevsky    150   7     6  tyrone         tex    250   3\n95      9   tanya tamashevsky    150   7     7  calvin    coolidge     75   3\n96      9   tanya tamashevsky    150   7     8  harold       hughs    190   2\n97      9   tanya tamashevsky    150   7     9   tanya tamashevsky    150   7\n98      9   tanya tamashevsky    150   7    10   jorge       jones    175   2\n99      9   tanya tamashevsky    150   7    11  claire    coolidge    110   7\n100    10   jorge       jones    175   2     1     ann     sanders    200  NA\n101    10   jorge       jones    175   2     2   floyd       flake     50   1\n102    10   jorge       jones    175   2     3 randall     rodgers     75   1\n103    10   jorge       jones    175   2     4  albert       aames     90   1\n104    10   jorge       jones    175   2     5   shawn     stevens    150   2\n105    10   jorge       jones    175   2     6  tyrone         tex    250   3\n106    10   jorge       jones    175   2     7  calvin    coolidge     75   3\n107    10   jorge       jones    175   2     8  harold       hughs    190   2\n108    10   jorge       jones    175   2     9   tanya tamashevsky    150   7\n109    10   jorge       jones    175   2    10   jorge       jones    175   2\n110    10   jorge       jones    175   2    11  claire    coolidge    110   7\n111    11  claire    coolidge    110   7     1     ann     sanders    200  NA\n112    11  claire    coolidge    110   7     2   floyd       flake     50   1\n113    11  claire    coolidge    110   7     3 randall     rodgers     75   1\n114    11  claire    coolidge    110   7     4  albert       aames     90   1\n115    11  claire    coolidge    110   7     5   shawn     stevens    150   2\n116    11  claire    coolidge    110   7     6  tyrone         tex    250   3\n117    11  claire    coolidge    110   7     7  calvin    coolidge     75   3\n118    11  claire    coolidge    110   7     8  harold       hughs    190   2\n119    11  claire    coolidge    110   7     9   tanya tamashevsky    150   7\n120    11  claire    coolidge    110   7    10   jorge       jones    175   2\n121    11  claire    coolidge    110   7    11  claire    coolidge    110   7",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#practice-question-rs-are-from-booksdatabase-word-document.",
    "href": "sql0040-crossJoin-leftJoin-subquery-selfJoins-v023.html#practice-question-rs-are-from-booksdatabase-word-document.",
    "title": "33  33.  more joins (cross/left/right/full), subqueries, set operations, case, self joins",
    "section": "33.22 Practice (question #rs are from booksDatabase Word document.)",
    "text": "33.22 Practice (question #rs are from booksDatabase Word document.)\n\n#######################################################################.\n# PRACTICE with subqueries, outer joins, unions and other stuff.\n# Note that many of the questions shown below come from the\n# booksDatabase word document. The numbers for the questions\n# are the numbers from that document.\n#######################################################################.\n\n\n33.22.1 Question 35\n\n# Show the title names  and number of pages for those books that are\n# longer than (ie.  more pages than) the average length of all books.\n\n\n#~~~~~~~~~~~~~~\n# ANSWER\n#~~~~~~~~~~~~~~\n\nsqldf(\"\nselect title_name , pages\nfrom titles\nwhere pages &gt;= (select avg(pages) from titles)\n\")\n\n                           title_name pages\n1        Ask Yor System Administrator  1226\n2           But I Did It Unconciously   510\n3                    How About Never?   473\n4    Perhaps It's a Glandular Problem   826\n5           Spontaneous, Not Annoying   507\n6 What Are The Civilian Applications?   802\n\n\n\n\n33.22.2 Question 36\n\n#   Show each publisher's name and the number of authors who have written books\n# for that publisher.\n\n\n#~~~~~~~~~~~~~~\n# ANSWER\n#~~~~~~~~~~~~~~\n\nsqldf(\"\nselect pub_name, count(DISTINCT authors.au_id) as '# of authors'\nfrom authors join title_authors on authors.au_id = title_authors.au_id \njoin titles on title_authors.title_id = titles.title_id \njoin publishers on publishers.pub_id = titles.pub_id\ngroup by publishers.pub_id, pub_name;\n\")\n\n              pub_name # of authors\n1    Abatis Publishers            5\n2      Core Dump Books            1\n3 Schandenfreude Press            3\n4       AAA Publishing            4\n\n# EXPLANATION\n\n# The following query shows the first step that would be necessariy to thinking about\n# how to find the final answer (shown above). The following shows a separate\n# row for each publisher and an author that wrote for that publisher. If you\n# order the results by publisher, it is easy to see that you can think of\n# each different publisher as a \"group\". That is what the group by\n# does in the answer (see above). The count(DISTINCT authors.au_id)\n# gets the number of \"distinct\" authors for the particular publisher.\n# The \"distinct\" is necessary to ensure that you don't count an author\n# twice for the same publisher (e.g. Sarah Buchman wrote two books\n# for Schandenfreude Press but should only be counted once as an\n# author for Scandenfreude Press)\n\nsqldf(\"\nselect pub_name, authors.au_fname, authors.au_lname\nfrom authors join title_authors on authors.au_id = title_authors.au_id \njoin titles on title_authors.title_id = titles.title_id \njoin publishers on publishers.pub_id = titles.pub_id\norder by pub_name, au_fname, au_lname\n\")\n\n               pub_name  au_fname  au_lname\n1        AAA Publishing    Hallie      Hull\n2        AAA Publishing    Harvey   Kellsey\n3        AAA Publishing      Klee      Hull\n4        AAA Publishing     Wendy Heydemark\n5        AAA Publishing     Wendy Heydemark\n6     Abatis Publishers    Hallie      Hull\n7     Abatis Publishers    Harvey   Kellsey\n8     Abatis Publishers    Harvey   Kellsey\n9     Abatis Publishers      Klee      Hull\n10    Abatis Publishers      Klee      Hull\n11    Abatis Publishers     Sarah   Buchman\n12    Abatis Publishers     Wendy Heydemark\n13      Core Dump Books Christian     Kells\n14 Schandenfreude Press      Klee      Hull\n15 Schandenfreude Press     Sarah   Buchman\n16 Schandenfreude Press     Sarah   Buchman\n17 Schandenfreude Press     Wendy Heydemark\n\n\n\n\n33.22.3 Question 36.5\n\n# Show the pub_name and the number of titles published by that publisher. \n\n\n#~~~~~~~~~~~~~~\n# ANSWER\n#~~~~~~~~~~~~~~\n\nsqldf(\"select pub_name, count(*) as num_titles\n       from publishers join titles on publishers.pub_id = titles.pub_id\n       group by publishers.pub_id, pub_name\n       order by pub_name\")\n\n              pub_name num_titles\n1       AAA Publishing          3\n2    Abatis Publishers          6\n3      Core Dump Books          1\n4 Schandenfreude Press          3\n\n\n\n\n33.22.4 Question 37\n\n# Show the pub_name and the number of titles published by that publisher. \n# Only include publishers for whom at least 4 authors have worked. \n#\n# (this question combines the concepts of the previous two questions)\n\n\n#~~~~~~~~~~~~~~~~.\n# ANSWER\n#~~~~~~~~~~~~~~~~.\n\nsqldf(\"\nselect pub_name, count(*) as 'Number of Titles'\nfrom publishers as pub1 join titles on pub1.pub_id = titles.pub_id\ngroup by pub1.pub_id, pub_name\nhaving   4 &lt;= ( select count(DISTINCT authors.au_id)\n                from authors join title_authors on authors.au_id = title_authors.au_id \n                join titles on title_authors.title_id = titles.title_id \n                join publishers as pub2 on pub2.pub_id = titles.pub_id\n                where pub2.pub_id = pub1.pub_id ) \norder by pub_name\n\")\n\n           pub_name Number of Titles\n1    AAA Publishing                3\n2 Abatis Publishers                6\n\n# Note - the subquery is a correlated subquery. \n# It returns the number of authors who published with a particular publisher.\n# The following are the acutal subqueries that are run for each publisher:\n# This happens automatically.\n\n# For Abatis - 'P01'\n\nsqldf(\" select count(DISTINCT authors.au_id)\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                     join titles on title_authors.title_id = titles.title_id \n                     join publishers as pub2 on pub2.pub_id = titles.pub_id\n        where pub2.pub_id = 'P01'\")\n\n  count(DISTINCT authors.au_id)\n1                             5\n\n# For Core Dump Books - 'P02'\n\nsqldf(\" select count(DISTINCT authors.au_id)\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                     join titles on title_authors.title_id = titles.title_id \n                     join publishers as pub2 on pub2.pub_id = titles.pub_id\n        where pub2.pub_id = 'P02'\")\n\n  count(DISTINCT authors.au_id)\n1                             1\n\n\n\n\n33.22.5 Question 38\n\n# Show the names of publishers who did not publish any books.\n# (I guess they are just getting started in the business ... )\n\n\n#~~~~~~~~~~~~~~~~.\n# ANSWER\n#~~~~~~~~~~~~~~~~.\n\n# ANSWER - with a subquery\n\nsqldf(\"\nselect pub_name \nfrom publishers  as   pub1\nwhere not exists\n( select * \n    from publishers  as pub2  join titles on pub2.pub_id = titles.pub_id\n  where pub2.pub_id = pub1.pub_id );\n\")\n\n           pub_name\n1 Tneterhooks Press\n\n# ANSWER - with a left join\n\nsqldf(\"\nselect pub_name from publishers left join titles on publishers.pub_id = titles.pub_id\nwhere title_id is NULL;\n\")\n\n           pub_name\n1 Tneterhooks Press\n\n\n\n\n33.22.6 Question 39(a)\n\n# List the name of each publisher and the total number of books that\n# each publisher has published. Sort the results so that the publishers who\n# published the most books appear at the top. If two or more publishers\n# published the same number of books then they should be listed in alphabetical order. \n#\n# a.    The answer should only include publisher who have published some books.\n\n\n#~~~~~~~~~~~~~~~~.\n# ANSWER\n#~~~~~~~~~~~~~~~~.\n\nsqldf(\"\nselect pub_name, count(*)\nfrom titles join publishers on publishers.pub_id = titles.pub_id\ngroup by pub_name\norder by count(*) desc, pub_name;\n\")\n\n              pub_name count(*)\n1    Abatis Publishers        6\n2       AAA Publishing        3\n3 Schandenfreude Press        3\n4      Core Dump Books        1\n\n\n\n\n33.22.7 Question 39(b)\n\n# (2nd part of previous question)\n#\n# b.    This time make sure to also include publishers who have published zero books\n\n\n#~~~~~~~~~~~~~~~~.\n# ANSWER\n#~~~~~~~~~~~~~~~~.\n\nsqldf(\"\nselect pub_name, count(titles.title_id)\nfrom publishers left join titles on publishers.pub_id = titles.pub_id\ngroup by pub_name\norder by count(*) desc, pub_name;\n\")\n\n              pub_name count(titles.title_id)\n1    Abatis Publishers                      6\n2       AAA Publishing                      3\n3 Schandenfreude Press                      3\n4      Core Dump Books                      1\n5    Tneterhooks Press                      0\n\n# ANSWER - with a UNION - however the previous answer is shorter and more to the point\n\nsqldf(\"\nselect pub_name , 0  as  NumTitles\nfrom publishers left join titles on publishers.pub_id = titles.pub_id\nwhere title_id is NULL \n\nUNION\n\nselect pub_name, count(*) as NumTitles\nfrom titles join publishers on publishers.pub_id = titles.pub_id\ngroup by pub_name;\n\")\n\n              pub_name NumTitles\n1       AAA Publishing         3\n2    Abatis Publishers         6\n3      Core Dump Books         1\n4 Schandenfreude Press         3\n5    Tneterhooks Press         0\n\n\n\n\n33.22.8 Question 46\n\n#################.\n# QUESTION\n#################.\n\n# Show the names of the most prolific authors, i.e. the authors \n# who have written the most books. Note that there could be\n# several different authors who are \"tied\" for the most number of books \n# written. Sort the names in alphabetical order (last name, first name)\n# Also show the au_id and the number of titles that the author wrote.\n\n\n#~~~~~~~~~~~~~~~~.\n# ANSWER\n#~~~~~~~~~~~~~~~~.\n\n# This is a rather complex answer.\n# This approach uses a subquery within a subquery.\n# There may be other approaches too.\n# There may be simpler answers too.\n# \n# To explain we will show the subqueries and what they produce.\n# Then we will show the full query that includes the subquery.\n#\n# The following query is NOT the answer. It is simply a query that \n# shows the number of books that were written by each author. This\n# will be used as a subquery in the answer shown below.\nsqldf(\"\n          select authors.au_id, count(*) as numTitles\n          from authors join title_authors on authors.au_id = title_authors.au_id\n          group by authors.au_id, au_fname, au_lname\n\")\n\n  au_id numTitles\n1   A01         3\n2   A02         4\n3   A03         2\n4   A04         4\n5   A05         1\n6   A06         3\n\n# We'd like to get the max number of titles that an author wrote. \n# This amounts to getting the highest number from the numTitles column\n# in the previous query. We can do that by using the above query \n# as a subquery in the from clause.\n\nsqldf(\"\n          select max(numTitles2) \n          from \n               (select authors.au_id, count(*) as numTitles2\n                from authors join title_authors on authors.au_id = title_authors.au_id\n                group by authors.au_id, au_fname, au_lname)\n\n      \")\n\n  max(numTitles2)\n1               4\n\n# Finally we can put this all together to answer our question. \n# The following is the FULL ANSWER to the original question.\n# It uses the above query as a subquery in the having clause (see \n# the code below). Note that in the following code there is a\n# subquery inside of a subquery.\n#\n# FINAL ANSWER:\n\nsqldf(\"\nselect authors.au_id, au_fname, au_lname, count(*) as numTitles\nfrom authors join title_authors on authors.au_id = title_authors.au_id\ngroup by authors.au_id, au_fname, au_lname\nhaving numTitles = \n          (select max(numTitles2) \n           from \n               (select authors.au_id, count(*) as numTitles2\n                from authors join title_authors on authors.au_id = title_authors.au_id\n                group by authors.au_id, au_fname, au_lname))\norder by au_lname, au_fname\n\")\n\n  au_id au_fname  au_lname numTitles\n1   A02    Wendy Heydemark         4\n2   A04     Klee      Hull         4",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'><yrChapterNumber>33. </yrChapterNumber> more joins (cross/left/right/full), subqueries, set operations, case, self joins</span>"
    ]
  },
  {
    "objectID": "sql0050-commonTableExpressions-v001.html",
    "href": "sql0050-commonTableExpressions-v001.html",
    "title": "34  34.  Common Table Expressions (CTE)",
    "section": "",
    "text": "34.1 SQL Common Table Expressions (CTE)\nYou can also see the following to learn about this important feature of modern SQL:\nhttps://learnsql.com/blog/what-is-common-table-expression/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'><yrChapterNumber>34. </yrChapterNumber> Common Table Expressions (CTE)</span>"
    ]
  },
  {
    "objectID": "sql0050-commonTableExpressions-v001.html#cte-with-one-or-more-selects",
    "href": "sql0050-commonTableExpressions-v001.html#cte-with-one-or-more-selects",
    "title": "34  34.  Common Table Expressions (CTE)",
    "section": "34.2 CTE with one or more SELECTs",
    "text": "34.2 CTE with one or more SELECTs\n“Common Table Expressions (CTE)” are designed to make complex queries easier to read and think about. Instead of embedding subqueries in the middle of an outer query CTEs allow you to several SELECT statements in an organized way. These SELECT statements are named and can be referred to as if they were tables inside of the main query. See below for exmaples.\n(Note - these examples work but were generated by chatgpt. I should modify these to make them more interesting, but the current code works to get across the main ideas of CTEs)\nThe following is the database that we will be using to demo the CTEs below.\n\n\n\n\n\n\nClick here to hide/show this section\n\n\n\n\n\n34.3 The “Books Database”\nThe “books database” is a collection of several tables that contains data relating to books (i.e. “titles”), authors, publishers, etc.\nYou can download the data for the database in the following ways:\n\nas a set of several CSV files or (see below)\nas a single sqlite3 database file (see below)\n\nSee below for details.\n\n34.3.1 SQLite file for “Books Database”\nClick here to download all the data for the books datbase as a single “SQLite” database file.\nTo use this file you can do one of the following:\n\nInstall SQLite software on your computer. There are many such programs avaiable. I recommend “DB Viewer for SQLite” which is a free open source program.\nWebsite (download from here): https://sqlitebrowser.org/\nGithub repository: https://github.com/sqlitebrowser/sqlitebrowser\nYou can search online for other similar programs for working with sqlite3 files.\nFree web based tool: https://y-rosenthal.github.io/yrSqliteViewer/yrSqliteViewer.html\nThis is a website that lets you upload a sqlite3 file and run SQL Select statements against the data. I created this website myself (with the help of some AI coding assistants).\nPROs: you don’t need to install anything\nCONs: currently only works with SQL SELECT statment but not any other types of sql statements.\nSee this chapter (in this book) https://y-rosenthal.github.io/DataManagementUsingR/sql0950-workingWithDatabasesInR-v010.html for instructions on how to access this database file directly via R commands.\n\n\n\n34.3.2 CSV files for “Books Database”\nDownload the CSV files for the “books database”. These CSV files together comprise the data for the “books database”.\n\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties.csv\n\nDownload a tutorial on using Relational databases.\n\nbooksDatabase description and Questions\n\nThis word document contains:\n\na complete description of the “books database”\na tutorial on how “relational databases” are structured\na tutorial on Entity Relationship Diagrams (ERDs)\na set of questions and answers\n\nOnce you’ve downloaded the various CSV files you can run the following commands to import the data into R.\n\n\n34.3.3 Import the data (books database)\n\nif(!require(readr)){install.packages(\"readr\");require(readr);}\n\nLoading required package: readr\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"royalties.csv\", na=\"NULL\", show_col_types=FALSE)\n\n\n\n34.3.4 Books Database - Entity Relationship Diagram (ERD)\nThe following is an “Entity Relationship Diagram (ERD)” that describes the relationships between the tables. See the Word document linked above for more info:\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#0000ff', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'mainBkg': '#ffffff', 'background': '#ffffff', 'edgeLabelBackground': '#ffffff' }}}%%\nerDiagram\n    authors ||--o{ title_authors : \"\"\n    title_authors }|--|| titles : \"\"\n    titles ||--o| royalties : \"\"\n    publishers |o--o{ titles : \"\"\n    \n    authors {\n        string au_id PK\n        string au_fname\n        string au_lname\n        string phone\n        string address\n        string city\n        string state\n        string zip\n    }\n    \n    title_authors {\n        string title_id FK1\n        string au_id FK2\n        int au_order\n        float royalty_share\n    }\n    \n    titles {\n        string title_id PK\n        string title_name\n        string type\n        string pub_id FK\n        int pages\n        float price\n        int sales\n        date pubdate\n    }\n    \n    publishers {\n        string pub_id PK\n        string pub_name\n        string city\n        string state\n        string country\n    }\n    \n    royalties {\n        string title_id PK\n        float advance\n        float royalty_rate\n    }\n\n\n\n\n\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here - titles table\n\n\n\n\n\n\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price\n1       T01                               1977!    history    P01   107 21.99\n2       T02           200 Years of German Humor    history    P03    14 19.95\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95\n4       T04           But I Did It Unconciously psychology    P01   510 12.99\n5       T05              Exchange of Platitudes psychology    P01   201  6.95\n6       T06                    How About Never?  biography    P01   473 19.95\n7       T07                   I Blame My Mother  biography    P03   333 23.95\n8       T08        Just Wait Until After School   children    P01    86 10.00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99\n     sales         pubdate\n1      566   8/1/2000 0:00\n2     9566   4/1/1998 0:00\n3    25667   9/1/2000 0:00\n4    13001  5/31/1999 0:00\n5   201440   1/1/2001 0:00\n6    11320  7/31/2000 0:00\n7  1500200  10/1/1999 0:00\n8     4095   6/1/2001 0:00\n9     5000  5/31/2002 0:00\n10      NA            &lt;NA&gt;\n11   94123 11/30/2000 0:00\n12  100001  8/31/2000 0:00\n13   10467  5/31/1999 0:00\n\n\n\n\n\n\n\n\n\n\n\nClick here - publishers table\n\n\n\n\n\n\nsqldf(\"select * from publishers\")\n\n  pub_id             pub_name          city state country\n1    P01    Abatis Publishers      New York    NY     USA\n2    P02      Core Dump Books San Francisco    CA     USA\n3    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    P04    Tneterhooks Press      Berkeley    CA     USA\n5    P05       AAA Publishing      Berkeley    CA     USA\n\n\n\n\n\n\n\n\n\n\n\nClick here - title_authors table\n\n\n\n\n\n\nsqldf(\"select * from title_authors\")\n\n   title_id au_id au_order royalty_shares\n1       T01   A01        1            1.0\n2       T02   A01        1            1.0\n3       T03   A05        1            1.0\n4       T04   A03        1            0.6\n5       T04   A04        2            0.4\n6       T05   A04        1            1.0\n7       T06   A02        1            1.0\n8       T07   A02        1            0.5\n9       T07   A04        2            0.5\n10      T08   A06        1            1.0\n11      T09   A06        1            1.0\n12      T10   A02        1            1.0\n13      T11   A03        2            0.3\n14      T11   A04        3            0.3\n15      T11   A06        1            0.4\n16      T12   A02        1            1.0\n17      T13   A01        1            1.0\n\n\n\n\n\n\n\n\n\n\n\nClick here - authors table\n\n\n\n\n\n\nsqldf(\"select * from authors\")\n\n  au_id  au_fname    au_lname        phone              address          city\n1   A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx\n2   A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder\n3   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco\n4   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco\n5   A05 Christian       Kells 212-771-4680       114 Horatio St      New York\n6   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto\n7   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota\n  state   zip\n1    NY 10468\n2    CO 80303\n3    CA 94123\n4    CA 94123\n5    NY 10014\n6    CA 94305\n7    FL 34236\n\n\n\n\n\n\n\n\n\n\n\nClick here - royalties table\n\n\n\n\n\n\nsqldf(\"select * from royalties\")\n\n   title_id advance royalty_rate\n1       T01   10000         0.05\n2       T02    1000         0.06\n3       T03   15000         0.07\n4       T04   20000         0.08\n5       T05  100000         0.09\n6       T06   20000         0.08\n7       T07 1000000         0.11\n8       T08       0         0.04\n9       T09       0         0.05\n10      T10      NA           NA\n11      T11  100000         0.07\n12      T12   50000         0.09\n13      T13   20000         0.06\n\n\n\n\n\n\n\n\n\nGoal: Show each author’s full name, how many books they’ve written, and a list with one row per book and a total count included.\n\nsqldf(\"\nWITH author_titles_cte AS (\n  SELECT \n    a.au_id,\n    a.au_fname || ' ' || a.au_lname AS author,\n    t.title_name\n  FROM authors a\n  JOIN title_authors ta ON a.au_id = ta.au_id\n  JOIN titles t ON ta.title_id = t.title_id\n),\nauthor_book_counts AS (\n  SELECT au_id, COUNT(*) AS book_count\n  FROM author_titles_cte\n  GROUP BY au_id\n)\nSELECT \n  at.author,\n  at.title_name,\n  abc.book_count\nFROM author_titles_cte at\nJOIN author_book_counts abc ON at.au_id = abc.au_id\nORDER BY abc.book_count DESC, at.author, at.title_name\n\")\n\n            author                          title_name book_count\n1        Klee Hull           But I Did It Unconciously          4\n2        Klee Hull              Exchange of Platitudes          4\n3        Klee Hull                   I Blame My Mother          4\n4        Klee Hull    Perhaps It's a Glandular Problem          4\n5  Wendy Heydemark                    How About Never?          4\n6  Wendy Heydemark                   I Blame My Mother          4\n7  Wendy Heydemark         Not Without My Fabrerge Egg          4\n8  Wendy Heydemark           Spontaneous, Not Annoying          4\n9   Harvey Kellsey        Just Wait Until After School          3\n10  Harvey Kellsey                     Kiss My Boo Boo          3\n11  Harvey Kellsey    Perhaps It's a Glandular Problem          3\n12   Sarah Buchman                               1977!          3\n13   Sarah Buchman           200 Years of German Humor          3\n14   Sarah Buchman What Are The Civilian Applications?          3\n15     Hallie Hull           But I Did It Unconciously          2\n16     Hallie Hull    Perhaps It's a Glandular Problem          2\n17 Christian Kells        Ask Yor System Administrator          1\n\n\n\nGoal: Find authors who have written books with more than 10,000 sales and show the average sales per author.\n\nsqldf(\"\nWITH high_selling_titles AS (\n  SELECT title_id, sales\n  FROM titles\n  WHERE sales &gt; 10000\n),\nauthors_of_hits AS (\n  SELECT a.au_id, a.au_fname || ' ' || a.au_lname AS author, t.sales\n  FROM high_selling_titles h\n  JOIN title_authors ta ON h.title_id = ta.title_id\n  JOIN authors a ON ta.au_id = a.au_id\n  JOIN titles t ON h.title_id = t.title_id\n)\nSELECT author, AVG(sales) AS avg_sales\nFROM authors_of_hits\nGROUP BY author\nORDER BY avg_sales DESC\n\")\n\n           author avg_sales\n1 Wendy Heydemark  537173.7\n2       Klee Hull  452191.0\n3  Harvey Kellsey   94123.0\n4     Hallie Hull   53562.0\n5 Christian Kells   25667.0\n6   Sarah Buchman   10467.0",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'><yrChapterNumber>34. </yrChapterNumber> Common Table Expressions (CTE)</span>"
    ]
  },
  {
    "objectID": "sql0050-commonTableExpressions-v001.html#yrBooksDatabase",
    "href": "sql0050-commonTableExpressions-v001.html#yrBooksDatabase",
    "title": "34  34.  Common Table Expressions (CTE)",
    "section": "34.3 The “Books Database”",
    "text": "34.3 The “Books Database”\nThe “books database” is a collection of several tables that contains data relating to books (i.e. “titles”), authors, publishers, etc.\nYou can download the data for the database in the following ways:\n\nas a set of several CSV files or (see below)\nas a single sqlite3 database file (see below)\n\nSee below for details.\n\n34.3.1 SQLite file for “Books Database”\nClick here to download all the data for the books datbase as a single “SQLite” database file.\nTo use this file you can do one of the following:\n\nInstall SQLite software on your computer. There are many such programs avaiable. I recommend “DB Viewer for SQLite” which is a free open source program.\nWebsite (download from here): https://sqlitebrowser.org/\nGithub repository: https://github.com/sqlitebrowser/sqlitebrowser\nYou can search online for other similar programs for working with sqlite3 files.\nFree web based tool: https://y-rosenthal.github.io/yrSqliteViewer/yrSqliteViewer.html\nThis is a website that lets you upload a sqlite3 file and run SQL Select statements against the data. I created this website myself (with the help of some AI coding assistants).\nPROs: you don’t need to install anything\nCONs: currently only works with SQL SELECT statment but not any other types of sql statements.\nSee this chapter (in this book) https://y-rosenthal.github.io/DataManagementUsingR/sql0950-workingWithDatabasesInR-v010.html for instructions on how to access this database file directly via R commands.\n\n\n\n34.3.2 CSV files for “Books Database”\nDownload the CSV files for the “books database”. These CSV files together comprise the data for the “books database”.\n\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties.csv\n\nDownload a tutorial on using Relational databases.\n\nbooksDatabase description and Questions\n\nThis word document contains:\n\na complete description of the “books database”\na tutorial on how “relational databases” are structured\na tutorial on Entity Relationship Diagrams (ERDs)\na set of questions and answers\n\nOnce you’ve downloaded the various CSV files you can run the following commands to import the data into R.\n\n\n34.3.3 Import the data (books database)\n\nif(!require(readr)){install.packages(\"readr\");require(readr);}\n\nLoading required package: readr\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"royalties.csv\", na=\"NULL\", show_col_types=FALSE)\n\n\n\n34.3.4 Books Database - Entity Relationship Diagram (ERD)\nThe following is an “Entity Relationship Diagram (ERD)” that describes the relationships between the tables. See the Word document linked above for more info:\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#0000ff', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'mainBkg': '#ffffff', 'background': '#ffffff', 'edgeLabelBackground': '#ffffff' }}}%%\nerDiagram\n    authors ||--o{ title_authors : \"\"\n    title_authors }|--|| titles : \"\"\n    titles ||--o| royalties : \"\"\n    publishers |o--o{ titles : \"\"\n    \n    authors {\n        string au_id PK\n        string au_fname\n        string au_lname\n        string phone\n        string address\n        string city\n        string state\n        string zip\n    }\n    \n    title_authors {\n        string title_id FK1\n        string au_id FK2\n        int au_order\n        float royalty_share\n    }\n    \n    titles {\n        string title_id PK\n        string title_name\n        string type\n        string pub_id FK\n        int pages\n        float price\n        int sales\n        date pubdate\n    }\n    \n    publishers {\n        string pub_id PK\n        string pub_name\n        string city\n        string state\n        string country\n    }\n    \n    royalties {\n        string title_id PK\n        float advance\n        float royalty_rate\n    }\n\n\n\n\n\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here - titles table\n\n\n\n\n\n\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price\n1       T01                               1977!    history    P01   107 21.99\n2       T02           200 Years of German Humor    history    P03    14 19.95\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95\n4       T04           But I Did It Unconciously psychology    P01   510 12.99\n5       T05              Exchange of Platitudes psychology    P01   201  6.95\n6       T06                    How About Never?  biography    P01   473 19.95\n7       T07                   I Blame My Mother  biography    P03   333 23.95\n8       T08        Just Wait Until After School   children    P01    86 10.00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99\n     sales         pubdate\n1      566   8/1/2000 0:00\n2     9566   4/1/1998 0:00\n3    25667   9/1/2000 0:00\n4    13001  5/31/1999 0:00\n5   201440   1/1/2001 0:00\n6    11320  7/31/2000 0:00\n7  1500200  10/1/1999 0:00\n8     4095   6/1/2001 0:00\n9     5000  5/31/2002 0:00\n10      NA            &lt;NA&gt;\n11   94123 11/30/2000 0:00\n12  100001  8/31/2000 0:00\n13   10467  5/31/1999 0:00\n\n\n\n\n\n\n\n\n\n\n\nClick here - publishers table\n\n\n\n\n\n\nsqldf(\"select * from publishers\")\n\n  pub_id             pub_name          city state country\n1    P01    Abatis Publishers      New York    NY     USA\n2    P02      Core Dump Books San Francisco    CA     USA\n3    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    P04    Tneterhooks Press      Berkeley    CA     USA\n5    P05       AAA Publishing      Berkeley    CA     USA\n\n\n\n\n\n\n\n\n\n\n\nClick here - title_authors table\n\n\n\n\n\n\nsqldf(\"select * from title_authors\")\n\n   title_id au_id au_order royalty_shares\n1       T01   A01        1            1.0\n2       T02   A01        1            1.0\n3       T03   A05        1            1.0\n4       T04   A03        1            0.6\n5       T04   A04        2            0.4\n6       T05   A04        1            1.0\n7       T06   A02        1            1.0\n8       T07   A02        1            0.5\n9       T07   A04        2            0.5\n10      T08   A06        1            1.0\n11      T09   A06        1            1.0\n12      T10   A02        1            1.0\n13      T11   A03        2            0.3\n14      T11   A04        3            0.3\n15      T11   A06        1            0.4\n16      T12   A02        1            1.0\n17      T13   A01        1            1.0\n\n\n\n\n\n\n\n\n\n\n\nClick here - authors table\n\n\n\n\n\n\nsqldf(\"select * from authors\")\n\n  au_id  au_fname    au_lname        phone              address          city\n1   A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx\n2   A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder\n3   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco\n4   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco\n5   A05 Christian       Kells 212-771-4680       114 Horatio St      New York\n6   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto\n7   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota\n  state   zip\n1    NY 10468\n2    CO 80303\n3    CA 94123\n4    CA 94123\n5    NY 10014\n6    CA 94305\n7    FL 34236\n\n\n\n\n\n\n\n\n\n\n\nClick here - royalties table\n\n\n\n\n\n\nsqldf(\"select * from royalties\")\n\n   title_id advance royalty_rate\n1       T01   10000         0.05\n2       T02    1000         0.06\n3       T03   15000         0.07\n4       T04   20000         0.08\n5       T05  100000         0.09\n6       T06   20000         0.08\n7       T07 1000000         0.11\n8       T08       0         0.04\n9       T09       0         0.05\n10      T10      NA           NA\n11      T11  100000         0.07\n12      T12   50000         0.09\n13      T13   20000         0.06",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'><yrChapterNumber>34. </yrChapterNumber> Common Table Expressions (CTE)</span>"
    ]
  },
  {
    "objectID": "sql0050-commonTableExpressions-v001.html#cte-referring-to-an-earlier-cte",
    "href": "sql0050-commonTableExpressions-v001.html#cte-referring-to-an-earlier-cte",
    "title": "34  34.  Common Table Expressions (CTE)",
    "section": "34.4 CTE Referring to an Earlier CTE",
    "text": "34.4 CTE Referring to an Earlier CTE\nGoal: Find top-earning books and list their authors. Top-earning = (sales × price) &gt; 500,000.\n\nsqldf(\"\nWITH book_earnings AS (\n  SELECT title_id, title_name, sales * price AS revenue\n  FROM titles\n),\ntop_books AS (\n  SELECT title_id, title_name, revenue\n  FROM book_earnings\n  WHERE revenue &gt; 500000\n),\ntop_books_authors AS (\n  SELECT tb.title_name, tb.revenue, a.au_fname || ' ' || a.au_lname AS author\n  FROM top_books tb\n  JOIN title_authors ta ON tb.title_id = ta.title_id\n  JOIN authors a ON ta.au_id = a.au_id\n)\nSELECT * FROM top_books_authors\nORDER BY revenue DESC\n\")\n\n                        title_name    revenue          author\n1                I Blame My Mother 35929790.0 Wendy Heydemark\n2                I Blame My Mother 35929790.0       Klee Hull\n3           Exchange of Platitudes  1400008.0       Klee Hull\n4        Spontaneous, Not Annoying  1299013.0 Wendy Heydemark\n5     Ask Yor System Administrator  1025396.7 Christian Kells\n6 Perhaps It's a Glandular Problem   752042.8     Hallie Hull\n7 Perhaps It's a Glandular Problem   752042.8       Klee Hull\n8 Perhaps It's a Glandular Problem   752042.8  Harvey Kellsey",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'><yrChapterNumber>34. </yrChapterNumber> Common Table Expressions (CTE)</span>"
    ]
  },
  {
    "objectID": "sql0050-commonTableExpressions-v001.html#recursive-ctes",
    "href": "sql0050-commonTableExpressions-v001.html#recursive-ctes",
    "title": "34  34.  Common Table Expressions (CTE)",
    "section": "34.5 Recursive CTEs",
    "text": "34.5 Recursive CTEs\nThis is a more advanced topic. See here for info on how to use. https://learnsql.com/blog/sql-recursive-cte/\nThe following is an example from the books database:\nGoal: Given an author, find all other authors who co-authored any book with them, directly or through a chain of co-authors.\nLet’s assume we start from author A03.\nsqldf(\"\n  WITH RECURSIVE coauthors_cte(au_id, coauthor_id, level) AS (\n    -- Anchor: Get direct co-authors of A03\n    SELECT ta1.au_id, ta2.au_id, 1\n    FROM title_authors ta1\n    JOIN title_authors ta2 ON ta1.title_id = ta2.title_id\n    WHERE ta1.au_id = 'A03' AND ta2.au_id != 'A03'\n  \n    UNION\n  \n    -- Recursive: Get co-authors of the previous level\n    SELECT c.coauthor_id, ta2.au_id, level + 1\n    FROM coauthors_cte c\n    JOIN title_authors ta1 ON c.coauthor_id = ta1.au_id\n    JOIN title_authors ta2 ON ta1.title_id = ta2.title_id\n    WHERE ta2.au_id != c.coauthor_id\n  )\n  SELECT DISTINCT coauthor_id\n  FROM coauthors_cte\n  ORDER BY coauthor_id;\n\")",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'><yrChapterNumber>34. </yrChapterNumber> Common Table Expressions (CTE)</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html",
    "href": "sql0200-otherSqlCommands-v002.html",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "",
    "text": "35.1 db-fiddle website\nsqldf cannot run SQL commands other than SELECT.\nWe will use the following website to demonstrate other SQL commands.\nJust create the database on db-fiddle\nhttps://www.db-fiddle.com/f/uWwmfDJJeCdeS6Xiy3e1bm/0\nsome sample select statements with the same tables\nhttps://www.db-fiddle.com/f/uWwmfDJJeCdeS6Xiy3e1bm/11",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#update",
    "href": "sql0200-otherSqlCommands-v002.html#update",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.2 update",
    "text": "35.2 update\nThe UPDATE command is used for modifying the data in a table. See the following for more info:\n\nhttps://www.sqlitetutorial.net/sqlite-update/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#delete",
    "href": "sql0200-otherSqlCommands-v002.html#delete",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.3 delete",
    "text": "35.3 delete\nThe DELETE command is used to delete entire rows of data from a table. See the following for more info:\n\nhttps://www.sqlitetutorial.net/sqlite-delete/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#create-table",
    "href": "sql0200-otherSqlCommands-v002.html#create-table",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.4 create table",
    "text": "35.4 create table\nThe CREATE TABLE command is used to create tables in a relational database. See the following for more info:\n\nhttps://www.sqlitetutorial.net/sqlite-create-table/\n\nAs part of the create table command, you can specify certain “constraints” on the columns of the table. For example, you can specify that a particular column or columns should never be allowed to contain any NULL values or that all of the values in a particular column should be unique (in addition to this requirement for the primary key). See the following for info about how to apply various constraints to the columns in a table:\n\nhttps://www.sqlitetutorial.net/sqlite-not-null-constraint/\nhttps://www.sqlitetutorial.net/sqlite-unique-constraint/\nhttps://www.sqlitetutorial.net/sqlite-check-constraint/\nhttps://www.sqlitetutorial.net/sqlite-autoincrement/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#insert-a-new-row-into-a-table",
    "href": "sql0200-otherSqlCommands-v002.html#insert-a-new-row-into-a-table",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.5 insert a new row into a table",
    "text": "35.5 insert a new row into a table\nThe INSERT command is used to insert one or more new rows into a table. See the following for more info:\n\nhttps://www.sqlitetutorial.net/sqlite-insert/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#drop-table",
    "href": "sql0200-otherSqlCommands-v002.html#drop-table",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.6 drop table",
    "text": "35.6 drop table\nThe DROP TABLE command removes an entire table (including the structure of the table and all rows of data) from a database. See the following for more info:\n\nhttps://www.sqlitetutorial.net/sqlite-drop-table/",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0200-otherSqlCommands-v002.html#sql-dml-vs-ddl-commands",
    "href": "sql0200-otherSqlCommands-v002.html#sql-dml-vs-ddl-commands",
    "title": "35  35.  other commands: update/delete, create/drop/insert",
    "section": "35.7 SQL “DML” vs “DDL” commands",
    "text": "35.7 SQL “DML” vs “DDL” commands\nThe commands in SQL are often categorized into the following two distinct categories:\n\n35.7.1 Data Manipulation Language (DML) SQL commands\nThe commands in SQL that are used for creating, reading, updating and deleting data (collectively referred to as “CRUD” operations) are known as “Data Manipulation Language” (DML) SQL commands. These include the following SQL commands:\n\ninsert (for “creating” rows of data)\nselect (for “reading” rows of data)\nupdate (for “updating” rows of data)\ndelete (for “deleting” rows of data)\n\nThe term DML or “Data Manipulation Language” is often misunderstood. DML is NOT a new language. DML is simply the term that is used to refer to the parts of SQL for “manipulating” data (i.e. performing CRUD operations).\n\n\n35.7.2 Data Definition Language (DML) SQL commands\nThe commands in SQL that are used for creating and managing the database tables are known as “Data Definition Language” (DML) SQL commands. These include the following.\n\nCREATE TABLE - used to create a table\nDROP TABLE - used to destroy an entire table (its structure and its data)\nALTER TABLE - used to change the structure of a table (e.g. add columns)\n\nMany RDBMS products include other DDL commands for example to rename tables as well as to work with other more advanced structure. For more info, see the documentation for your particular RDBMS product.\n\n# There are many SQL commands other than select.\n# The following is a list of some of the more common SQL commands. \n# The list below doesn't include the entire syntax. \n# See this page for more info on the syntax for individual\n# SQL commands: https://www.w3schools.com/sql/default.asp\n#\n#  # create a new table structure\n#  # see : https://www.w3schools.com/sql/sql_create_table.asp\n#  CREATE TABLE someTable ...\n# \n#  # \"drop\" or entirely remove the table structure and all rows of data\n#  DROP TABLE someTable    \n#\n#  # Insert a new row into a table\n#  INSERT INTO someTable ...\n#\n#  # modify the values in a table\n#  UPDATE someTable ...\n#\n#  # delete some or all of the rows in a table\n#  DELETE FROM someTable ...\n#\n#\n#\n# FOR MORE DETAILS ABOUT THESE STATEMENTS, SEE THE catering DATABASE AND\n# THE VARIOUS FILES CONTAINED IN cateringDatabase-v003.zip",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'><yrChapterNumber>35. </yrChapterNumber> other commands: update/delete, create/drop/insert</span>"
    ]
  },
  {
    "objectID": "sql0900-sqlWindowFunctions-v002.html",
    "href": "sql0900-sqlWindowFunctions-v002.html",
    "title": "36  36.  SQL “window” functions",
    "section": "",
    "text": "See the following for more info:\n\nIntro to SQL Window Functions\n\ntext\n\nhttps://learnsql.com/blog/why-should-i-learn-sql-window-functions/\n\nvideos\n\nSQL Window Functions in 10 minutes (Colt Steele, 2022) https://www.youtube.com/watch?v=y1KCM8vbYe4\n[SQL Window Functions | Clearly Explained (Maven Analytics, 2024)] https://www.youtube.com/watch?v=rIcB4zMYMas",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'><yrChapterNumber>36. </yrChapterNumber> SQL \"window\" functions</span>"
    ]
  },
  {
    "objectID": "sql0915-workingWithSqliteDatabasesInR-v001.html",
    "href": "sql0915-workingWithSqliteDatabasesInR-v001.html",
    "title": "37  37a.  Working with SQLite databases in R",
    "section": "",
    "text": "NOTE: This section give a very brief overview for how to work specifically with SQLite databases in R. This section gives just enough information to get working. More elaborate coverage for how to work with any type of RDBMS (relations database management system software) in R appears in the next section.\n\nlibrary(DBI)\nlibrary(RSQLite)\nlibrary(sqldf)\n\n# let's start with a new database.\n# I'll delete the database file in case it was left over\n# from before.\n\nif(!require(fs)){install.packages(\"fs\");rquire(fs);}\nfs::file_delete(\"mydb.sqlite\")\n\ncon = RSQLite::dbConnect(RSQLite::SQLite(), \"mydb.sqlite\")\n\n# To run any SQL command other than SELECT you can use\n# the dbExecute function from the DBI package.\n#\n# You need to supply the connection that you made as\n# the first argument ot the dbExecute function.\n\nDBI::dbExecute(conn = con,\n  \"create table authors (\n   au_id text,\n   au_fname text,\n   au_lname text)\"\n  )\n\n[1] 0\n\n# As always, you can use the sqldf function to run SELECT statements.\n# To run the select against the SQLite database, you \n# must provied the connection variable that you created above\n# to the connection argument to sqldf.\n\n# This should come up with zero rows\nsqldf(\"select * from authors\", connection = con)\n\n[1] au_id    au_fname au_lname\n&lt;0 rows&gt; (or 0-length row.names)\n\n# We can use the insert SQL statement to insert a row\ndbExecute(con, \"insert into authors values('a100','joe','smith')\", connection = con)\n\n[1] 1\n\n# We now can see that row in the database.\nsqldf(\"select * from authors\", connection = con)\n\n  au_id au_fname au_lname\n1  a100      joe    smith\n\n########################################################.\n# Dynamically creating an INSERT statement from data.\n########################################################.\n\n# Suppose you wanted to insert the following data\n# into a row of the authors table.\n\nauthorId = 'a200'\nauthorFname = 'bill'\nauthorLname = 'jones'\n\n# We are using the following code to build up the following SQL code:\n#\n#    insert into authors values( 'a200' , 'bill' , 'jones' )\n\ninsertStatement = paste0(\n                           \"insert into authors \",\n                           \"values(\", \n                              \"'\", authorId, \"'\", \",\" , \n                              \"'\", authorFname, \"'\", \",\" , \n                              \"'\", authorLname, \"'\", \n                           \")\"\n                        )\n\n# This is the insert statement that we created.\ninsertStatement\n\n[1] \"insert into authors values('a200','bill','jones')\"\n\n# We can now use dbExecute to enter the data into the database\ndbExecute(conn = con, insertStatement)          \n\n[1] 1\n\n# We can now query the database to make sure that the data is there.\nsqldf(\"select * from authors\", connection = con)\n\n  au_id au_fname au_lname\n1  a100      joe    smith\n2  a200     bill    jones\n\n##########################################################.\n# Using an API and dynamically inserting the returned\n# data into a database.\n##########################################################.\n\n# Suppose that you retrieved some data from an API\n# and that you got the data from the API\n# into an R dataframe by using jsonlite::fromJSON( ... )\n#\n# We will pretend that we did so and got back the following\n# dataframe with two authors.\n\ndf = data.frame(authorFirstName = c(\"bill\", \"sue\"),\n                authorLastName = c(\"berger\", \"smith\"))\n\n# We can use a loop to get the authors names from each row of the \n# the dataframe into a separate insert statment and then \n# insert that author's row. \n#\n# Notice that there is no author ID for these authors. \n# We can create our own IDs inside the loop using R code.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'><yrChapterNumber>37a. </yrChapterNumber> Working with SQLite databases in R</span>"
    ]
  },
  {
    "objectID": "sql0950-workingWithDatabasesInR-v010.html",
    "href": "sql0950-workingWithDatabasesInR-v010.html",
    "title": "38  37b.  Working directly with databases in R",
    "section": "",
    "text": "38.1 Download files for use with this material\nClick here to download the books.sqlite file. We will use this with the code below. Note that a sqlite file can also have different extensions, such as .db (or anything else). There is no official standard for the extension.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'><yrChapterNumber>37b. </yrChapterNumber> Working directly with databases in R</span>"
    ]
  },
  {
    "objectID": "sql0950-workingWithDatabasesInR-v010.html#going-beyond-sqldf",
    "href": "sql0950-workingWithDatabasesInR-v010.html#going-beyond-sqldf",
    "title": "38  37b.  Working directly with databases in R",
    "section": "38.2 Going beyond sqldf",
    "text": "38.2 Going beyond sqldf\n\n###############################################################################.\n#\n# \"going beyond sqldf\"   -   Connecting to databases through R  -  \n#\n# In this section we will go beyond using sqldf to manipulate dataframes in R.\n# We will learn about how to interact with an actual database management system\n# from within R.\n#\n###############################################################################.\n\n###############################################################################.\n#\n# Resources for learning more about working with databases in R\n#\n# Best Practices in Working with Databases (in RStudio)\n#    https://solutions.posit.co/connections/db/\n#\n# Datacamp Tutorial\n#    https://www.datacamp.com/community/tutorials/sqlite-in-r\n#\n# vignette(\"spec\", package=\"DBI\")\n#\n# help(package=\"DBI\")\n#\n# Functions we will use:\n#    dbConnect\n#    dbExecute    # create table, insert , update, delete, etc.\n#\n#    dbGetQuery   # select (interactive work)\n#\n#    dbSendQuery  # select (batched work)\n#    dbFetch\n#    dbClearResult\n###############################################################################.\n\n\n#-------------------------------------\n# sqldf is not enough\n#-------------------------------------\n\n# As you have already learned, SQL is a standard language for manipulating data\n# that is stored in a relational database. There are many different relational\n# database management system products and each one uses SQL to manipulate the\n# data in the database.\n\n# We started learning about the SQL select statement by using the sqldf\n# function from the sqldf package to manipulate R dataframes with \n# SQL select statements. However, sqldf does NOT allow you to manipulate\n# data that is stored in an actual database. The purpose for sqldf is to allow\n# R programmers who are familiar with SQL SELECT statements \n# to use their knowledge to manipulate R dataframes. This is very useful as \n# SQL is an extremely popular language that is widely known and understood.\n# However, in order to use SQL to manipulate data in an ACTUAL database, R programers\n# must use functions from packages other than sqldf. \n\n\n#-------------------------------------------------------------------\n# More about Relational Database Management System (RDBMS) software\n#-------------------------------------------------------------------\n\n# Relational database management system (RDBMS) products aim to provide a highly\n# efficient mechanism to store and retrieve data. Depending on the specific\n# RDBMS product, the type of computer (and many other details) a relational\n# database management system may able to read and write thousands or even\n# millions or billions of data values per second. To achieve this level of\n# efficiency often requires these products to integrate tightly with the\n# underlying computer in technical ways that go beyond what many other software\n# products require. Therefore installing and getting these products to work\n# correctly can sometimes be a little complex and require more than just running\n# a simple install program.\n#\n# Relational Database Management Systems differ from programs such as Excel that\n# are designed for people to interact with them directly. Relational Database\n# Management System (RDBMS) software products are designed to allow other\n# computer programs to store and retrieve data from them. For example a program\n# written in R (or any other language) can store and retrieve data from an\n# RDBMS. These other computer programs (such as an R program) are in turn used\n# by business people to do their work. Typically, data technologists (e.g.\n# programmers data analysts, etc) write the programs (e.g. in R) that are used\n# by business people (e.g. traders, accountants). The data technologists use SQL\n# and R (or another programming language) to create easy to use tools for the\n# business people. In our class we will learn how to write R programs to send\n# and retrieve data from an RDBMS.\n\n#--------------------------------------------------------------------------\n# CLIENTS AND SERVERS\n#\n#   - client programs\n#   - server programs\n#\n#   - client computers\n#   - server computers\n#--------------------------------------------------------------------------\n\n# An RDBMS software program is known as a \"server program\" (it \"serves up\"\n# information). The computer programs that send and retrieve data from the RDBMS\n# program are known as \"client programs\". In a similar way, the programs that \n# run websites are known as \"(web) server programs\" (they also serve up information)\n# while web browsers are known as \"client programs\".\n\n# Most relational database management systems are intended to be used\n# \"remotely\". In other words, the relational database management system is\n# installed one computer and the computer programs that send and retrieve data\n# from the RDBMS are located on other computers. The computer that the RDBMS is\n# installed on is known as a \"server computer\" (the \"server program\" is\n# installed on the \"server computer\"). A computer that has software installed on\n# it that interacts with the RDBMS is known as a \"client computer\" (a \"client\n# program\" is installed on a \"client computer\"). The client program and server\n# program may communicate over the Internet or over a local computer network\n# (e.g. a network of computers in a company that is not accessible from the\n# larger Internet).\n#\n# It is also possible for client programs and server programs to be\n# installed on the same computer. The details as to which computer each \n# program is installed on (whether the same computer or different computers)\n# is a detail that should not affect the way client programs are written.\n# Often when creating a new program (e.g. an R program) the developer has \n# both the server program (e.g. the RDBMS software) and the client program\n# (e.g. the program they are creating) installed on their personal computer.\n# When the program is eventually used by business users, the program could \n# then be configured to communicate with a different server computer. The\n# specifics of how exactly this is done is beyond the scope of what we are learning\n# in this class. \n#\n\n#--------------------------------------------------------------------\n# Different RDBMS software packages\n#--------------------------------------------------------------------\n# Some popular RDBMS packages that can be downloaded and installed on your\n# computer for free are:\n#\n#   - MySQL available for Windows/Mac/Linux https://www.mysql.com/\n#\n#     The installation of MySql can be somewhat complex. There are multiple\n#     different products to choose between and may require \"administrator privileges\"\n#     to install correctly. Nevertheless, it should be possible for you to install\n#     and use MySql without too much hassle. However, the complications in my mind\n#     do not make this an ideal solution for use in a classroom setting.\n#\n#   - PostgreSQL available for Windows/Mac/Linux https://www.postgresql.org/\n#\n#   - Microsoft SQL Server Express \n#     available for Windows/Linux (not for Mac)\n#     https://www.microsoft.com/en-us/sql-server/sql-server-downloads\n#     NOTE: There are many different versions of \"Microsoft SQL Server\" software\n#           the \"Express\" version is the \"free\" version.\n#\n#     For Windows users, this is a wonderful product that is much easier to use\n#     than MySql. However, Microsoft SQL Server is  NOT available \"out of the box\" for Mac. \n#     While, you can find information online about how to install SQL Server on Macs,\n#     this requires using Docker software - which itself has a learning curve.\n#     If you own a mac and want to try it anyway\n#     you can see this page - https://database.guide/how-to-install-sql-server-on-a-mac/\n#     or search online for many other websites that have similar information.\n#\n#\n#   - SQLite (https://www.sqlite.org/)\n#\n#     This is a \"simple\" relational database management system. It is designed\n#     to be used only \"locally\", not over a computer network. In other words,\n#     the client program (e.g. an R program) that interacts with the RDBMS is\n#     expected to be installed on the same computer as the SQLite software. This\n#     software lacks many of the more sophisticated features found in other\n#     products. SQLite is not designed to be used with tremendous amounts of\n#     information. However, SQLite does allow programs that communicate via SQL\n#     to store their information in a database without too much hassle. This\n#     product is often used by computer programs and apps that don't require\n#     tremendous amounts of data to store and retrieve data in a relational\n#     database.\n#\n#\n#     DIFFERENT OPTIONS TO INSTALL SQLite\n#\n#     Installation of SQLite is relatively straight forward. The entire SQLite\n#     program is contained in a single \"executable file\" that can be downloaded\n#     from the SQLite website. This software does NOT have a \"graphical user\n#     interface (GUI)\". Rather the program presents a simple \"command line\"\n#     interface - similar to R - in which you can type commands and see the\n#     results.\n#\n#     In addition to the \"plain vanilla\" SQLite that is distributed by\n#     sqlite.org, there are other add-on programs that you can download to\n#     provide a more \"friendly\" GUI user interface. One such program is \"SQLite\n#     Studio\" (https://sqlitestudio.pl/). The latest version of SQLite Studio\n#     also does not come with an installation program. Just a zip file that\n#     contains all the files that the program needs to run. To use this program,\n#     unzip the downloaded file and run the program that is in the file named\n#     SQLiteStudio.exe.\n#\n#     To use SQLite from within R does not require downloading any files from\n#     the SQLite website or any other website. Rather the RSQLite package in R\n#     has all of the functionality needed to use SQLite built into it. In our\n#     class we will be using the RSQLite package to interface with the SQLite\n#     database.\n#--------------------------------------------------------------------.\n\n#--------------------------------------------------------------------.\n# sqlfiddle.com\n#--------------------------------------------------------------------.\n# sqlfiddle.com - you can use this website to try different flavors of sql.\n# You can choose the flavor of SQL to use from a dropdown list, (e.g.\n# MySQL, PostgreSQL, SQLite, Oracle)\n#\n# On the left side of the sqlfiddle.com screen you create tables and \n# and insert and modify data by using CREATE TABLE, INSERT, UPDATE and DELETE\n# SQL statements.\n#\n# On the right side of the sqlfiddle.com screen you can enter \n# SELECT statements that manipulate the data.\n#\n# EXAMPLE - click on the following URL:\n#            http://sqlfiddle.com/#!17/8efb57/8\n# To see the following database tables and sample data running in the\n# sqlfiddle.com website.\n#   \n#   +---------------------+           +----------------------+\n#   |  venues             |           |  parties             |\n#   +---------------------+           +----------------------+\n#   | PK   venueId        |           | PK  partyId          |\n#   |                     |           |                      |\n#   |      venueName      |           | FK  venueId          | \n#   |                     | |        /|                      |\n#   |      city           +-+---------+     numGuests        |\n#   |                     | |        \\|                      |\n#   |      state          |           |     cuisine          |\n#   |                     |           |                      |\n#   |      kosherKitchen  |           |     customerFirstName|\n#   |                     |           |                      |\n#   |                     |           |     customerLastName |\n#   |                     |           |                      |\n#   |                     |           |     partyDate        |\n#   |                     |           |                      |\n#   +---------------------+           |     durationInHours  |  \n#                                     +----------------------+                    \n#                                               \\|/\n#                                                |\n#                                                |\n#                                                |\n#                                               ---\n#                                                |\n#   +---------------------+           +----------------------+\n#   |  waiters            |           |  parties_waiters     |\n#   +---------------------+           +----------------------+\n#   | PK   waiterId       |           | PK,FK1  partyId      |\n#   |                     |           |                      |\n#   |      firstName      |           | PK,FK2  waiterId     | \n#   |                     | |        /|                      |\n#   |      lastName       +-+---------+         hourlyWage   |\n#   |                     | |        \\|                      |\n#   |      gender         |           |                      |\n#   |                     |           |                      |\n#   +---------------------+           +----------------------+\n#\n#--------------------------------------------------------------------.\n\n#----------------------------------------------------------------------------.\n# DDL vs DML statements\n#\n# SQL statements that create or modify the structure of the database \n# tables are known collectively as Data Definition Language (DDL) SQL statements. \n# These include commands such as \n#\n#     CREATE TABLE (that creates the table structures),\n#     DROP TABLE (that destroys an entire table, including the structure), \n#     ALTER TABLE (that alters or changes the structure of tables, e.g. add columns)\n#     other similar commands \n#\n# SQL statements such as SELECT, INSERT, UPDATE and DELETE that create, modify\n# retrieve and delte \"data\" in the tables are collectively known as \n# Data Manipulation Language (DML) SQL statements.\n#\n#\n# NOTE: (not very important, but may clarify something ...)\n#\n# sqlfiddle.com instructs you to put DDL statements in the left hand side\n# of the window and SELECT statements in the right hand side. The website\n# (incorrectly, IMHO) refers to INSERT, UPDATE and DELETE statemens as DDL \n# statements. I believe most people in the industry refer to these as DML\n# statements.\n#----------------------------------------------------------------------------.\n\n\n\n\n#-----------------------------------------------------------------------.\n# More about SQLite databases.\n#\n# SQLite is designed to be used without the need for a server program.\n# All the data in a SQLite database is packed into a single file.\n# To connect to a SQLite database, you need to know the path to the \n# SQLite file for the database.\n#\n# The extension for a SQLite file is not standardized - sometimes\n# you will see .db  sometimes .sqlite  sometimes other extensions\n#-----------------------------------------------------------------------.\n\n\n#########################################################################.\n# Using DBI to connect to a database from R\n#------------------------------------------------------------------------.\n# To connect to a database from R, you can use functions from the \n# DBI package. You will also need to use an additional package that is\n# specific for the type of database software you are going to connect to. \n#\n# For example:\n#   - to connect to a sqlite database you will need to use\n#     functions from both the DBI package, as well as the RSQLite package. \n#\n#   - to connect to a PostgreSQL database you will need to use\n#     functions from both the DBI package, as well as the RPostgreSQL package. \n#\n# Fortunately, the code to write for both types of database software products\n# as well as many other database software products is VERY similar.\n#\n# Below, we will demonstrate how to connect to both a sqlite database and a\n# PostgreSQL database using the DBI package. \n#\n# NOTE: There is an alternative to the DBI approach described above.\n#       Another popular alternative to connect to a database from R is to \n#       use the RODBC package. You can research that on your own if \n#       you like. The approach we take here with the DBI package\n#       is just as powerful as ODBC - just an alternative. They are both\n#       popular approaches. \n#########################################################################.\n\n\n#------------------------------------------------------------------------.\n# How to connect to a sqlite database using the DBI package.\n#------------------------------------------------------------------------.\n# Use R's DBI package to establish a \"connection\" with a database.\n# You also need to use a package for the particular database software \n# you are going to be connecting with. For example, the RSQLite package\n# is used for the sqlite database.\n#\n# The DBI package contains a function called dbConnect. \n# The DBMS specific packages, such as RSQLite also contain a dbConnect function.\n# The DBMS specific version of the dbConnect function, calls the DBI version\n# of the dbConnect function as part of its processing.\n#\n# The RSQLite package also provides the SQLite() function. This function\n# returns a \"driver\" object that knows the specifics of how to work with\n# sqlite databases. \n#\n# The dbConnect function in the DBI package takes a \"driver\" as a first\n# argument. The other arguments to dbConnect depend on the type of\n# database software you are trying to connect to. \n#\n# See the code below for more info.\n#########################################################################.\n\n#---------------------------------------------------------------------.\n# Establish a connection to the SQLite database:\n#---------------------------------------------------------------------.\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# You will need both the DBI and RSQLite packages\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nif(!require(DBI)) {install.packages(\"DBI\"); require(DBI)}\n\nLoading required package: DBI\n\nif(!require(RSQLite)) {install.packages(\"RSQLite\"); require(RSQLite)}\n\nLoading required package: RSQLite\n\n\nWarning: package 'RSQLite' was built under R version 4.4.3\n\nhelp(package=\"DBI\")\nhelp(package=\"RSQLite\")\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Create a connection to the Books database\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# this should be a path to the .sqlite file on your computer\n\nsqliteBooksFile = \"data/books.sqlite\"   \n\n# Create the connection variable, conBooks (you can name this anything you want)\n# You will use conBooks whenever you want to access the database.\n# See below for more info.\n\nconBooks &lt;- dbConnect(RSQLite::SQLite(), dbname = sqliteBooksFile)\n\n#---------------------------------------------------------------------.\n# Show the tables in the database\n#---------------------------------------------------------------------.\n\ndbListTables(conBooks)\n\n[1] \"authors\"       \"publishers\"    \"royalties\"     \"title_authors\" \"titles\"       \n\n#---------------------------------------------------------------------.\n# The following functions from the DBI package can be used to \n# run SQL statements in the database:\n#\n# FOR SELECT STATEMENTS USE\n#      dbGetQuery() or     # use to get entire results all at once\n#      dbSendQuery()       # use for retrieving large amounts of info a few rows at a time\n# \n# FOR OTHER SQL STATEMENTS USE \n#      dbExecute() or dbSendStatement() \n#---------------------------------------------------------------------.\n\n#......................................\n# Get everything all at once\n#......................................\n\ndbGetQuery(conBooks, \"select * from authors\")\n\n  au_id  au_fname    au_lname        phone              address          city state   zip\n1   A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468\n2   A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303\n3   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n4   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n5   A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014\n6   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305\n7   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236\n\ndbGetQuery(conBooks, \"select * from titles \n                                  where type='biography'\")\n\n  title_id                 title_name      type pub_id pages price   sales             pubdate\n1      T06           How About Never? biography    P01   473 19.95   11320 2000-07-31 00:00:00\n2      T07          I Blame My Mother biography    P03   333 23.95 1500200 1999-10-01 00:00:00\n3      T10 Not Without My Faberge Egg biography    P05                                        \n4      T12  Spontaneous, Not Annoying biography    P05   507 12.99  100001 2000-08-31 00:00:00\n\n#.................................................\n# To get large amounts of data a few rows at a time use the following functions:\n# \n#  # Call dbSendQuery to get a \"handle\"\n#\n#  RESULT_HANDLE = dbSendQuery( conn = CONNECTION ,\n#                               statement = SOME_SELECT_STATEMENT )\n#\n#  # Call dbFetch multiple times.\n#  # Each time you get back a dataframe with a few rows.\n#  # You could also call dbFetch in a loop\n#\n#  ROWS      = dbFetch( res = RESULT_HANDLE , n = NUMBER_OF_ROWS)\n#  MORE_ROWS = dbFetch( res = RESULT_HANDLE , n = NUMBER_OF_ROWS)\n#  MORE_ROWS = dbFetch( res = RESULT_HANDLE , n = NUMBER_OF_ROWS)\n#  MORE_ROWS = dbFetch( res = RESULT_HANDLE , n = NUMBER_OF_ROWS)\n#  etc. \n#\n#  # It is a very good idea to call dbClearResult when you are done processing\n#  # all the rows. That will free up any computer memory that was allocated\n#  # to processing the results. Also most DBMS products have a limited number\n#  # of queries that can be processed simultaneously. dbClearResult\n#  # will free up the resources used by this query so that there are \n#  # more resources available in the database for others who might be trying\n#  # to access the database simultaneously.\n#\n#  dbClearResult( RESULT_HANDLE )\n#.................................................\n\n# Get the rows from the titles table 5 at a time.\n# You can see a summary of where you're up to in the entire result set \n# by displaying the results variable.\n\nresults &lt;- dbSendQuery(conBooks, \"select * from titles\")\n\ndbFetch(results, n=5)   # get first 5 rows\n\n  title_id                    title_name       type pub_id pages price  sales             pubdate\n1      T01                         1977!    history    P01   107 21.99    566 2000-08-01 00:00:00\n2      T02     200 Years of German Humor    history    P03    14 19.95   9566 1998-04-01 00:00:00\n3      T03 Ask Your System Administrator   computer    P02  1226 39.95  25667 2000-09-01 00:00:00\n4      T04    But I Did It Unconsciously psychology    P01   510 12.99  13001 1999-05-31 00:00:00\n5      T05        Exchange of Platitudes psychology    P01   201  6.95 201440 2001-01-01 00:00:00\n\nresults                 # show a summary of where we are up to\n\n&lt;SQLiteResult&gt;\n  SQL  select * from titles\n  ROWS Fetched: 5 [incomplete]\n       Changed: 0\n\ndbFetch(results, n=5)   # get the next 5 rows\n\n  title_id                   title_name      type pub_id pages price   sales             pubdate\n1      T06             How About Never? biography    P01   473 19.95   11320 2000-07-31 00:00:00\n2      T07            I Blame My Mother biography    P03   333 23.95 1500200 1999-10-01 00:00:00\n3      T08 Just Wait Until After School  children    P01    86 10.00    4095 2001-06-01 00:00:00\n4      T09              Kiss My Boo-Boo  children    P01    22 13.95    5000 2002-05-31 00:00:00\n5      T10   Not Without My Faberge Egg biography    P05                                        \n\nresults                 # show a summary of where we are up to\n\n&lt;SQLiteResult&gt;\n  SQL  select * from titles\n  ROWS Fetched: 10 [incomplete]\n       Changed: 0\n\ndbFetch(results, n=5)   # get the next 5 rows\n\n  title_id                          title_name       type pub_id pages price  sales             pubdate\n1      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 2000-11-30 00:00:00\n2      T12           Spontaneous, Not Annoying  biography    P05   507 12.99 100001 2000-08-31 00:00:00\n3      T13 What Are The Civilian Applications?    history    P03   802 29.99  10467 1999-05-31 00:00:00\n\nresults                 # show a summary of where we are up to\n\n&lt;SQLiteResult&gt;\n  SQL  select * from titles\n  ROWS Fetched: 13 [complete]\n       Changed: 0\n\ndbClearResult(results)   # we're done so free up any \"resources\" (e.g. memory or connections to database)\n\n# Let's do that again ...\n# You can also use the following functions to return specific info about the \n# results, by passing the function the RESULTS_HANDLE that you got back from \n# the dbSendQuery function.\n#\n#   dbGetRowCount(RESULTS_HANDLE)  # return the # of rows returned so far\n#   dbHasCompleted(RESULTS_HANDLE) # FALSE if more rows to be retrieved, TRUE otherwise\n#\n# There are other functions that could be used with the RESULTS_HANDLE too.\n# See the documentation for more info.\n\nresults &lt;- dbSendQuery(conBooks, \"select * from titles\")\n\ndbFetch(results, n=5)   # get first 5 rows\n\n  title_id                    title_name       type pub_id pages price  sales             pubdate\n1      T01                         1977!    history    P01   107 21.99    566 2000-08-01 00:00:00\n2      T02     200 Years of German Humor    history    P03    14 19.95   9566 1998-04-01 00:00:00\n3      T03 Ask Your System Administrator   computer    P02  1226 39.95  25667 2000-09-01 00:00:00\n4      T04    But I Did It Unconsciously psychology    P01   510 12.99  13001 1999-05-31 00:00:00\n5      T05        Exchange of Platitudes psychology    P01   201  6.95 201440 2001-01-01 00:00:00\n\ndbGetRowCount(results)  # 5\n\n[1] 5\n\ndbHasCompleted(results) # FALSE\n\n[1] FALSE\n\ndbFetch(results, n=5)   # get the next 5 rows\n\n  title_id                   title_name      type pub_id pages price   sales             pubdate\n1      T06             How About Never? biography    P01   473 19.95   11320 2000-07-31 00:00:00\n2      T07            I Blame My Mother biography    P03   333 23.95 1500200 1999-10-01 00:00:00\n3      T08 Just Wait Until After School  children    P01    86 10.00    4095 2001-06-01 00:00:00\n4      T09              Kiss My Boo-Boo  children    P01    22 13.95    5000 2002-05-31 00:00:00\n5      T10   Not Without My Faberge Egg biography    P05                                        \n\ndbGetRowCount(results)  # 10\n\n[1] 10\n\ndbHasCompleted(results) # FALSE\n\n[1] FALSE\n\ndbFetch(results, n=5)   # try to get the next 5 rows (but there are only 3 more)\n\n  title_id                          title_name       type pub_id pages price  sales             pubdate\n1      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99  94123 2000-11-30 00:00:00\n2      T12           Spontaneous, Not Annoying  biography    P05   507 12.99 100001 2000-08-31 00:00:00\n3      T13 What Are The Civilian Applications?    history    P03   802 29.99  10467 1999-05-31 00:00:00\n\ndbGetRowCount(results)  # 13\n\n[1] 13\n\ndbHasCompleted(results) # TRUE\n\n[1] TRUE\n\ndbClearResult(results)   # we're done so free up any \"resources\" (e.g. memory or connections to database)\n\n\n# You can do that in a loop to get back 5 rows at a time, \n# do some processing and then continue.\n\n# Start the query and get the first few rows.\nresults &lt;- dbSendQuery(conBooks, \"select * from titles\")\nrows = dbFetch(results, n=5)   # get first 5 rows\n\n# Keep looping until we get all of the rows.\nwhile ( dbHasCompleted(results) == FALSE ){\n  newRows = dbFetch(results, n=5)   # get first 5 rows\n  rows = rbind(rows, newRows)\n}\n\n# We're done getting all the rows so clear the results handle.\n# This releases any memory or other database \"resources\" that were being used\n# to process the results.\ndbClearResult(results)   \n\n# These are all the rows\nrows \n\n   title_id                          title_name       type pub_id pages price   sales             pubdate\n1       T01                               1977!    history    P01   107 21.99     566 2000-08-01 00:00:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566 1998-04-01 00:00:00\n3       T03       Ask Your System Administrator   computer    P02  1226 39.95   25667 2000-09-01 00:00:00\n4       T04          But I Did It Unconsciously psychology    P01   510 12.99   13001 1999-05-31 00:00:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440 2001-01-01 00:00:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320 2000-07-31 00:00:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200 1999-10-01 00:00:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095 2001-06-01 00:00:00\n9       T09                     Kiss My Boo-Boo   children    P01    22 13.95    5000 2002-05-31 00:00:00\n10      T10          Not Without My Faberge Egg  biography    P05                                        \n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 2000-11-30 00:00:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001 2000-08-31 00:00:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467 1999-05-31 00:00:00\n\n# The default for dbFetch is to retrieve ALL of the records.\n# In the following examples all of the records are retrieved at once.\n\nresults &lt;- dbSendQuery(conBooks, \"select * from titles \n                                  where type='biography'\")\nresults           # This is NOT the actual data - it is a \"handle\" that you need to use with other functions\n\n&lt;SQLiteResult&gt;\n  SQL  select * from titles \n                                  where type='biography'\n  ROWS Fetched: 0 [incomplete]\n       Changed: 0\n\ndbFetch(results)  # get all of the data\n\n  title_id                 title_name      type pub_id pages price   sales             pubdate\n1      T06           How About Never? biography    P01   473 19.95   11320 2000-07-31 00:00:00\n2      T07          I Blame My Mother biography    P03   333 23.95 1500200 1999-10-01 00:00:00\n3      T10 Not Without My Faberge Egg biography    P05                                        \n4      T12  Spontaneous, Not Annoying biography    P05   507 12.99  100001 2000-08-31 00:00:00\n\ndbFetch(results)  # can't get it again\n\n[1] title_id   title_name type       pub_id     pages      price      sales      pubdate   \n&lt;0 rows&gt; (or 0-length row.names)\n\ndbClearResult(results)  # clear the computer memory/resources after you're finished\n\n# You can also retrieve the rest of the rows that haven't been retrieved yet.\nresults &lt;- dbSendQuery(conBooks, \"select * from authors\")\ndbFetch(results, n=2)  # get the first 2 rows \n\n  au_id au_fname  au_lname        phone          address    city state   zip\n1   A01    Sarah   Buchman 718-496-7223   75 West 205 St   Bronx    NY 10468\n2   A02    Wendy Heydemark 303-986-7020 2922 Baseline Rd Boulder    CO 80303\n\ndbFetch(results, n=-1)  # get the rest of the rows \n\n  au_id  au_fname    au_lname        phone              address          city state   zip\n1   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n2   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n3   A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014\n4   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305\n5   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236\n\ndbClearResult(results)  # clear the computer memory/resources after you're finished\n\n\n\n\ndbListTables(conBooks)    # packages DBI, odbc\n\n[1] \"authors\"       \"publishers\"    \"royalties\"     \"title_authors\" \"titles\"       \n\n# create a table\ndbExecute(conBooks, 'CREATE TABLE test_table(id int, name text)') # packages DBI\n\n[1] 0\n\ndbListTables(conBooks)                                            # packages DBI, odbc\n\n[1] \"authors\"       \"publishers\"    \"royalties\"     \"test_table\"    \"title_authors\" \"titles\"       \n\n# insert a row\ndbExecute(conBooks, \"insert into test_table values (1, 'hello')\")\n\n[1] 1\n\n# query the data in the table\ndbGetQuery(conBooks, \"select * from test_table\")\n\n  id  name\n1  1 hello\n\n# drop the table\ndbExecute(conBooks, 'DROP TABLE IF EXISTS test_table') \n\n[1] 0\n\ndbListTables(conBooks)      \n\n[1] \"authors\"       \"publishers\"    \"royalties\"     \"title_authors\" \"titles\"       \n\n#---------------------------------------------------------------------.\n# Connecting to a PostgreSQL database\n#\n# See the info here:  https://hevodata.com/learn/rpostgresql/\n#---------------------------------------------------------------------.\n# Many RDBMS (Relational Database Management System) software products\n# are designed for use with massive amounts of data. They are\n# often more complicated software products that have many options for optimizing\n# how they work. These types of RDBMS products are often run on a separate\n# computer. PostgreSQL is such a product. Connecting to it requires\n# knowledge of the following info:\n#\n#   - The name of the database. A database is a collection of tables\n#     The same PostgreSQL software can manage several different databases.\n#\n#   - The \"name\" of the computer that is running the PostgreSQL software.\n#     This could be in the form of\n#\n#         *  a \"domain name\", e.g. somedomain.com,\n#         *  an IP address, e.g. 192.168.15.1\n#         * \"domain name\", e.g. somedomain.com,\n#     \n#     Every website on the internet is running on a computer. The \n#     domain name \n#\n#\ndsn_database = \"catering\"   # Specify the name of your Database\n\n# Specify host name e.g.:\"aws-us-east-1-portal.4.dblayer.com\"\n# 127.0.0.1 is an IP address that refers to your computer\ndsn_hostname = \"127.0.0.1\"  \n\n# see: https://dba.stackexchange.com/questions/41458/changing-postgresql-port-using-command-line\n#  or: https://stackoverflow.com/questions/15100368/postgresql-port-confusion-5433-or-5432\n#\n# find the file \"postgresql.conf\" \n# on my Windows computer it is in the following folder:\n#   \\Program Files\\PostgreSQL\\15\\data\n# There should be a line in that file that says: port = SOME_NUMBER\ndsn_port = \"5432\"     # Specify your port number. e.g. 98939\n#dsn_port = \"5050\"     # Specify your port number. e.g. 98939\n#dsn_port = \"65335\"     # Specify your port number. e.g. 98939\n\n# this is the default user id\ndsn_uid = \"postgres\"         # Specify your username. e.g. \"admin\"\n#dsn_uid = \"pgadmin4\"         # Specify your username. e.g. \"admin\"\n#dsn_uid = \"pgadmin4\"         # Specify your username. e.g. \"admin\"\n\n# this is whatever you set your password to\ndsn_pwd = \"password\"        # Specify your password. e.g. \"xxx\"\n#\n# On March 30, 2023,\n# I got the following error when trying to connect R \n# to postgres 15.2 on Windows 10 Home:\n#\n#      SCRAM authentication requires libpq version 10 or above\n#\n# This webpage:\n#   https://stackoverflow.com/questions/62807717/how-can-i-solve-postgresql-scram-authentication-problem\n# says the following:\n#\n#   &gt; Your application uses an API that is linked with the PostgreSQL client C library libpq.\n#   &gt; The version of that library must be 9.6 or older, and SCRAM authentication was introduced in v10.\n#   &gt; Upgrade libpq on the application end and try again.\n#   &gt; \n#   &gt; If you don't need scram-sha-256 authentication, you can revert to md5:\n#   &gt; \n#   &gt; * set password_encryption = md5 in postgresql.conf\n#   &gt; * change the authentication method to md5 in pg_hba.conf\n#   &gt; * reload PostgreSQL\n#   &gt; * change the password of the user to get an MD5 encrypted password\n#\n# I followed the 2nd suggestion - i.e. \n#   &gt; * set password_encryption = md5 in postgresql.conf\n#   &gt; * change the authentication method to md5 in pg_hba.conf\n#   &gt; * reload PostgreSQL\n#   &gt; * change the password of the user to get an MD5 encrypted password\n#\n# -YR \n#---------------------------------------------------------------------.\n\n# You will need both the DBI and RPostgreSQL packages\n\nif(!require(DBI)) {install.packages(\"DBI\"); require(DBI)}\nif(!require(RPostgreSQL)) {install.packages(\"RPostgreSQL\"); require(RPostgreSQL)}\n\nLoading required package: RPostgreSQL\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : there is no package called 'RPostgreSQL'\n\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\nhelp(package=\"DBI\")\nhelp(package=\"RPostgreSQL\")\n\nError in find.package(pkgName, lib.loc, verbose = verbose): there is no package called 'RPostgreSQL'\n\ndsn_database = \"catering\"   # Specify the name of your Database\n\n# Specify host name e.g.:\"aws-us-east-1-portal.4.dblayer.com\"\n# 127.0.0.1 is an IP address that refers to your computer\ndsn_hostname = \"127.0.0.1\"  \n\n# see: https://dba.stackexchange.com/questions/41458/changing-postgresql-port-using-command-line\n#  or: https://stackoverflow.com/questions/15100368/postgresql-port-confusion-5433-or-5432\n#\n# find the file \"postgresql.conf\" \n# on my Windows computer it is in the following folder:\n#   \\Program Files\\PostgreSQL\\15\\data\n# There should be a line in that file that says: port = SOME_NUMBER\ndsn_port = \"5432\"     # Specify your port number. e.g. 98939\n#dsn_port = \"5050\"     # Specify your port number. e.g. 98939\n#dsn_port = \"65335\"     # Specify your port number. e.g. 98939\n\n# this is the default user id\ndsn_uid = \"postgres\"         # Specify your username. e.g. \"admin\"\n#dsn_uid = \"pgadmin4\"         # Specify your username. e.g. \"admin\"\n#dsn_uid = \"pgadmin4\"         # Specify your username. e.g. \"admin\"\n\n# this is whatever you set your password to\ndsn_pwd = \"password\"        # Specify your password. e.g. \"xxx\"\n\n# Call dbConnect to connect to the database\ntryCatch({\n  drv &lt;- dbDriver(\"PostgreSQL\")      \n  print(\"Connecting to Database…\")\n  conCatering &lt;- dbConnect(drv, \n                      dbname = dsn_database,\n                      host = dsn_hostname, \n                      port = dsn_port,\n                      user = dsn_uid, \n                      password = dsn_pwd)\n  print(\"Database Connected!\")\n},\nerror=function(cond) {\n  print(\"Unable to connect to Database.\")\n  print(cond)\n})\n\n[1] \"Unable to connect to Database.\"\n&lt;simpleError: Couldn't find driver PostgreSQL. Looked in:\n* global namespace\n* in package called PostgreSQL\n* in package called RPostgreSQL&gt;\n\n# You can now use any of the DBI functions by passing the connection\n# object for the catering database.\n\n# Use dbGetQuery to get instant results from select statements.\ndbGetQuery(conCatering, \"select * from waiters\")\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbGetQuery': object 'conCatering' not found\n\n# Use dbSendQuery, dbFetch, dbClearResult to get only some rows at a time.\nresults &lt;- dbSendQuery(conCatering, \"select * from parties\") \n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbSendQuery': object 'conCatering' not found\n\ndbFetch(results, n=2)  # get first two rows\n\nError: Invalid result set\n\ndbFetch(results)       # get the rest of the rows\n\nError: Invalid result set\n\ndbClearResult(results)  # clear the computer memory/resources after you're finished\n\nWarning: Expired, result set already closed\n\n# Use dbListTables to see the tables in the database\n# Use dbExecute to run other SQL commands - eg. create or drop a table\ndbListTables(conCatering)      \n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbListTables': object 'conCatering' not found\n\ndbExecute(conCatering, 'CREATE TABLE test_table(id int, name text)') # packages DBI\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbExecute': object 'conCatering' not found\n\ndbListTables(conCatering)                                            # packages DBI, odbc\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbListTables': object 'conCatering' not found\n\n# insert a row, query the row, then drop the table\ndbExecute (conCatering, \"insert into test_table values (1, 'hello')\")\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbExecute': object 'conCatering' not found\n\ndbGetQuery(conCatering, \"select * from test_table\")\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbGetQuery': object 'conCatering' not found\n\ndbExecute (conCatering, 'DROP TABLE IF EXISTS test_table') \n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbExecute': object 'conCatering' not found\n\ndbListTables(conCatering)      # the table is gone\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbListTables': object 'conCatering' not found",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'><yrChapterNumber>37b. </yrChapterNumber> Working directly with databases in R</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html",
    "href": "sql0935-dbNormalization-v001.html",
    "title": "39  38.  Database Normalization",
    "section": "",
    "text": "39.1 Normal Forms\nDatabase normalization is accomplished in several steps. There are different “levels” of normalization, each level progressing towards a more perfect organization of the data. Each level is called a “Normal Form”. We say that a database is “in” a particular normal form when the tables are structured in a way that conforms to the rules of that normal form. This will all become much more clear as you progress through the material below. In a fully normalized database the only data that should be repeated are the primary/foreign keys.\nThe following are the various normal forms. You are not expected to understand the technical terms in the list below. These will be covered in the material that follows this section.\nMost database designers aim to have their databases at least in 3rd Normal Form. However, the following additional normal forms are used to further “normalize” a database to deal with special cases.\nIn this material we will only be covering 1st, 2nd and 3rd normal forms. The list below is provided in case you want to do your own research.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html#normal-forms",
    "href": "sql0935-dbNormalization-v001.html#normal-forms",
    "title": "39  38.  Database Normalization",
    "section": "",
    "text": "1NF (1st Normal Form) - only atomic values in columns, no repeating groups or arrays\n2NF (2nd Normal Form) - no “partial functional dependencies (pFD)”\n3NF (3rd Normal Form) - no “transitive functional dependencies (tFD)”\n\n\n\n\nBocye-Codd Normal Form (BCNF) - every determinant (attribute that determines other attributes) must be a candidate key\n4NF - no multi-valued dependencies\n5NF - tables cannot be losslessly decomposed into smaller tables\n6NF - tables must be irreducible, containing no non-trivial join dependencies",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html#first-normal-form-1nf",
    "href": "sql0935-dbNormalization-v001.html#first-normal-form-1nf",
    "title": "39  38.  Database Normalization",
    "section": "39.2 First Normal Form (1NF)",
    "text": "39.2 First Normal Form (1NF)\nFirst normal form requires that\n\nThere be only one value in each “cell” of a table.\n(AKA “all attributes contain only atomic values”)\nThere should NOT be multiple columns for the same type of data\n(AKA There are no “repeating groups”.)\n\nThe following tables are not in 1st normal form (1NF).\n\nThis table contains multiple values in a single cell which violates the rules of 1NF\n\n\n\norders\n\n\norderNum\nproducts\n\n\n\n\nord100\n3 apples, 2 plums\n\n\nord200\n5 apples, 3 pears, 10 plums\n\n\n\n\n\nThis table contains multiple columns for similar types of data. This also violates the rules of 1NF.\n\n\n\norders\n\n\n\n\n\n\n\n\n\n\n\norderNum\nproduct1\nquantity1\nproduct2\nquantity2\nproduct1.1\nquantity3\n\n\n\n\nord100\napples\n3\nplums\n2\nNULL\nNULL\n\n\nord200\napples\n5\npears\n3\nplums\n10\n\n\n\n\n\n\n\n39.2.1 Converting the data to 1NF\nThe following table contains the same data in first normal form:\n::: {.cell} ::: {.cell-output-display}\n\norders\n\n\norderNum\nproduct\nquantity\n\n\n\n\nord100\napples\n3\n\n\nord100\nplums\n2\n\n\nord200\napples\n5\n\n\nord200\npears\n3\n\n\nord200\nplums\n10\n\n\n\n::: :::",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html#example---college-class-offerings",
    "href": "sql0935-dbNormalization-v001.html#example---college-class-offerings",
    "title": "39  38.  Database Normalization",
    "section": "39.3 Example - college class offerings",
    "text": "39.3 Example - college class offerings\nThe data shown below about classes, sections and professors can be arranged in many different formats.\n\n39.3.1 All data in a single table\nFor example, the following data could be in a single table. This makes it very easy for people to read the info.\n\n\n\nclassOfferings\n\n\n\n\n\n\n\n\n\n\n\n\n\nclassName\nclassNum\ncredits\nsection\nroom\nprofessor\noffice\nemail\ntel\n\n\n\n\nIntro to IDS\nIDS1020\n3\n231\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n241\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n331\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nIntro to IDS\nIDS1020\n3\n341\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nDatabase Design\nIDS2040\n4\n133\nF250\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\n\n\n\n\n\n39.3.2 “Normalize” tables to reduce duplication of data\nIf you look closely at the data in the example above, you’ll notice that the details for each professor (i.e. name, office, email, tel) is duplicated in multiple rows. This makes it very easy for people to read the data. However, this is NOT a great way to store information in a relational database (keep reading).\nHaving the same data in more than one place in a database can lead to inconsistencies in the data. For example if a database records a person’s name in more than one place and the person changes their name, then all copies of the person’s name would need to be updated. If due to some miscalculation, some copies of the person’s name were updated and some weren’t updated, the database would be inconsistent. One of the major goals of a good database design is to eliminate redundant data. All data, other than primary keys and foreign keys, should be recorded in one and only one place in the database.\n\n\n39.3.3 split professor data into a different table\nThe data shown above can be reorganized into the following structure to avoid duplicating each professor’s details.\n\n\n\nofferings\n\n\nclassName\nclassNum\ncredits\nsection\nroom\nprof_id\n\n\n\n\nIntro to IDS\nIDS1020\n3\n231\nF201\nP100\n\n\nIntro to IDS\nIDS1020\n3\n241\nF201\nP100\n\n\nIntro to IDS\nIDS1020\n3\n331\nF310\nP200\n\n\nIntro to IDS\nIDS1020\n3\n341\nF310\nP200\n\n\nDatabase Design\nIDS2040\n4\n133\nF250\nP100\n\n\n\n\n\n\nprofessors\n\n\nprof_id\nfirst\nlast\noffice\nemail\ntel\n\n\n\n\nP100\njoe\njones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nP200\nsue\nsmith\nB231\nsue@abc.edu\n(212)999-9999\n\n\n\n\n\n\nSQL SELECT to regenerate original “all in one” table\nWe are able to see the data as it was originally presented by using the following SQL query:\n\nqueryResults = \n  sqldf(\"\n    select className, classNum, credits, section, room, first || ' ' || last as professor, office, email, tel\n    from offerings join professors on offerings.prof_id = professors.prof_id\n    order by classNum, section\n  \")\n\nkable(queryResults, caption = \"queryResults\")\n\n\nqueryResults\n\n\n\n\n\n\n\n\n\n\n\n\n\nclassName\nclassNum\ncredits\nsection\nroom\nprofessor\noffice\nemail\ntel\n\n\n\n\nIntro to IDS\nIDS1020\n3\n231\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n241\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n331\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nIntro to IDS\nIDS1020\n3\n341\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nDatabase Design\nIDS2040\n4\n133\nF250\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\n\n\n\n\n\n\n39.3.4 Split common class info into a different table\nThe same data can be still further reorganized into the following structure to avoid duplicating the details for each class:\n\n\n\nofferings\n\n\ncrn\nclassName\nclassNum\ncredits\n\n\n\n\n12100\nIntro to IDS\nIDS1020\n3\n\n\n14301\nDatabase Design\nIDS2040\n4\n\n\n\n\n\n\nofferings\n\n\ncrn\nsection\nroom\nprof_id\n\n\n\n\n12100\n231\nF201\nP100\n\n\n12100\n241\nF201\nP100\n\n\n12100\n331\nF310\nP200\n\n\n12100\n341\nF310\nP200\n\n\n14301\n133\nF250\nP100\n\n\n\n\n\n\nprofessors\n\n\nprof_id\nfirst\nlast\noffice\nemail\ntel\n\n\n\n\nP100\njoe\njones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nP200\nsue\nsmith\nB231\nsue@abc.edu\n(212)999-9999\n\n\n\n\n\n\nSQL SELECT to regenerate original “all in one” table\nWe are able to see the data as it was originally presented by using the following SQL query:\n\nqueryResults = \n  sqldf(\"\n    select className, classNum, credits, section, room, \n           first || ' ' || last as professor, office, email, tel\n    from classes join offerings on classes.crn = offerings.crn \n                 join professors on offerings.prof_id = professors.prof_id\n    order by classNum, section\n  \")\n\nkable(queryResults, caption = \"queryResults\")\n\n\nqueryResults\n\n\n\n\n\n\n\n\n\n\n\n\n\nclassName\nclassNum\ncredits\nsection\nroom\nprofessor\noffice\nemail\ntel\n\n\n\n\nIntro to IDS\nIDS1020\n3\n231\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n241\nF201\njoe jones\nB102\njoe@abc.edu\n(212)555-5555\n\n\nIntro to IDS\nIDS1020\n3\n331\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nIntro to IDS\nIDS1020\n3\n341\nF310\nsue smith\nB231\nsue@abc.edu\n(212)999-9999\n\n\nDatabase Design\nIDS2040\n4\n133\nF250\njoe jones\nB102\njoe@abc.edu\n(212)555-5555",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html#second-normal-form-2nf-and-third-normal-form-3nf",
    "href": "sql0935-dbNormalization-v001.html#second-normal-form-2nf-and-third-normal-form-3nf",
    "title": "39  38.  Database Normalization",
    "section": "39.4 Second Normal Form (2NF) and Third Normal Form (3NF)",
    "text": "39.4 Second Normal Form (2NF) and Third Normal Form (3NF)\nA prerequisite to understanding 2nd and 3rd normal forms is to understand what a “functional dependency (FD)” is.\n\n39.4.1 What is a Functional Dependency (FD)\nA functional dependency describes a relationship between columns (attributes) in a table.\nA functional dependency is a relationship between the columns in a table where the values in one column (or a set of columns) uniquely determines the values in another column. If the value in column A determines the value in column B (denoted as A → B), then for any given value of A, there can only ever be exactly one value of B associated with it. For example, if StudentID → StudentName, then knowing a specific StudentID uniquely identifies the corresponding StudentName.\n\n\n39.4.2 Functional Dependency Rules: X, Y → A, B, C\nA functional dependency rule can be written in the following format where A,B,C,D are column names.\nA -&gt; B,C,D\n\nThe left-hand side (LHS) contains the determinant(s) — the column(s) that determine the values of other columns.\nThe right-hand side (RHS) contains the dependent column(s) — the values that are determined.\nBoth the left hand side and the right hand side can have any number of columns.\n\nFor example a functional dependency can also take a form similar to:\nA,B -&gt; C,D,E\nWhere the values in columns A and B taken together functionally determine the values of columns C,D and E.\n\n\n39.4.3 Example Table: StudentCourse\n\n\n\n\n\n\n\n\n\n\n\n\nStudentID\nFirstName\nLastName\nCourseID\nCourseName\nCredits\nGrade\n\n\n\n\n001\nAlice\nSmith\nC101\nIntro to Math\n3\nA\n\n\n002\nBob\nJones\nC101\nIntro to Math\n3\nB\n\n\n001\nAlice\nSmith\nC102\nEnglish 101\n4\nA-\n\n\n\nThe functional dependencies are:\n\nStudentID → FirstName, LastName\n\nA student ID uniquely determines the student’s name.\n\nCourseID → CourseName, Credits\n\nA course ID determines the course details.\n\nStudentID, CourseID → Grade\n\nThe composite key of student and course determines the grade.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql0935-dbNormalization-v001.html#primary-key-pk-candidate-pk-composite-pk",
    "href": "sql0935-dbNormalization-v001.html#primary-key-pk-candidate-pk-composite-pk",
    "title": "39  38.  Database Normalization",
    "section": "39.5 Primary Key (PK), candidate PK, composite PK",
    "text": "39.5 Primary Key (PK), candidate PK, composite PK\nWe simplified the last functional dependency rule a little in order to highlight that StudentId and CourseId determine the Grade value.\nHowever, if you look carefully you should notice that since SudentId determines all of the student related details and CourseId determines all of the Course details, and these are ALL of the columns in the table. We can write that as:\nStudentID, CourseID → FirstName, LastName, CourseName, Credicts, Grade\nSince this functional dependency rule includes all of the columns in the table, we say that StudentId, CourseId is a candidate primary key for the table. If there would be more than one candidate PK (there isn’t in this table) then one of the candidate PK’s would be chosen as THE primary key for the table.\nA Primary Key that is the combination of more than one column is known as a composite primary key.\n\n39.5.1 Starting Point: A 1NF table\nLet’s begin with a table that tracks student course enrollments along with associated information. The following table is already in first normal form (see above).\n\n# Create the initial non-normalized table\nfirst_normal_form &lt;- data.frame(\n  StudentID = c(\"001\", \"002\", \"001\", \"003\", \"002\"),\n  FirstName = c(\"Alice\", \"Bob\", \"Alice\", \"Carol\", \"Bob\"),\n  LastName = c(\"Smith\", \"Jones\", \"Smith\", \"Davis\", \"Jones\"),\n  CourseID = c(\"C101\", \"C101\", \"C102\", \"C103\", \"C103\"),\n  CourseName = c(\"Intro to Math\", \"Intro to Math\", \"English 101\", \"Calculus I\", \"Calculus I\"),\n  Credits = c(3, 3, 4, 4, 4),\n  DepartmentID = c(\"D01\", \"D01\", \"D02\", \"D01\", \"D01\"),\n  DepartmentName = c(\"Mathematics\", \"Mathematics\", \"English\", \"Mathematics\", \"Mathematics\"),\n  DepartmentHead = c(\"Dr. Johnson\", \"Dr. Johnson\", \"Dr. Williams\", \"Dr. Johnson\", \"Dr. Johnson\"),\n  DeptPhone = c(\"(555) 123-4567\", \"(555) 123-4567\", \"(555) 987-6543\", \"(555) 123-4567\", \"(555) 123-4567\"),\n  DeptOffice = c(\"Room A100\", \"Room A100\", \"Room B200\", \"Room A100\", \"Room A100\"),\n  Grade = c(\"A\", \"B\", \"A-\", \"B+\", \"C+\"),\n  stringsAsFactors = FALSE\n)\n\n# Display the non-normalized table\nlibrary(knitr)\nkable(first_normal_form, caption = \"First Normal Form Student Course Data\")\n\n\nFirst Normal Form Student Course Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudentID\nFirstName\nLastName\nCourseID\nCourseName\nCredits\nDepartmentID\nDepartmentName\nDepartmentHead\nDeptPhone\nDeptOffice\nGrade\n\n\n\n\n001\nAlice\nSmith\nC101\nIntro to Math\n3\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nA\n\n\n002\nBob\nJones\nC101\nIntro to Math\n3\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nB\n\n\n001\nAlice\nSmith\nC102\nEnglish 101\n4\nD02\nEnglish\nDr. Williams\n(555) 987-6543\nRoom B200\nA-\n\n\n003\nCarol\nDavis\nC103\nCalculus I\n4\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nB+\n\n\n002\nBob\nJones\nC103\nCalculus I\n4\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nC+\n\n\n\n\n\nThis table contains information about students, their courses, grades, and detailed department information. Let’s normalize this table step by step.\n\n\n39.5.2 2NF - Second Normal Form\nA table is in Second Normal Form (2NF) if:\n\nIt is already in 1NF\nAll non-key columns are fully functionally dependent on the entire primary key (not just part of it)\n\nIn our table, the primary key is the composite key (StudentID, CourseID) since together they uniquely identify each row.\nLet’s identify the functional dependencies:\n\nStudentID → FirstName, LastName\n\nA student ID uniquely determines the student’s name.\n\nCourseID → CourseName, Credits, DepartmentID\n\nA course ID determines the course details.\n\nDepartmentID → DepartmentName, DepartmentHead, DeptPhone, DeptOffice\n\nA department ID determines the department details.\n\nStudentID, CourseID → Grade\n\nThe combination of student and course determines the grade.\n\n\nWe have partial dependencies: - Student column values depend only on StudentID (part of the primary key) - Course column values depend only on CourseID (part of the primary key) - Department column values depend only on DepartmentID (which depends on CourseID)\nTo achieve 2NF, we need to remove these partial dependencies by creating separate tables:\n\n# Create StudentCourse table (2NF)\nstudent_course_2nf &lt;- first_normal_form %&gt;%\n  select(StudentID, CourseID, Grade)\n\n# Create Student table (2NF)\nstudent_2nf &lt;- first_normal_form %&gt;%\n  select(StudentID, FirstName, LastName) %&gt;%\n  distinct()\n\n# Create Course table (2NF but not yet in 3NF)\ncourse_2nf &lt;- first_normal_form %&gt;%\n  select(CourseID, CourseName, Credits, DepartmentID, DepartmentName, DepartmentHead, DeptPhone, DeptOffice) %&gt;%\n  distinct()\n\n# Display the 2NF tables\nkable(student_course_2nf, caption = \"StudentCourse Table (2NF)\")\n\n\nStudentCourse Table (2NF)\n\n\nStudentID\nCourseID\nGrade\n\n\n\n\n001\nC101\nA\n\n\n002\nC101\nB\n\n\n001\nC102\nA-\n\n\n003\nC103\nB+\n\n\n002\nC103\nC+\n\n\n\n\nkable(student_2nf, caption = \"Student Table (2NF)\")\n\n\nStudent Table (2NF)\n\n\nStudentID\nFirstName\nLastName\n\n\n\n\n001\nAlice\nSmith\n\n\n002\nBob\nJones\n\n\n003\nCarol\nDavis\n\n\n\n\nkable(course_2nf, caption = \"Course Table (2NF but not yet in 3NF)\")\n\n\nCourse Table (2NF but not yet in 3NF)\n\n\n\n\n\n\n\n\n\n\n\n\nCourseID\nCourseName\nCredits\nDepartmentID\nDepartmentName\nDepartmentHead\nDeptPhone\nDeptOffice\n\n\n\n\nC101\nIntro to Math\n3\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\n\n\nC102\nEnglish 101\n4\nD02\nEnglish\nDr. Williams\n(555) 987-6543\nRoom B200\n\n\nC103\nCalculus I\n4\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\n\n\n\n\n\nNow we have resolved the partial dependencies related to StudentID. The Course table is in 2NF with respect to its primary key CourseID, but we have another issue to address for 3NF.\n\n\n39.5.3 3NF - Third Normal Form\nA table is in Third Normal Form (3NF) if:\n\nIt is already in 2NF\nIt has no transitive dependencies (non-key attributes depending on other non-key attributes)\n\nLooking at the Course table, we notice a transitive dependency:\n\nCourseID → DepartmentID (directly)\nDepartmentID → DepartmentName, DepartmentHead, DeptPhone, DeptOffice (directly)\nTherefore, CourseID → DepartmentName, DepartmentHead, DeptPhone, DeptOffice (transitively)\n\nThis is a classic transitive dependency where course details transitively determine department details through the DepartmentID. To achieve 3NF, we need to remove this transitive dependency:\n\n# Create Course table (3NF)\ncourse_3nf &lt;- course_2nf %&gt;%\n  select(CourseID, CourseName, Credits, DepartmentID) %&gt;%\n  distinct()\n\n# Create Department table (3NF)\ndepartment_3nf &lt;- course_2nf %&gt;%\n  select(DepartmentID, DepartmentName, DepartmentHead, DeptPhone, DeptOffice) %&gt;%\n  distinct()\n\n# Display the 3NF tables\nkable(course_3nf, caption = \"Course Table (3NF)\")\n\n\nCourse Table (3NF)\n\n\nCourseID\nCourseName\nCredits\nDepartmentID\n\n\n\n\nC101\nIntro to Math\n3\nD01\n\n\nC102\nEnglish 101\n4\nD02\n\n\nC103\nCalculus I\n4\nD01\n\n\n\n\nkable(department_3nf, caption = \"Department Table (3NF)\")\n\n\nDepartment Table (3NF)\n\n\n\n\n\n\n\n\n\nDepartmentID\nDepartmentName\nDepartmentHead\nDeptPhone\nDeptOffice\n\n\n\n\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\n\n\nD02\nEnglish\nDr. Williams\n(555) 987-6543\nRoom B200\n\n\n\n\n\nNow all tables are in 3NF: - The StudentCourse table has only the Grade dependent on the full primary key (StudentID, CourseID) - The Student table has FirstName and LastName dependent on StudentID - The Course table has CourseName and Credits dependent on CourseID, with DepartmentID as a foreign key - The Department table has all department details dependent on DepartmentID\nAll non-key attributes in each table now depend directly on the primary key, not on any other non-key attributes.\n\n\n39.5.4 Summary of Our Normalization Process\nLet’s visualize the relationships between our normalized tables:\n\n# Display all tables in their final form\ncat(\"Final Normalized Tables:\\n\\n\")\n\nFinal Normalized Tables:\n\ncat(\"StudentCourse Table (Primary Key: StudentID, CourseID):\\n\")\n\nStudentCourse Table (Primary Key: StudentID, CourseID):\n\nkable(student_course_2nf)\n\n\n\n\nStudentID\nCourseID\nGrade\n\n\n\n\n001\nC101\nA\n\n\n002\nC101\nB\n\n\n001\nC102\nA-\n\n\n003\nC103\nB+\n\n\n002\nC103\nC+\n\n\n\n\ncat(\"\\n\\n\")\ncat(\"Student Table (Primary Key: StudentID):\\n\")\n\nStudent Table (Primary Key: StudentID):\n\nkable(student_2nf)\n\n\n\n\nStudentID\nFirstName\nLastName\n\n\n\n\n001\nAlice\nSmith\n\n\n002\nBob\nJones\n\n\n003\nCarol\nDavis\n\n\n\n\ncat(\"\\n\\n\")\ncat(\"Course Table (Primary Key: CourseID):\\n\")\n\nCourse Table (Primary Key: CourseID):\n\nkable(course_3nf)\n\n\n\n\nCourseID\nCourseName\nCredits\nDepartmentID\n\n\n\n\nC101\nIntro to Math\n3\nD01\n\n\nC102\nEnglish 101\n4\nD02\n\n\nC103\nCalculus I\n4\nD01\n\n\n\n\ncat(\"\\n\\n\")\ncat(\"Department Table (Primary Key: DepartmentID):\\n\")\n\nDepartment Table (Primary Key: DepartmentID):\n\nkable(department_3nf)\n\n\n\n\n\n\n\n\n\n\n\nDepartmentID\nDepartmentName\nDepartmentHead\nDeptPhone\nDeptOffice\n\n\n\n\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\n\n\nD02\nEnglish\nDr. Williams\n(555) 987-6543\nRoom B200\n\n\n\n\n\n\n\n39.5.5 Join Example to Reconstruct Original Data\nWe can use joins to reconstruct the original data from our normalized tables:\n\n# Join all tables to reconstruct the original data\nreconstructed_data &lt;- student_course_2nf %&gt;%\n  left_join(student_2nf, by = \"StudentID\") %&gt;%\n  left_join(course_3nf, by = \"CourseID\") %&gt;%\n  left_join(department_3nf, by = \"DepartmentID\") %&gt;%\n  select(StudentID, FirstName, LastName, CourseID, CourseName, Credits, \n         DepartmentID, DepartmentName, DepartmentHead, DeptPhone, DeptOffice, Grade)\n\n# Display the reconstructed data\nkable(reconstructed_data, caption = \"Reconstructed Data from Normalized Tables\")\n\n\nReconstructed Data from Normalized Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudentID\nFirstName\nLastName\nCourseID\nCourseName\nCredits\nDepartmentID\nDepartmentName\nDepartmentHead\nDeptPhone\nDeptOffice\nGrade\n\n\n\n\n001\nAlice\nSmith\nC101\nIntro to Math\n3\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nA\n\n\n002\nBob\nJones\nC101\nIntro to Math\n3\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nB\n\n\n001\nAlice\nSmith\nC102\nEnglish 101\n4\nD02\nEnglish\nDr. Williams\n(555) 987-6543\nRoom B200\nA-\n\n\n003\nCarol\nDavis\nC103\nCalculus I\n4\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nB+\n\n\n002\nBob\nJones\nC103\nCalculus I\n4\nD01\nMathematics\nDr. Johnson\n(555) 123-4567\nRoom A100\nC+\n\n\n\n\n\n\n\n39.5.6 Benefits of Normalization\n\nEliminating redundancy: Notice how department information was repeated multiple times in our original table but appears only once in our final 3NF design.\nPreventing update anomalies: If Dr. Johnson’s phone number changes, we only need to update one row in the Department table, not multiple rows as in the original table.\nImproving data integrity: Each entity (Student, Course, Department) is modeled in its own table with appropriate primary keys and relationships.\nSimplifying queries: Complex data can be reconstructed using JOIN operations while maintaining a clean, normalized structure.\n\nEach normal form builds upon the previous one, addressing specific types of data redundancy and potential anomalies. By following these normalization principles, you can create database designs that are more efficient, less prone to errors, and easier to maintain.",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'><yrChapterNumber>38. </yrChapterNumber> Database Normalization</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html",
    "href": "sql3000-sqlQuestions-v001.html",
    "title": "40   Database Questions (and Answers)",
    "section": "",
    "text": "40.1.5 Single Table queries",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#yrBooksDatabase",
    "href": "sql3000-sqlQuestions-v001.html#yrBooksDatabase",
    "title": "40   Database Questions (and Answers)",
    "section": "40.1 The “Books Database”",
    "text": "40.1 The “Books Database”\nThe “books database” is a collection of several tables that contains data relating to books (i.e. “titles”), authors, publishers, etc.\nYou can download the data for the database in the following ways:\n\nas a set of several CSV files or (see below)\nas a single sqlite3 database file (see below)\n\nSee below for details.\n\n40.1.1 SQLite file for “Books Database”\nClick here to download all the data for the books datbase as a single “SQLite” database file.\nTo use this file you can do one of the following:\n\nInstall SQLite software on your computer. There are many such programs avaiable. I recommend “DB Viewer for SQLite” which is a free open source program.\nWebsite (download from here): https://sqlitebrowser.org/\nGithub repository: https://github.com/sqlitebrowser/sqlitebrowser\nYou can search online for other similar programs for working with sqlite3 files.\nFree web based tool: https://y-rosenthal.github.io/yrSqliteViewer/yrSqliteViewer.html\nThis is a website that lets you upload a sqlite3 file and run SQL Select statements against the data. I created this website myself (with the help of some AI coding assistants).\nPROs: you don’t need to install anything\nCONs: currently only works with SQL SELECT statment but not any other types of sql statements.\nSee this chapter (in this book) https://y-rosenthal.github.io/DataManagementUsingR/sql0950-workingWithDatabasesInR-v010.html for instructions on how to access this database file directly via R commands.\n\n\n\n40.1.2 CSV files for “Books Database”\nDownload the CSV files for the “books database”. These CSV files together comprise the data for the “books database”.\n\ntitles.csv\nauthors.csv\npublishers.csv\ntitle_authors.csv\nroyalties.csv\n\nDownload a tutorial on using Relational databases.\n\nbooksDatabase description and Questions\n\nThis word document contains:\n\na complete description of the “books database”\na tutorial on how “relational databases” are structured\na tutorial on Entity Relationship Diagrams (ERDs)\na set of questions and answers\n\nOnce you’ve downloaded the various CSV files you can run the following commands to import the data into R.\n\n\n40.1.3 Import the data (books database)\n\nif(!require(readr)){install.packages(\"readr\");require(readr);}\n\nLoading required package: readr\n\n# Read in the data for the books database - see the \ntitles = read_csv(\"titles.csv\", na=\"NULL\", show_col_types=FALSE)\nauthors = read_csv(\"authors.csv\", na=\"NULL\", show_col_types=FALSE)\npublishers = read_csv(\"publishers.csv\", na=\"NULL\", show_col_types=FALSE)\ntitle_authors = read_csv(\"title_authors.csv\", na=\"NULL\", show_col_types=FALSE)\nroyalties = read_csv(\"royalties.csv\", na=\"NULL\", show_col_types=FALSE)\n\n\n\n40.1.4 Books Database - Entity Relationship Diagram (ERD)\nThe following is an “Entity Relationship Diagram (ERD)” that describes the relationships between the tables. See the Word document linked above for more info:\n\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#0000ff', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'mainBkg': '#ffffff', 'background': '#ffffff', 'edgeLabelBackground': '#ffffff' }}}%%\nerDiagram\n    authors ||--o{ title_authors : \"\"\n    title_authors }|--|| titles : \"\"\n    titles ||--o| royalties : \"\"\n    publishers |o--o{ titles : \"\"\n    \n    authors {\n        string au_id PK\n        string au_fname\n        string au_lname\n        string phone\n        string address\n        string city\n        string state\n        string zip\n    }\n    \n    title_authors {\n        string title_id FK1\n        string au_id FK2\n        int au_order\n        float royalty_share\n    }\n    \n    titles {\n        string title_id PK\n        string title_name\n        string type\n        string pub_id FK\n        int pages\n        float price\n        int sales\n        date pubdate\n    }\n    \n    publishers {\n        string pub_id PK\n        string pub_name\n        string city\n        string state\n        string country\n    }\n    \n    royalties {\n        string title_id PK\n        float advance\n        float royalty_rate\n    }\n\n\n\n\n\n\nBelow are the contents of these tables:\n\n\n\n\n\n\nClick here - titles table\n\n\n\n\n\n\nsqldf(\"select * from titles\")\n\n   title_id                          title_name       type pub_id pages price   sales         pubdate\n1       T01                               1977!    history    P01   107 21.99     566   8/1/2000 0:00\n2       T02           200 Years of German Humor    history    P03    14 19.95    9566   4/1/1998 0:00\n3       T03        Ask Yor System Administrator   computer    P02  1226 39.95   25667   9/1/2000 0:00\n4       T04           But I Did It Unconciously psychology    P01   510 12.99   13001  5/31/1999 0:00\n5       T05              Exchange of Platitudes psychology    P01   201  6.95  201440   1/1/2001 0:00\n6       T06                    How About Never?  biography    P01   473 19.95   11320  7/31/2000 0:00\n7       T07                   I Blame My Mother  biography    P03   333 23.95 1500200  10/1/1999 0:00\n8       T08        Just Wait Until After School   children    P01    86 10.00    4095   6/1/2001 0:00\n9       T09                     Kiss My Boo Boo   children    P01    22 13.95    5000  5/31/2002 0:00\n10      T10         Not Without My Fabrerge Egg  biography    P05    NA    NA      NA            &lt;NA&gt;\n11      T11    Perhaps It's a Glandular Problem psychology    P05   826  7.99   94123 11/30/2000 0:00\n12      T12           Spontaneous, Not Annoying  biography    P05   507 12.99  100001  8/31/2000 0:00\n13      T13 What Are The Civilian Applications?    history    P03   802 29.99   10467  5/31/1999 0:00\n\n\n\n\n\n\n\n\n\n\n\nClick here - publishers table\n\n\n\n\n\n\nsqldf(\"select * from publishers\")\n\n  pub_id             pub_name          city state country\n1    P01    Abatis Publishers      New York    NY     USA\n2    P02      Core Dump Books San Francisco    CA     USA\n3    P03 Schandenfreude Press       Hamburg  &lt;NA&gt; Germany\n4    P04    Tneterhooks Press      Berkeley    CA     USA\n5    P05       AAA Publishing      Berkeley    CA     USA\n\n\n\n\n\n\n\n\n\n\n\nClick here - title_authors table\n\n\n\n\n\n\nsqldf(\"select * from title_authors\")\n\n   title_id au_id au_order royalty_shares\n1       T01   A01        1            1.0\n2       T02   A01        1            1.0\n3       T03   A05        1            1.0\n4       T04   A03        1            0.6\n5       T04   A04        2            0.4\n6       T05   A04        1            1.0\n7       T06   A02        1            1.0\n8       T07   A02        1            0.5\n9       T07   A04        2            0.5\n10      T08   A06        1            1.0\n11      T09   A06        1            1.0\n12      T10   A02        1            1.0\n13      T11   A03        2            0.3\n14      T11   A04        3            0.3\n15      T11   A06        1            0.4\n16      T12   A02        1            1.0\n17      T13   A01        1            1.0\n\n\n\n\n\n\n\n\n\n\n\nClick here - authors table\n\n\n\n\n\n\nsqldf(\"select * from authors\")\n\n  au_id  au_fname    au_lname        phone              address          city state   zip\n1   A01     Sarah     Buchman 718-496-7223       75 West 205 St         Bronx    NY 10468\n2   A02     Wendy   Heydemark 303-986-7020     2922 Baseline Rd       Boulder    CO 80303\n3   A03    Hallie        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n4   A04      Klee        Hull 415-549-4278 3800 Waldo Ave, #14F San Francisco    CA 94123\n5   A05 Christian       Kells 212-771-4680       114 Horatio St      New York    NY 10014\n6   A06    Harvey     Kellsey 650-836-7128       390 Serra Mall     Palo Alto    CA 94305\n7   A07     Paddy O'Furniture 941-925-0752         1442 Main St      Sarasota    FL 34236\n\n\n\n\n\n\n\n\n\n\n\nClick here - royalties table\n\n\n\n\n\n\nsqldf(\"select * from royalties\")\n\n   title_id advance royalty_rate\n1       T01   10000         0.05\n2       T02    1000         0.06\n3       T03   15000         0.07\n4       T04   20000         0.08\n5       T05  100000         0.09\n6       T06   20000         0.08\n7       T07 1000000         0.11\n8       T08       0         0.04\n9       T09       0         0.05\n10      T10      NA           NA\n11      T11  100000         0.07\n12      T12   50000         0.09\n13      T13   20000         0.06",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#single-table-queries-with-group-by.",
    "href": "sql3000-sqlQuestions-v001.html#single-table-queries-with-group-by.",
    "title": "40   Database Questions (and Answers)",
    "section": "40.2 Single table queries with “group by”.",
    "text": "40.2 Single table queries with “group by”.\nThese can use aggregate functions but will return at most one row in the output for each “group” of rows as defined in the “group by”\n\nQuestion 11\nFor each “type” of book (e.g. biography, children, etc) list the number of pages in the shortest book of that type and the number of pages in the longest book of that type. Sort the results alphabetically by the type of book.\nUsing the data shown above, the output should be:\n\n\n        type Length of Shortest Length of Longest\n1  biography                333               507\n2   children                 22                86\n3   computer               1226              1226\n4    history                 14               802\n5 psychology                201               826\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select type, min(pages) AS 'Length of Shortest' ,  max(pages)  as 'Length of Longest'\n        from titles\n        group by type\n        order by type;\n\")\n\n\n\n\n\n\nQuestion 12\nList the types of books (e.g. history, biography, etc) and the average price of those books for which the average price of books in that category is at least $12.00.\nUsing the data shown above, the output should be:\n\n\n       type avg(price)\n1 biography   18.96333\n2  computer   39.95000\n3   history   23.97667\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select type, avg(price)\n        from titles\n        group by type\n        having avg(price) &gt;= 12;\n\")\n\n\n\n\n\n\nQuestion 13\nFor each type of book (e.g. biography, children, etc), show the number of those books that are 450 pages or longer. The column heading should be ‘# of long books’. If there are no titles of a particular type that are so long, then don’t show that type at all.\nUsing the data shown above, the output should be:\n\n\n        type # of long books\n1  biography               2\n2   computer               1\n3    history               1\n4 psychology               2\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select type, count(*) as '# of long books'\n        from titles\n        where pages &gt;= 450\n        group by type;\n\")\n\n\n\n\n\n\nQuestion 14\nModify the answer to the previous question so that only those types for which there are at least 2 long books are displayed.\nUsing the data shown above, the output should be:\n\n\n        type # of long books\n1  biography               2\n2 psychology               2\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nNOTE: the only change to the above answer is the addition of the HAVING clause at the end.\n\nsqldf(\"\n        select type, count(*) as '# of long books'\n        from titles\n        where pages &gt;= 450\n        group by type\n        having count(*) &gt;= 2        -- this is the only change.\n\")",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#multiple-table-queries-with-group-by",
    "href": "sql3000-sqlQuestions-v001.html#multiple-table-queries-with-group-by",
    "title": "40   Database Questions (and Answers)",
    "section": "40.3 Multiple table queries with group by",
    "text": "40.3 Multiple table queries with group by\n\nQuestion 15\nFor each publisher, list the name of the publisher and total number of pages that the publisher has published in all their books (ie. add up all the pages in all books for each publisher). Show the publishers who have published the most pages at the top of the result list.\nUsing the data shown above, the output should be:\n\n\n              pub_name sum(pages)\n1    Abatis Publishers       1399\n2       AAA Publishing       1333\n3      Core Dump Books       1226\n4 Schandenfreude Press       1149\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, sum(pages)\n        from publishers join titles on publishers.pub_id = titles.pub_id\n        group by pub_name\n        order by sum(pages) desc;\n\")\n\n\n\n\n\n\nQuestion 16\nModify the previous query so that only publishers who have published at least 1250 pages will show up.\nUsing the data shown above, the output should be:\n\n\n           pub_name sum(pages)\n1 Abatis Publishers       1399\n2    AAA Publishing       1333\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, sum(pages)\n        from publishers join titles on publishers.pub_id = titles.pub_id\n        group by pub_name\n        having  sum(pages)  &gt;= 1250      -- THIS LINE IS THE ONLY CHANGE \n        order by sum(pages) desc;\n\")\n\n\n\n\n\n\nQuestion 17\nList each publisher’s name and the numbers of pages in their longest and shortest books. Sort the results by the publisher’s name.\nUsing the data shown above, the output should be:\n\n\n              pub_name Length of Shortest Length of Longest\n1       AAA Publishing                507               826\n2    Abatis Publishers                 22               510\n3      Core Dump Books               1226              1226\n4 Schandenfreude Press                 14               802\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, min(pages) AS 'Length of Shortest' ,  max(pages)  as 'Length of Longest'\n        from titles join publishers on titles.pub_id = publishers.pub_id\n        group by pub_name\n        order by pub_name;\n\")",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#multiple-table-queries",
    "href": "sql3000-sqlQuestions-v001.html#multiple-table-queries",
    "title": "40   Database Questions (and Answers)",
    "section": "40.4 Multiple table queries",
    "text": "40.4 Multiple table queries\n\nQuestion 18\nList the authors first and last names, the author’s state, the title name, the publisher name and the publishers state for all books.\nUsing the data shown above, the output should be:\n\n\n    au_fname  au_lname state                          title_name             pub_name state\n1      Sarah   Buchman    NY                               1977!    Abatis Publishers    NY\n2      Sarah   Buchman    NY           200 Years of German Humor Schandenfreude Press  &lt;NA&gt;\n3      Sarah   Buchman    NY What Are The Civilian Applications? Schandenfreude Press  &lt;NA&gt;\n4      Wendy Heydemark    CO                    How About Never?    Abatis Publishers    NY\n5      Wendy Heydemark    CO                   I Blame My Mother Schandenfreude Press  &lt;NA&gt;\n6      Wendy Heydemark    CO         Not Without My Fabrerge Egg       AAA Publishing    CA\n7      Wendy Heydemark    CO           Spontaneous, Not Annoying       AAA Publishing    CA\n8     Hallie      Hull    CA           But I Did It Unconciously    Abatis Publishers    NY\n9     Hallie      Hull    CA    Perhaps It's a Glandular Problem       AAA Publishing    CA\n10      Klee      Hull    CA           But I Did It Unconciously    Abatis Publishers    NY\n11      Klee      Hull    CA              Exchange of Platitudes    Abatis Publishers    NY\n12      Klee      Hull    CA                   I Blame My Mother Schandenfreude Press  &lt;NA&gt;\n13      Klee      Hull    CA    Perhaps It's a Glandular Problem       AAA Publishing    CA\n14 Christian     Kells    NY        Ask Yor System Administrator      Core Dump Books    CA\n15    Harvey   Kellsey    CA        Just Wait Until After School    Abatis Publishers    NY\n16    Harvey   Kellsey    CA                     Kiss My Boo Boo    Abatis Publishers    NY\n17    Harvey   Kellsey    CA    Perhaps It's a Glandular Problem       AAA Publishing    CA\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select au_fname, au_lname, authors.state, title_name, pub_name, publishers.state\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id;\n\")\n\n\n\n\n\n\nQuestion 19\nList the title, number of pages, authors first and last names and author’s state for all books that are longer than 150 pages and whose author lives in NY or CA.\nUsing the data shown above, the output should be:\n\n\nWarning: Ignoring remaining part of query: -- (parentheses ARE necessary since by default AND is done before OR)\n\n\n                           title_name pages  au_fname au_lname state\n1 What Are The Civilian Applications?   802     Sarah  Buchman    NY\n2           But I Did It Unconciously   510    Hallie     Hull    CA\n3    Perhaps It's a Glandular Problem   826    Hallie     Hull    CA\n4           But I Did It Unconciously   510      Klee     Hull    CA\n5              Exchange of Platitudes   201      Klee     Hull    CA\n6                   I Blame My Mother   333      Klee     Hull    CA\n7    Perhaps It's a Glandular Problem   826      Klee     Hull    CA\n8        Ask Yor System Administrator  1226 Christian    Kells    NY\n9    Perhaps It's a Glandular Problem   826    Harvey  Kellsey    CA\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select title_name, pages, au_fname, au_lname, authors.state \n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n    where pages &gt; 150 and   (state = 'NY'  or   state = 'CA') ;  -- (parentheses ARE necessary since by default AND is done before OR)\n\")\n\n\n\n\n\n\nQuestion 20\nList the authors first and last names, the author’s state, the title name, the publisher name and the publishers state only for those books where the author and publisher are from the same state. (HINT: specify authors.state [note: there is a period between author and state] in the criteria for the publisher’s state).\nUsing the data shown above, the output should be:\n\n\n  au_fname au_lname state                       title_name          pub_name state\n1    Sarah  Buchman    NY                            1977! Abatis Publishers    NY\n2   Hallie     Hull    CA Perhaps It's a Glandular Problem    AAA Publishing    CA\n3     Klee     Hull    CA Perhaps It's a Glandular Problem    AAA Publishing    CA\n4   Harvey  Kellsey    CA Perhaps It's a Glandular Problem    AAA Publishing    CA\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select au_fname, au_lname, authors.state, title_name, pub_name, publishers.state\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\n                where authors.state = publishers.state;\n\")\n\n\n\n\n\n\nQuestion 21\nList the title and authors of all books published by “Abatis Publishers” sort the results in alphabetical order by title and then by the au_order column. Do NOT show that au_order column in the output.\nUsing the data shown above, the output should be:\n\n\n  au_fname  au_lname                   title_name          pub_name\n1    Sarah   Buchman                        1977! Abatis Publishers\n2   Hallie      Hull    But I Did It Unconciously Abatis Publishers\n3     Klee      Hull    But I Did It Unconciously Abatis Publishers\n4     Klee      Hull       Exchange of Platitudes Abatis Publishers\n5    Wendy Heydemark             How About Never? Abatis Publishers\n6   Harvey   Kellsey Just Wait Until After School Abatis Publishers\n7   Harvey   Kellsey              Kiss My Boo Boo Abatis Publishers\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select au_fname, au_lname, title_name, pub_name\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\n    where pub_name = 'Abatis Publishers'\n    order by title_name, au_order;\n\")\n\n\n\n\n\n\nQuestion 22\nList the publisher name, author name, and title name for all books that have at least 100 pages and that sold at least 200 copies. Sort the results in alphabetical order first by publisher and then by author.\nUsing the data shown above, the output should be:\n\n\n               pub_name  au_fname  au_lname                          title_name\n1        AAA Publishing     Wendy Heydemark           Spontaneous, Not Annoying\n2        AAA Publishing    Hallie      Hull    Perhaps It's a Glandular Problem\n3        AAA Publishing      Klee      Hull    Perhaps It's a Glandular Problem\n4        AAA Publishing    Harvey   Kellsey    Perhaps It's a Glandular Problem\n5     Abatis Publishers     Sarah   Buchman                               1977!\n6     Abatis Publishers     Wendy Heydemark                    How About Never?\n7     Abatis Publishers    Hallie      Hull           But I Did It Unconciously\n8     Abatis Publishers      Klee      Hull           But I Did It Unconciously\n9     Abatis Publishers      Klee      Hull              Exchange of Platitudes\n10      Core Dump Books Christian     Kells        Ask Yor System Administrator\n11 Schandenfreude Press     Sarah   Buchman What Are The Civilian Applications?\n12 Schandenfreude Press     Wendy Heydemark                   I Blame My Mother\n13 Schandenfreude Press      Klee      Hull                   I Blame My Mother\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n  select pub_name, au_fname, au_lname, title_name\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id\n                where   pages &gt;= 100 and sales &gt;= 200\n    order by  pub_name,   au_lname,  au_fname;\n\")\n\n\n\n\n\n\nQuestion 23\nList authors first and last names and the titles they wrote for publishers whose name (ie. the publisher’s name) begins with “A”, “B” or “C”. Sort the rows in alphabetical order by the publisher. For rows for the same publisher, sort the rows alphabetically by the names of the authors.\nUsing the data shown above, the output should be:\n\n\n    au_fname  au_lname                       title_name          pub_name\n1      Wendy Heydemark      Not Without My Fabrerge Egg    AAA Publishing\n2      Wendy Heydemark        Spontaneous, Not Annoying    AAA Publishing\n3     Hallie      Hull Perhaps It's a Glandular Problem    AAA Publishing\n4       Klee      Hull Perhaps It's a Glandular Problem    AAA Publishing\n5     Harvey   Kellsey Perhaps It's a Glandular Problem    AAA Publishing\n6      Sarah   Buchman                            1977! Abatis Publishers\n7      Wendy Heydemark                 How About Never? Abatis Publishers\n8     Hallie      Hull        But I Did It Unconciously Abatis Publishers\n9       Klee      Hull        But I Did It Unconciously Abatis Publishers\n10      Klee      Hull           Exchange of Platitudes Abatis Publishers\n11    Harvey   Kellsey     Just Wait Until After School Abatis Publishers\n12    Harvey   Kellsey                  Kiss My Boo Boo Abatis Publishers\n13 Christian     Kells     Ask Yor System Administrator   Core Dump Books\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThree different answers.\nThe only differences between the answers are the WHERE clauses.\n\nAnswer 1\n\n\n    select au_fname, au_lname, title_name, pub_name\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id  where substr(pub_name, 1,1) IN ('A','B','C')  order by pub_name, au_lname, au_fname\n\n\n\n\n    au_fname  au_lname                       title_name          pub_name\n1      Wendy Heydemark      Not Without My Fabrerge Egg    AAA Publishing\n2      Wendy Heydemark        Spontaneous, Not Annoying    AAA Publishing\n3     Hallie      Hull Perhaps It's a Glandular Problem    AAA Publishing\n4       Klee      Hull Perhaps It's a Glandular Problem    AAA Publishing\n5     Harvey   Kellsey Perhaps It's a Glandular Problem    AAA Publishing\n6      Sarah   Buchman                            1977! Abatis Publishers\n7      Wendy Heydemark                 How About Never? Abatis Publishers\n8     Hallie      Hull        But I Did It Unconciously Abatis Publishers\n9       Klee      Hull        But I Did It Unconciously Abatis Publishers\n10      Klee      Hull           Exchange of Platitudes Abatis Publishers\n11    Harvey   Kellsey     Just Wait Until After School Abatis Publishers\n12    Harvey   Kellsey                  Kiss My Boo Boo Abatis Publishers\n13 Christian     Kells     Ask Yor System Administrator   Core Dump Books\n\n\n\n\nAnswer 2\n\n\n    select au_fname, au_lname, title_name, pub_name\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id  where substr(pub_name, 1,1) = 'A' or \n        substr(pub_name, 1,1) = 'B' or\n               substr(pub_name, 1,1) = 'C'  order by pub_name, au_lname, au_fname\n\n\n\n\n    au_fname  au_lname                       title_name          pub_name\n1      Wendy Heydemark      Not Without My Fabrerge Egg    AAA Publishing\n2      Wendy Heydemark        Spontaneous, Not Annoying    AAA Publishing\n3     Hallie      Hull Perhaps It's a Glandular Problem    AAA Publishing\n4       Klee      Hull Perhaps It's a Glandular Problem    AAA Publishing\n5     Harvey   Kellsey Perhaps It's a Glandular Problem    AAA Publishing\n6      Sarah   Buchman                            1977! Abatis Publishers\n7      Wendy Heydemark                 How About Never? Abatis Publishers\n8     Hallie      Hull        But I Did It Unconciously Abatis Publishers\n9       Klee      Hull        But I Did It Unconciously Abatis Publishers\n10      Klee      Hull           Exchange of Platitudes Abatis Publishers\n11    Harvey   Kellsey     Just Wait Until After School Abatis Publishers\n12    Harvey   Kellsey                  Kiss My Boo Boo Abatis Publishers\n13 Christian     Kells     Ask Yor System Administrator   Core Dump Books\n\n\n\n\nAnswer 3\n\n\n    select au_fname, au_lname, title_name, pub_name\n    from authors join title_authors on authors.au_id = title_authors.au_id \n                          join titles on title_authors.title_id = titles.title_id \n                          join publishers on publishers.pub_id = titles.pub_id  where pub_name like 'A%'  or  pub_name like 'B%'  or pub_name like 'C%'  order by pub_name, au_lname, au_fname\n\n\n\n\n    au_fname  au_lname                       title_name          pub_name\n1      Wendy Heydemark      Not Without My Fabrerge Egg    AAA Publishing\n2      Wendy Heydemark        Spontaneous, Not Annoying    AAA Publishing\n3     Hallie      Hull Perhaps It's a Glandular Problem    AAA Publishing\n4       Klee      Hull Perhaps It's a Glandular Problem    AAA Publishing\n5     Harvey   Kellsey Perhaps It's a Glandular Problem    AAA Publishing\n6      Sarah   Buchman                            1977! Abatis Publishers\n7      Wendy Heydemark                 How About Never? Abatis Publishers\n8     Hallie      Hull        But I Did It Unconciously Abatis Publishers\n9       Klee      Hull        But I Did It Unconciously Abatis Publishers\n10      Klee      Hull           Exchange of Platitudes Abatis Publishers\n11    Harvey   Kellsey     Just Wait Until After School Abatis Publishers\n12    Harvey   Kellsey                  Kiss My Boo Boo Abatis Publishers\n13 Christian     Kells     Ask Yor System Administrator   Core Dump Books\n\n\n\n\n\n\n\n\nQuestion 24\nList the publisher name, author’s name and phone number for all authors that have written books for the publisher’s whose name begins with a letter from A through J. (HINT: specify the publisher’s name twice but only show one of them. In the criteria for one of them list &gt;=“A” and in the criteria for the other list &lt;“K”).\nUsing the data shown above, the output should be:\n\n\n            pub_name  au_fname  au_lname        phone\n1  Abatis Publishers     Sarah   Buchman 718-496-7223\n2  Abatis Publishers    Hallie      Hull 415-549-4278\n3  Abatis Publishers      Klee      Hull 415-549-4278\n4  Abatis Publishers      Klee      Hull 415-549-4278\n5  Abatis Publishers     Wendy Heydemark 303-986-7020\n6  Abatis Publishers    Harvey   Kellsey 650-836-7128\n7  Abatis Publishers    Harvey   Kellsey 650-836-7128\n8    Core Dump Books Christian     Kells 212-771-4680\n9     AAA Publishing     Wendy Heydemark 303-986-7020\n10    AAA Publishing    Hallie      Hull 415-549-4278\n11    AAA Publishing      Klee      Hull 415-549-4278\n12    AAA Publishing    Harvey   Kellsey 650-836-7128\n13    AAA Publishing     Wendy Heydemark 303-986-7020\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, au_fname, au_lname, phone\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join publishers on publishers.pub_id = titles.pub_id\n        where substr(pub_name,1,1) &gt;= 'A' and substr(pub_name,1,1) &lt; 'K';\n\")\n\n\n\n\n\n\nQuestion 25\nList the titles of all books and the number of authors for the book (NOT the actual author names). Sort the results so that the books with the most authors appear on top. All books that have the same number of authors should be listed in alphabetical order.\nUsing the data shown above, the output should be:\n\n\n                            title_name Number of Authors\n1     Perhaps It's a Glandular Problem                 3\n2            But I Did It Unconciously                 2\n3                    I Blame My Mother                 2\n4                                1977!                 1\n5            200 Years of German Humor                 1\n6         Ask Yor System Administrator                 1\n7               Exchange of Platitudes                 1\n8                     How About Never?                 1\n9         Just Wait Until After School                 1\n10                     Kiss My Boo Boo                 1\n11         Not Without My Fabrerge Egg                 1\n12           Spontaneous, Not Annoying                 1\n13 What Are The Civilian Applications?                 1\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select title_name, count(*) as 'Number of Authors'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n        group by titles.title_id, title_name\n        order by count(*) desc, title_name;\n\")\n\n\n\n\n\n\nQuestion 26\nList the titles of all books that have 2 or more authors and that are longer than 50 pages.\nUsing the data shown above, the output should be:\n\n\n                        title_name Number of Authors\n1 Perhaps It's a Glandular Problem                 3\n2        But I Did It Unconciously                 2\n3                I Blame My Mother                 2\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select title_name, count(*) as 'Number of Authors'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n        where pages &gt;= 50\n        group by titles.title_id, title_name\n        having count(*) &gt;= 2\n        order by count(*) desc, title_name;\n\")\n\n\n\n\n\n\nQuestion 27\nList all publishers and the number of books that the publisher has published (not the actual titles, just the number of books). Sort the list so that the publisher with the most books appears on top.\nUsing the data shown above, the output should be:\n\n\n              pub_name # of Books\n1    Abatis Publishers          6\n2 Schandenfreude Press          3\n3       AAA Publishing          3\n4      Core Dump Books          1\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select pub_name, count(*) as '# of Books'\n        from titles join publishers on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name\n        order by count(*) desc;\n\")\n\n\n\n\n\n\nQuestion 28\nList the publisher name and total revenue for each for each publisher. (Total revenue for a publisher is the total revenue for all books published by that publisher. Revenue for a book is the number sold times the price of the book.)\nUsing the data shown above, the output should be:\n\n\n              pub_name Total Revenue\n1    Abatis Publishers       1917871\n2      Core Dump Books       1025397\n3 Schandenfreude Press      36434537\n4       AAA Publishing       2051056\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, sum(sales * price) as 'Total Revenue'\n        from titles join publishers on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name    ;\n\")\n\n\n\n\n\n\nQuestion 29\nModify the previous query to only list those publishers who have at least ten thousand dollars in total revenue.\nUsing the data shown above, the output should be:\n\n\nWarning: Ignoring remaining part of query: having sum(sales * price) &gt;= 10000;\n\n\n              pub_name Total Revenue\n1    Abatis Publishers       1917871\n2      Core Dump Books       1025397\n3 Schandenfreude Press      36434537\n4       AAA Publishing       2051056\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, sum(sales * price) as 'Total Revenue'\n        from titles join publishers on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name    ;\n        having sum(sales * price) &gt;= 10000;\n\")\n\n\n\n\n\n\nQuestion 30\nList the title and author of each book. For each author for a particular book, list the amount of the “advance” that (s)he got for that book.\nUsing the data shown above, the output should be:\n\n\n                            title_name  au_fname  au_lname advance\n1                                1977!     Sarah   Buchman   10000\n2            200 Years of German Humor     Sarah   Buchman    1000\n3  What Are The Civilian Applications?     Sarah   Buchman   20000\n4                     How About Never?     Wendy Heydemark   20000\n5                    I Blame My Mother     Wendy Heydemark 1000000\n6          Not Without My Fabrerge Egg     Wendy Heydemark      NA\n7            Spontaneous, Not Annoying     Wendy Heydemark   50000\n8            But I Did It Unconciously    Hallie      Hull   20000\n9     Perhaps It's a Glandular Problem    Hallie      Hull  100000\n10           But I Did It Unconciously      Klee      Hull   20000\n11              Exchange of Platitudes      Klee      Hull  100000\n12                   I Blame My Mother      Klee      Hull 1000000\n13    Perhaps It's a Glandular Problem      Klee      Hull  100000\n14        Ask Yor System Administrator Christian     Kells   15000\n15        Just Wait Until After School    Harvey   Kellsey       0\n16                     Kiss My Boo Boo    Harvey   Kellsey       0\n17    Perhaps It's a Glandular Problem    Harvey   Kellsey  100000\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select title_name, au_fname, au_lname, advance\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join royalties on royalties.title_id = titles.title_id;\n\")\n\n\n\n\n\n\nQuestion 31\nList the title and author of each book. For each author for a particular book, list the total amount in royalties that (s)he got for that book. Hint: the royalty dollars for a particular book is the “price” times the “sales” times the “royalty_rate” times the “royalty_rate”. Sort the results so that the authors who made the most royalty dollars are listed at the top. (NOTE: make sure to read the database description at the top of this document to understand the fields ROYALTIES.ROYALTY_RATE and the field TITLE_AUTHORS.royalty_rate).\nUsing the data shown above, the output should be:\n\n\n                            title_name  au_fname  au_lname Total Royalties for Author\n1                                1977!     Sarah   Buchman                   31.11585\n2            200 Years of German Humor     Sarah   Buchman                  687.03012\n3  What Are The Civilian Applications?     Sarah   Buchman                 1130.05919\n4                     How About Never?     Wendy Heydemark                 1445.33760\n5                    I Blame My Mother     Wendy Heydemark               434750.45900\n6          Not Without My Fabrerge Egg     Wendy Heydemark                         NA\n7            Spontaneous, Not Annoying     Wendy Heydemark                10522.00522\n8            But I Did It Unconciously    Hallie      Hull                 1080.85114\n9     Perhaps It's a Glandular Problem    Hallie      Hull                 3685.00957\n10           But I Did It Unconciously      Klee      Hull                 1080.85114\n11              Exchange of Platitudes      Klee      Hull                11340.06480\n12                   I Blame My Mother      Klee      Hull               434750.45900\n13    Perhaps It's a Glandular Problem      Klee      Hull                 3685.00957\n14        Ask Yor System Administrator Christian     Kells                 5024.44359\n15        Just Wait Until After School    Harvey   Kellsey                   65.52000\n16                     Kiss My Boo Boo    Harvey   Kellsey                  174.37500\n17    Perhaps It's a Glandular Problem    Harvey   Kellsey                 3685.00957\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select title_name, au_fname, au_lname, royalty_rate * royalty_rate * sales * price as  'Total Royalties for Author'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join royalties on royalties.title_id = titles.title_id;       \")\n\n\n\n\n\n\nQuestion 32\nList the name of each author and the total amount of royalty dollars that they received for all of their books (name this column “RoyaltyDollars”). Hint: the royalty dollars for a particular book is the “price” times the “sales” times the “royalty_rate” times the “royalty_rate”. Sort the results so that the authors who made the most royalty dollars are listed at the top.\nUsing the data shown above, the output should be:\n\n\n   au_fname  au_lname Total Royalties for Author\n1      Klee      Hull                 450856.385\n2     Wendy Heydemark                 446717.802\n3 Christian     Kells                   5024.444\n4    Hallie      Hull                   4765.861\n5    Harvey   Kellsey                   3924.905\n6     Sarah   Buchman                   1848.205\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select au_fname, au_lname, sum(royalty_rate * royalty_rate * sales * price)  as  'Total Royalties for Author'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join royalties on royalties.title_id = titles.title_id\n        group by authors.au_id, au_fname, au_lname\n        order by sum(royalty_rate * royalty_rate * sales * price)  desc;\n\")\n\n\n\n\n\n\nQuestion 33\nModify the query from the previous question so that the list only includes authors who made more than $50,000.00 in royalties.\nUsing the data shown above, the output should be:\n\n\n  au_fname  au_lname Total Royalties for Author\n1     Klee      Hull                   450856.4\n2    Wendy Heydemark                   446717.8\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select au_fname, au_lname, sum(royalty_rate * royalty_rate * sales * price)  as  'Total Royalties for Author'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join royalties on royalties.title_id = titles.title_id\n        group by authors.au_id, au_fname, au_lname\n        having sum(royalty_rate * royalty_rate * sales * price) &gt; 50000\n        order by sum(royalty_rate * royalty_rate * sales * price)  desc;\n\")",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#subqueries-outer-joins-unions-and-other-stuff",
    "href": "sql3000-sqlQuestions-v001.html#subqueries-outer-joins-unions-and-other-stuff",
    "title": "40   Database Questions (and Answers)",
    "section": "40.5 Subqueries, outer joins, unions and other stuff …",
    "text": "40.5 Subqueries, outer joins, unions and other stuff …\n\n\nQuestion 34\nShow the title names and number of pages for those books that are longer than (ie. more pages) the average length of all books.\nUsing the data shown above, the output should be:\n\n\n                           title_name pages\n1        Ask Yor System Administrator  1226\n2           But I Did It Unconciously   510\n3                    How About Never?   473\n4    Perhaps It's a Glandular Problem   826\n5           Spontaneous, Not Annoying   507\n6 What Are The Civilian Applications?   802\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select title_name , pages\n    from titles\n    where pages &gt;= (select avg(pages) from titles);\n\")\n\n\n\n\n\n\nQuestion 35\nShow the publisher’s name and number of authors who have written books for that publisher:\nUsing the data shown above, the output should be:\n\n\nError: ambiguous column name: pub_id\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, count(DISTINCT authors.au_id) as '# of authors'\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join publishers on publishers.pub_id = titles.pub_id\n        group by pub_id, pub_name;\n\")\n\nError: ambiguous column name: pub_id\n\n\n\n\n\n\n\nQuestion 36\nShow the pub_name and the number of titles published by that publisher. Only include publishers for whom at least 5 authors have worked.\nUsing the data shown above, the output should be:\n\n\n           pub_name Number of Titles\n1 Abatis Publishers                6\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select pub_name, count(*) as 'Number of Titles'\n    from publishers as pub1 join titles on pub1.pub_id = titles.pub_id\n    group by pub1.pub_id, pub_name\n    having   5 &lt;= ( select count(DISTINCT authors.au_id)\n        from authors join title_authors on authors.au_id = title_authors.au_id \n                              join titles on title_authors.title_id = titles.title_id \n                              join publishers as pub2 on pub2.pub_id = titles.pub_id\n        where pub2.pub_id = pub1.pub_id ) ;\n\")\n\n\n\n\n\n\nQuestion 37\nShow the names of publishers who did not publish any books (I guess they are just getting started  )\nUsing the data shown above, the output should be:\n\n\n           pub_name\n1 Tneterhooks Press\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nANSWER – with a subquery\n\nsqldf(\"\n    select pub_name \n    from publishers  as   pub1\n    where not exists\n        ( select * \n        from publishers  as pub2  join titles on pub2.pub_id = titles.pub_id\n        where pub2.pub_id = pub1.pub_id )\n\")\n\n           pub_name\n1 Tneterhooks Press\n\n\n\nClick for answer\nANSWER – with a left join\n\nsqldf(\"\n    select pub_name from publishers left join titles on publishers.pub_id = titles.pub_id\n    where title_id is NULL\n\")\n\n           pub_name\n1 Tneterhooks Press\n\n\n\n\n\n\n\n\nQuestion 38\nList the name of each publisher and the total number of books that each publisher has published. Sort the results so that the publishers who published the most books appear at the top. If two or more publishers published the same number of books then they should be listed in alphabetical order.\n\nThe answer should only include publishers who have published some books.\nUsing the data shown above, the output should be:\n\n\n\n              pub_name count(*)\n1    Abatis Publishers        6\n2       AAA Publishing        3\n3 Schandenfreude Press        3\n4      Core Dump Books        1\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n        select pub_name, count(*)\n        from titles join publishers on publishers.pub_id = titles.pub_id\n        group by pub_name\n        order by count(*) desc, pub_name;\n\")\n\n\n\n\n\nSame as previous question but this time make sure to also include publishers who have published zero books\nUsing the data shown above, the output should be:\n\n\n\n              pub_name count(titles.title_id)\n1    Abatis Publishers                      6\n2       AAA Publishing                      3\n3 Schandenfreude Press                      3\n4      Core Dump Books                      1\n5    Tneterhooks Press                      0\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nANSWER - with a left join\n\nsqldf(\"\n        select pub_name, count(titles.title_id)\n        from publishers left join titles on publishers.pub_id = titles.pub_id\n        group by pub_name\n        order by count(*) desc, pub_name;\n\")\n\n              pub_name count(titles.title_id)\n1    Abatis Publishers                      6\n2       AAA Publishing                      3\n3 Schandenfreude Press                      3\n4      Core Dump Books                      1\n5    Tneterhooks Press                      0\n\n\nANSWER – with a UNION\n\nsqldf(\"\n    select pub_name , 0  as  NumTitles\n    from publishers left join titles on publishers.pub_id = titles.pub_id\n    where title_id is NULL \n\nUNION\n\n    select pub_name, count(*) as NumTitles\n    from titles join publishers on publishers.pub_id = titles.pub_id\n    group by pub_name;\n\")\n\n              pub_name NumTitles\n1       AAA Publishing         3\n2    Abatis Publishers         6\n3      Core Dump Books         1\n4 Schandenfreude Press         3\n5    Tneterhooks Press         0",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "sql3000-sqlQuestions-v001.html#additional-questions",
    "href": "sql3000-sqlQuestions-v001.html#additional-questions",
    "title": "40   Database Questions (and Answers)",
    "section": "40.6 Additional questions",
    "text": "40.6 Additional questions\n\nQuestion 39\nShow the names of the authors and the publishers who published their books. Only show those authors/publishers where the author lives in the same state as the publisher who published the book. Do not show any author/pbulisher names twice.\nUsing the data shown above, the output should be:\n\n\n  au_fname au_lname          pub_name state state\n1    Sarah  Buchman Abatis Publishers    NY    NY\n2   Hallie     Hull    AAA Publishing    CA    CA\n3     Klee     Hull    AAA Publishing    CA    CA\n4   Harvey  Kellsey    AAA Publishing    CA    CA\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect distinct au_fname, au_lname, pub_name, authors.state, publishers.state\nfrom authors join title_authors on authors.au_id = title_authors.au_id\n             join titles on title_authors.title_id = titles.title_id\n             join publishers on titles.pub_id = publishers.pub_id\nwhere authors.state = publishers.state\n      \")\n\n\n\n\n\n\nQuestion 40\nThe exchange rate for nis/usd is 3.3.\nWrite a query that shows the titles and prices for biographies that cost between 10 and 20 dollars. Show two columns for the prices. One column for USD and one column for NIS. Name the columns “price in USD” and “price in NIS”. Sort the results so that the most expensive books are displayed first.\nUsing the data shown above, the output should be:\n\n\n                 title_name price in NIS price in USD      type\n1          How About Never?       65.835        19.95 biography\n2 Spontaneous, Not Annoying       42.867        12.99 biography\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect title_name, price * 3.3 as 'price in NIS', price as 'price in USD', type\nfrom titles\nwhere type = 'biography' and\n      price &gt;= 10 and price &lt;= 20\norder by price desc\n      \")\n\n\n\n\n\n\nQuestion 41\nShow those titles for which the price of the book is greater than the average price of all books and the number of pages of the book is less than the average number of pages for all books.\nUsing the data shown above, the output should be:\n\n\n                 title_name price pages\n1                     1977! 21.99   107\n2 200 Years of German Humor 19.95    14\n3         I Blame My Mother 23.95   333\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect title_name, price, pages\nfrom titles\nwhere price &gt; (select avg(price) from titles) and\n      pages &lt; (select avg(pages) from titles)\n      \")\n\nThe answer above uses the following two “non-correlated” subqueries. It might help your understanding of the answer above to see the output of the subqueries when they are run by themselves.\n\n# The average price of all books\nsqldf(\"select avg(price) from titles\")\n\n  avg(price)\n1    18.3875\n\n# The average number of pages of all books\nsqldf(\"select avg(pages) from titles\")\n\n  avg(pages)\n1   425.5833\n\n\n\n\n\n\n\nQuestion 42\nShow the authors who have not written any books\nUsing the data shown above, the output should be:\n\n\n  au_fname    au_lname\n1    Paddy O'Furniture\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect au_fname, au_lname\nfrom authors left join title_authors on authors.au_id = title_authors.au_id\nwhere title_authors.au_id is null\n\")\n\n\n\n\n\n\nQuestion 43\nIn each row show an author’s name and the number of books that author wrote.\nOnly show those authors who wrote at least two books.\nSort the results alphabetically (last name, first name).\nUsing the data shown above, the output should be:\n\n\n  au_fname  au_lname count(*)\n1   Hallie      Hull        2\n2   Harvey   Kellsey        3\n3     Klee      Hull        4\n4    Sarah   Buchman        3\n5    Wendy Heydemark        4\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect au_fname, au_lname, count(*)\nfrom authors join title_authors on authors.au_id = title_authors.au_id\ngroup by au_fname, au_lname\nhaving count(*) &gt;= 2\n      \")\n\n\n\n\n\n\nQuestion 44\nSame as previous question. However, this time, only show those authors who wrote biographies, the count of the number of biographies they wrote and only show those authors who wrote at least 2 biographies.\nUsing the data shown above, the output should be:\n\n\n  au_fname  au_lname count(*)\n1    Wendy Heydemark        4\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\nselect au_fname, au_lname, count(*)\nfrom authors join title_authors on authors.au_id = title_authors.au_id\n             join titles on title_authors.title_id = titles.title_id\nwhere type = 'biography'\ngroup by au_fname, au_lname\nhaving count(*) &gt;= 2\n\")\n\n\n\n\n\n\nQuestion 45 - a\nWrite a query to show the books in the titles table for which both the author and the publisher have a higher than average price for books they have worked on. Sort the rows alphabetically by publisher name. Sort the rows that have the same publisher name by the author’s name.\nUsing the data shown above, the output should be:\n\n\n              pub_name  au_fname  au_lname                          title_name\n1      Core Dump Books Christian     Kells        Ask Yor System Administrator\n2 Schandenfreude Press     Sarah   Buchman           200 Years of German Humor\n3 Schandenfreude Press     Sarah   Buchman What Are The Civilian Applications?\n4 Schandenfreude Press     Wendy Heydemark                   I Blame My Mother\n\n\n\n\n\n\n\n\nClick for answer - using subqueries in WHERE\n\n\n\n\n\n\nsqldf(\"\nselect pub_name, au_fname, au_lname, title_name\nfrom authors join title_authors on authors.au_id = title_authors.au_id\n             join titles on title_authors.title_id = titles.title_id\n             join publishers on titles.pub_id = publishers.pub_id\nwhere authors.au_id in \n         (\n           select authors.au_id \n           from authors join title_authors on authors.au_id = title_authors.au_id\n                        join titles on title_authors.title_id = titles.title_id\n           group by authors.au_id \n           having avg(price) &gt; (select avg(price) from titles)\n         )\n      and publishers.pub_id in\n         (\n           select publishers.pub_id \n           from titles join publishers on titles.pub_id = publishers.pub_id\n           group by publishers.pub_id\n           having avg(price) &gt; (select avg(price) from titles)\n         )\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname                          title_name\n1      Core Dump Books Christian     Kells        Ask Yor System Administrator\n2 Schandenfreude Press     Sarah   Buchman           200 Years of German Humor\n3 Schandenfreude Press     Sarah   Buchman What Are The Civilian Applications?\n4 Schandenfreude Press     Wendy Heydemark                   I Blame My Mother\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using subqueries in FROM\n\n\n\n\n\n\nsqldf(\"\nselect pub_name, au_fname, au_lname, title_name\nfrom    (\n          select authors.*\n          from authors join title_authors on authors.au_id = title_authors.au_id\n                       join titles on title_authors.title_id = titles.title_id\n          group by authors.au_id \n          having avg(price) &gt; (select avg(price) from titles)\n        ) as highEndAuthors \n\n      join title_authors on highEndAuthors.au_id = title_authors.au_id\n\n      join titles on title_authors.title_id = titles.title_id\n\n      join \n        (\n          select publishers.*\n          from titles join publishers on titles.pub_id = publishers.pub_id\n          group by publishers.pub_id\n          having avg(price) &gt; (select avg(price) from titles)\n        )\n      as highEndPublishers on titles.pub_id = highEndPublishers.pub_id\n\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname                          title_name\n1      Core Dump Books Christian     Kells        Ask Yor System Administrator\n2 Schandenfreude Press     Sarah   Buchman           200 Years of German Humor\n3 Schandenfreude Press     Sarah   Buchman What Are The Civilian Applications?\n4 Schandenfreude Press     Wendy Heydemark                   I Blame My Mother\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using Common Table Expressions (CTE)\n\n\n\n\n\nYou can modify either of the previous queries to use CTE instead of subqueries. The following modifies the previous query.\n\nsqldf(\"\nwith \n  highEndAuthors as \n  (\n    select authors.*\n    from authors join title_authors on authors.au_id = title_authors.au_id\n                 join titles on title_authors.title_id = titles.title_id\n    group by authors.au_id \n    having avg(price) &gt; (select avg(price) from titles)\n  ),\n\n  highEndPublishers as \n  (\n    select publishers.*\n    from titles join publishers on titles.pub_id = publishers.pub_id\n    group by publishers.pub_id\n    having avg(price) &gt; (select avg(price) from titles)\n  )\n\nselect pub_name, au_fname, au_lname, title_name\nfrom highEndAuthors join title_authors on highEndAuthors.au_id = title_authors.au_id\n             join titles on title_authors.title_id = titles.title_id\n             join highEndPublishers on titles.pub_id = highEndPublishers.pub_id\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname                          title_name\n1      Core Dump Books Christian     Kells        Ask Yor System Administrator\n2 Schandenfreude Press     Sarah   Buchman           200 Years of German Humor\n3 Schandenfreude Press     Sarah   Buchman What Are The Civilian Applications?\n4 Schandenfreude Press     Wendy Heydemark                   I Blame My Mother\n\n\n\n\n\n\n\nQuestion 45 - b\nA literary agent is trying to connect authors with publishers who have not worked together before. Write a SQL query to help the agent identify these potential matches. Each row of your output should show the name of a publisher and the name of an author who have NOT worked together yet.\nSort the rows alphabetically by publisher name. For rows that have the same publisher name, sort them by the author’s name.\n\n\n\n\n\n\nClick for answer - Method 1: Using EXCEPT\n\n\n\n\n\n\nsqldf(\"\n-- All existing and non-existing publisher/author relationships\nSELECT publishers.pub_name, authors.au_fname, authors.au_lname \nFROM authors CROSS JOIN publishers\n\nEXCEPT\n\n-- Only the existing publisher/author relationships\nSELECT publishers.pub_name, authors.au_fname, authors.au_lname \nFROM authors JOIN title_authors on authors.au_id = title_authors.au_id\n             JOIN titles on title_authors.title_id = titles.title_id\n             JOIN publishers on titles.pub_id = publishers.pub_id\n \nORDER BY pub_name, au_lname, au_fname\n\")\n\n               pub_name  au_fname    au_lname\n1        AAA Publishing     Sarah     Buchman\n2        AAA Publishing Christian       Kells\n3        AAA Publishing     Paddy O'Furniture\n4     Abatis Publishers Christian       Kells\n5     Abatis Publishers     Paddy O'Furniture\n6       Core Dump Books     Sarah     Buchman\n7       Core Dump Books     Wendy   Heydemark\n8       Core Dump Books    Hallie        Hull\n9       Core Dump Books      Klee        Hull\n10      Core Dump Books    Harvey     Kellsey\n11      Core Dump Books     Paddy O'Furniture\n12 Schandenfreude Press    Hallie        Hull\n13 Schandenfreude Press Christian       Kells\n14 Schandenfreude Press    Harvey     Kellsey\n15 Schandenfreude Press     Paddy O'Furniture\n16    Tneterhooks Press     Sarah     Buchman\n17    Tneterhooks Press     Wendy   Heydemark\n18    Tneterhooks Press    Hallie        Hull\n19    Tneterhooks Press      Klee        Hull\n20    Tneterhooks Press Christian       Kells\n21    Tneterhooks Press    Harvey     Kellsey\n22    Tneterhooks Press     Paddy O'Furniture\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - Method 2: Using LEFT JOIN\n\n\n\n\n\n\nsqldf(\"\nSELECT publishers.pub_id, pub_name, authors.au_id, au_fname, au_lname\nFROM authors CROSS JOIN publishers\n             LEFT JOIN (\n                SELECT DISTINCT title_authors.au_id, titles.pub_id\n                FROM title_authors JOIN titles \n                                   ON title_authors.title_id = titles.title_id\n             ) AS author_publisher \n               ON authors.au_id = author_publisher.au_id AND \n                  publishers.pub_id = author_publisher.pub_id\nWHERE author_publisher.au_id IS NULL\nORDER BY pub_name, au_lname, au_fname;\n\")\n\n   pub_id             pub_name au_id  au_fname    au_lname\n1     P05       AAA Publishing   A01     Sarah     Buchman\n2     P05       AAA Publishing   A05 Christian       Kells\n3     P05       AAA Publishing   A07     Paddy O'Furniture\n4     P01    Abatis Publishers   A05 Christian       Kells\n5     P01    Abatis Publishers   A07     Paddy O'Furniture\n6     P02      Core Dump Books   A01     Sarah     Buchman\n7     P02      Core Dump Books   A02     Wendy   Heydemark\n8     P02      Core Dump Books   A03    Hallie        Hull\n9     P02      Core Dump Books   A04      Klee        Hull\n10    P02      Core Dump Books   A06    Harvey     Kellsey\n11    P02      Core Dump Books   A07     Paddy O'Furniture\n12    P03 Schandenfreude Press   A03    Hallie        Hull\n13    P03 Schandenfreude Press   A05 Christian       Kells\n14    P03 Schandenfreude Press   A06    Harvey     Kellsey\n15    P03 Schandenfreude Press   A07     Paddy O'Furniture\n16    P04    Tneterhooks Press   A01     Sarah     Buchman\n17    P04    Tneterhooks Press   A02     Wendy   Heydemark\n18    P04    Tneterhooks Press   A03    Hallie        Hull\n19    P04    Tneterhooks Press   A04      Klee        Hull\n20    P04    Tneterhooks Press   A05 Christian       Kells\n21    P04    Tneterhooks Press   A06    Harvey     Kellsey\n22    P04    Tneterhooks Press   A07     Paddy O'Furniture\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - Method 3: Using NOT EXISTS\n\n\n\n\n\n\nsqldf(\"\nSELECT publishers.pub_id, pub_name, authors.au_id, au_fname, au_lname\nFROM authors CROSS JOIN publishers \nWHERE NOT EXISTS (\n    -- NOTE 1 - This is a correlated subquery.\n    --\n    -- NOTE 2 - (unrelated to Note 1) With an EXISTS or NOT EXISTS subquery,\n    -- it doesn't make a difference what data is returned by the subquery. \n    -- As long as ANYTHING is returned the query is considered to 'EXIST'\n    -- Threfore it's more efficient to return as little info as possilble.\n    -- SELECT 1 just returns the number 1 for each row that matches as \n    -- opposed to returning actual data from the tables. The query would\n    -- work in the exact same way if we said SELECT * (or anything else) \n    -- it's just for efficiency that we say SELECT 1.\n    SELECT 1 \n    FROM titles JOIN title_authors ON titles.title_id = title_authors.title_id\n    WHERE title_authors.au_id = authors.au_id\n    AND titles.pub_id = publishers.pub_id\n)\nORDER BY pub_name, au_lname, au_fname;\n\")\n\n   pub_id             pub_name au_id  au_fname    au_lname\n1     P05       AAA Publishing   A01     Sarah     Buchman\n2     P05       AAA Publishing   A05 Christian       Kells\n3     P05       AAA Publishing   A07     Paddy O'Furniture\n4     P01    Abatis Publishers   A05 Christian       Kells\n5     P01    Abatis Publishers   A07     Paddy O'Furniture\n6     P02      Core Dump Books   A01     Sarah     Buchman\n7     P02      Core Dump Books   A02     Wendy   Heydemark\n8     P02      Core Dump Books   A03    Hallie        Hull\n9     P02      Core Dump Books   A04      Klee        Hull\n10    P02      Core Dump Books   A06    Harvey     Kellsey\n11    P02      Core Dump Books   A07     Paddy O'Furniture\n12    P03 Schandenfreude Press   A03    Hallie        Hull\n13    P03 Schandenfreude Press   A05 Christian       Kells\n14    P03 Schandenfreude Press   A06    Harvey     Kellsey\n15    P03 Schandenfreude Press   A07     Paddy O'Furniture\n16    P04    Tneterhooks Press   A01     Sarah     Buchman\n17    P04    Tneterhooks Press   A02     Wendy   Heydemark\n18    P04    Tneterhooks Press   A03    Hallie        Hull\n19    P04    Tneterhooks Press   A04      Klee        Hull\n20    P04    Tneterhooks Press   A05 Christian       Kells\n21    P04    Tneterhooks Press   A06    Harvey     Kellsey\n22    P04    Tneterhooks Press   A07     Paddy O'Furniture\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - Method 4: Using NOT IN\n\n\n\n\n\n\nsqldf(\"\nSELECT publishers.pub_id, pub_name, authors.au_id, au_fname, au_lname\nFROM authors CROSS JOIN publishers\nWHERE (authors.au_id, publishers.pub_id) NOT IN (\n    SELECT title_authors.au_id, titles.pub_id\n    FROM title_authors\n    JOIN titles ON title_authors.title_id = titles.title_id\n)\nORDER BY pub_name, au_lname, au_fname;\n\")\n\n   pub_id             pub_name au_id  au_fname    au_lname\n1     P05       AAA Publishing   A01     Sarah     Buchman\n2     P05       AAA Publishing   A05 Christian       Kells\n3     P05       AAA Publishing   A07     Paddy O'Furniture\n4     P01    Abatis Publishers   A05 Christian       Kells\n5     P01    Abatis Publishers   A07     Paddy O'Furniture\n6     P02      Core Dump Books   A01     Sarah     Buchman\n7     P02      Core Dump Books   A02     Wendy   Heydemark\n8     P02      Core Dump Books   A03    Hallie        Hull\n9     P02      Core Dump Books   A04      Klee        Hull\n10    P02      Core Dump Books   A06    Harvey     Kellsey\n11    P02      Core Dump Books   A07     Paddy O'Furniture\n12    P03 Schandenfreude Press   A03    Hallie        Hull\n13    P03 Schandenfreude Press   A05 Christian       Kells\n14    P03 Schandenfreude Press   A06    Harvey     Kellsey\n15    P03 Schandenfreude Press   A07     Paddy O'Furniture\n16    P04    Tneterhooks Press   A01     Sarah     Buchman\n17    P04    Tneterhooks Press   A02     Wendy   Heydemark\n18    P04    Tneterhooks Press   A03    Hallie        Hull\n19    P04    Tneterhooks Press   A04      Klee        Hull\n20    P04    Tneterhooks Press   A05 Christian       Kells\n21    P04    Tneterhooks Press   A06    Harvey     Kellsey\n22    P04    Tneterhooks Press   A07     Paddy O'Furniture\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - Method 5: Using Common Table Expressions (CTE)\n\n\n\n\n\nAny of the previous queries could be reorganized by using Common Table Expressions (CTE). For example:\n\nsqldf(\"\nWITH \n\nall_pairs AS (\n    SELECT authors.au_id, au_fname, au_lname, publishers.pub_id, pub_name\n    FROM authors CROSS JOIN publishers\n),\n\nexisting_pairs AS (\n    SELECT DISTINCT title_authors.au_id, titles.pub_id\n    FROM title_authors JOIN titles ON title_authors.title_id = titles.title_id\n)\n\nSELECT all_pairs.pub_id,   all_pairs.pub_name, all_pairs.au_id, \n       all_pairs.au_fname, all_pairs.au_lname\nFROM all_pairs\nWHERE NOT EXISTS (\n    SELECT 1 FROM existing_pairs\n    WHERE existing_pairs.au_id = all_pairs.au_id \n    AND existing_pairs.pub_id = all_pairs.pub_id\n)\nORDER BY pub_name, au_lname, au_fname;\n\")\n\n   pub_id             pub_name au_id  au_fname    au_lname\n1     P05       AAA Publishing   A01     Sarah     Buchman\n2     P05       AAA Publishing   A05 Christian       Kells\n3     P05       AAA Publishing   A07     Paddy O'Furniture\n4     P01    Abatis Publishers   A05 Christian       Kells\n5     P01    Abatis Publishers   A07     Paddy O'Furniture\n6     P02      Core Dump Books   A01     Sarah     Buchman\n7     P02      Core Dump Books   A02     Wendy   Heydemark\n8     P02      Core Dump Books   A03    Hallie        Hull\n9     P02      Core Dump Books   A04      Klee        Hull\n10    P02      Core Dump Books   A06    Harvey     Kellsey\n11    P02      Core Dump Books   A07     Paddy O'Furniture\n12    P03 Schandenfreude Press   A03    Hallie        Hull\n13    P03 Schandenfreude Press   A05 Christian       Kells\n14    P03 Schandenfreude Press   A06    Harvey     Kellsey\n15    P03 Schandenfreude Press   A07     Paddy O'Furniture\n16    P04    Tneterhooks Press   A01     Sarah     Buchman\n17    P04    Tneterhooks Press   A02     Wendy   Heydemark\n18    P04    Tneterhooks Press   A03    Hallie        Hull\n19    P04    Tneterhooks Press   A04      Klee        Hull\n20    P04    Tneterhooks Press   A05 Christian       Kells\n21    P04    Tneterhooks Press   A06    Harvey     Kellsey\n22    P04    Tneterhooks Press   A07     Paddy O'Furniture\n\n\n\n\n\n\n\nQuestion 45 - c\n(continued from the previous question) The literary agent became more selective. He now wants to focus on introducing only those authors who command a higher than average price for their books with publishers who command a higher than average price for their books.\nWrite a SQL query to help the agent identify these potential matches. Each row of your output should show the name of a publisher and the name of an author who have NOT worked together yet. The publishers should be those whose average book price is above the average price of all books. Similarly, the authors should be those whose average book price is above the average price of all books.\nSort the rows alphabetically by publisher name. For rows that have the same publisher name, sort them by the author’s name.\nUsing the data shown above, the output should be:\n\n\n              pub_name  au_fname  au_lname\n1      Core Dump Books     Sarah   Buchman\n2      Core Dump Books     Wendy Heydemark\n3 Schandenfreude Press Christian     Kells\n\n\nThis is a relatively complex query. It might help to build up your answer step by step.\n\n\n\n\n\n\nClick here for some hints\n\n\n\n\n\nIn order to check your work, it might be helpful to know the following:\n\nThe average selling price of all books is:\n\nsqldf(\"select avg(price) from titles\")\n\n  avg(price)\n1    18.3875\n\n\nThe average selling price of books by different publishers are: (The rows are sorted in decreasing order by average price.)\n\nsqldf(\"\nselect round(avg(price),2) as 'avg price of publishers books', pub_name\nfrom publishers join titles on publishers.pub_id = titles.pub_id\ngroup by publishers.pub_id, pub_name\norder by avg(price) desc, pub_name\n\")\n\n  avg price of publishers books             pub_name\n1                         39.95      Core Dump Books\n2                         24.63 Schandenfreude Press\n3                         14.30    Abatis Publishers\n4                         10.49       AAA Publishing\n\n\nTherefore the publishers who are potential introduction candidates are those whose avg book price is higher than average, ie.\n\nsqldf(\"\nselect pub_name\nfrom publishers join titles on publishers.pub_id = titles.pub_id\ngroup by publishers.pub_id, pub_name\nhaving avg(price) &gt; (select avg(price) from titles)\norder by avg(price) desc, pub_name\n\")\n\n              pub_name\n1      Core Dump Books\n2 Schandenfreude Press\n\n\nThese are the average selling prices of the books for the various authors (The rows are sorted in decreasing order by average price.)\n\nsqldf(\"\nselect round(avg(price),2) as 'avg price of authors books', au_fname,   au_lname\nfrom authors join title_authors on authors.au_id = title_authors. au_id\n             join titles on title_authors.title_id = titles.title_id\ngroup by authors.au_id, au_fname, au_lname\norder by avg(price) desc, au_lname, au_fname\n\")\n\n  avg price of authors books  au_fname  au_lname\n1                      39.95 Christian     Kells\n2                      23.98     Sarah   Buchman\n3                      18.96     Wendy Heydemark\n4                      12.97      Klee      Hull\n5                      10.65    Harvey   Kellsey\n6                      10.49    Hallie      Hull\n\n\nTherefore the authors who are potential introduction candidates are those whose avg book price is higher than average, ie.\n\nsqldf(\"\nselect au_fname,   au_lname\nfrom authors join title_authors on authors.au_id = title_authors. au_id\n             join titles on title_authors.title_id = titles.title_id\ngroup by authors.au_id, au_fname, au_lname\nhaving avg(price) &gt; (select avg(price) from titles)\norder by avg(price) desc, au_lname, au_fname\n\")\n\n   au_fname  au_lname\n1 Christian     Kells\n2     Sarah   Buchman\n3     Wendy Heydemark\n\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using EXCEPT with subqueries\n\n\n\n\n\nAnswer using EXCEPT with subqueries\n\nsqldf(\"\n-- These are the potential matchups for highEnd authors/publishers\nselect pub_name, au_fname, au_lname\nfrom (\n        select authors.au_id, au_fname, au_lname\n        from authors join title_authors on authors.au_id = title_authors.au_id\n                     join titles on title_authors.title_id = titles.title_id\n        group by authors.au_id, au_fname, au_lname\n        having avg(price) &gt; \n                 (select avg(price) from titles)\n     ) as highEndAuthors\n     \n     cross join\n\n     (\n        select publishers.pub_id, pub_name\n        from publishers join titles on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name\n        having avg(price) &gt; \n                  (select avg(price) from titles)\n     ) as highEndPublishers\n\nEXCEPT\n\n-- These are the existing publisher/author relationships\nselect pub_name , au_fname, au_lname\nfrom authors join title_authors on authors.au_id = title_authors. au_id\n             join titles on title_authors.title_id = titles.title_id\n             join publishers on titles.pub_id = publishers.pub_id\n\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname\n1      Core Dump Books     Sarah   Buchman\n2      Core Dump Books     Wendy Heydemark\n3 Schandenfreude Press Christian     Kells\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using EXCEPT with Common Table Expressions (CTE)\n\n\n\n\n\n\n\n              pub_name  au_fname  au_lname\n1      Core Dump Books     Sarah   Buchman\n2      Core Dump Books     Wendy Heydemark\n3 Schandenfreude Press Christian     Kells\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using NOT EXISTS with subqueries\n\n\n\n\n\n\nsqldf(\"\nselect pub_name, au_fname, au_lname\nfrom (\n        select authors.au_id, au_fname, au_lname\n        from authors join title_authors on authors.au_id = title_authors.au_id\n                     join titles on title_authors.title_id = titles.title_id\n        group by authors.au_id, au_fname, au_lname\n        having avg(price) &gt; \n                 (select avg(price) from titles)\n     ) as highEndAuthors\n     \n     cross join\n\n     (\n        select publishers.pub_id, pub_name\n        from publishers join titles on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name\n        having avg(price) &gt; \n                  (select avg(price) from titles)\n     ) as highEndPublishers\n\nwhere not exists\n     (  -- correlated subquery\n        select 1\n        from authors join title_authors on authors.au_id = title_authors. au_id\n                     join titles on title_authors.title_id = titles.title_id\n                     join publishers on titles.pub_id = publishers.pub_id\n        where publishers.pub_id = highEndPublishers.pub_id and\n              authors.au_id = highEndAuthors.au_id\n     )\n\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname\n1      Core Dump Books     Sarah   Buchman\n2      Core Dump Books     Wendy Heydemark\n3 Schandenfreude Press Christian     Kells\n\n\n\n\n\n\n\n\n\n\n\nClick for answer - using NOT EXISTS with Common Table Expressions (CTE)**\n\n\n\n\n\n\nsqldf(\"\nwith\n  highEndAuthors as \n  (\n        select authors.au_id, au_fname, au_lname\n        from authors join title_authors on authors.au_id = title_authors.au_id\n                     join titles on title_authors.title_id = titles.title_id\n        group by authors.au_id, au_fname, au_lname\n        having avg(price) &gt; \n                 (select avg(price) from titles)\n  ),\n  \n  highEndPublishers as\n  (\n        select publishers.pub_id, pub_name\n        from publishers join titles on publishers.pub_id = titles.pub_id\n        group by publishers.pub_id, pub_name\n        having avg(price) &gt; \n                  (select avg(price) from titles)\n  ),\n\n  existingRelationships as \n     (  -- correlated subquery\n        select 1\n        from authors join title_authors on authors.au_id = title_authors. au_id\n                     join titles on title_authors.title_id = titles.title_id\n                     join publishers on titles.pub_id = publishers.pub_id\n        where publishers.pub_id = highEndPublishers.pub_id and\n              authors.au_id = highEndAuthors.au_id\n     )\n  \nselect pub_name, au_fname, au_lname\nfrom highEndAuthors cross join highEndPublishers\nwhere not exists\n     ( select 1 from existingRelationships )\n\norder by pub_name, au_lname, au_fname\n\")\n\n              pub_name  au_fname  au_lname\n1      Core Dump Books     Sarah   Buchman\n2      Core Dump Books     Wendy Heydemark\n3 Schandenfreude Press Christian     Kells\n\n\n\n\n\n\n\nQuestion 46\nShow the names of the most prolific authors, i.e. the authors who have written the most books. Note that there could be several different authors who are “tied” for the most number of books.\nSort the names in alphabetical order (last name, first name) Also show the au_id and the number of titles that the author wrote.\nUsing the data shown above, the output should be:\n\n\n  au_id au_fname  au_lname numTitles\n1   A02    Wendy Heydemark         4\n2   A04     Klee      Hull         4\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\nThis is a rather complex answer. This approach uses a subquery within a subquery. There may be other approaches too. There may be simpler answers too.\nTo explain we will show the subqueries and what they produce. Then we will show the full query that includes the subquery.\nThe following query is NOT the answer. It is simply a query that shows the number of books that were written by each author. This will be used as a subquery in the answer shown below.\n\nsqldf(\"\n          select authors.au_id, count(*) as numTitles\n          from authors join title_authors on authors.au_id = title_authors.au_id\n          group by authors.au_id, au_fname, au_lname\n\")\n\n  au_id numTitles\n1   A01         3\n2   A02         4\n3   A03         2\n4   A04         4\n5   A05         1\n6   A06         3\n\n\nWe’d like to get the max number of titles that an author wrote. This amounts to getting the highest number from the numTitles column in the previous query. We can do that by using the above query as a subquery in the from clause.\n\nsqldf(\"\n          select max(numTitles2) \n          from \n               (select authors.au_id, count(*) as numTitles2\n                from authors join title_authors on authors.au_id = title_authors.au_id\n                group by authors.au_id, au_fname, au_lname)\n      \")\n\n  max(numTitles2)\n1               4\n\n\nFinally we can put this all together to answer our question. The following is the FULL ANSWER to the original question. It uses the above query as a subquery in the having clause (see the code below). Note that in the following code there is a subquery inside of a subquery.\nFINAL ANSWER:\n\nsqldf(\"\nselect authors.au_id, au_fname, au_lname, count(*) as numTitles\nfrom authors join title_authors on authors.au_id = title_authors.au_id\ngroup by authors.au_id, au_fname, au_lname\nhaving numTitles = \n          (select max(numTitles2) \n           from \n               (select authors.au_id, count(*) as numTitles2\n                from authors join title_authors on authors.au_id = title_authors.au_id\n                group by authors.au_id, au_fname, au_lname))\norder by au_lname, au_fname\n\")\n\n  au_id au_fname  au_lname numTitles\n1   A02    Wendy Heydemark         4\n2   A04     Klee      Hull         4\n\n\n\n\n\n\n\nQuestion 47\nShow all topics and the authors who wrote books on those topics. Sort the ouput in alphabetical order by topic and within each topic in alphabetical order by the author. The first name and last name of each author should be combined into a single column called ‘author’.\nUsing the data shown above, the output should be:\n\n\n        type          author\n1  biography Wendy Heydemark\n2  biography       Klee Hull\n3   children  Harvey Kellsey\n4   computer Christian Kells\n5    history   Sarah Buchman\n6 psychology     Hallie Hull\n7 psychology       Klee Hull\n8 psychology  Harvey Kellsey\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n    select type, au_fname || ' ' || au_lname as author\n        from titles join title_authors on titles.title_id = title_authors.title_id\n                join authors on title_authors.au_id = authors.au_id\n        group by type, authors.au_id, au_fname, au_lname\n        order by type, au_lname, au_fname;\n\")\n\n\n\n\n\n\nQuestion 48\nAnswer all parts of this question\n\nShow all authors who wrote at least one biography or a history book for Abatis Publishers. Do NOT show an author’s name more than once even if they wrote several books.\nUsing the data shown above, the output should be:\n\n\n  au_fname  au_lname\n1    Sarah   Buchman\n2    Wendy Heydemark\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n  select distinct au_fname, au_lname\n  from authors join title_authors on authors.au_id = title_authors.au_id\n               join titles on title_authors.title_id = titles.title_id\n               join publishers on titles.pub_id = publishers.pub_id\n  where publishers.pub_name = 'Abatis Publishers' and\n        (titles.type = 'biography' or titles.type = 'history')\n  order by au_lname, au_fname\n\")\n\n\n\n\nShow all authors who wrote at least one psychology book for Abatis Publishers. Do NOT show an author’s name more than once even if they wrote several books.\nUsing the data shown above, the output should be:\n\n\n  au_fname au_lname\n1   Hallie     Hull\n2     Klee     Hull\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n  select distinct au_fname, au_lname\n  from authors join title_authors on authors.au_id = title_authors.au_id\n               join titles on title_authors.title_id = titles.title_id\n               join publishers on titles.pub_id = publishers.pub_id\n  where publishers.pub_name = 'Abatis Publishers' and\n        titles.type = 'psychology'\n  order by au_lname, au_fname\n\")\n\n\n\n\nAbatis Publishers believes publishing a book on the history of psychology during Sigmund Freud’s era would be a worthwhile endeavor. They think it would be best to get two authors to collaborate on the book - one author who has a psychology background and one author who has a history or biography background.\nIn order to brainstorm which authors might work together well, Abatis Publishers would like to see a list of each author who wrote a history or biography book with them matched up with each author who wrote a pschology book with them. The publisher will then look through the list and think about which combination of authors might make the most sense - then they’ll try to contact the chosen authors and persuade them to take on the project.\nWrite a query that returns two columns.\nThe first column should be named ‘history/biography author’ and contain names of authors who wrote a history or biography book with Abatis (combine the authors first and last names into a single column).\nThe second column should be named ‘psychology author’ and contain names of authors who wrote a psychology book with Abatis.\nThe output should show every combination of a history/biogrphy author who wrote for Abatis with a psychology author who wrote for Abatis.\nUsing the data shown above, the output should be:\n\n\n  history/biography author psychology author\n1            Sarah Buchman       Hallie Hull\n2            Sarah Buchman         Klee Hull\n3          Wendy Heydemark       Hallie Hull\n4          Wendy Heydemark         Klee Hull\n\n\n\n\n\n\n\n\nClick for answer\n\n\n\n\n\n\nsqldf(\"\n  select hist_bio_authors.au_fname || ' ' || hist_bio_authors.au_lname \n             as 'history/biography author', \n         psych_authors.au_fname || ' ' || psych_authors.au_lname\n             as 'psychology author' \n  from \n      ( -- this is the query from part (a)\n        select distinct au_fname, au_lname\n          from authors join title_authors on authors.au_id = title_authors.au_id\n                       join titles on title_authors.title_id = titles.title_id\n                       join publishers on titles.pub_id = publishers.pub_id\n          where publishers.pub_name = 'Abatis Publishers' and\n                (titles.type = 'biography' or titles.type = 'history')\n          order by au_lname, au_fname\n        ) as hist_bio_authors\n\n      cross join\n\n      ( -- this is the query from part (b)\n          select distinct au_fname, au_lname\n          from authors join title_authors on authors.au_id = title_authors.au_id\n                       join titles on title_authors.title_id = titles.title_id\n                     join publishers on titles.pub_id = publishers.pub_id\n          where publishers.pub_name = 'Abatis Publishers' and\n              titles.type = 'psychology'\n        order by au_lname, au_fname\n      ) as psych_authors\n\")",
    "crumbs": [
      "Relational databases and SQL",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> Database Questions (and Answers)</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html",
    "href": "github-000700-markdownGithubVsQuarto.html",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "",
    "text": "41.1 Feature Comparison Tables",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html#sec-feature-tables",
    "href": "github-000700-markdownGithubVsQuarto.html#sec-feature-tables",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "",
    "text": "41.1.1 Features Supported by Both GitHub and Quarto\n\n\n\nFeature\nGitHub Markdown\nQuarto (.qmd)\nNotes\n\n\n\n\nBasic Markdown\n✅\n✅\nStandard Markdown syntax (headers, lists, links, etc.)\n\n\nYAML Front Matter\n⚠️ Basic support (click here for more info)\n✅ Extensive support\nUsed for metadata and document configuration\n\n\nMath Notation\n⚠️ Basic LaTeX (click here for more info)\n✅ Advanced LaTeX\nMathematical equations and formulas\n\n\nTask Lists\n✅ Advanced\n⚠️ Basic support (click here for more info)\nCheckboxes for task items\n\n\nHTML Support\n⚠️ Restricted (click here for more info)\n✅ Extensive\nEmbedding raw HTML within Markdown\n\n\n\n\n\n41.1.2 Features Only in GitHub Markdown\n\n\n\nFeature\nDescription\n\n\n\n\nAuto-linking\nAutomatically converts issue numbers, commit SHAs, and usernames to links\n\n\nEmoji Shortcodes\nSupports emoji codes like :smile:\n\n\nUser Mentions\nSupports @username mentions\n\n\nIssue References\nLinks to issues with #123\n\n\nPR References\nLinks to pull requests with #123\n\n\nCommit References\nLinks to commits with SHA hashes\n\n\n\n\n\n41.1.3 Features Only in Quarto Markdown\n\n\n\nFeature\nDescription\n\n\n\n\nCode Execution\nCan execute code chunks and display results inline\n\n\nMultiple Languages\nSupports multiple programming languages in one document\n\n\nCitations\nBibTeX/CSL integration for academic referencing\n\n\nCross-references\nReferences to figures, tables, sections, and equations\n\n\nMulti-format Output\nPublishes to PDF, HTML, DOCX, presentations\n\n\nCallouts/Admonitions\nSpecial highlighted boxes for notes, warnings, tips\n\n\nTabsets\nTab interfaces for organizing content\n\n\nLayout Options\nColumn layouts, panel arrangements, page breaks\n\n\nExecution Options\nControl over code visibility, evaluation, and caching",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html#sec-examples-table",
    "href": "github-000700-markdownGithubVsQuarto.html#sec-examples-table",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "41.2 Feature Examples Table",
    "text": "41.2 Feature Examples Table\n\n\n\n\n\n\n\n\nFeature\nGitHub Markdown Example\nQuarto (.qmd) Example\n\n\n\n\nYAML Front Matter\n---\ntitle: Document Title\n---\n---\ntitle: \"Document Title\"\nauthor: \"Author Name\"\ndate: \"2023-01-15\"\nformat: \n  html:\n    toc: true\n    code-fold: true\n  pdf:\n    geometry: margin=1in\nbibliography: references.bib\n---\n\n\nCode Blocks\n```python\ndef hello_world():\n    print(\"Hello, world!\")\n```\n```{python}\n#| label: fig-scatter\n#| fig-cap: \"A scatter plot\"\n#| echo: false\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmpg = sns.load_dataset(\"mpg\")\nsns.scatterplot(data=mpg, \n                x=\"horsepower\", y=\"mpg\")\n```\n\n\nMath Notation\nWhen $a \\ne 0$, there are two \nsolutions to $ax^2 + bx + c = 0$:\n$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$\nWhen $a \\ne 0$, there are two \nsolutions to $ax^2 + bx + c = 0$:\n\n$$x = {-b \\pm \\sqrt{b^2-4ac} \n\\over 2a}$$ {#eq-quadratic}\n\nAs shown in @eq-quadratic...\n\n\nCitations\nNot supported natively\nAccording to @smith2020, the \nanalysis shows significant results.\n\n# References {.unnumbered}\n\n\nTask Lists\n- [x] Complete documentation\n- [ ] Write tests\n- [ ] Deploy to production\n::: {.task-list}\n- [x] Complete documentation\n- [ ] Write tests\n- [ ] Deploy to production\n:::\n\n\n\nAuto-linking\nSee issue #42 for details.\n@username can you review this?\nSee commit 8a3f79e for the fix.\nNot supported natively\n\n\nCross-references\nNot supported natively\nSee @fig-scatter for the visualization.\n\nAs shown in @sec-methods and \n@tbl-results, the approach works.\n\n\nCallouts/Admonitions\nNot supported natively\n::: {.callout-note}\nThis is an important note.\n:::\n\n::: {.callout-warning}\nWarning: Proceed with caution!\n:::\n\n\nTabsets\nNot supported natively\n::: {.panel-tabset}\n### Tab A\nContent for tab A\n\n### Tab B\nContent for tab B\n:::\n\n\nColumn Layout\nNot supported natively\n:::: {.columns}\n::: {.column width=\"40%\"}\nLeft column content\n:::\n::: {.column width=\"60%\"}\nRight column content\n:::\n::::",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html#sec-conflicts",
    "href": "github-000700-markdownGithubVsQuarto.html#sec-conflicts",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "41.3 Potential Conflicts Table",
    "text": "41.3 Potential Conflicts Table\n\n\n\nElement\nGitHub Markdown\nQuarto (.qmd)\nPotential Issue\n\n\n\n\nCode Blocks\n```python\n```{python} with options\nCode blocks from one system may not execute in the other\n\n\nTable Formatting\nBasic pipe tables\nEnhanced tables with options\nComplex tables may not render correctly across systems\n\n\nHeading IDs\nAuto-generated\nCan be explicitly set with {#id}\nLinks to headings may break when moving between systems\n\n\nImage Handling\n![alt](src)\n![alt](src){width=50%}\nImage formatting options won't work in GitHub\n\n\nFootnotes\n[^1] with note at bottom\nSimilar but with more options\nMinor syntax differences may cause rendering issues\n\n\nHTML Elements\nLimited, sanitized\nMore extensive support\nHTML that works in Quarto may be stripped in GitHub",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html#sec-example",
    "href": "github-000700-markdownGithubVsQuarto.html#sec-example",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "41.4 Self-Contained Example",
    "text": "41.4 Self-Contained Example\nHere’s how you might reference different sections of this document in Quarto:\nAs we can see in Section 41.1, Quarto offers many capabilities that GitHub Markdown doesn’t support.\nThe Section 41.2 section demonstrates syntax differences between the two formats.\nSection 41.3 highlights potential issues when converting between formats.",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "github-000700-markdownGithubVsQuarto.html#sec-notes",
    "href": "github-000700-markdownGithubVsQuarto.html#sec-notes",
    "title": "41   GitHub Markdown vs. Quarto (.qmd) Comparison",
    "section": "41.5 Notes and Details",
    "text": "41.5 Notes and Details\n\n41.5.1 Note 1: Basic YAML Front Matter Support in GitHub\nGitHub recognizes YAML front matter primarily for Jekyll integration. It supports basic metadata like title, description, and permalink, but lacks Quarto’s extensive formatting controls, bibliography management, execution options, and multi-format configuration capabilities.\nIn GitHub, YAML front matter is primarily used for Jekyll site generation, while in Quarto it controls virtually every aspect of document rendering and behavior.\n[Return to table]\n\n\n41.5.2 Note 2: Basic LaTeX Support in GitHub\nGitHub’s LaTeX support is implemented through MathJax and handles basic equations well, but lacks features such as equation numbering, cross-referencing of equations, alignment environments (like align, gather), and theorem environments.\nIt also doesn’t support LaTeX packages or custom commands that Quarto can handle through the full LaTeX engine it uses for PDF output.\n[Return to table]\n\n\n41.5.3 Note 3: Restricted HTML in GitHub\nGitHub sanitizes HTML for security reasons, restricting certain tags (like &lt;script&gt;, &lt;style&gt;, some &lt;iframe&gt; usages) and attributes (especially JavaScript event handlers). Many advanced HTML features that work in Quarto will be stripped in GitHub.\nGitHub also doesn’t support custom styling through CSS classes to the extent Quarto does. Quarto allows direct HTML integration with much fewer restrictions when generating HTML output.\n[Return to table]\n\n\n41.5.4 Note 4: Basic Task List Support in Quarto\nWhile Quarto does support task lists with checkbox syntax, GitHub’s implementation is more feature-rich and integrated with its platform capabilities:\n\nGitHub Task List Features:\n\nIntegration with GitHub Issues and Pull Requests\nInteractive checkboxes that can be toggled directly in the interface\nAutomatic task tracking and progress reporting\nTask assignment and project management integration\nNotifications when tasks are completed\n\nQuarto Task List Limitations:\n\nPrimarily for display purposes\nNo interactive toggling in standard output formats\nLacks integration with project management systems\nNo automatic tracking or status updates\nRequires a div container with the .task-list class for proper rendering\n\n\nQuarto’s task lists are functionally static in most output formats, while GitHub’s implementation provides a more interactive, workflow-oriented experience.\n[Return to table]",
    "crumbs": [
      "Git and Github",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'><yrChapterNumber></yrChapterNumber> GitHub Markdown vs. Quarto (.qmd) Comparison</span>"
    ]
  },
  {
    "objectID": "050050-tdyvrsGgplot2-v0001.html",
    "href": "050050-tdyvrsGgplot2-v0001.html",
    "title": "42  39.  Intro to ggplot2",
    "section": "",
    "text": "42.1 Code from sections 1.1 thorugh 1.3 of r4ds2e\nBelow is a summary of the code that we went through.\nPlease see the following webpage for more info: https://r4ds.hadley.nz/data-visualize\nif(!require(palmerpenguins)){install.packages(\"palmerpenguins\");require(palmerpenguins);}\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\n\nif(!require(dplyr)){install.packages(\"dplyr\");require(dplyr);} # needed for glimpse\nif(!require(ggplot2)){install.packages(\"ggplot2\");require(ggplot2);}\n# Here are the first few rows data.\n# When viewing a tibble, you may not see all the columns if your screen is too narrow.\n\npenguins\n\nError: object 'penguins' not found\n\n# You can use the dplyr::glimpse function to \n# view the names and datatypes of ALL the columns as well as \n# view the first few values of each column.\nglimpse(penguins)\n\nError: object 'penguins' not found\n\n# You can also use the following command to \"View\" the entire tibble in the \n# RStudio viewer window:\n#\n#View(penguins)  # It is a capital \"V\" in \"View\"\n\n# Setting up the \"aesthetics\"\n# This doesn't display any actual data.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\nError: object 'penguins' not found\n\n# You will start seeing \"data\" on the plot once you set the \"geometry\".\n# Here we set the \"geometry\" to be geom_point().\n# Each \"dot\" on the plot represents a row of data from the tibble, i.e. one penguin.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nError: object 'penguins' not found\n\n# We can add some color and shapes to each dot on the plot based on the species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species, shape=species)\n) +\n  geom_point()\n\nError: object 'penguins' not found\n\n# The function call, geom_smooth(method = \"lm\") \n# adds linear regressions lines, one for each species. \n#\n# Since \"color=species, shape=species\" was mapped in the ggplot function, the data was\n# divided into 3 different subsets, one for each species. \n# That is why there are 3 different linear regression lines, one for\n# each species (compare this with the next plot).\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species, shape=species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nError: object 'penguins' not found\n\n# In the following plot, \"color=species, shape=species\", was moved\n# from the ggplot function to the geom_point function.\n# Since we did not set the color in the ggplot function we no longer\n# consider the data as three different subsets and we get a single linear \n# regression line for the entire set of data.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\nError: object 'penguins' not found\n\n# Finally, the following plot adds a title and subtitle to the graph\n# and labels for the x-axis, y-axis and legend.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Penguin Species\", shape = \"Penguin Species\"\n  )\n\nError: object 'penguins' not found",
    "crumbs": [
      "visualizations with ggplot2",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'><yrChapterNumber>39. </yrChapterNumber> Intro to ggplot2</span>"
    ]
  },
  {
    "objectID": "050050-tdyvrsGgplot2-v0001.html#other-stuff",
    "href": "050050-tdyvrsGgplot2-v0001.html#other-stuff",
    "title": "42  39.  Intro to ggplot2",
    "section": "42.2 Other stuff",
    "text": "42.2 Other stuff\nThe info above goes through the main ideas of how to use ggplot2. Using that knowledge you should be in good shape for learning on your own how to use other more advanced features of ggplot2.\nThe rest of the webpage, https://r4ds.hadley.nz/data-visualize, shows how to use several other features of ggplot2. The following topics are described on the rest of that webpage:\n\nOther geometries (histograms, box plots, etc)\nHow to use several other geometries (i.e. bar blots, histograms, density plots, box plots, stacked bar plots).\nFacets\nHow to break up a graph into several smaller graphs using the facet_wrap function.\nHow to save a plot to an image file\nYou can use the ggsave function to save an image file with a copy of the last plot that you created. You can then import the image file to other files, e.g. a Word document, a powerpoint, etc.\nSee ?ggsave for more info.",
    "crumbs": [
      "visualizations with ggplot2",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'><yrChapterNumber>39. </yrChapterNumber> Intro to ggplot2</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html",
    "href": "web-000300-internetArchitecture-v001.html",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "",
    "text": "43.1 Files for this presentation\nClick here to download a zip file that contains several files we will use in this presentation:\nhtmlExamples-v001.zip",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#browsers-communicate-with-web-servers",
    "href": "web-000300-internetArchitecture-v001.html#browsers-communicate-with-web-servers",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.2 browsers communicate with “web servers”",
    "text": "43.2 browsers communicate with “web servers”\nEvery website is managed by a computer. When you point your browser at a URL you are really initiating a conversation between your browser and and a program on the computer that is hosting the website. The program that manages the website is known as a web-server program. The computer that the website is hosted on is known as a web-server computer. Your browser is known as a “client program”.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#the-parts-of-a-url",
    "href": "web-000300-internetArchitecture-v001.html#the-parts-of-a-url",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.3 The parts of a URL",
    "text": "43.3 The parts of a URL\nA URL such as https://www.mygreatsite.com/trips/europe/eifelTower.html is comprised of several parts. The URL above is comprised of the following parts: (see this link for a more detailed discussion of the different part of a URL: https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_URL )\n\nhttps\nThis is known as the “scheme” (or the “protocol”). This is the “language” that is used by the browser to communicate with the web server program. For webpages, you will commonly see either “http” or “https”, however other schemes are possible, such as “ftp”, “mailto” as well as others. (Note that “https” and “http” are very different from “HTML”).\n://\nUsed to separate between the scheme (or protocol) and the rest of the URL (In more complicated URLs you may see a number between : and // but that is beyond the scope of our discussion here.)\nwww.mygreatwebsite.com\nThis is called the “domain name”. It is the “name” of the computer that is hosting the website.\n/trips/europe/eifelTower.html\nWhen the browser requests a URL, the web-server program sends a file back to the browser. Classically, these files are stored on the hard drive of the server computer. This part of the URL represents the path to the file on the server computer. (NOTE: For modern websites the response from the web-server to the browser is often generated “dynamically” in “real time” instead of being stored on the server computer in files. However, these responses can and often are stored in files. In any case, this part of the URL can be thought of as a path that helps the server computer understand exactly which information needs to be sent back to the browser.)",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#diagram",
    "href": "web-000300-internetArchitecture-v001.html#diagram",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.4 Diagram",
    "text": "43.4 Diagram\nWe’ll disucss the following diagram in class.\n\nThe diagram shows how computers on the Internet communicate with each other. Every computer has a unique number - called an IP address. In order for one computer to engage in a conversation with another computer, the first computer must know the IP address of the 2nd computer. (In this way an IP address is similar to a telephone number.) The diagram shows how your computer that is running the browser finds out the IP address (similar to a telephone number) of the computer that manages the website.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#ip-addresses-and-dns-servers",
    "href": "web-000300-internetArchitecture-v001.html#ip-addresses-and-dns-servers",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.5 IP addresses and DNS servers",
    "text": "43.5 IP addresses and DNS servers\nEach domain name, which represents a computer on the Internet, (e.g. www.mygreatwebsite.com) has a corresponding IP address. You can think of the domain name and IP address for a computer on the Internet as being analogous to a person’s name and the person’s phone number. In order for one computer on the Internet to contact another computer, the first computer must know the IP address of the computer it wants to contact.\nThe first time your browser tries to access a new website, the browser generally does NOT know the IP address for the computer running the website. Your browser must “look up” the IP address. If you don’t know a person’s phone number but you know their name you can “look up” their phone number. In the mid 1900s it was common for someone to “call an operator” to get someone’s phone number. This is similar to what happens with a browser when contacting a website for the first time. The browser needs to find out the IP address for the website. To find the IP address, the browser contacts another computer, known as a Domain Name Server (AKA, DNS server). The browser asks the DNS server to provide the IP address for the website’s domain name (e.g. www.mygreatwebsite.com). This IP address (e.g. 89.123.92.5) is then sent back to the browser. This communication between the browser and the DNS server is labeled in the diagram as steps (1) and (2). This all happens “behind the scenes” and you as a typical computer user is totally unaware that all of this is happening.\nThe browser then uses the IP address to contact the web-server for the website. This is shown in the diagram as steps (3) and (4).",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#how-does-a-computer-know-the-ip-address-of-the-dns-server",
    "href": "web-000300-internetArchitecture-v001.html#how-does-a-computer-know-the-ip-address-of-the-dns-server",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.6 How does a computer know the IP address of the DNS server?",
    "text": "43.6 How does a computer know the IP address of the DNS server?\nWe said that in order to communicate, a computer must know the IP address of the computer that it is trying to contact. If so, how does your computer know the IP address of the DNS server computer? The answer is simple. Your computer is configured with that information. This all happens when your computer is setup with Internet service.\nEvery Internet Service Provider has DNS servers that their users computers are configured to contact. Your computer has been configured to contact the DNS server for your Internet Service Provider. The following shows how to see your DNS server settings on Windows and Mac. Note that these instructions can become outdated with newer releases of Windows or Mac OS. If by the time you read this, these instructions are out of date, search online for something like “how do I see what DNS server I’m using on Mac” (or Windows 10, etc) https://www.lifewire.com/check-dns-settings-5215603",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#large-websites-are-run-by-many-different-computers",
    "href": "web-000300-internetArchitecture-v001.html#large-websites-are-run-by-many-different-computers",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.7 Large websites are run by many different computers",
    "text": "43.7 Large websites are run by many different computers\nLarge websites such as google.com and amazon.com have many different computers that respond to web requests.\nWe said that a domain name such as www.mygreatwebsite.com corresponds to a specific computer that is running a website. That’s not exactly true. Actually a specific IP address corresponds to a specific computer. When asked for the IP address of a domain name, the DNS server might respond with different IP addresses to different requests for the same domain name. In this way google, amazon and other large websites can spread the work (i.e. the load) of supporting the web requests over many different server computers. The details of how all of this is done is way beyond the scope of our discussion.\nThe main ideas you should walk away from this discussion with are:\n\na domain name represents a computer (or multiple computers) on the Internet\nan IP address represents a specific computer on the Internet\nGiven a domain name, a DNS server, responds with an IP address for that domain name.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#the-server-sends-html-css-and-javascript-code-back-to-your-browser",
    "href": "web-000300-internetArchitecture-v001.html#the-server-sends-html-css-and-javascript-code-back-to-your-browser",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.8 The server sends HTML, CSS and Javascript code back to your browser",
    "text": "43.8 The server sends HTML, CSS and Javascript code back to your browser\nWhen the server receives the request for the webpage it sends back one or more files that contain the “code” for the webpage. It is this code that the browser uses to know how to display the contents of the webpage. The code for a webpage is comprised of the following three languages.\n\nHTML: this language is used to describe the text and overall structure (e.g. headings, paragraphs, lists, tables, etc) of a webpage.\nCascading Style Sheets (CSS): this language is used to describe the styling (e.g. colors, fonts, etc) of the webpage\nJavaScript: this language is used to add “dynamic” features to the webpage, (e.g. menus that pop-up when users click on them).\n\nIt is possible for all three languages to be combined into a single file or for the different languages to be located in different files that the server sends to the browser. The rest of this document describes these three different languages.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#how-to-see-the-code-for-a-webpage",
    "href": "web-000300-internetArchitecture-v001.html#how-to-see-the-code-for-a-webpage",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.9 How to see the “code” for a webpage",
    "text": "43.9 How to see the “code” for a webpage\n\n43.9.1 view page source\nIf you right-click on any webpage you should see an option to “view page source” (or something similar, depending on which browser you are using). If you click on this option you will see the “source code” for the webpage.\n\n\n43.9.2 inspect\nSimilarly, if you right click on the webpage and choose “inspect” you will also see the source code for the webpage along with many tools for analyzing the underlying technologies that the browser uses to present the webpage. The details of how to use all of these tools are beyond the scope of this material. However, we will use some of these tools when we learn about how to do “web scraping”.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#html-example",
    "href": "web-000300-internetArchitecture-v001.html#html-example",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.10 HTML example",
    "text": "43.10 HTML example\nWe will be working with the following files from the htmlExamples.zip file (see the link at the top of this page). Download the zip file and extract the following files and place them in the same folder.\n\nstuffToDo.html\nyellowUmbrella.png\n\nDouble click on this file stuffToDo.html to open it in a browser.\nAlso open stuffToDo.html in a text editor (e.g. RStudio - File | Open File)\nSee this link for an overview of HTML. https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics\n\n43.10.1 HTML Tags you should know\n\nh1,h2,h3,h4,h5,h6 (for different level headings)\np - for a paragraph\nul,ol,li - for unordered lists, order lists and list items\nhr (also width attribute) - for horizontal “rules” (i.e. lines)\nimg (also src, width and alt attributes) - for images (i.e. pictures)\na (also href attribute) - for links to other pages\nstrong - to make text bold\nem - to make text italicized (em stands for emphasized)\nspan, div - these don’t have a noticeable effect on the page. They are mainly used in conjunction with CSS (see the next section to learn more about CSS). The span and div tags are used to apply id and class attributes to portions of the text that aren’t otherwise surrounded by other tags. The div tag is used to surround entire portions of the page. The span tag is used to surround portions of text that are inside other tags.\n\n\n\n43.10.2 HTML concepts you must know\n\nHTML element - e.g. &lt;h1&gt;My Stuff&lt;/h1&gt; - this is broken down into the following:\n\nstart tag - e.g. &lt;h1&gt;\nend tag - e.g. &lt;/h1&gt;\ncontents - e.g. My Stuff\n\nHTML empty tags - e.g. &lt;hr /&gt; or &lt;hr \"width=50%\" /&gt; or &lt;img src=\"smiley.jpg\" /&gt;\n\nthese types of tags do NOT have an end tag. They can end with a space followed by a forward slash to indicate that there is no end tag. Modern HTML allows for leaving off the space/ (e.g. &lt;hr&gt; instead of &lt;hr /&gt;, but even in modern HTML the space/ is often still included.\n\nHTML attributes\n\nAre placed inside of start tags or empty tags. An attribute contains a name and a value. The attributes affect how the HTML elements work.\nExample: in &lt;a href=\"https://google.com\"&gt;my favorite browser&lt;/a&gt;\n\nthe start tag is &lt;a href=\"https://google.com/\"&gt;\n\nhref=\"https://google.com/\" is an attribute.\n\nhref is the NAME of the attribute\n\"https://google.com/\" is the VALUE of the attribute\n\n\nthe end tag is &lt;/a&gt;",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#css-example",
    "href": "web-000300-internetArchitecture-v001.html#css-example",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.11 CSS example",
    "text": "43.11 CSS example\nHTML is used to layout the basic structure of a webpage.\nCSS is used to apply styles (e.g. colors, fonts, etc.) to a webpage.\nSee this link for an overview of using CSS selectors: https://developer.mozilla.org/en-US/docs/Learn/CSS/First_steps/Getting_started\nThe CSS for a webpage can be placed in the same file as the HTML code or in a different file. In the overview on the developer.mozilla.org website linked above the CSS is placed in separate file from the HTML code. This is done with the use of the &lt;link&gt; tag as shown in the snippet below. The &lt;link&gt; tag, which identifies the name of the file that contains the CSS code, is placed inside the HTML\n\nelement.\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"nameOfCssFile.css\" /&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    Rest of your webpage goes here\n  &lt;/body&gt;\n&lt;/html&gt;\nAnother option is to place the CSS code directly inside the HTML file. This is done by typing the CSS styling rules directly inside a &lt;style&gt; element that is placed inside the\n\nelement. For example:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;style&gt;\n       /*  CSS Styling rules go here */\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    Rest of your webpage goes here\n  &lt;/body&gt;\n&lt;/html&gt;\nWe will be working with the following files from the htmlExamples.zip file (see the link at the top of this page). This example uses the 2nd approach listed above (i.e. putting the CSS directly in the HTML file with the &lt;style&gt; element). Download the zip file and extract the following files and place them in the same folder.\n\nstuffToDo-withCss-v002.html\nyellowUmbrella.png\n\nDouble click on this file stuffToDo-withCss-v002.html to open it in a browser.\nAlso open stuffToDo-withCss-v002.html in a text editor (e.g. RStudio - File | Open File)\n\n43.11.1 Practice with CSS selectors\n\nUse the following website to learn about different CSS selectors and to practice using them:\nhttps://flukeout.github.io/\nSuggested answers are here:\nhttps://gist.github.com/chrisman/fcb0a88459cd98239dbe6d2d200b02d1\nAnother website for testing your knowledge of CSS selectors:\nhttps://css-speedrun.netlify.app/\nSuggested answers are here:\nhttps://github.com/Vincenius/css-speedrun\nThe following website is very useful for learning more about CSS as well as many other topics in technology: https://www.w3schools.com/w3css/defaulT.asp",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000300-internetArchitecture-v001.html#javascript-example",
    "href": "web-000300-internetArchitecture-v001.html#javascript-example",
    "title": "43  40.  How a webpage gets to your browser",
    "section": "43.12 Javascript example",
    "text": "43.12 Javascript example\nWe will be working with the following files from the htmlExamples.zip file (see the link at the top of this page). Download the zip file and extract the following file from the zip file before continuing.\n\njavascriptExample-v002.html\n\nDouble click on javascriptExample-v002.html to open it in a browser.\nAlso open javascriptExample-v002.html in a text editor (e.g. RStudio - File | Open File)",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'><yrChapterNumber>40. </yrChapterNumber> How a webpage gets to your browser</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html",
    "href": "web-000400-webscrapingContinued-015.html",
    "title": "44  41.  Web Scraping with R",
    "section": "",
    "text": "44.1 Ethics of webscraping\nSee the first section of this page for a discussion of the ethics and legal issues related to webscraping:\nhttps://r4ds.hadley.nz/webscraping",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#how-to-webscrape-with-r",
    "href": "web-000400-webscrapingContinued-015.html#how-to-webscrape-with-r",
    "title": "44  41.  Web Scraping with R",
    "section": "44.2 How to webscrape with R",
    "text": "44.2 How to webscrape with R\n\n##################################################################.\n##################################################################.\n##\n## In this file:\n##\n## - Using the rvest package to scrape information from a\n##   basic HTML website\n##\n## - Packaging up the code to scrape a page into a function\n##\n## - Using a loop to scrape several pages of information\n##   whose URLs differ in a predictable way.\n##\n## - Fixing fragile CSS selectors\n## \n## - Intro to regular expressions (regex)\n#################################################################.\n#################################################################.\n\n#-------------------------------------------------------------------------------\n# Some online resources\n#\n# video demonstrating basics of web scraping in R\n# - https://www.youtube.com/watch?v=v8Yh_4oE-Fs&t=275s\n#\n# Tips for editing HTML in VSCode\n# - https://code.visualstudio.com/docs/languages/html\n#\n# Tips on how to use VSCode in general:\n#   https://code.visualstudio.com/docs/getstarted/tips-and-tricks\n#\n# timing R code\n# - https://www.r-bloggers.com/2017/05/5-ways-to-measure-running-time-of-r-code/\n#\n# video - using RSelenium to scrape dynamic (ie. javascript) webpage\n# - https://www.youtube.com/watch?v=CN989KER4pA\n#\n# CSS selector rules\n# - https://flukeout.github.io/ \n# - answers to the \"game\": https://gist.github.com/humbertodias/b878772e823fd9863a1e4c2415a3f7b6\n#\n# Intro to regular expressions\n# - https://ryanstutorials.net/regular-expressions-tutorial/\n#-------------------------------------------------------------------------------\n\n# Install (if necessary) and load the rvest package\n\n# The following two lines will install and load the rvest package.\n# I commented out these lines in favor of the one line below\n#\n#install.packages(\"rvest\")    # installs it on your machine (only need to do this once on your machine)\n#library(rvest)              # do this everytime your start RStudio  (also require(rvest) works)\n\n# This one line will accomplish what the two lines above do. However,\n# this one line will not install the package if it's not necessary.\nif(!require(rvest)) install.packages(\"rvest\")   # this automatically also loads the \"xml2\" package\n\nLoading required package: rvest\n\nhelp(package=\"rvest\")\nhelp(package=\"xml2\")",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#download-weather-data",
    "href": "web-000400-webscrapingContinued-015.html#download-weather-data",
    "title": "44  41.  Web Scraping with R",
    "section": "44.3 Download weather data",
    "text": "44.3 Download weather data\nWe’ll be working with the following webpage\nhttps://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196#.Xl0j6BNKhTY",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#scraping-a-piece-of-data",
    "href": "web-000400-webscrapingContinued-015.html#scraping-a-piece-of-data",
    "title": "44  41.  Web Scraping with R",
    "section": "44.4 Scraping a piece of data",
    "text": "44.4 Scraping a piece of data\n\n###########################################################################\n#\n# Code to download high/low temperature data from the following website\n#\n###########################################################################\n\nurl = \"https://forecast.weather.gov/MapClick.php?lat=37.7771&lon=-122.4196#.Xl0j6BNKhTY\"\n\n\n# You can get a very sepecific CSS selector for a particular \n# part of a webpage by using the \"copy css selector\" feature of\n# your browser. Every browser has a slightly different way of doing this\n# but they are all similar. In Chrome do the following:\n#\n# (1) right click on the portion of the page you want to scrape\n#     and choose \"inspect\"\n# (2) right click on the portion of the HTML that contains what\n#     you want and choose \"Copy\" and then \"Copy Selector\"\n# \n# This gives you a very, very specific selector to get this info.\n# \n# For exmaple, we followed this process for the first high temperature \n# for this webpage and got the following selector. \n#\n# CSS SELECTOR: (note the leading # on the line below is part of the selector)\n#    #seven-day-forecast-list &gt; li:nth-child(1) &gt; div &gt; p.temp.temp-high\n#\n# WARNING - this is a VERY specific selector! You will often need\n# to modify this selector so that it is LESS specific and gets \n# more than just the one piece of info you clicked on.\n\nweatherPage = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ncssSelector = \"#seven-day-forecast-list &gt; li:nth-child(1) &gt; div &gt; p.temp.temp-high\"\n\ntemperature = weatherPage %&gt;%\n  html_elements( cssSelector ) %&gt;%\n  html_text2()\n\nError: object 'weatherPage' not found\n\ntemperature\n\nError: object 'temperature' not found\n\nx = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nx\n\nError: object 'x' not found\n\ny = html_elements(x, cssSelector )\n\nError: object 'x' not found\n\ny\n\nError: object 'y' not found\n\nz = html_text(y)\n\nError: object 'y' not found\n\nz\n\nError: object 'z' not found\n\n# What is the structure of x?\nstr(x)\n\nError: object 'x' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#choosing-better-css-selectors",
    "href": "web-000400-webscrapingContinued-015.html#choosing-better-css-selectors",
    "title": "44  41.  Web Scraping with R",
    "section": "44.5 Choosing better CSS Selectors",
    "text": "44.5 Choosing better CSS Selectors\n\n####################################################################.\n# If we analyze the HTML code, a MUCH BETTER selector is .temp\n# This selects all html elements that have class=\"temp\"\n####################################################################.\n\n#------------------------------------------------------------------.\n# NOTE: \n#\n# When webscraping you should \"play nicely\" with the website.\n# We already read the page above using the following line of code:\n#\n#    weatherPage = read_html(url)\n#\n# Don't do this again. Every time you read the page you are causing\n# the website to do some work to generate the page. You also use\n# network \"bandwidth\". \n#------------------------------------------------------------------.\n\n# Don't do this again - we already did it. Read comment above for more info.\n#\n# weatherPage = read_html(url)\n\ncssSelector = \".temp\"\n\nforecasts &lt;- weatherPage %&gt;%\n  html_elements( cssSelector ) %&gt;%\n  html_text()\n\nError: object 'weatherPage' not found\n\nforecasts\n\nError: object 'forecasts' not found\n\nx = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nx\n\nError: object 'x' not found\n\ny = html_elements(x, cssSelector )\n\nError: object 'x' not found\n\ny\n\nError: object 'y' not found\n\nz = html_text(y)\n\nError: object 'y' not found\n\nz\n\nError: object 'z' not found\n\n# What is the structure of x?\nstr(x)\n\nError: object 'x' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#scraping-country-names",
    "href": "web-000400-webscrapingContinued-015.html#scraping-country-names",
    "title": "44  41.  Web Scraping with R",
    "section": "44.6 Scraping country names",
    "text": "44.6 Scraping country names\n\n###########################################################################\n#\n# Get the country names off the following website\n#\n###########################################################################\n\nurl = \"https://scrapethissite.com/pages/simple/\"\ncssSelector = \".country-name\"\n\ncountries &lt;- read_html(url) %&gt;%\n  html_elements( cssSelector ) %&gt;%\n  html_text()\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nx = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nx\n\nError: object 'x' not found\n\ny = html_elements(x, cssSelector)\n\nError: object 'x' not found\n\ny\n\nError: object 'y' not found\n\nz = html_text(y)\n\nError: object 'y' not found\n\nz\n\nError: object 'z' not found\n\n#..............................................................................\n# Let's examine the results.\n# \n# Notice that the results includes the newlines (\\n) and blanks from the .html\n# file. This is because we picked up EVERYTHING that appears between the\n# start tag and end tag in the HTML.\n#\n# Below, we will see how to fix up the results below by using the gsub function.\n#..............................................................................\n\ncountries  \n\nError: object 'countries' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#same-thing-without-pipes",
    "href": "web-000400-webscrapingContinued-015.html#same-thing-without-pipes",
    "title": "44  41.  Web Scraping with R",
    "section": "44.7 Same thing without pipes",
    "text": "44.7 Same thing without pipes\n\n###########################################################################\n#\n# same thing a slightly different way (without magrittr pipes)\n#\n###########################################################################\n\nurl = \"https://scrapethissite.com/pages/simple/\"\ncssSelector = \".country-name\"\n\nwhole_html_page = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ncountry_name_html =  html_elements( whole_html_page, cssSelector )\n\nError: object 'whole_html_page' not found\n\njust_the_text =   html_text(country_name_html)\n\nError: object 'country_name_html' not found\n\n#...........................\n# Let's examine each part \n#...........................\n\n# the contents of the entire HTML page\nwhole_html_page      \n\nError: object 'whole_html_page' not found\n\n# just the HTML tags that we targeted with the cssSelector\ncountry_name_html    \n\nError: object 'country_name_html' not found\n\n# remove the actual start and end tags ... leaving just the text.\n# again ... notice that this picked up the newlines (\\n) and blanks from the .html file\njust_the_text  \n\nError: object 'just_the_text' not found\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# Let's look a little closer at the contents of what is returned by read_html\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n# We already had this above, just reviewing it again ...\nurl = \"https://scrapethissite.com/pages/simple/\"\nwhole_html_page = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ncountry_name_html =  html_elements( whole_html_page, cssSelector )\n\nError: object 'whole_html_page' not found\n\njust_the_text =   html_text(country_name_html)\n\nError: object 'country_name_html' not found\n\n# What is the structure of this data?\nstr(whole_html_page) # list of 2 externalptr objects (see below)\n\nError: object 'whole_html_page' not found\n\n# What is the class of this data? \nclass(whole_html_page)   # \"xml_document\" \"xml_node\"\n\nError: object 'whole_html_page' not found\n\nmethods(print)    # ... print.xml_document* ... print.xml_node*\n\n  [1] print.acf*                                          \n  [2] print.activeConcordance*                            \n  [3] print.AES*                                          \n  [4] print.anova*                                        \n  [5] print.aov*                                          \n  [6] print.aovlist*                                      \n  [7] print.ar*                                           \n  [8] print.Arima*                                        \n  [9] print.arima0*                                       \n [10] print.AsIs                                          \n [11] print.aspell*                                       \n [12] print.aspell_inspect_context*                       \n [13] print.bibentry*                                     \n [14] print.Bibtex*                                       \n [15] print.browseVignettes*                              \n [16] print.by                                            \n [17] print.cache_info*                                   \n [18] print.changedFiles*                                 \n [19] print.check_bogus_return*                           \n [20] print.check_code_usage_in_package*                  \n [21] print.check_compiled_code*                          \n [22] print.check_demo_index*                             \n [23] print.check_depdef*                                 \n [24] print.check_details*                                \n [25] print.check_details_changes*                        \n [26] print.check_doi_db*                                 \n [27] print.check_dotInternal*                            \n [28] print.check_make_vars*                              \n [29] print.check_nonAPI_calls*                           \n [30] print.check_package_code_assign_to_globalenv*       \n [31] print.check_package_code_attach*                    \n [32] print.check_package_code_data_into_globalenv*       \n [33] print.check_package_code_startup_functions*         \n [34] print.check_package_code_syntax*                    \n [35] print.check_package_code_unload_functions*          \n [36] print.check_package_compact_datasets*               \n [37] print.check_package_CRAN_incoming*                  \n [38] print.check_package_datalist*                       \n [39] print.check_package_datasets*                       \n [40] print.check_package_depends*                        \n [41] print.check_package_description*                    \n [42] print.check_package_description_encoding*           \n [43] print.check_package_license*                        \n [44] print.check_packages_in_dir*                        \n [45] print.check_packages_used*                          \n [46] print.check_po_files*                               \n [47] print.check_pragmas*                                \n [48] print.check_Rd_line_widths*                         \n [49] print.check_Rd_metadata*                            \n [50] print.check_Rd_xrefs*                               \n [51] print.check_RegSym_calls*                           \n [52] print.check_S3_methods_needing_delayed_registration*\n [53] print.check_so_symbols*                             \n [54] print.check_T_and_F*                                \n [55] print.check_url_db*                                 \n [56] print.check_vignette_index*                         \n [57] print.checkDocFiles*                                \n [58] print.checkDocStyle*                                \n [59] print.checkFF*                                      \n [60] print.checkRd*                                      \n [61] print.checkRdContents*                              \n [62] print.checkReplaceFuns*                             \n [63] print.checkS3methods*                               \n [64] print.checkTnF*                                     \n [65] print.checkVignettes*                               \n [66] print.citation*                                     \n [67] print.cli_ansi_html_style*                          \n [68] print.cli_ansi_string*                              \n [69] print.cli_ansi_style*                               \n [70] print.cli_boxx*                                     \n [71] print.cli_diff_chr*                                 \n [72] print.cli_doc*                                      \n [73] print.cli_progress_demo*                            \n [74] print.cli_rule*                                     \n [75] print.cli_sitrep*                                   \n [76] print.cli_spark*                                    \n [77] print.cli_spinner*                                  \n [78] print.cli_tree*                                     \n [79] print.codoc*                                        \n [80] print.codocClasses*                                 \n [81] print.codocData*                                    \n [82] print.colorConverter*                               \n [83] print.compactPDF*                                   \n [84] print.condition                                     \n [85] print.connection                                    \n [86] print.CRAN_package_reverse_dependencies_and_views*  \n [87] print.curl_handle*                                  \n [88] print.curl_multi*                                   \n [89] print.data.frame                                    \n [90] print.Date                                          \n [91] print.default                                       \n [92] print.dendrogram*                                   \n [93] print.density*                                      \n [94] print.difftime                                      \n [95] print.dist*                                         \n [96] print.Dlist                                         \n [97] print.DLLInfo                                       \n [98] print.DLLInfoList                                   \n [99] print.DLLRegisteredRoutines                         \n[100] print.document_context*                             \n[101] print.document_position*                            \n[102] print.document_range*                               \n[103] print.document_selection*                           \n[104] print.dummy_coef*                                   \n[105] print.dummy_coef_list*                              \n[106] print.ecdf*                                         \n[107] print.eigen                                         \n[108] print.evaluate_evaluation*                          \n[109] print.factanal*                                     \n[110] print.factor                                        \n[111] print.family*                                       \n[112] print.fileSnapshot*                                 \n[113] print.findLineNumResult*                            \n[114] print.form_data*                                    \n[115] print.form_file*                                    \n[116] print.formula*                                      \n[117] print.fseq*                                         \n[118] print.ftable*                                       \n[119] print.function                                      \n[120] print.getAnywhere*                                  \n[121] print.glm*                                          \n[122] print.glue*                                         \n[123] print.handle*                                       \n[124] print.hashtab*                                      \n[125] print.hclust*                                       \n[126] print.help_files_with_topic*                        \n[127] print.hexmode                                       \n[128] print.HoltWinters*                                  \n[129] print.hsearch*                                      \n[130] print.hsearch_db*                                   \n[131] print.htest*                                        \n[132] print.html*                                         \n[133] print.html_dependency*                              \n[134] print.htmltools.selector*                           \n[135] print.htmltools.selector.list*                      \n[136] print.htmlwidget*                                   \n[137] print.infl*                                         \n[138] print.integrate*                                    \n[139] print.isoreg*                                       \n[140] print.json*                                         \n[141] print.key_missing*                                  \n[142] print.kmeans*                                       \n[143] print.knitr_kable*                                  \n[144] print.Latex*                                        \n[145] print.LaTeX*                                        \n[146] print.libraryIQR                                    \n[147] print.lifecycle_warnings*                           \n[148] print.listof                                        \n[149] print.lm*                                           \n[150] print.loadings*                                     \n[151] print.loess*                                        \n[152] print.logLik*                                       \n[153] print.ls_str*                                       \n[154] print.medpolish*                                    \n[155] print.MethodsFunction*                              \n[156] print.mtable*                                       \n[157] print.NativeRoutineList                             \n[158] print.news_db*                                      \n[159] print.nls*                                          \n[160] print.noquote                                       \n[161] print.numeric_version                               \n[162] print.oauth_app*                                    \n[163] print.oauth_endpoint*                               \n[164] print.object_size*                                  \n[165] print.octmode                                       \n[166] print.opts_list*                                    \n[167] print.packageDescription*                           \n[168] print.packageInfo                                   \n[169] print.packageIQR*                                   \n[170] print.packageStatus*                                \n[171] print.paged_df*                                     \n[172] print.pairwise.htest*                               \n[173] print.person*                                       \n[174] print.POSIXct                                       \n[175] print.POSIXlt                                       \n[176] print.power.htest*                                  \n[177] print.ppr*                                          \n[178] print.prcomp*                                       \n[179] print.princomp*                                     \n[180] print.proc_time                                     \n[181] print.quosure*                                      \n[182] print.quosures*                                     \n[183] print.R6*                                           \n[184] print.R6ClassGenerator*                             \n[185] print.raster*                                       \n[186] print.Rconcordance*                                 \n[187] print.Rd*                                           \n[188] print.recordedplot*                                 \n[189] print.request*                                      \n[190] print.response*                                     \n[191] print.restart                                       \n[192] print.RGBcolorConverter*                            \n[193] print.RGlyphFont*                                   \n[194] print.rlang:::list_of_conditions*                   \n[195] print.rlang_box_done*                               \n[196] print.rlang_box_splice*                             \n[197] print.rlang_data_pronoun*                           \n[198] print.rlang_dict*                                   \n[199] print.rlang_dyn_array*                              \n[200] print.rlang_envs*                                   \n[201] print.rlang_error*                                  \n[202] print.rlang_fake_data_pronoun*                      \n[203] print.rlang_lambda_function*                        \n[204] print.rlang_message*                                \n[205] print.rlang_trace*                                  \n[206] print.rlang_warning*                                \n[207] print.rlang_zap*                                    \n[208] print.rle                                           \n[209] print.rlib_bytes*                                   \n[210] print.rlib_error_3_0*                               \n[211] print.rlib_trace_3_0*                               \n[212] print.roman*                                        \n[213] print.rvest_field*                                  \n[214] print.rvest_form*                                   \n[215] print.rvest_session*                                \n[216] print.SavedPlots*                                   \n[217] print.scalar*                                       \n[218] print.sessionInfo*                                  \n[219] print.shiny.tag*                                    \n[220] print.shiny.tag.env*                                \n[221] print.shiny.tag.list*                               \n[222] print.shiny.tag.query*                              \n[223] print.simple.list                                   \n[224] print.smooth.spline*                                \n[225] print.socket*                                       \n[226] print.srcfile                                       \n[227] print.srcref                                        \n[228] print.stepfun*                                      \n[229] print.stl*                                          \n[230] print.StructTS*                                     \n[231] print.subdir_tests*                                 \n[232] print.summarize_CRAN_check_status*                  \n[233] print.summary.aov*                                  \n[234] print.summary.aovlist*                              \n[235] print.summary.ecdf*                                 \n[236] print.summary.glm*                                  \n[237] print.summary.lm*                                   \n[238] print.summary.loess*                                \n[239] print.summary.manova*                               \n[240] print.summary.nls*                                  \n[241] print.summary.packageStatus*                        \n[242] print.summary.ppr*                                  \n[243] print.summary.prcomp*                               \n[244] print.summary.princomp*                             \n[245] print.summary.table                                 \n[246] print.summary.warnings                              \n[247] print.summaryDefault                                \n[248] print.suppress_viewer*                              \n[249] print.table                                         \n[250] print.tables_aov*                                   \n[251] print.terms*                                        \n[252] print.ts*                                           \n[253] print.tskernel*                                     \n[254] print.TukeyHSD*                                     \n[255] print.tukeyline*                                    \n[256] print.tukeysmooth*                                  \n[257] print.undoc*                                        \n[258] print.vignette*                                     \n[259] print.warnings                                      \n[260] print.xfun_md_viewable*                             \n[261] print.xfun_raw_string*                              \n[262] print.xfun_record_results*                          \n[263] print.xfun_rename_seq*                              \n[264] print.xfun_strict_list*                             \n[265] print.xgettext*                                     \n[266] print.xml_document*                                 \n[267] print.xml_missing*                                  \n[268] print.xml_namespace*                                \n[269] print.xml_node*                                     \n[270] print.xml_nodeset*                                  \n[271] print.xngettext*                                    \n[272] print.xtabs*                                        \nsee '?methods' for accessing help and source code\n\nattributes(whole_html_page)\n\nError: object 'whole_html_page' not found\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# An externalptr cannot be accessed directly via R code. It is a \"blob\"\n# (a \"binary large object\") i.e. a piece of data that is only accessible\n# via C language (not R) code that is used to build some of the packages\n# that are used to extend R's functionality. You must use the built in\n# functions in the xml2 and the rvest packages to access this data.\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#working-with-the-data-you-scraped",
    "href": "web-000400-webscrapingContinued-015.html#working-with-the-data-you-scraped",
    "title": "44  41.  Web Scraping with R",
    "section": "44.8 Working with the data you scraped",
    "text": "44.8 Working with the data you scraped\n\n#.....................................................................\n#\n# we can eliminate extra newlines and the blanks with the gsub function\n# \n#.....................................................................\n\n?gsub   # see the documentation\n\nstarting httpd help server ... done\n\nwithout_backslash_n = gsub(\"\\\\n\", \"\", just_the_text)    \n\nError: object 'just_the_text' not found\n\nwithout_backslash_n\n\nError: object 'without_backslash_n' not found\n\n# The following has a bug in that it removes ALL spaces.\n# countries = gsub(\" \" , \"\" , without_backslash_n)\n\n# Just remove the spaces that appear BEFORE the country name\ncountries1 = gsub(\"^ *\", \"\", without_backslash_n)\n\nError: object 'without_backslash_n' not found\n\ncountries2 = gsub(\" *$\", \"\", countries1)\n\nError: object 'countries1' not found\n\ncountries = countries2\n\nError: object 'countries2' not found\n\n# alternatively, you can eliminate spaces and \\n characters \n# before and after the text all in one shot\ncountries3 = gsub(\"(^[ \\\\n]*)|([ \\\\n]*$)\", \"\", without_backslash_n)\n\nError: object 'without_backslash_n' not found\n\ncountries3\n\nError: object 'countries3' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#get-all-data-into-a-dataframe",
    "href": "web-000400-webscrapingContinued-015.html#get-all-data-into-a-dataframe",
    "title": "44  41.  Web Scraping with R",
    "section": "44.9 Get all data into a dataframe",
    "text": "44.9 Get all data into a dataframe\n\n########################################################\n#\n# Now that we understand the basics we can get\n# all of the country data and put it in a dataframe\n#\n########################################################\n\nurl = \"https://scrapethissite.com/pages/simple/\"\n\nthe_full_html = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ncountries = the_full_html %&gt;%\n              html_elements( \".country-name\" ) %&gt;%\n              html_text()\n\nError: object 'the_full_html' not found\n\ncapitals = the_full_html %&gt;%\n              html_elements( \".country-capital\" ) %&gt;%\n              html_text()\n\nError: object 'the_full_html' not found\n\npopulation = the_full_html %&gt;%\n  html_elements( \".country-population\" ) %&gt;%\n  html_text()\n\nError: object 'the_full_html' not found\n\narea = the_full_html %&gt;%\n  html_elements( \".country-area\" ) %&gt;%\n  html_text()\n\nError: object 'the_full_html' not found\n\ncountries\n\nError: object 'countries' not found\n\n# remove the spaces and newline characters from countries\ncountries= gsub(\"\\\\n\", \"\", countries)\n\nError: object 'countries' not found\n\ncountries= gsub(\"(^ *)|( *$)\", \"\" , countries)\n\nError: object 'countries' not found\n\ncountries\n\nError: object 'countries' not found\n\nhead(countries,5)  # the United Arab Emirates has no spaces! Let's fix that ...\n\nError: object 'countries' not found\n\ncountries = the_full_html %&gt;%\n  html_elements( \".country-name\" ) %&gt;%\n  html_text()\n\nError: object 'the_full_html' not found\n\n# remove the spaces and newline characters from countries\n# Do not remove spaces between words\ncountries= gsub(\"\\\\n\", \"\", countries)\n\nError: object 'countries' not found\n\ncountries= gsub(\"^ *\", \"\" , countries)  # get rid of spaces before the text\n\nError: object 'countries' not found\n\ncountries= gsub(\" *$\", \"\" , countries)  # get rid of spaces after the text\n\nError: object 'countries' not found\n\ncountries\n\nError: object 'countries' not found\n\ncapitals\n\nError: object 'capitals' not found\n\npopulation\n\nError: object 'population' not found\n\narea\n\nError: object 'area' not found\n\ndf = data.frame ( country=countries,\n                  capital = capitals,\n                  pop = population,\n                  area = area,\n                  stringsAsFactors = FALSE\n                  )\n\nError: object 'countries' not found\n\nhead(df, 5)\n\n                                              \n1 function (x, df1, df2, ncp, log = FALSE)    \n2 {                                           \n3     if (missing(ncp))                       \n4         .Call(C_df, x, df1, df2, log)       \n5     else .Call(C_dnf, x, df1, df2, ncp, log)\n\nnrow(df)\n\nNULL",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#html_text2-removes-leadingtrailing-whitespace",
    "href": "web-000400-webscrapingContinued-015.html#html_text2-removes-leadingtrailing-whitespace",
    "title": "44  41.  Web Scraping with R",
    "section": "44.10 html_text2() removes leading/trailing whitespace",
    "text": "44.10 html_text2() removes leading/trailing whitespace\n\n#####################################################\n# UPDATE:\n#\n# The latest release of the rvest package includes\n# a function named html_text2. This function automatically\n# removes leading and trailing whitespace.\n# It's still important to understand how to modify the \n# data with gsub for situations for which you want to modify\n# the data other than leading and trailing whitespace.\n#####################################################\n\n\nurl = \"https://scrapethissite.com/pages/simple/\"\ncssSelector = \".country-name\"\n\nwhole_html_page = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ncountry_name_html =  html_elements( whole_html_page, cssSelector )\n\nError: object 'whole_html_page' not found\n\n# Using html_text2 instead of html_text.\n# html_text2 automatically removes leading and trailing whitespace\n# from the text.\njust_the_text =   html_text2(country_name_html)\n\nError: object 'country_name_html' not found\n\njust_the_text  \n\nError: object 'just_the_text' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#create-a-function-to-do-the-scraping",
    "href": "web-000400-webscrapingContinued-015.html#create-a-function-to-do-the-scraping",
    "title": "44  41.  Web Scraping with R",
    "section": "44.11 Create a function to do the scraping",
    "text": "44.11 Create a function to do the scraping\n\n#####################################################\n#\n# Package up the technique into a function\n#\n#####################################################\n\ngetCountryData &lt;- function() {\n \n  \n  # Get all of the country data and put it in a dataframe\n  \n  url = \"https://scrapethissite.com/pages/simple/\"\n  \n  the_full_html = read_html(url)\n  \n  countries = the_full_html %&gt;%\n    html_elements( \".country-name\" ) %&gt;%\n    html_text()\n  \n  capitals = the_full_html %&gt;%\n    html_elements( \".country-capital\" ) %&gt;%\n    html_text()\n  \n  population = the_full_html %&gt;%\n    html_elements( \".country-population\" ) %&gt;%\n    html_text()\n  \n  area = the_full_html %&gt;%\n    html_elements( \".country-area\" ) %&gt;%\n    html_text()\n\n  # remove the spaces and newline characters from countries\n  countries= gsub(\"\\\\n\", \"\", countries)\n  countries= gsub(\" \", \"\" , countries)\n\n  df = data.frame ( country=countries,\n                    capital = capitals,\n                    pop = population,\n                    area = area,\n                    stringsAsFactors = FALSE\n  )\n  \n  return (df)\n}\n\n\n# Data on websites can change. We can now call this function whenever\n# we want to get the latest versions of the data from the website.\n\nmydata = getCountryData()\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nhead(mydata)\n\nError: object 'mydata' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#scraping-multiple-webpages-in-a-loop",
    "href": "web-000400-webscrapingContinued-015.html#scraping-multiple-webpages-in-a-loop",
    "title": "44  41.  Web Scraping with R",
    "section": "44.12 Scraping multiple webpages in a loop",
    "text": "44.12 Scraping multiple webpages in a loop\n\n44.12.1 Figure out the code\n\n######################################################\n#\n# Getting data from multiple web pages in a loop.\n#\n######################################################\n\n\n# The data on the following page is only one page of multiple pages of similar\n# data. \n#\n#   https://scrapethissite.com/pages/forms/?page_num=1\n#\n# The links to the other pages appear on the bottom of the page. Clicking on \n# the the link for the 2nd page, reveals that the 2nd page of data is at \n# the following URL:\n#\n#   https://scrapethissite.com/pages/forms/?page_num=2\n#\n# These URLs differ ONLY in the page number. This allows us to scrape\n# ALL of the pages by reconstructing the URL for the page we want by\n# inserting the correct page number. \n\n\n# CSS FROM INSPECT\ncss = \"#hockey &gt; div &gt; table &gt; tbody &gt; tr:nth-child(2) &gt; td.name\"\n\n# After looking at the full HTML code, we could see the the tbody\n# in the selector should not have been there? There could be several \n# reasons (beyond the scope of today's class) as to why the tbody was \n# in the css selector that we got back. By looking at the full HTML\n# and understanding a little about how HTML and css selectors work, we were\n# able to realize that \"tbody\" didn't belong and take it out.\ncss = \"#hockey &gt; div &gt; table &gt; tr:nth-child(2) &gt; td.name\"\n\n# When we tried the above css selector we ONLY got back the \"Colorado Avalance\"\n# but no other team name. We quickly realized that was because of \n# :nth-child(2) that was limiting the results. We we took off :nth-child(2)\n# that got us all of the team names. \ncss = \"#hockey &gt; div &gt; table &gt; tr &gt; td.name\"\n\n# The following is what we got from selector gadget\ncss = \".name\"\n\n# according to selector gadget this will match all data in the table\n# and it does. However, the data comes back in a single vector \n# instead of a dataframe.\ncss = \"td\"\n\n# Instead of getting all the data in a single vector it's probably better to\n# get the Name column in one vector and the Year column in another vector\ncssName = \".name\"\ncssYear = \".year\"\n\nurl= \"https://www.scrapethissite.com/pages/forms/?page_num=7\"\n\nfullPage = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\nfullPage %&gt;%\n  html_elements(css) %&gt;%\n  html_text2()\n\nError: object 'fullPage' not found\n\n# Get the name data\nteamNames = fullPage %&gt;%\n  html_elements(cssName) %&gt;%\n  html_text2()\n\nError: object 'fullPage' not found\n\n# Reminder about the %&gt;% pipe symbold\n# Same as above without using the %&gt;% pipe symbol\nhtml_text2(html_elements(fullPage, cssName))\n\nError: object 'fullPage' not found\n\n# The %&gt;% symbol takes the ootput of the command on the left and \n# sends it into the first argument of the command on the right.\nrep(c(100,200), 3)\n\n[1] 100 200 100 200 100 200\n\nc(100,200) %&gt;%\n  rep(3)\n\n[1] 100 200 100 200 100 200\n\nteamNames\n\nError: object 'teamNames' not found\n\n# Get the year data\nteamYears = fullPage %&gt;%\n  html_elements(cssYear) %&gt;%\n  html_text2()\n\nError: object 'fullPage' not found\n\nteamYears\n\nError: object 'teamYears' not found\n\n# now combine the values in a dataframe\ndfTeamInfo = data.frame (name=teamNames, year=teamYears)\n\nError: object 'teamNames' not found\n\ndfTeamInfo\n\nError: object 'dfTeamInfo' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#put-it-into-a-function",
    "href": "web-000400-webscrapingContinued-015.html#put-it-into-a-function",
    "title": "44  41.  Web Scraping with R",
    "section": "44.13 Put it into a function",
    "text": "44.13 Put it into a function\n\n#-----------------------------------------------\n# Let's see if we can get the first page of data\n#-----------------------------------------------\n\n# The following function will be useful to get rid of extra \"whitespace\"\n# if necessary.\n\nremoveWhitespace = function(text){\n  text = gsub(\"\\\\n\",\"\", text)\n  text = gsub(\"\\\\t\",\"\", text )\n  \n  return ( gsub(\" \", \"\", text))\n}\n  \nx = \"    this is some info \\n\\nanother line\\nanother line\\tafter a tab\"\ncat(x)\n\n    this is some info \n\nanother line\nanother line    after a tab\n\nremoveWhitespace(x)\n\n[1] \"thisissomeinfoanotherlineanotherlineafteratab\"\n\n# Create a function to scrape the hocky data from one of the \n# pages as specified in the url argument.\ngetHockeyData &lt;- function(url) {\n\n  the_full_html = read_html(url)\n  \n  teamNames = the_full_html %&gt;%\n    # Analyze the HTML from one of the pages to figure out which CSS selector\n    # is best to use. We did so and figured out that the hockey team names\n    # were surrounded with an HTML that had class=\"name\". Therefore the\n    # best css slector to use was \".name\"\n    html_elements( \".name\" ) %&gt;%     \n    html_text()\n  \n  teamNames = removeWhitespace(teamNames)   # the team names seem to be surrounded by whitespace\n\n  wins = the_full_html %&gt;%\n    # Analyze the HTML from one of the pages to figure out which CSS selector\n    # is best to use. We did so and figured out that the number of wins\n    # were surrounded with an HTML that had class=\"wins\". Therefore the\n    # best css slector to use was \".wins\"\n    html_elements( \".wins\" ) %&gt;%   # analyze the HTML to find the appropriate css selector\n    html_text()\n  \n  wins = removeWhitespace(wins)\n\n  # ... we can keep doing this to get all the other data on the page \n  # for each team ... We didn't do that here but feel free to fill in \n  # the missing code to scrape the rest of the data from the webpages.\n  \n  return(data.frame(team=teamNames, wins=wins, stringsAsFactors=FALSE))  \n}",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#some-urls-includes-info-about-the-page-number",
    "href": "web-000400-webscrapingContinued-015.html#some-urls-includes-info-about-the-page-number",
    "title": "44  41.  Web Scraping with R",
    "section": "44.14 Some URLs includes info about the page number",
    "text": "44.14 Some URLs includes info about the page number\n\npage1Url = \"https://scrapethissite.com/pages/forms/?page_num=20\"\ngetHockeyData(page1Url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\n#--------------------------------------------------------------------\n# Let's figure out how to write a loop to get multiple pages of data\n#--------------------------------------------------------------------\n\n# The following is the url without the page number\nbaseurl = \"https://scrapethissite.com/pages/forms/?page_num=\"\n\n# get data for the first page\npageNum = \"1\"\nurl = paste0(baseurl, pageNum)\ngetHockeyData (url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\n# get data for the 2nd page\nurl=paste0(baseurl, \"2\")\ngetHockeyData(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#function-to-get-multiple-pages",
    "href": "web-000400-webscrapingContinued-015.html#function-to-get-multiple-pages",
    "title": "44  41.  Web Scraping with R",
    "section": "44.15 Function to get multiple pages",
    "text": "44.15 Function to get multiple pages\n\n# We can now write a function to get the data from multiple pages of the hockey data.\n# The pages argument is expected to be a vector with the numbers of \n# the pages you want to retrieve.\n\ngetMultiplePages &lt;- function(pages = 1:10){\n  # This is the URL without the page number\n  baseurl = \"https://scrapethissite.com/pages/forms/?page_num=\"\n\n  #   baseurl = \"https://scrapethissite.com/pages/forms/?page_num=THE_PAGE_NUMBER&league=american\n  \n  \n  # baseurl = \"https://finance.yahoo.com/quote/&lt;&lt;&lt;TICKER&gt;&gt;&gt;?p=&lt;&lt;&lt;TICKER&gt;&gt;&gt;&.tsrc=fin-srch\"\n  \n  allData = NA\n  \n  # Loop through all the pages\n  for ( page in pages ){\n    # Create the URL for the current page\n    url = paste0(baseurl, page)\n    \n    # Get the data for the current page\n    hd = getHockeyData(url)\n    \n    # Combine the data for the current page with all of the data\n    # from the pages we have already retreived.\n    \n    if (!is.data.frame(allData)){\n      # This will only happen for the first page of retrieved data.\n      allData = hd\n    } else {\n      # This will happen for all pages other than the first page retrieved.\n      # rbind will only work if allData is already a dataframe. Therefore\n      # we cannot use rbind for the first page of retrieved data. \n      allData = rbind(allData, hd)\n    }\n    # We don't want to overwhelm the web server with too many requests\n    # so we will pause (i.e. sleep) for 1 second after every time we\n    # retrieve a page before getting the next page of data.\n    Sys.sleep(1)\n  }\n  return(allData)\n}\n\ngetMultiplePages(4:6)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\n# BE CAREFUL - the next line may take a little time to run\n# allPages = getMultiplePages(1:24)\n#\n# Let's try it with just 2 pages for now ...\nallPages = getMultiplePages(1:2)\n\nError in open.connection(x, \"rb\"): cannot open the connection",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#scrape-the-number-of-pages-to-get",
    "href": "web-000400-webscrapingContinued-015.html#scrape-the-number-of-pages-to-get",
    "title": "44  41.  Web Scraping with R",
    "section": "44.16 Scrape the number of pages to get",
    "text": "44.16 Scrape the number of pages to get\n\n# Figure out automatically how many pages of data there are on the website\n\nurl= \"https://www.scrapethissite.com/pages/forms/?page_num=1\"\nfullPage = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\n# css that targets the 24 that represents the 24th page of data\ncss = \"li:nth-child(24) a\"\ncss = \"li:last-child a\"\ncss = \".pagination-area li:nth-last-child(2) a\"\n\nfullPage %&gt;%\n  html_elements(css) %&gt;%\n  html_text2()\n\nError: object 'fullPage' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#robots.txt-file",
    "href": "web-000400-webscrapingContinued-015.html#robots.txt-file",
    "title": "44  41.  Web Scraping with R",
    "section": "44.17 robots.txt file",
    "text": "44.17 robots.txt file\n\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n# DEFINITION: root directory (or root folder) ####\n# \n# The \"root\" folder is the top level folder\n# on a computer harddrive or on a website.\n# It is named \"/\" on Mac and \"\\\" on Windows.\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n# EXAMPLE\ndir(\"/\")   # show the files and folders in the root of the harddrive\n\n [1] \"$Recycle.Bin\"              \"$SysReset\"                \n [3] \"BGInfo\"                    \"Brother\"                  \n [5] \"Documents and Settings\"    \"DumpStack.log\"            \n [7] \"DumpStack.log.tmp\"         \"hiberfil.sys\"             \n [9] \"Intel\"                     \"OneDriveTemp\"             \n[11] \"pagefile.sys\"              \"PerfLogs\"                 \n[13] \"Program Files\"             \"Program Files (x86)\"      \n[15] \"ProgramData\"               \"Recovery\"                 \n[17] \"swapfile.sys\"              \"System Volume Information\"\n[19] \"Users\"                     \"Windows\"                  \n\n# robots.txt ####\n# \n# Websites may place a file named robots.txt\n# at the very top of their website.\n#\n# Examples\n#   https://finance.yahoo.com/robots.txt    # pretty simple\n#   https://www.amazon.com/robots.txt       # very complex\n#   https://www.gutenberg.org/robots.txt    # pretty typical\n#\n#\n# Example entries in robots.txt\n#     Disallow all robots to scrape the entire site\n#     Since Disallow specifies the \"root\" or top level folder\n#\n#        User-agent: *\n#        Disallow: /\n#\n#     Allow all robots to scrape the entire site\n#     since \"Disallow: \" specifies no path\n#\n#        User-agent: *\n#        Disallow: \n#\n#     Sleep 10 seconds between each page request\n#\n#        crawl-delay: 10\n#\n# More info about robots.txt\n#\n#    Quick overview and a \"test tool\"\n#      https://en.ryte.com/free-tools/robots-txt/\n#\n#    Ultimate robots.txt guide\n#      https://yoast.com/ultimate-guide-robots-txt/\n#\n#    Another test tool\n#      https://technicalseo.com/tools/robots-txt/\n#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#techniques-for-finding-css-selectors",
    "href": "web-000400-webscrapingContinued-015.html#techniques-for-finding-css-selectors",
    "title": "44  41.  Web Scraping with R",
    "section": "44.18 Techniques for finding CSS selectors",
    "text": "44.18 Techniques for finding CSS selectors\n\n############################################################################.\n# Techniques for finding CSS selectors\n#\n# 1. Analyze the entire HTML code.\n#    Right-click on blank area of page and choose \"view page source\"\n#    (or something similar)\n#\n# 2. Use the built in developer tools in most browsers.\n#    Right click on the data you want to scrape and choose \"inspect\".\n#    That brings you to the browser's developers tools. \n#    Right click on the HTML Tag you want to get a CSS selector for\n#    and choose \"Copy selector\". Then paste the selector into \n#    a text editor (e.g. VSCode or RStudio's editor).\n#    \n#    Warning that these selectors will be too overly specific and\n#    do not generalize well to scrape the same type of data from\n#    the entire webpage.\n#\n#    For example the following css selector was retrieved in this way.\n#    It gets a very specific position on the page. However, if the page\n#    code changes in the future, this selector may stop to work. \n#    This selector is very \"fragile\":\n#\n#        css selector to find the last page \n#        #hockey &gt; div &gt; div.row.pagination-area &gt; div.col-md-10.text-center &gt; ul &gt; li:nth-child(24) &gt; a\n#\n# 3. Use the \"selector gadget\" Chrome extension. \n#    This is very helpful but is also not 100% guaranteed to work \n#    correctly.\n############################################################################.",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#scrape-the-total-number-of-pages.",
    "href": "web-000400-webscrapingContinued-015.html#scrape-the-total-number-of-pages.",
    "title": "44  41.  Web Scraping with R",
    "section": "44.19 “scrape” the total number of pages.",
    "text": "44.19 “scrape” the total number of pages.\n\n############################################################################\n#\n# Write code to \"scrape\" the total number of pages.\n#\n# Find out how many pages there are (since the website may change, \n# we can't assume that there will always be the same number of pages)\n#\n###########################################################################\n\n\n#..................................................................\n# Find the CSS selector that pinpoints the number of the last page\n#..................................................................\n\n# Modern browsers have \"web developer tools\" built into them.\n# These tools can help you to find the css selector that you need.\n# Different browsers have slightly different tools. \n# For the following we will be using Chrome.\n#\n# Navigate to the first page ( https://scrapethissite.com/pages/forms/?page_num=1 )\n#\n# Right click on the last page number and choose \"inspect\". This will\n# reveal a new window that shows the HTML for the page. The part of the HTML\n# that corresponds to the page number is revealed. Make sure the page number\n# is there ...\n#\n# Right click on the highlighted HTML and choose the menu choices :\n# \"Copy | copy selector\". This will copy into the computer's clipboard \n# a CSS selector that will pinpoint that piece of info.\n# \n# Paste the CSS selector into your R code. The following is the \n# css selector that I got when I did this:\n#\n#   #hockey &gt; div &gt; div.row.pagination-area &gt; div.col-md-10.text-center &gt; ul &gt; li:nth-child(24) &gt; a\n#\n# Note that this css selector is \"fragile\" - i.e. it is likely not to work\n# in the future if more data is added to the stie. Therefore it is NOT a very \n# good CSS selector to use. We will examine this issue below. For now, let's \n# just use this selector and revisit this issue later.\n\nurl = \"https://scrapethissite.com/pages/forms/?page_num=1\"\ncssSelector = \"#hockey &gt; div &gt; div.row.pagination-area &gt; div.col-md-10.text-center &gt; ul &gt; li:nth-child(24) &gt; a\"\n\n# save the html for the page in a variable so we can use it \n# again later without needing to pull it down again from the website.\nfull_page = read_html(url)          \n\nError in open.connection(x, \"rb\"): cannot open the connection\n\n# Get last page number by searching the full_page html for the tags\n# identified by the cssSelector and stripping out the tags\nlast_page_number &lt;- full_page %&gt;%\n  html_elements( cssSelector ) %&gt;%\n  html_text()\n\nError: object 'full_page' not found\n\nlast_page_number\n\nError: object 'last_page_number' not found\n\n# Strip out the whitespace \nlast_page_number = removeWhitespace(last_page_number)\n\nError: object 'last_page_number' not found\n\nlast_page_number\n\nError: object 'last_page_number' not found\n\n# Convert the page nubmer to a numeric value\nlast_page_number = as.numeric(last_page_number)\n\nError: object 'last_page_number' not found\n\nlast_page_number\n\nError: object 'last_page_number' not found\n\n# Now that it works, let's create a function that packages up this functionality.\n# It's always a good idea to create a function that neatly packages up your\n# working functionality.\n#\n# The function will take the \"parsed\" html code as an argument.\n\ngetLastPageNumber &lt;- function(theFullHtml) {\n  cssSelector = \n    \"#hockey &gt; div &gt; div.row.pagination-area &gt; div.col-md-10.text-center &gt; ul &gt; li:nth-child(24) &gt; a\"\n  \n  # remember that a function that does not have a return statement\n  # will return the last value that is calculated.\n  theFullHtml %&gt;%\n    html_elements( cssSelector ) %&gt;%\n    html_text() %&gt;%\n    removeWhitespace() %&gt;%\n    as.numeric()\n}\n\nlast_page_number = getLastPageNumber(full_page)\n\nError: object 'full_page' not found\n\nlast_page_number\n\nError: object 'last_page_number' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000400-webscrapingContinued-015.html#timing-r-code-with-sys.time",
    "href": "web-000400-webscrapingContinued-015.html#timing-r-code-with-sys.time",
    "title": "44  41.  Web Scraping with R",
    "section": "44.20 Timing R code with Sys.time()",
    "text": "44.20 Timing R code with Sys.time()\n\n# Now that we have the total nubmer of pages, we can get ALL of the data\n# for all the pages as follows.\n#\n# NOTE that this may take a while to run.\n# We can figure out how much time by running all of the following commands\n# together. \n\nSys.time()    # shows the time on your computer\n\n[1] \"2025-04-01 13:30:50 EDT\"\n\nstart = Sys.time()\n# put some r code here\nend = Sys.time()\nend - start\n\nTime difference of 0.001088858 secs\n\nstart_time = Sys.time() # get the time before we start\n\ndataFromAllPages = getMultiplePages(1:last_page_number)   # get all the pages\n\nError: object 'last_page_number' not found\n\nend_time = Sys.time()   # get the time when it ended\n\nend_time - start_time   # show how long it took\n\nTime difference of 0.002022982 secs\n\n# Let's examine the results ...\nnrow(dataFromAllPages)  # how many rows?\n\nError: object 'dataFromAllPages' not found\n\nhead(dataFromAllPages, 4)  # first few rows ...\n\nError: object 'dataFromAllPages' not found\n\ntail(dataFromAllPages, 4)  # last few rows ...\n\nError: object 'dataFromAllPages' not found\n\n\n\n44.20.1 Finding a css selector that isn’t “fragile”\n\n#######################################################################\n#\n# Finding css selectors that aren't \"fragile\"\n#\n#######################################################################\n\n#--------------------------------------------\n# Quick review of CSS selector rules. \n#--------------------------------------------\n\n# To review the CSS selector rules see the website \n#   https://flukeout.github.io/ \n#\n# Read the tutorial information as well as try to solve the different \n# levels of the game. \n# The answers to the \"game\" on the above website can be found here:\n#   https://gist.github.com/humbertodias/b878772e823fd9863a1e4c2415a3f7b6\n#\n\n\n#-----------------------------------------------\n# Analyzing the CSS Selector we got from Chrome\n# (There is an issue with it. \n#  We will address the issue and fix it.) \n#------------------------------------------------\n\n# The following was the selector that we got from Chrome \n# to find the last page number:\n#\n#   #hockey &gt; div &gt; div.row.pagination-area &gt; div.col-md-10.text-center &gt; ul &gt; li:nth-child(24) &gt; a\n#\n# This selector is \"fragile\". It will work for now but might not work in the future. \n# This website has data for several years up until the present time. We \n# anticipate that in the future there will be additional pages of data.\n# The CSS selector that we found will work to find the 24th page number\n# (which is currently the last page). However, in the future if there are \n# more pages, the selector will continue to return the number 24. \n# To understand exactly why, let's analyze this selector:\n# For a review of the rules for how selectors are built see the following \n# page: https://flukeout.github.io/\n#\n\n# &lt;div&gt;                # div is the parent of h1 and ul\n#   &lt;h1&gt;My stuff&lt;/h1&gt;  # h1 is the child of div and the sibling of ul\n#   &lt;ul&gt;               # ul is the parent of 3 &lt;li&gt;s, the 2nd child of div and a sibling of h1\n#     &lt;li&gt;             # \n#       &lt;strong&gt;       \n#         Table\n#       &lt;/strong&gt;&lt;/li&gt;\n#     &lt;li&gt;Chairs&lt;/li&gt;\n#     &lt;li&gt;Fork&lt;/li&gt;\n#   &lt;/ul&gt;\n# &lt;/div&gt;\n\n\n# The following breaks down the selector that Chrome figured out into its \n# different parts:\n#\n#   #hockey                   # find the HTML start tag whose id has a value id=\"hockey\" \n#   &gt;                         # directly inside of that element\n#   div                       # there is a &lt;div&gt; tag\n#   &gt;                         # directly inside of that div tag\n#   div.row.pagination-area   # there is a &lt;div&gt; tag that has row and pagination-area classes, i.e. &lt;div class=\"row pagination-area\"&gt;\n#   &gt;                         # directly inside of that\n#   div.col-md-10.text-center # there is div tag with classes \"col-md-10\" and \"text-center\", i.e. &lt;div class=\"col-md-10 text-center\"&gt;\n#   &gt;                         # directly inside of that \n#   ul                        # there is a ul tag\n#   &gt;                         # directly inside that there is\n#   li:nth-child(24)          # an &lt;li&gt; tag that is the 24th tag inside of the enclosing &lt;ul&gt; tag\n#   &gt;                         # directly inside that there is \n#   a                         # an \"a\" tag\n#\n# You should take the time to convince yourself that the css selector is \n# accurate today by looking carefully at the HTML code and noting exactly where the \n# last page number is in the context of the other tags and attributes on the page. \n\n\n#................................................\n# Using VSCode (or another text ediotr) to analyze the HTML\n#................................................\n\n# It's much easier to navigate around the HTML code by analyzing the HTML in an HTML editor. \n# VSCode works well for this. To get the full HTML, right click on the page\n# and choose \"View page source\" (in Chrome or a similar link in other browsers).\n# Then copy all the code (ctrl-a on windows or cmd-a on Mac) and paste\n# it into a new VSCode file. Then save the file with a .html extension\n# to ensure that VSCode knows how to navigate around the file. \n# In VSCode you can now point to the left margin and press the arrows that appear\n# to \"collapse\" or \"expand\" the HTML tags. Doing so helps a lot in trying to \n# understand how the HTML file is organized.\n#\n# Other useful features in VSCode to help with editing HTML:\n#\n# - alt-shift-F   \n#    \n#     Remember that HTML does not require proper indentation to work in the webpage.\n#     However, without proper indentation, it is hard to read the HTML. Pressing\n#     shift-alt-F will automatically \"format\" the entire HTML file so that it \n#     is indented properly and is easier to read.\n#\n# - As noted above - point to left margin to see arrows to \n#   collapse or expand HTML elements.\n#\n# - ctrl-shift-P\n#\n#     VSCode has many features that are not directly available through the\n#     menus. ctrl-shift-P (Windows) or cmd-shift-P (mac) reveals the\n#     \"command palette\". This is a quick way to search for commands\n#     in VSCode which you might not otherwise know about. Try it. \n#     For example,\n#\n#     * press ctrl-shift-P and type \"comment\" (without the quotes). Then choose\n#       \"Add line comment\" to automatically insert &lt;!--   --&gt; into your HTML file.\n#       You can add comments to the HTML file as you are examining it. This may\n#       help you when you are trying to figure out the structure of a \n#       complex HTML file and how to \n#    \n#\n#     * Highlight some text, press ctrl-shift-P. \n#       Then type \"case\" (without the quotes). You will see options to transform \n#       the selected text upper or lowercase. \n#\n# - For more tips on how to use VSCode to edit HTML files: \n#   https://code.visualstudio.com/docs/languages/html\n#\n# - For more tips on how to use VSCode in general:\n#   https://code.visualstudio.com/docs/getstarted/tips-and-tricks\n\n\n#................................................\n# ... back to analyzing the CSS selector\n#................................................\n\n# From analyzing this HTML, we determine that each page number is in an &lt;li&gt;\n# element. Since there are 24 pages of data there are 24 &lt;li&gt; tags (each contains\n# a link to one of the page numbers). There is also one more &lt;li&gt; tag that\n# contains the \"next page\" button.     \n#\n# In the CSS Selector tha we got from Chrome \"li:nth-child(24)\" is specifically\n# targeting the 24th &lt;li&gt; tag. That is the last tag today. However, \n# if in the future more pages of data are added to the website, this selector\n# will still retrieve the 24th &lt;li&gt; tag and not the 2nd to last &lt;li&gt; tag\n# (remember, the last &lt;li&gt; tag is for the \"next page\" button).\n#\n# (NOTE: the \"next page\" link appears in the HTML code\n# as \"next page\" but is rendered in the browser as \"&gt;&gt;\". Replacing the\n# words \"next page\" in the HTML with \"&gt;&gt;\" on the rendered page\n# is accomplished through CSS rules - but this has no effect on  \n# what we're trying to accomplish. When scraping a page, you work with what's \n# in the HTML file, not what is rendered in the browser.)\n#\n# Therefore it is important to analyze the HTML and try \n# to find a \"less fragile\" css selector the LAST page number the same information.\n# After looking at the HTML code carefully, it seems like the &lt;ul&gt; tag\n# referenced in the CSS selector that we found has a class=\"pagination\" attribute, \n# ie. &lt;ul class=\"pagination\"&gt;. We can target that directly with \".pagination\".\n#\n# The li that points to the last page happens to be the 2nd to last \n# &lt;li&gt; tag that is directly inside this ul tag, i.e. \n#\n#    &lt;ul class=\"pagination\"&gt;\n#       &lt;li&gt; info about page 1 &lt;/li&gt;\n#       &lt;li&gt; info about page 2 &lt;/li&gt;\n#         etc ...\n#       &lt;li&gt; info about the last page &lt;/li&gt;         # THIS IS THE &lt;li&gt; WE WANT\n#       &lt;li&gt; info about the \"next\" button &lt;/li&gt;\n#    &lt;/ul&gt;\n#\n# Our css selector therefore becomes :    \".pagination &gt; li:nth-last-child(2)\"\n# Let's see if our code works with the new CSS selector\n\ngetLastPageNumber &lt;- function(theFullHtml) {\n  \n  # Selector specfies the \n  # 2nd to last li inside the a tag that contains class=\"pagination\"\n  selector = \".pagination &gt; li:nth-last-child(2)\"    \n  \n  # remember that a function that does not have a return statement\n  # will return the last value that is calculated.\n  theFullHtml %&gt;%\n    html_elements( selector ) %&gt;%\n    html_text() %&gt;%\n    removeWhitespace() %&gt;%\n    as.numeric()\n}\n\nfull_page = read_html(\"https://scrapethissite.com/pages/forms/?page_num=1\")\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ngetLastPageNumber(full_page)  # test the new version\n\nError: object 'full_page' not found",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'><yrChapterNumber>41. </yrChapterNumber> Web Scraping with R</span>"
    ]
  },
  {
    "objectID": "web-000130-WebscrapingJavascriptSites-v003.html",
    "href": "web-000130-WebscrapingJavascriptSites-v003.html",
    "title": "45  42.  Scraping Dynamic (JavaScript) Websites",
    "section": "",
    "text": "45.1 Files for the rest of this presentation\nClick here to download a zip file that contains several files we will use in this presentation:\nhtmlExamples-v001.zip",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'><yrChapterNumber>42. </yrChapterNumber> Scraping Dynamic (JavaScript) Websites</span>"
    ]
  },
  {
    "objectID": "web-000130-WebscrapingJavascriptSites-v003.html#intro-to-rselenium",
    "href": "web-000130-WebscrapingJavascriptSites-v003.html#intro-to-rselenium",
    "title": "45  42.  Scraping Dynamic (JavaScript) Websites",
    "section": "45.2 Intro to RSelenium",
    "text": "45.2 Intro to RSelenium\n\nThis is an example of a JavaScript enabled webpage.\njavascriptExample-v002.html\nClick here for a Word document that discusses scraping JavaScript enabled websites with RSelenium.\n000130-WebscrapingJavascriptSitesWithRselenium-v002.docx",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'><yrChapterNumber>42. </yrChapterNumber> Scraping Dynamic (JavaScript) Websites</span>"
    ]
  },
  {
    "objectID": "web-000420-practiceWithWebScraping-v001-ANSWERS-v002.html",
    "href": "web-000420-practiceWithWebScraping-v001-ANSWERS-v002.html",
    "title": "46  43.  CSS Selector Questions",
    "section": "",
    "text": "library(rvest)\n\n#########################################################.\n# CSS Selector questions and answers\n#########################################################.\n# For information about CSS selectors, see the explanations on the \n# various levels of the CSS selector Game found here:\n#\n#    https://flukeout.github.io/\n#\n# Here are some other websites for learning and practicing\n# CSS Selectors\n#\n#    https://css-speedrun.netlify.app/  \n#    # answers are here: https://github.com/Vincenius/css-speedrun\n#\n#    https://css-selector.netlify.app/\n#    # on this page write the selector that correctly selects the one\n#    # line that contains the words \"Select me\"\n#\n#########################################################.\n\n#########################################################################.\n# The following code is how we typically worked with local HTML files\n# in class:\n#\n#\n# This is the folder that contains the HTML file\n#    folder = \"/Users/yrose/Dropbox (Personal)/website/yu/ids2460-dataMgmtForAnal/83spr23-ids2460-dataManagement/000600-WebScraping\"\n#    setwd(folder)\n#\n# This is the html file name\n#    fullPage = read_html(\"000420-practiceWithWebScraping-v001.html\")\n#\n# This command reads the HTML file into R\n#    fullPage = read_html(\"000420-practiceWithWebScraping-v001.html\")\n#\n#########################################################################.\n#\n# Instead of reading from an HTML file, I put the text of the HTML directly\n# into the R file by using an R r\"(raw string)\".  You can enclose anything\n# between r\"(   and   )\"   without using backslashes on the contents. \n# This allows us to very simply create a single R variable that contains\n# the entire text of the HTML file directly in this R file. That makes \n# understanding the answers to the questions below easier since you can\n# see the HTML directly in this file.\n#\n#########################################################################.\n\ntextOfHtmlFile = r\"(\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;this is the title&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;one&lt;/h1&gt;\n\n    &lt;div class=\"a\"&gt;\n      &lt;h2&gt;three&lt;/h2&gt;\n      &lt;p class=\"c\"&gt;four&lt;/p&gt;\n      &lt;p class=\"d\"&gt;five&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"b\"&gt;\n      &lt;h2&gt;six&lt;/h2&gt;\n      &lt;p class=\"d\"&gt;seven&lt;/p&gt;\n      &lt;p class=\"c\"&gt;eight&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;h3&gt;nine&lt;/h3&gt;\n    &lt;div class=\"c\"&gt;\n      &lt;p&gt;ten&lt;/p&gt;\n      &lt;p&gt;eleven&lt;/p&gt;\n      &lt;p&gt;twelve&lt;/p&gt;\n      &lt;p&gt;thirteen&lt;/p&gt;\n      &lt;p&gt;fourteen&lt;/p&gt;\n      &lt;p&gt;fifteen&lt;/p&gt;\n      &lt;p&gt;sixteen&lt;/p&gt;\n      &lt;p&gt;seventeen&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;p id=\"x\"&gt;eighteen&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n)\"\n\nfullPage = read_html(textOfHtmlFile)\n\n#########################################################################.\n#\n# For your convenience you can use this function to view the HTML\n# directly in the RStudio \"Viewer\" tab \n# (by default Viewer is in the lower right corner of the RStudio window)\n#\n#########################################################################.\n\nviewHtmlInRStudio = function(htmlText){\n  filename = tempfile()   # create a temporary filename\n  filename = paste0(filename, \".html\")  # add an html extension to the filename\n  cat(\"Html is in \", filename , \"\\n\", sep=\"\") # show where the file is (you can ignore this)\n  writeLines(htmlText, filename) # write the html text to the file\n  rstudioapi::viewer(filename)  # display the html page in the RStudio viewer\n  \n  # Wait 2 seconds before removing the file to make sure it has already\n  # been displayed in the RStudio \"viewer\" tab\n  Sys.sleep(2)\n  unlink(filename)  # remove the temporary file\n}\n\n\n#viewHtmlInRStudio (textOfHtmlFile)\n\n\n\n\n\n########################################################################.\n# Question 1a.\n#\n# What CSS selector targets just the following words: \n#  eighteen\n########################################################################.\n\n\n# This is ONE POSSIBLE answer\n# Use the #id selector\n# See \"level 3\" and the explanation in the CSS selector game here:\n# https://flukeout.github.io/\ncss = \"#x\"\n  \n# test the answer to make sure it works\nfullPage %&gt;% \n  html_nodes(css) %&gt;%\n  html_text2()\n\n\n########################################################################.\n# Question 1b.\n#\n# What CSS selector targets just the following words: \n#  three\n#  six\n#  ten\n########################################################################.\n\n# This is ONE POSSIBLE answer\n# Use the following selector rules:\n#\n#   Descendant Selector (i.e. Selector1  Selctor2)  # see level 4 of CSS Game\n#   :first-child                                    # see level 15 of CSS Game\n\ncss = \"div  :first-child\"\n\n# test the answer to make sure it works\nfullPage %&gt;% \n  html_nodes(css) %&gt;%\n  html_text2()\n\n########################################################################.\n# Question 1c.\n#\n# What CSS selector targets just the following words: \n#  four\n#  seven\n########################################################################.\n\n# This is ONE POSSIBLE answer\n# Use the following selector rules:\n#\n#   Adjacent Sibling Selector  # see level 12 of CSS Game\n\ncss = \"h2 + p\"\n\n# test the answer to make sure it works\nfullPage %&gt;% \n  html_nodes(css) %&gt;%\n  html_text2()\n\n########################################################################.\n# Question 1d.\n#\n# What CSS selector targets just the following words:\n#  thirteen\n#  fifteen\n#  seventeen\n########################################################################.\n\n# This is ONE POSSIBLE answer\n# Use the following selector rules:\n#\n#   nth-of-type   # see level 22 of CSS Game\n\ncss = \"p:nth-of-type(2n+4)\"\n\n# test the answer to make sure it works\nfullPage %&gt;% \n  html_nodes(css) %&gt;%\n  html_text2()\n\n\n########################################################################.\n# Question 1e.\n#\n# What CSS selector targets just the following words: \n#  six\n#  seven\n#  eight\n########################################################################.\n\n\n# This is ONE POSSIBLE answer\n# Use the following selector rules:\n#\n#   Class Selector (i.e. .classname)                # see level 6 of CSS Game\n#   Descendant Selector (i.e. Selector1  Selctor2)  # see level 4 of CSS Game\n#   The Universal Selector (i.e. *)                 # see level 10 of CSS Game\ncss = \".b *\"\n\n# test the answer to make sure it works\nfullPage %&gt;% \n  html_nodes(css) %&gt;%\n  html_text2()\n\n\n\n########################################################################.\n# Question 2.\n#\n# Save the HTML code in a file and display it in your browser. Modify the code \n# so that the oddNumber numbered words appear in green text and the evenNumber numbered \n# words appear in red text. You may add any HTML or css code that you think is\n# appropriate. However, do not modify the overall layout of the page (e.g. all\n# headings should remain headings, all paragraphs should remain paragraphs,\n# etc.)\n########################################################################.\n\n# NOTE - instead of modifying the HTML in a file I will show how to \n# modify the HTML code directly in this R file.\n#\n# The following is the original HTML code from above with some modifications.\n#\n# Step1 :   add class=\"evenNumber\" to the tags that contain evenNumber numbers\n#           add class=\"oddNumber\" to the tags that contain evenNumber numbers\n#\n#    The words \"evenNumber\" and \"oddNumber\" are NOT part of CSS or HTML. \n#    They are just class names that I \"made up\".\n#\n#    If a tag already has a class=\"something\" just add the new class name\n#    by putting a space between the new name and the original name. \n#    For example  &lt;p class=\"oddNumber d\"&gt;seven&lt;/p&gt;\n#    means that the p tag has both a class named \"oddNumber\" and a class\n#    named \"d\".\n#\n# Step 2: Add the following &lt;style&gt; element with these CSS rules into\n#         the &lt;head&gt;...&lt;/head&gt; element of the page.\n#\n#             &lt;style&gt;\n#               .oddNumber { color: green; }\n#               .evenNumber { color: red; }\n#             &lt;/style&gt;\n  \nmodifiedHtml = r\"(\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;this is the title&lt;/title&gt;\n\n    &lt;!-- add the following css rules --&gt;\n    &lt;style&gt;\n      .oddNumber { color: green; }\n      .evenNumber { color: red; }\n    &lt;/style&gt;\n\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1 class=\"oddNumber\"&gt;one&lt;/h1&gt;\n\n    &lt;div class=\"a\"&gt;\n      &lt;h2 class=\"oddNumber\"&gt;three&lt;/h2&gt;\n      &lt;p class=\"evenNumber c\"&gt;four&lt;/p&gt;\n      &lt;p class=\"oddNumber d\"&gt;five&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"b\"&gt;\n      &lt;h2 class=\"evenNumber\"&gt;six&lt;/h2&gt;\n      &lt;p class=\"oddNumber d\"&gt;seven&lt;/p&gt;\n      &lt;p class=\"evenNumber c\"&gt;eight&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;h3 class=\"oddNumber\"&gt;nine&lt;/h3&gt;\n    &lt;div class=\"c\"&gt;\n      &lt;p class=\"evenNumber\"&gt;ten&lt;/p&gt;\n      &lt;p class=\"oddNumber\"&gt;eleven&lt;/p&gt;\n      &lt;p class=\"evenNumber\"&gt;twelve&lt;/p&gt;\n      &lt;p class=\"oddNumber\"&gt;thirteen&lt;/p&gt;\n      &lt;p class=\"evenNumber\"&gt;fourteen&lt;/p&gt;\n      &lt;p class=\"oddNumber\"&gt;fifteen&lt;/p&gt;\n      &lt;p class=\"evenNumber\"&gt;sixteen&lt;/p&gt;\n      &lt;p class=\"oddNumber\"&gt;seventeen&lt;/p&gt;\n    &lt;/div&gt;\n\n    &lt;p class=\"evenNumber\" id=\"x\"&gt;eighteen&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n)\"\n\n# Show the results\n#\n#viewHtmlInRStudio(modifiedHtml)",
    "crumbs": [
      "Web Scraping",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'><yrChapterNumber>43. </yrChapterNumber> CSS Selector Questions</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html",
    "href": "0000100-topics-v001.html",
    "title": "47  44.  Topics",
    "section": "",
    "text": "47.1 Quarto (newer version of RMarkdown)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#more-practice-with-base-r",
    "href": "0000100-topics-v001.html#more-practice-with-base-r",
    "title": "47  44.  Topics",
    "section": "47.2 More practice with “base” R",
    "text": "47.2 More practice with “base” R\nThroughout the course you will be working with R. You must understand all of the basics that were covered in the R programming course. This includes but is no limited to lists,dataframes,user defined functions, if/else/else if, while loop, for loop, etc. etc. etc.\nResources\n\nhttp://yrosenthal.com/rnotes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#r-packages",
    "href": "0000100-topics-v001.html#r-packages",
    "title": "47  44.  Topics",
    "section": "47.3 R Packages",
    "text": "47.3 R Packages\n\ninstall.packages\nlibrary vs require\nCRAN\ntidyverse packages - https://www.tidyverse.org/packages/\n\nggplot2\ndplyr\n\nhttps://dplyr.tidyverse.org/\nhttps://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html\n\ntidyr https://tidyr.tidyverse.org/\nreadr https://readr.tidyverse.org/\n\nread_csv(): comma-separated values (CSV)\nread_tsv(): tab-separated values (TSV)\nread_csv2(): semicolon-separated values with , as the decimal mark\nread_delim(): delimited files (CSV and TSV are important special cases)\nread_fwf(): fixed-width files\nread_table(): whitespace-separated files\nread_log(): web log files\n\npurr\ntibble https://tibble.tidyverse.org/\nstringr\nforcats\nOther tidyverse packages\n\nmagrittr\nglue\nlubridate\nhms\nblob\ndtplyr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#command-line-interfaces",
    "href": "0000100-topics-v001.html#command-line-interfaces",
    "title": "47  44.  Topics",
    "section": "48.1 Command Line Interfaces",
    "text": "48.1 Command Line Interfaces\n\nwhat is a command line interface (CLI)\nWindows (CMD, Powershell)\nUnix, Linux and Mac Terminal (Bash and others)\nBash\n\nTerminal tab in RStudio\nworking with the file system\n\npwd\nls -a -l -r -t\nwildcards: * [abc] [!abc] [a-cw-z] [!0-9] \\\nhttps://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm\ncd\n.. and .\n.hiddenFiles\n$HOME\necho\n$HOME\nmkdir\nrm\nrmdir\ntree\n\nediting files\n\nnano\nvim https://www.openvim.com/\ntouch\n\nworking with text\n\ncut\ngrep\n\nredirection\n\ncommand &gt; filename\ncommand &lt; filename\ncommand | command2 | command3\n\nscript files\n\nchmod command\n$PATH\nbasic script files\n$env\nvariable=value # no spaces\n49 comments",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#regular-expressions",
    "href": "0000100-topics-v001.html#regular-expressions",
    "title": "47  44.  Topics",
    "section": "49.1 Regular Expressions",
    "text": "49.1 Regular Expressions\n\nhttp://regextutorials.com/ Good examples with practice questions. Need to explain about /g and other forward slash options.\nhttps://regexcrossword.com/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#web-scraping",
    "href": "0000100-topics-v001.html#web-scraping",
    "title": "47  44.  Topics",
    "section": "49.2 Web scraping",
    "text": "49.2 Web scraping\n\nhttps://www.zenrows.com/blog/web-scraping-r#prerequisites",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000100-topics-v001.html#other-topics-beyond-the-scope-of-the-course",
    "href": "0000100-topics-v001.html#other-topics-beyond-the-scope-of-the-course",
    "title": "47  44.  Topics",
    "section": "49.3 Other topics (beyond the scope of the course)",
    "text": "49.3 Other topics (beyond the scope of the course)\n\nWorking with large data\n\ndata.table package - https://rdatatable.gitlab.io/data.table/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'><yrChapterNumber>44. </yrChapterNumber> Topics</span>"
    ]
  },
  {
    "objectID": "0000200-bibliography-v001.html",
    "href": "0000200-bibliography-v001.html",
    "title": "48  45.  Bibliography",
    "section": "",
    "text": "48.1 SQL (Structured Query Language)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'><yrChapterNumber>45. </yrChapterNumber> Bibliography</span>"
    ]
  },
  {
    "objectID": "0000200-bibliography-v001.html#sql-structured-query-language",
    "href": "0000200-bibliography-v001.html#sql-structured-query-language",
    "title": "48  45.  Bibliography",
    "section": "",
    "text": "online resources:\n\nhttps://www.w3schools.com/sql/default.asp\nhttps://www.sqlzoo.net/wiki/SQL_Tutorial\n\nBooks - don’t buy these (yet):\n\nMurach’s MySQL (4th Edition)\nby Joel Murach\n20 chapters, 648 pages, 266 figures\nPublished November 2023\nISBN 978-1-943873-10-4\npublisher: https://www.murach.com/shop/murach-s-mysql-4th-edition-detail amazon.com: https://www.amazon.com/Murachs-MySQL-Reference-Joel-Murach/dp/B0CHMPQ3FC\nSQL QuickStart Guide: The Simplified Beginner’s Guide\nto Managing, Analyzing, and Manipulating Data With SQL\n(QuickStart Guides™ - Technology)\nby Walter Shields (Author) Paperback\nNovember 18, 2019",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'><yrChapterNumber>45. </yrChapterNumber> Bibliography</span>"
    ]
  },
  {
    "objectID": "0000200-bibliography-v001.html#r-and-tidyverse",
    "href": "0000200-bibliography-v001.html#r-and-tidyverse",
    "title": "48  45.  Bibliography",
    "section": "48.2 R and tidyverse",
    "text": "48.2 R and tidyverse\n\nR 4 Data Science, 2nd edition (R4DS) by Hadly Wickham\n\nFree to read online: https://r4ds.hadley.nz/\nPurchase hardcopy: https://www.amazon.com/Data-Science-Transform-Visualize-Model/dp/1492097403/\n\nUnofficial r4ds solution manual - https://jrnold.github.io/r4ds-exercise-solutions/\ntidyverse packages - https://www.tidyverse.org/packages/\n\ntibble https://tibble.tidyverse.org/\nreadr https://readr.tidyverse.org/\n\nread_csv(): comma-separated values (CSV)\nread_tsv(): tab-separated values (TSV)\nread_csv2(): semicolon-separated values with , as the decimal mark\nread_delim(): delimited files (CSV and TSV are important special cases)\nread_fwf(): fixed-width files\nread_table(): whitespace-separated files\nread_log(): web log files\n\ndplyr\n\nhttps://dplyr.tidyverse.org/\nhttps://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html\n\nstringr\ntidyr https://tidyr.tidyverse.org/\nOther tidyverse packages\n\nggplot https://ggplot2.tidyverse.org/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'><yrChapterNumber>45. </yrChapterNumber> Bibliography</span>"
    ]
  },
  {
    "objectID": "0000200-bibliography-v001.html#various-topics---bash-command-line-html-css-regular-expressions-regex",
    "href": "0000200-bibliography-v001.html#various-topics---bash-command-line-html-css-regular-expressions-regex",
    "title": "48  45.  Bibliography",
    "section": "48.3 Various topics - bash command line, html, css, regular expressions (regex)",
    "text": "48.3 Various topics - bash command line, html, css, regular expressions (regex)\n\nhttps://ryanstutorials.net/\n\nhttps://ryanstutorials.net/html-tutorial/\nhttps://ryanstutorials.net/css-tutorial/\nhttps://ryanstutorials.net/regular-expressions-tutorial/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'><yrChapterNumber>45. </yrChapterNumber> Bibliography</span>"
    ]
  },
  {
    "objectID": "0000200-bibliography-v001.html#bash-command-line",
    "href": "0000200-bibliography-v001.html#bash-command-line",
    "title": "48  45.  Bibliography",
    "section": "48.4 Bash command line",
    "text": "48.4 Bash command line\n\nA Comprehensive Introduction to Command Line for R Users\n\nhttps://blog.rsquaredacademy.com/command-line-basics-for-r-users/\n\nUsing the RStudio Terminal\n\nhttps://support.posit.co/hc/en-us/articles/115010737148-Using-the-RStudio-Terminal\nhttps://support.posit.co/hc/en-us/articles/115010737148-Using-the-RStudio-Terminal-in-the-RStudio-IDE\n\nhttps://ryanstutorials.net/\n\nhttps://ryanstutorials.net/linuxtutorial/\nhttps://ryanstutorials.net/bash-scripting-tutorial/\n\nlinux man pages - describes commands you can use in Bash\n\nhttps://man7.org/linux/man-pages/dir_section_1.html\n\nvim text editor\n\nhttps://www.openvim.com/\nhttps://linuxconfig.org/vim-tutorial",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'><yrChapterNumber>45. </yrChapterNumber> Bibliography</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html",
    "href": "0000300-whereToFindDatasets-v001.html",
    "title": "49  46.  Where to find datasets",
    "section": "",
    "text": "49.1 Datasets in R packages\nThere are many R packages that come with datasets. By “dataset” I mean any data that is referenced in an R variable. This can be a dataframe, a vector, a list, etc. etc. etc. Below are just a few example R packages that contain datasets. There are many, many others. Also make sure to see the section below about R pacakges that offer access to APIs.\nTo use the data in a package you should first install the package. As an example we’ll use the “nycflights13” package which contains data about all airline flights that came into or out of New York City in 2013.\nif(!require(\"nycflights13\")){install.packages(\"nycflights13\");require(\"nycflights13\");}\n\nError in contrib.url(repos, \"source\"): trying to use CRAN without setting a mirror\nWe can now view which datasets are in the package with the following command:\ndata(package=\"nycflights13\")\n\nError in find.package(package, lib.loc, verbose = verbose): there is no package called 'nycflights13'\nTo see the list of datasets in ALL packages that you have installed, you can use the following command\ndata(package=rownames(installed.packages()))\nTo access the data you can use the syntax PACKAGE_NAME::VARIABLE_NAME. Alternatively you can load the package by using the library or require functions and then access the data by just using the variable name.\n# This will work after you've installed the nycflights13 package\n# even before you've loaded it.\n\nhead(nycflights13::flights)   # show the first few rows\n\nError in loadNamespace(x): there is no package called 'nycflights13'\n\n# Alternatively if you have already loaded the package with the \n# library or require functions (which we did above) you can access\n# the flights variable without specifying nycflights::\n\nhead(flights)   # show the first few rows\n\nError: object 'flights' not found",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html#datasets-in-r-packages",
    "href": "0000300-whereToFindDatasets-v001.html#datasets-in-r-packages",
    "title": "49  46.  Where to find datasets",
    "section": "",
    "text": "Package name\nDescription\n\n\n\n\nnycflights13\ndata about all flights coming into or out of NYC in 2013.\n\n\nLahman\nExtensive data about baseball teams, players, etc.\n\n\nbabynames\ndata provided by USA Social Security administration about births from 1880 through 2017\n\n\nstringr\nContains vectors fruits,sentences,words that the help uses in examples.\n\n\netc …\n…\n\n\n\n\n\n\n\n\n\n\n\n\nnycflights13 package\n\ninstall.packages(“nycflights13”) # install the package library(nycflights13) # load the package\nhelp(package=“nycflights13”) # see what is included in the package",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html#project-gutenberg",
    "href": "0000300-whereToFindDatasets-v001.html#project-gutenberg",
    "title": "49  46.  Where to find datasets",
    "section": "49.2 Project Gutenberg",
    "text": "49.2 Project Gutenberg\nhttps://www.gutenberg.org/\nThis website hosts complete text of many many books whose copyright has expired. If you are planning to work with “text” files, then you should search for the “UTF-8” or “ASCII” or “Plain Text” versions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html#search-for-open-data",
    "href": "0000300-whereToFindDatasets-v001.html#search-for-open-data",
    "title": "49  46.  Where to find datasets",
    "section": "49.3 Search for “Open Data”",
    "text": "49.3 Search for “Open Data”\nhttps://www.google.com/search?q=open+data\nSearch for “open data” and MANY websites, especially government based websites will come up. For example:\n\nhttps://data.gov/ - US Government Open Data\nhttps://data.ny.gov/ - New York State\nhttps://opendata.cityofnewyork.us/ - New York City\nhttps://data.nasa.gov/\nmany, many others …",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html#google-dataset-search",
    "href": "0000300-whereToFindDatasets-v001.html#google-dataset-search",
    "title": "49  46.  Where to find datasets",
    "section": "49.4 Google dataset search",
    "text": "49.4 Google dataset search\nGoogle search engine for datasets: https://datasetsearch.research.google.com/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000300-whereToFindDatasets-v001.html#kaggle",
    "href": "0000300-whereToFindDatasets-v001.html#kaggle",
    "title": "49  46.  Where to find datasets",
    "section": "49.5 Kaggle",
    "text": "49.5 Kaggle\nhttps://www.kaggle.com/datasets\nThis is a popular website for machine learning and data science competitions, projects and tutorials. There are many datasets hosted on this platform",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'><yrChapterNumber>46. </yrChapterNumber> Where to find datasets</span>"
    ]
  },
  {
    "objectID": "0000310-whereToFindApis-v001.html",
    "href": "0000310-whereToFindApis-v001.html",
    "title": "50  47.  Where to find APIs",
    "section": "",
    "text": "51 How to use APIs\nSee this video https://www.youtube.com/watch?v=8tilyqp4bZY\nYou can use the curl command line tool to get info from the APIs too.\nNote that the following was compiled circa April 2025.\nThere is a incredible amount of data that can be accessed online by using an API. Some APIs are free to use but many will require some form of payment. Unfortunately, every API has its own set of rules and regulations. You must read the documentation for each API to understand what type of data is available through the API, how to use the API and how much it may cost.\nIn the past it was possible to go to “API Directory” website and “API Hubs” to find APIs. For example, see the following.\nHowever, the API landscape has changed a lot in recent years. There are so many APIs, it has become almost impossible to maintain a list of available APIs. Many companies have an API that make their services available programatically via APIs. However, these services are hard to find if you don’t know what to look for. Many companies maintain a “developers” portion of their websites. You can also look for “API” in the fineprint of the website info.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'><yrChapterNumber>47. </yrChapterNumber> Where to find APIs</span>"
    ]
  },
  {
    "objectID": "0000310-whereToFindApis-v001.html#packages-in-r-for-specific-apis",
    "href": "0000310-whereToFindApis-v001.html#packages-in-r-for-specific-apis",
    "title": "50  47.  Where to find APIs",
    "section": "51.1 Packages in R for specific APIs",
    "text": "51.1 Packages in R for specific APIs\nUsually, accessing data from an API requires a technical understanding about how to use APIs in general. However, there are many R packages that make the process of accessing data from specific APIs much easier. These R packages “wrap” the technical details of how to access the API in R functions.\nTo get an idea of what APIs have R packages, you can search the list packages on CRAN (https://cran.r-project.org/web/packages/available_packages_by_name.html) for the word API.\n\n# Find the number of packages that have the letters API in \n# the package name or in the description.\nif(!require(rvest)){install.packages(\"rvest\");require(rvest);}\n\nLoading required package: rvest\n\nurl = \"https://cran.r-project.org/web/packages/available_packages_by_name.html\"\npage = read_html(url)\n\nError in open.connection(x, \"rb\"): cannot open the connection\n\ntables = html_elements(page, \"table\")\n\nError in UseMethod(\"xml_find_all\"): no applicable method for 'xml_find_all' applied to an object of class \"function\"\n\npkgs = as.data.frame(html_table(tables[[1]]))\n\nError: object 'tables' not found\n\nnames(pkgs) = c(\"package\", \"description\")\n\nError: object 'pkgs' not found\n\nrows = grepl(\"API\", pkgs[,\"package\"], ignore.case = TRUE) | \n       grepl(\"API\", pkgs[,\"description\"], ignore.case = TRUE)\n\nError: object 'pkgs' not found\n\nnumberOfApiPackages = sum(rows)\n\nError: object 'rows' not found\n\n\nAs of the date this document was compiled there were\n\n\nError: object 'numberOfApiPackages' not found\n\n\npackages on CRAN that have the letters API in either the name of the package or the description.\nHowever, be aware that the API ecosystem can change rapidly and the R packages don’t always keep up. For example, twitter used to allow searching for tweets for free using their API. However, they started charging for those services. There are many youtube videos explaining how to search twitter for free using the twitter api via the “twittR” packager. These videos are no longer applicable.\n\n51.1.1 R for finance\nBelow are some R resources for working with financial data Feb 2, 2024\nR package for financial data - getting, charting, modeling\n\ngithub: (this seems to be the most up to date info)\nhttps://github.com/joshuaulrich/quantmod\nwebsite: (this seems VERY out of date - but it’s much easier to get a quick idea of what is possible circa 2009. For more up to date info look at the github page or the official documentation accessible from the CRAN website) https://www.quantmod.com/\nhttps://www.quantmod.com/examples/intro/\n“official” documentation from the “Comprehensive R Archive Network (CRAN)” website\nhttps://cran.r-project.org/web/packages/quantmod/index.html\n\nRelated R package for doing technical analysis\n\nhttps://github.com/joshuaulrich/TTR",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'><yrChapterNumber>47. </yrChapterNumber> Where to find APIs</span>"
    ]
  },
  {
    "objectID": "0000310-whereToFindApis-v001.html#apis",
    "href": "0000310-whereToFindApis-v001.html#apis",
    "title": "50  47.  Where to find APIs",
    "section": "51.2 APIs",
    "text": "51.2 APIs\nYou learn about many different APIs via this website https://rapidapi.com/collection/amazon-products",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'><yrChapterNumber>47. </yrChapterNumber> Where to find APIs</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html",
    "href": "0999000-toDo-v001.html",
    "title": "51  48.  Additional topics",
    "section": "",
    "text": "51.1 Importing Data\nhttps://rafalab.dfci.harvard.edu/dsbook-part-1/R/importing-data.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#importing-data",
    "href": "0999000-toDo-v001.html#importing-data",
    "title": "51  48.  Additional topics",
    "section": "",
    "text": "51.1.1 Excel\nRead excel file with formulas in cells into R\nhttps://stackoverflow.com/questions/32014692/read-excel-file-with-formulas-in-cells-into-r\nXLConnect\nhttps://www.rdocumentation.org/packages/XLConnect/versions/1.0.8/topics/getCellFormula-methods https://cran.r-project.org/web/packages/XLConnect/vignettes/XLConnect.pdf https://financetrain.com/using-xlconnect-in-r-programming",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#using-ai-to-code",
    "href": "0999000-toDo-v001.html#using-ai-to-code",
    "title": "51  48.  Additional topics",
    "section": "51.2 Using AI to code",
    "text": "51.2 Using AI to code\nUsing Gihub Copilot with RStudio\nhttps://www.youtube.com/watch?v=t7NrkAeosog&list=PLSjaBj3ul8GpuMUaSGME2_NomnIsYWIDj",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#apis",
    "href": "0999000-toDo-v001.html#apis",
    "title": "51  48.  Additional topics",
    "section": "51.3 APIs",
    "text": "51.3 APIs\nHow to work with APIs uing R (httr2 package tutorial) https://www.youtube.com/watch?v=8tilyqp4bZY\nTrends in API world https://www.forbes.com/sites/forbestechcouncil/2024/02/07/four-trends-shaping-apis-in-2024/?sh=14a432ea3369",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#code-for-graphs-and-charts---e.g.-mermaid",
    "href": "0999000-toDo-v001.html#code-for-graphs-and-charts---e.g.-mermaid",
    "title": "51  48.  Additional topics",
    "section": "51.4 Code for graphs and charts - e.g. mermaid",
    "text": "51.4 Code for graphs and charts - e.g. mermaid\nInfo about mermaid: https://mermaid.js.org/\nUsing in Quarto: https://quarto.org/docs/authoring/diagrams.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#online-books",
    "href": "0999000-toDo-v001.html#online-books",
    "title": "51  48.  Additional topics",
    "section": "51.5 Online books",
    "text": "51.5 Online books\nIntroduction to Data Science Data Wrangling and Visualization with R\nhttps://rafalab.dfci.harvard.edu/dsbook-part-1/\nAdvanced Data Science Statistics and Prediction Algorithms Through Case Studies\nhttps://rafalab.dfci.harvard.edu/dsbook-part-2/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  },
  {
    "objectID": "0999000-toDo-v001.html#online-exercises-and-other-resources",
    "href": "0999000-toDo-v001.html#online-exercises-and-other-resources",
    "title": "51  48.  Additional topics",
    "section": "51.6 Online exercises and other resources",
    "text": "51.6 Online exercises and other resources\nhttps://docs.dodona.be/en/ https://dodona.be/en/activities/?tab=all&programming_language=R\nhttps://financetrain.com/using-xlconnect-in-r-programming",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'><yrChapterNumber>48. </yrChapterNumber> Additional topics</span>"
    ]
  }
]